# The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities

CorpusID: 264832783 - [https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825](https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825)

Fields: Linguistics, Computer Science

## (s0) Introduction
Number of References: 9

(p0.0) Emergent abilities such as in-context learning (ICL) and chain-of-thought (CoT) prompting have become evident in large language models (LLMs) when they are scaled to certain levels (Wei et al., 2022b).These capabilities are receiving heightened attention due to their remarkable adaptability and their parameter-free nature.As shown in The concept of emergent abilities within LLMs was originally introduced by Wei et al. (2022b), defining them as capabilities that manifest in largescale models but are absent in their smaller-scale counterparts.They further classified these abilities into two categories: 1) few-shot prompting abilities, referring to the capacity of LLMs to achieve significantly better results than random chance on certain tasks such as BIG-Bench (Srivastava et al., 2022) and TruthfulQA (Lin et al., 2022), when presented with only a small number of demonstration examples; 2) augmented prompting strategies, where certain strategies produce less impressive outcomes compared to established baselines until applied to models of sufficient scale.For example, this includes the chain-of-thought prompting strategy (Wei et al., 2022c;Suzgun et al., 2022) and instruction tuning (Brown et al., 2020;Wei et al., 2022a;Chung et al., 2022).
## (s6) Gradient Descent & Meta-Optimization
Number of References: 4

(p6.0) In the realm of gradient descent, Dai et al. (2023) adopted a perspective of viewing LLMs as metaoptimizers and interpreting ICL as a form of implicit fine-tuning.They first conducted a qualitative analysis of Transformer attention, representing it in a relaxed linear attention form, and identified a dual relationship between it and gradient descent.Through a comparative analysis between ICL and explicit fine-tuning, Dai et al. (2023) interpreted ICL as a meta-optimization process.They further provided evidence that the transformer attention head possesses a dual nature similar to gradient descent (Irie et al., 2022), where the optimizer produces meta-gradients based on the provided examples for ICL through forward computation.Concurrently, von Oswald et al. (2022) also proposed a connection between the training of Transformers on auto-regressive objectives and gradient-based meta-learning formulations.They specifically examined how Transformers define a loss function based on the given examples and, subsequently, the mechanisms by which Transformers assimilate knowledge using the gradients of this loss function.Their findings suggest that ICL may manifest as an emergent property, approximating gradient-based few-shot learning within the forward pass of the model.
## (s12) Demonstration Order
Number of References: 7

(p12.0) The order of the demonstrations has a significant impact on downstream task performance.Lu et al. (2022) showed that it be the deciding factor between achieving near stateof-the-art and random guessing.They designed demonstrations containing four samples with a balanced label distribution and conducted experiments involving all 24 possible permutations of sample orders.The experimental results showed that the performance variations among different permutations exist across various model sizes, especially for smaller models.Besides, is was observed that effective prompts are not transferrable across models, indicating that the optimal order is model-dependent, and what works well for one model does not guarantee good results for another model.Zhao et al. (2021) identified a phenomenon that LLMs tend to repeat answers found at the end of demonstrations, which they termed "recency bias".Similarly, in multi-document question answering and key-value retrieval tasks, Liu et al. ( 2023) made analogous observations.These tasks involve identifying relevant information within lengthy input contexts.The results showed that LLMs performed best when the relevant information is located at the beginning or end of their input contexts.However, their performance degraded when they are forced to use information from the middle of their input.In addition, they noted that model performance declines as the input context length increases, suggesting that current models struggle to effectively reason over their entire context window.Although these studies offer insights into how demonstration order influences emergent abilities, they do not delve into the underlying reasons of these obesrvations.In an effort to investigate the impact of semantic similarity between ICL examples and test examples on downstream task, Liu et al. (2022) proposed retrieving examples semantically similar to a test example for creating its demonstration.They utilized the CLS embeddings from a pre-trained RoBERTa-large (Liu et al., 2019) model to represent sentences and assessed the semantic similarity between two sentences by computing the cosine similarity of their respective representations.For each test example, they identified the nearest K neighbors from the training set and concatenated them in descending order of semantic similarity to create the demonstration.Their experiments on Web Questions (Berant et al., 2013) and Trivia Question Answering (Joshi et al., 2017) benchmarks showed that the default order performed slightly better than the reverse order.However, the reverse order performed better on the Natural Questions (Kwiatkowski et al., 2019) benchmark.Consequently, the choice of order appears to be dependent on the specific dataset in use.
