# Pretraining in Deep Reinforcement Learning: A Survey

CorpusID: 253397510 - [https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096](https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096)

Fields: Computer Science

## (s6) Data Coverage Maximization
Number of References: 4

(p6.0) CBB (Bellemare et al., 2016) rt  rt ∝ i∈Iprototype log φ(st) − h i RE3 (Seo et al., 2021) rt ∝ log ( φ(st) − KNN (φ(st)) + 1) unsupervised RL, has been actively studied in recent years (Burda et al., 2019a;Srinivas & Abbeel, 2021).
## (s10) CHALLENGES & FUTURE DIRECTIONS
Number of References: 10

(p10.0) A major issue for MI-based skill discovery approaches is that the objective does not necessarily lead to strong state coverage as one can maximize I(s; z) even with the smallest state variations (Campos et al., 2020;Park et al., 2022). This lack of coverage can greatly limit their applicability to downstream tasks with complex environments (Campos et al., 2020). To resolve this issue, some existing work explicitly uses x-y coordinates as features to enforce state coverage induced by skills (Eysenbach et al., 2019;Sharma et al., 2020). It is also explored to separate the learning process to first maximize H(s) via maximum entropy estimation, followed by behavior learning (Campos et al., 2020;Liu & Abbeel, 2021a).

(p10.1) Moreover, it is empirically shown that skill discovery methods underperform other kinds of online pretraining methods, which may be due to restricted skill spaces . This calls attention to dissecting what skills are learned. In order to live up to their full potential, the discovered skills must strike a balance between generality (i.e., the applicability to a large variety of downstream tasks) and specificity (i.e., the quality of being useful to induce specific behaviors) (Gehring et al., 2021). It is also desired to avoid learning trivial skills (Sharma et al., 2020;Baumli et al., 2021).
## (s11) Data Coverage Maximization
Number of References: 4

(p11.0) Previously we have discussed how to obtain knowledge or skills, measured by the agent's own capability, from unsupervised interaction. Albeit indirectly related to the agent's ability, data diversity induced by online pretraining plays an essential role in deciding how well the agent obtains prior knowledge. In the field of supervised learning, recent advances have shown that diverse data can enhance out-of-distribution generalization (Hendrycks et al., 2020b) and robustness (Hendrycks et al., 2020a). Another supporting evidence is that most of the famed datasets are large and diverse (Deng et al., 2009;Wang et al., 2019). Motivated by the above considerations, it is desired to use data coverage maximization, usually measured by state visitation, as an objective to stimulate unsupervised learning.
## (s12) COUNT-BASED EXPLORATION
Number of References: 11

(p12.0) The first category of data coverage maximization is count-based exploration. Count-based exploration methods directly use visit counts to guide the agent towards underexplored states (Bellemare et al., 2016;Ostrovski et al., 2017). For tabular MDPs, Model-based Interval Estimation with Exploration Bonuses (Strehl & Littman, 2008) provably turn state-action N (s, a) counts into an exploration bonus reward:

(p12.1) Built on Equation 2, a series of work has studied how to tractably generalize count bonuses to high-dimensional state spaces (Bellemare et al., 2016;Ostrovski et al., 2017;Tang et al., 2017). To approximate these counts in high dimensions, Bellemare et al. (2016) introduce pseudo-counts derived from a density model. Specifically, the pseudo-count is defined as:

(p12.2) where ρ is a density model over state space S, ρ t (s) is the density assigned to s after training on a sequence of states s 1 , . . . , s t , and ρ t (s) is the density of s if ρ were to be trained on s one additional time. Based on similar ideas, it has been shown that a better density model (Ostrovski et al., 2017) or a hash function (Tang et al., 2017;Rashid et al., 2020) for computing state statistics can further improve performance. Besides, a self-supervised inverse dynamics model as discussed in Section 3.1 can also be used to bias the count-based bonuses towards what the agent can control (Badia et al., 2020).
## (s27) POLICY INITIALIZATION
Number of References: 6

(p27.0) Recent advances bridge the gap between reinforcement learning and sequential modeling (Chen et al., 2021a;Janner et al., 2021;Zheng et al., 2022;Furuta et al., 2022), opening up opportunities to borrow sequential models to RL tasks. Despite the clear distinction, pretrained LLMs could arguably provide reusable knowledge via weight initialization. Reid et al. (2022) investigate whether pretrained LLMs can provide good weight initialization for Transformer-based offline RL models, and conclude with very positive results. Li et al. (2022b) also demonstrate that pretrained LLMs can be used to initialize policies and facilitate behavior cloning as well as online reinforcement learning for embodied tasks. They also suggest using sequential input representations and fintuning the pretrained weights for better generalization.
## (s28) Multi-task and Multi-modal Pretraining
Number of References: 14

(p28.0) With recent advances in building powerful sequence models to handle different modalities and tasks (Lu et al., 2019;Jaegle et al., 2022;Wang et al., 2022), the wave of using large general-propose models (Bommasani et al., 2021) has been sweeping through the field of supervised learning. The key ingredient is Transformer (Vaswani et al., 2017), a highly capable neural architecture built on the self-attention mechanism (Bahdanau et al., 2015) that excels at capturing long-range dependencies in sequential data. Due to its strong generality where various tasks in different domains can be formulated as sequence modeling, Transformer is believed to be a unified architecture for developing foundation models (Bommasani et al., 2021).

(p28.1) Recently, Transformer-based architectures have also been extended to the field of offline RL (Chen et al., 2021a;Janner et al., 2021) and then online RL (Zheng et al., 2022), in which the agent is trained auto-regressively in a supervised manner via likelihood maximization. This opens up the possibility of replicating previous success achieved with Transformer in the field of supervised learning. Specifically, it is expected that by combining large-scale data, open-ended objectives, and Transformer-based architectures, we are ready to build general-purpose decision-making agents that are capable of various downstream tasks in different environments.

(p28.2) Pioneering work in this direction is Gato (Reed et al., 2022), a generalist agent trained on various tasks from control environments, vision datasets, and language datasets in a supervised manner. To handle multi-task and multi-modal data, Gato uses demonstrations as prompt sequences  at inference time.  extend Decision Transformer (Chen et al., 2021a) to train a generalist agent called Multi-Game DT that can play 41 Atari games simultaneously. Both Gato and Multi-Game DT show impressive scaling law properties. Fan et al. (2022) make use of large-scale multi-modal data from YouTube videos, Wikipedia pages, and Reddit posts to train an agent able to solve various tasks in Minecraft. To provide dense reward signals, a pretrained vision-language model based on CLIP (Radford et al., 2021) is introduced as a proxy of human evaluation.
## (s31) Representation Transfer
Number of References: 7

(p31.0) In the field of supervised learning, recent advances (Devlin et al., 2019;He et al., 2020;Chen et al., 2020) have demonstrated that good representations can be pretrained on large-scale unlabeled dataset, as evidenced by their impressive downstream performances. The most common practice is to freeze the weights of the pretrained feature encoder and train a randomly initialized task-specific network on top of that during adaptation. The success of this paradigm is essentially based on the promise that related tasks can usually be solved using similar representations.

(p31.1) For RL, it has been shown that directly reusing pretrained task-agnostic representations can significantly improve sample efficiency on downstream tasks. For instance, Schwarzer et al. (2021b) conduct experiments on the Atari 100K benchmark and find that frozen representations pretrained on exploratory offline data already form a basis of data-efficient RL. This success also extends to the cases where domain discrepancy exists between upstream and downstream tasks (Shah & Kumar, 2021;Parisi et al., 2022). However, the issue of negative transfer in the face of domain discrepancy might be exacerbated for RL due to its complexity (Shah & Kumar, 2021).
## (s33) Challenges & Future Directions
Number of References: 6

(p33.0) Parameter Efficiency. Despite that existing pretrained models for RL have much fewer parameters as compared with those in the field of supervised learning, the issue of parameter efficiency is still important with the ever-increasing number of model parameters. More concretely, it is desired to design parameter-efficient transfer learning that updates only a small fraction of parameters while keeping most of the pretrained parameters intact. It has been actively studied in natural language processing (He et al., 2022) with solutions like adding small neural modules as adapters (Houlsby et al., 2019) and prepending learnable prefix tokens as soft prompts (Lester et al., 2021). Built on these techniques, several efforts have been made to enable parameter-efficient transfer with prompting Reed et al., 2022), which we believe has a large room to improve with tailored methods.

(p33.1) Domain adaptation. In this section, we mainly consider task adaptation where unseen tasks are given in the same environment. A more challenging but practical scenario is domain adaptation. In domain adaptation, there exist environmental shifts between the upstream and downstream tasks. Despite that these environmental shifts are commonly seen in real-world applications, it remains a challenging problem to transfer across different domains (Eysenbach et al., 2021;Huang et al., 2022a). However, we believe that this direction will rapidly evolve by bringing related techniques from supervised learning to reinforcement learning.
