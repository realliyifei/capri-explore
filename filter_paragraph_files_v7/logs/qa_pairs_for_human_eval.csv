corpusid,title,domain,section,QA pair,subquestions
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s0),"What are the fundamental aspects and stages in BERT's architecture and functionality as highlighted since its inception in 2017?

1. Since their introduction in 2017, Transformers (Vaswani et al., 2017) took NLP by storm, offering enhanced parallelization and better modeling of long-range dependencies.
2. The best known Transformer-based model is BERT (Devlin et al., 2019) which obtained state-of-the-art results in numerous benchmarks, and was integrated in Google search 1 , improving an estimated 10% of queries.
3. While it is clear that BERT and other Transformer-based models work remarkably well, it is less clear why, which limits further hypothesisdriven improvement of the architecture.
4. Unlike CNNs, the Transformers have little cognitive motivation, and the size of these models limits our ability to experiment with pre-training and perform ablation studies.
5. This explains a large number of studies over the past year that attempted to understand the reasons behind BERT's performance.
6. This paper provides an overview of what has been learned to date, highlighting the questions which are still unresolved.
7. We focus on the studies investigating the types of knowledge learned by BERT, where this knowledge is represented, how it is learned, and the methods proposed to improve it.
8. 1 https://blog.google/products/search/ search-language-understanding-bert 2 Overview of BERT architecture Fundamentally, BERT is a stack of Transformer encoder layers (Vaswani et al., 2017) which consist of multiple ""heads"", i.e., fully-connected neural networks augmented with a self-attention mechanism.
9. For every input token in a sequence, each head computes key, value and query vectors, which are used to create a weighted representation.
10. The outputs of all heads in the same layer are combined and run through a fully-connected layer.
11. Each layer is wrapped with a skip connection and layer normalization is applied after it.
12. The conventional workflow for BERT consists of two stages: pre-training and fine-tuning.
13. Pretraining uses two semi-supervised tasks: masked language modeling (MLM, prediction of randomly masked input tokens) and next sentence prediction (NSP, predicting if two input sentences are adjacent to each other).
14. In fine-tuning for downstream applications, one or more fully-connected layers are typically added on top of the final encoder layer.
15. The input representations are computed as fol-  lows: BERT first tokenizes the given word into wordpieces (Wu et al., 2016b), and then combines three embedding layers (token, position, and segment) to obtain a fixed-length vector.
16. Special token
17. [CLS] is used for classification predictions, and [SEP] separates input segments.
18. The original BERT comes in two versions: base and large, varying in the number of layers, their hidden size, and number of attention heads.","1. What are the fundamental aspects of BERT's architecture?
2. What are the stages in BERT's functionality as highlighted since its inception in 2017?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s1),"How do BERT's contextualized embeddings differ from conventional static embeddings and what have studies discovered about their characteristics?

1. Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.
2. In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final)
3. Transformer layer.
4. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
5. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
6. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
7. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
8. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).","1. How do BERT's contextualized embeddings differ from conventional static embeddings?
2. What have studies discovered about their characteristics?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s2),"What types of knowledge have studies found to be encoded in BERT's weights, and how is this information structured?

1. A number of studies have looked at the types of knowledge encoded in BERT's weights.
2. The popular approaches include fill-in-the-gap probes of BERT's MLM, analysis of self-attention weights, and probing classifiers using different BERT representations as inputs.
3. showed that BERT representations are hierarchical rather than linear, i.e. there is something akin to syntactic tree structure in addition to the word order information.
4. Tenney et al. (2019b) and  also showed that BERT embeddings encode information about parts of speech, syntactic chunks and roles.
5. However, BERT's knowledge of syntax is partial, since probing classifiers could not recover the labels of distant parent nodes in the syntactic tree .","1. What types of knowledge have studies found to be encoded in BERT's weights?
2. How is this information structured?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s3),"How does BERT's handling of syntactic structure, subject-predicate agreement, and negation, indicate its syntactic knowledge and limitations?

1. As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it.
2. Htut et al.(2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.
3. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation.
4. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2).
5. Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).
6. Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.
7. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences.
8. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations.
9. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input.
10. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019).
11. This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","1. How does BERT's handling of syntactic structure indicate its syntactic knowledge and limitations?
2. How does BERT's handling of subject-predicate agreement indicate its syntactic knowledge and limitations?
3. How does BERT's handling of negation indicate its syntactic knowledge and limitations?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s4),"How does BERT perform in understanding semantic roles and numerical representations according to recent studies?

1. To date, more studies were devoted to BERT's knowledge of syntactic rather than semantic phenomena.
2. However, we do have evidence from an MLM probing study that BERT has some knowledge for semantic roles (Ettinger, 2019).
3. BERT is even able to prefer the incorrect fillers for semantic roles that are semantically related to the correct ones, to those that are unrelated (e.g. ""to tip a chef"" should be better than ""to tip a robin"", but worse than ""to tip a waiter"").
4. Tenney et al. (2019b) showed that BERT encodes information about entity types, relations, semantic roles, and proto-roles, since this information can be detected with probing classifiers.
5. BERT struggles with representations of numbers.
6. Addition and number decoding tasks showed that BERT does not form good representations for floating point numbers and fails to generalize away from the training data (Wallace et al., 2019b).
7. A part of the problem is BERT's wordpiece tokenization, since numbers of similar values can be divided up into substantially different word chunks.","1. How does BERT perform in understanding semantic roles according to recent studies?
2. How does BERT perform in understanding numerical representations according to recent studies?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s5),"How does BERT adapt for knowledge induction, and what are its limitations and strengths in world knowledge extraction according to recent studies?

1. MLM component of BERT is easy to adapt for knowledge induction by filling in the blanks (e.g. ""Cats like to chase [ ]"").
2. There is at least one probing study of world knowledge in BERT (Ettinger, 2019), but the bulk of evidence comes from  (Petroni et al., 2019) numerous practitioners using BERT to extract such knowledge.
3. Petroni et al. (2019) showed that, for some relation types, vanilla BERT is competitive with methods relying on knowledge bases (Figure 3).
4. Davison et al. (2019) suggest that it generalizes better to unseen data.
5. However, to retrieve BERT's knowledge we need good template sentences, and there is work on their automatic extraction and augmentation (Bouraoui et al., 2019; However, BERT cannot reason based on its world knowledge.
6. Forbes et al. (2019) show that BERT can ""guess"" the affordances and properties of many objects, but does not have the information about their interactions (e.g. it ""knows"" that people can walk into houses, and that houses are big, but it cannot infer that houses are bigger than people.)
7. and  also show that the performance drops with the number of necessary inference steps.
8. At the same time, Poerner et al. (2019) show that some of BERT's success in factoid knowledge retrieval comes from learning stereotypical character combinations, e.g. it would predict that a person with an Italian-sounding name is Italian, even when it is factually incorrect.","1. How does BERT adapt for knowledge induction?
2. What are BERT's limitations in world knowledge extraction according to recent studies?
3. What are BERT's strengths in world knowledge extraction according to recent studies?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s7),"What conclusions have researchers drawn about the linguistic functions and limitations of BERT's self-attention heads in syntactic and semantic processing?

1. Attention is widely considered to be useful for understanding Transformer models, and several studies proposed classification of attention head types:• attending to the word itself, to previous/next words and to the end of the sentence (Raganato and Tiedemann, 2018);• attending to previous/next tokens,    (Kovaleva et al., 2019)
2. According to Clark et al. (2019), ""attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word"".
3. However, Kovaleva et al. (2019) showed that most selfattention heads do not directly encode any nontrivial linguistic information, since less than half of them had the ""heterogeneous"" pattern 2 .
4. Much of the model encoded the vertical pattern (attention to [CLS], [SEP], and punctuation tokens), consistent with the observations by Clark et al. (2019).
5. This apparent redundancy must be related to the overparametrization issue (see section 7).
6. Attention to [CLS] is easy to interpret as attention to an aggregated sentence-level representation, but BERT also attends a lot to [SEP] and punctuation.
7. Clark et al. (2019) hypothesize that periods and commas are simply almost as frequent as [CLS] and [SEP], and the model learns to rely on them.
8. They suggest also that the function of [SEP] might be one of ""no-op"", a signal to ignore the head if its pattern is not applicable to the current case.
9. [SEP] gets increased attention starting in layer 5, but its importance for prediction drops.
10. If this hypothesis is correct, attention probing studies that excluded the [SEP] and [CLS] tokens (as e.g.  and Htut et al. (2019)) should perhaps be revisited.
11. Proceeding to the analysis of the ""heterogeneous"" self-attention pattern, a number of studies looked for specific BERT heads with linguistically interpretable functions.
12. Some BERT heads seem to specialize in certain types of syntactic relations.
13. Htut et al.(2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions.
14. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline.
15. The evidence for nsubj, advmod, and amod has some variation between these two studies.
16. The overall conclusion is also supported by Voita et al.
17. (2019)'s data for the base Transformer in machine translation context.
18. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis.
19. Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information, in line with evidence of partial knowledge of syntax (see subsection 4.1).
20. present evidence that attention weights are weak indicators of subjectverb agreement and reflexive anafora.
21. Instead of serving as strong pointers between tokens that should be related, BERT's self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data.
22. Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,.
23. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks.
24. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998).
25. Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.","1. What conclusions have researchers drawn about the linguistic functions of BERT's self-attention heads in syntactic and semantic processing?
2. What are the limitations of BERT's self-attention heads in syntactic and semantic processing?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s8),"How does syntactic information distribution vary across different layers in BERT models, and how does this relate to task-specific performance in final layers?

1. The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings.
2. It stands to reason that the lower layers have the most linear word order information.
3. report a decrease in the knowledge of linear word order around layer 4 in BERT-base.
4. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.
5. There is a wide consensus among studies with different tasks, datasets and methodologies that syntactic information is the most prominent in the middle BERT 3 layers.
6. Hewitt and Manning (2019) had the most success reconstructing syntactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large).
7. Goldberg (2019) report the best subject-verb agreement around layers 8-9, and the performance on syntactic probing tasks used by Jawahar et al. (2019) also seemed to peak around the middle of the model.
8. The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5).
9. There is conflicting evidence about syntactic chunks.
10. Tenney et al. (2019a) conclude that ""the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers"", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling.
11. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing.
12. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.
13. The final layers of BERT are the most taskspecific.
14. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable .
15. In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019).
16. At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance.
17. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers.
18. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182).
19. But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial.
20. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.
21. The above view is disputed by Jawahar et al. (2019), who place ""surface features in lower layers, syntactic features in middle layers and semantic features in higher layers"".
22. However, the conclusion with regards to the semantic features seems surprising, given that only one SentEval semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers.","1. How does syntactic information distribution vary across different layers in BERT models?
2. How does this relate to task-specific performance in final layers?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s10),"What alternative training objectives and data strategies have researchers explored to improve upon or augment the original BERT's capabilities, as indicated in recent studies?

1. The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM).
2. Multiple studies have come up with alternative training objectives to improve on BERT.
3. • Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting .
4. Wang et al. (2019a) Mikolov et al. (2013b).
5. • Permutation language modeling.
6. Yang et al. (2019) replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order.
7. See also the n-gram word order reconstruction task (Wang et al., 2019a).
8. •
9. Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);• Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;• Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).
10. • Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (e.g. sentiment analysis).
11. • Clinchant et al. (2019) propose replacing the MASK token with [UNK] token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.
12. Another obvious source of improvement is pretraining data.
13. Liu et al. (2019c) explore the benefits of increasing the corpus volume and longer training.
14. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (subsection 4.3), there are ongoing efforts to incorporate structured knowledge resources .
15. Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE .
16. Alternatively, SemBERT  integrates semantic role information with BERT representations.
17. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides.
18. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6).
19. However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).","1. What alternative training objectives have researchers explored to improve upon or augment the original BERT's capabilities?
2. What data strategies have researchers explored to improve upon or augment the original BERT's capabilities?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s11),"How do variations in BERT architecture layers, heads, and batch size impact its training efficiency and task performance, based on systematic studies?

1. To date, the most systematic study of BERT architecture was performed by .
2. They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others.
3. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable.
4. Larger hidden representation size was consistently better, but the gains varied by setting.
5. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance.
6. They also publish their recommendations for other model parameters.
7. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.
8. observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.
9. Gong et al.
10. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers.
11. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.","1. How do variations in BERT architecture layers impact its training efficiency and task performance, based on systematic studies?
2. How do variations in BERT architecture heads impact its training efficiency and task performance, based on systematic studies?
3. How do variations in BERT batch size impact its training efficiency and task performance, based on systematic studies?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s12),"What challenges and strategies are identified in the process of fine-tuning BERT for various NLP tasks, including its computational cost and the effects of initialization?

1. Pre-training + fine-tuning workflow is a crucial part of BERT.
2. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand.
3. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns.
4. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).
5. • Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).
6. With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules.
7. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost.
8. Artetxe et al.(2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.
9. An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b).
10. Initialization can have a dramatic effect on the training process (Petrov, 2010).
11. However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018).
12. Dodge et al.
13. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation.
14. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.
15. 7 How big should BERT be?","1. What challenges are identified in the process of fine-tuning BERT for various NLP tasks?
2. What strategies are identified in the process of fine-tuning BERT for various NLP tasks?
3. What is the computational cost associated with fine-tuning BERT?
4. What are the effects of initialization on fine-tuning BERT for various NLP tasks?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s13),"What are the implications of overparametrization in transformer-based models, including environmental concerns and research accessibility, and how effective is head and layer pruning in these models?

1. Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT.
2. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.
3. Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have.
4. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance.
5. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a)
6. ×1.6 97% ×1.9 BERT 6
7. No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head.
8. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance.
9. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019).
10. Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).
11. Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case.
12. In particular, the opposite was observed for subjectverb agreement (Goldberg, 2019) and sentence subject detection .
13. Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers.
14. Clark et al.(2019) suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.","1. What are the implications of overparametrization in transformer-based models?
   1. What are the environmental concerns related to overparametrization in transformer-based models?
   2. What does overparametrization imply for research accessibility?
2. How effective is head and layer pruning in transformer-based models?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s14),"What are the two main approaches for BERT compression, and what does each entail?

1. Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss.
2. Such efforts to date are summarized in Table 1.
3. Two main approaches include knowledge distillation and quantization.
4. The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase).
5. This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).
6. The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019).
7. Note that this strategy often requires compatible hardware.
8. Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","1. What are the two main approaches for BERT compression?
2. What does each entail?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s15),"How does Multilingual BERT (mBERT) facilitate cross-lingual transfer, and what are its capabilities and limitations in language representation and generation tasks?

1. Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary).
2. Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing.
3. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019).
4. The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019).
5. Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).
6. mBERT generalizes across some scripts (Pires et al., 2019), and can retrieve parallel sentences, although Libovický et al.
7. (2019) note that this task could be solvable by simple lexical matches.
8. Pires et al. (2019) conclude that mBERT representation space shows some systematicity in betweenlanguage mappings, which makes it possible in some cases to ""translate"" between languages by shifting the representations by the average parallel sentences offset for a given language pair.
9. mBERT is simply trained on a multilingual corpus, with no language IDs, but it encodes language identities (Wu and Dredze, 2019;Libovický et al., 2019), and adding the IDs in pre-training was not beneficial .
10. It is also aware of at least some typological language features (Libovický et al., 2019;Singh
11. et al., 2019), and transfer between structurally similar languages works better Pires et al., 2019).
12. Singh et al. (2019) argue that if typological features structure its representation space, it could not be considered as interlingua.
13. However, Artetxe et al. (2019) show that cross-lingual transfer can be achieved by only retraining the input embeddings while keeping monolingual BERT weights, which suggests that even monolingual models learn generalizable linguistic abstractions.
14. At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019).
15. Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages.
16. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary.
17. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.
18. To date, the following proposals were made for improving mBERT: As shown in section 4, multiple probing studies report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge.
19. However, as Tenney et al. (2019a) aptly stated, ""the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used"".
20. There is also the issue of tradeoff between the complexity of the probe and the tested hypothesis .
21. A more complex probe might be able to recover more information, but it becomes less clear whether we are still talking about the original model.
22. Furthermore, different probing methods may reveal complementary or even contradictory information, in which case a single test (as done in most studies) would not be sufficient (Warstadt et al., 2019).
23. Certain methods might also favor a certain model, e.g., RoBERTa is trailing BERT with one tree extraction method, but leading with another (Htut et al., 2019).
24. Head and layer ablation studies (Michel et al., 2019;Kovaleva et al., 2019) inherently assume that certain knowledge is contained in heads/layers, but there is evidence of more diffuse representations spread across the full network: the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a), the absence of heads that would perform parsing ""in general"" (Clark et al., 2019;Htut et al., 2019).
25. Ablations are also problematic if the same information was duplicated elsewhere in the network.
26. To mitigate that, Michel et al. (2019) prune heads in the order set by a proxy importance score, and Voita et al.
27. (2019) fine-tune the pretrained Transformer with a regularized objective that has the head-disabling effect.
28. Many papers are accompanied by attention visualizations, with a growing number of visualization tools (Vig, 2019;Hoover et al., 2019).
29. However, there is ongoing debate on the merits of attention as a tool for interpreting deep learning models (Jain and Wallace, 2019;Serrano and Smith, 2019;Wiegreffe and Pinter, 2019;Brunner et al., 2020).
30. Also, visualization is typically limited to qualitative analysis (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence.","1. How does Multilingual BERT (mBERT) facilitate cross-lingual transfer?
2. What are its capabilities in language representation and generation tasks?
3. What are its limitations in language representation and generation tasks?"
211532403,A Primer in BERTology: What we know about how BERT works,"Computer Science, Linguistics",(s16),"What are the most promising directions for further BERTology research, as identified in the available literature?

1. BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works.
2. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.
3. Benchmarks that require verbal reasoning.
4. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems.
5. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020).
6. As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it.
7. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.
8. Developing methods to ""teach"" reasoning.
9. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3).
10. For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination.
11. Learning what happens at inference time.
12. Most of the BERT analysis papers focused on different probes of the model, but we know much less about what knowledge actually gets used.
13. At the moment, we know that the knowledge represented in BERT does not necessarily get used in downstream tasks (Kovaleva et al., 2019).
14. As starting points for work in this direction, we also have other head ablation studies (Voita et al., 2019;Michel et al., 2019) and studies of how BERT behaves in reading comprehension task (van Aken et al., 2019; Arkhangelskaia and Dutta, 2019).","1. What are the most promising directions for further BERTology research, as identified in the available literature?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s0),"What is the significance and focus of research in the area of Representation and Neuron Analysis within the field of Deep Neural Networks in Natural Language Processing?

1. Models trained using Deep Neural Networks (DNNs) have constantly pushed the state-ofthe-art in various Natural Language Processing (NLP) problems, for example Language Modeling (Mikolov et al., 2013;Devlin et al., 2019) and Machine Translation (Sutskever et al., 2014;Bahdanau et al., 2014) to name a few.
2. Despite this remarkable revolution, the black-box nature of deep neural networks has remained major bottleneck in their large scale adaptability -especially in the applications where fairness, trust, accountability, reliability and ethical decision-making are considered critically important metrics or at least as important as model's performance (Lipton, 2016).
3. This opaqueness of Deep Neural Networks has spurred a new area of research to analyze and understand these models.
4. A plethora of papers have *
5. The authors contributed equally †
6. The work was done while the author was at QCRI been written in the past five years on interpreting deep NLP models and to answer one question in particular: What knowledge is learned within representations?
7. We term this work as the Representation Analysis.
8. Representation Analysis thrives on post-hoc decomposability, where we analyze the embeddings to uncover linguistic (and non-linguistic) concepts * that are captured as the network is trained towards an NLP task (Adi et al., 2016;Belinkov et al., 2017a;Conneau et al., 2018;Liu et al., 2019;Tenney et al., 2019).
9. A majority of the work on Representation Analysis has focused on a holistic view of the representations i.e. how much knowledge of a certain concept is learned within representations as a whole (See Belinkov et al. (2020a) for a survey done on this line of work).
10. Recently, a more fine-grained neuron interpretation has started to gain attention.
11. In addition to the holistic view of the representation, Neuron Analysis provides insight into a fundamental question: How is knowledge structured within these representations?
12. In particular, it targets questions such as:• What concepts are learned within neurons of the network?• Are there neurons that specialize in learning particular concepts?• How localized/distributed and redundantly is the knowledge preserved within neurons of the network?Answers to these questions entail potential benefits beyond understanding the inner workings of models, for example: i) controlling bias and manipulating system's behaviour by identifying relevant neurons with respect to a prediction, ii) model distillation by removing less useful neurons, iii) efficient feature selection by selecting the most salient neurons and removing the redundant ones, iv) neural architecture search by guiding the search with important neurons.
13. The work on neuron analysis has explored various directions such as: proposing novel methods to discover concept neurons (Mu and Andreas, 2020;Hennigen et al., 2020), analyzing and comparing architectures using neuron distributions (Wu et al., 2020;Suau et al., 2020;, and enabling applications of neuron analysis (Bau et al., 2019;Dai et al., 2021).
14. In this survey, we aim to provide a broad perspective of the field with an in-depth coverage of each of these directions.
15. We propose a matrix of seven attributes to compare various neuron analysis methods.
16. Moreover, we discuss the open issues and promising future directions in this area.
17. The survey is organized as follows: Section 2 defines the terminologies and formally introduces neuron analysis.
18. Section 3 covers various neuron analysis methods and compares them using seven attributes.
19. Section 4 presents the techniques that have been used to evaluate the effectiveness of neuron analysis methods.
20. Section 5 discusses the findings of neuron analysis methods.
21. Lastly Section 6 showcases various applications of the presented methods and Section 7 touches upon the open issues and future research directions.","1. What is the significance of research in the area of Representation and Neuron Analysis?
2. What is the focus of research in the area of Representation and Neuron Analysis within the field of Deep Neural Networks in Natural Language Processing?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s1),"What are the key definitions and objectives of neuron analysis in neural network models such as RNNs and Transformers?

1. In this section, we define the terminologies used in the paper and the objective of neuron analysis more formally.
2. Neuron Neural networks, such as RNNs or Transformer models consist of various components such as gates/cells, blocks, layers, attention heads, etc.
3. We use the term neuron (also called as features, experts, and units in the literature) to refer to the output of a single dimension from any neural network component.
4. For example, in the BERT base model, the output of a layer block has 768 neurons and the output of an attention head has 64 neurons.
5. Moreover, we refer to individual neurons that learn a single concept as focused neurons, and a set of neurons that in combination represent a concept as group neurons.
6. Concept A concept represents a coherent fragment of knowledge, such as ""a class containing certain objects as elements, where the objects have certain properties"" (Stock, 2010).
7. For example, a concept could be lexical: e.g. words ending with suffix ""ed"", morphological: e.g. gerund verbs, or semantic: e.g. names of cities.
8. We loosely define a concept C as a group of words that are coherent w.r.t to a linguistic property.
9. Table 1 shows an example sentence with different concept annotations.
10. Objective Figure 1 presents an overview of various objectives in neuron analysis.
11. Formally, given a model M and a set of neurons N (which may consist of all the neurons in the network or a specific subset from particular components like a layer or an attention head) and a concept C, neuron analysis aims to achieve one of the following objectives:•
12. For a concept C, find a ranked list of |N | neurons with respect to the concept (dotted blue line)•
13. Given a neuron n i ∈ N , find a set of concepts |C| the neuron represents (dashed purple line)•
14. Given a set of neurons, find a subset of neurons that encode similar knowledge (solid green line)
15. The former two aim to understand what concepts are encoded within the learned representation.
16. The last objective analyzes how knowledge is distributed across neurons.
17. Each neuron n i ∈ N is represented as a vector of activation values over some dataset D. Here, every element of the vector corresponds to a word.
18. For phrase or sentence-level concepts, an aggregation of neuron activations over words in the phrase/sentence is used.
19. Alternatively, [CLS] token representation is also used for transformer models that are transfer learned towards a downstream NLP task.","1. What are the key definitions of neuron analysis in neural network models?
2. What are the objectives of neuron analysis in neural network models such as RNNs and Transformers?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s2),"What are the five main categories for neuron analysis methods, and how do attributes like scope, input/output, scalability, HITL, and supervision differentiate them?

1. We have classified the work done on neuron analysis into 5 broader categories of methods, namely: i) visualizations, ii) corpus-based, iii) probingbased, iv) causation-based and v) miscellaneous methods, based on a set of attributes we describe below:• Scope: Does the method provide global or local interpretation?
2. Global methods accumulate statistics across a set of examples to discover the role of a neuron.
3. Local methods   provide interpretation of a neuron in a particular example and may not necessarily reflect its role over a large corpus.
4. • Input and Output: What is the input (e.g. a set of neurons or concepts) to the method and what does it output?
5. • Scalability: Can the method be scaled to a larger set of neurons?• HITL: Does the method require a human-inthe-loop for interpretation?
6. • Supervision: Does the method depend on labeled data to provide interpretation?• Causation: Is the interpretation connected with the model's prediction?
7. Table 2 summarizes and compares each method in the light of these attributes.
8. We discuss them in detail below.
9. †","1. What are the five main categories for neuron analysis methods?
2. How do the attributes like scope, input/output, scalability, HITL, and supervision differentiate them?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s3),"What are the limitations of using visualization to understand the role of neurons in deep NLP models, and how can it still be considered useful?

1. A simple way to discover the role of a neuron is by visualizing its activations and manually identifying the underlying concept over a set of sentences (Karpathy et al., 2015;Fyshe et al., 2015;Li et al., 2016a).
2. Given that deep NLP models are † Table 3 in Appendix gives a more comprehensive list.
3. trained using billions of neurons, it is impossible to visualize all the neurons.
4. A number of clues have been used to shortlist the neurons for visualization, for example, selecting saturated neurons, high/low variance neurons, or ignoring dead neurons (Karpathy et al., 2015) when using ReLU activation function.
5. ‡ Limitation
6. While visualization is a simple approach to find an explanation for a neuron, it has some major limitations: i) it is qualitative and subjective, ii) it cannot be scaled to the entire network due to an extensive human-in-the-loop effort, iii) it is difficult to interpret polysemous neurons that acquire multiple roles in different contexts, iv) it is ineffective in identifying group neurons, and lastly and v) not all neurons are visually interpretable.
7. Visualization nevertheless remains a useful tool when applied in combination to other interpretation methods that are discussed below.","1. What are the limitations of using visualization to understand the role of neurons in deep NLP models?
2. How can visualization still be considered useful in understanding the role of neurons in deep NLP models?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s4),"How do corpus-based methods facilitate the discovery of the relationship between neurons and concepts in neural networks, and what are their classifications?

1. Corpus-based methods discover the role of neurons by aggregating statistics over data activations.
2. They establish a connection between a neuron and a concept using co-occurrence between a neuron's activation values and existence of the concept in ‡ Saturated neurons have a gradient value of zero.
3. Dead neurons have an activation value of zero.
4. the underlying input instances (e.g. word, phrases or the entire sentence).
5. Corpus-based methods are global interpretation methods as they interpret the role of a neuron over a set of inputs.
6. They can be effectively used in combination with the visualization method to reduce the search space for finding the most relevant portions of data that activates a neuron, thus significantly reducing the humanin-the-loop effort.
7. Corpus-based methods can be broadly classified into two sets: i) the methods that take a neuron as an input and identify the concept the neuron has learned (Concept Search), ii) and others that take a concept as input and identify the neurons learning the concept (Neuron Search).
8. Concept Search This set of methods take a neuron as an input and search for a concept that the neuron has learned.
9. They sort the input instances based on the activation values of the given neuron.
10. The top activating instances represent a concept the neuron represents.
11. Kádár et al. (2017) discovered neurons that learn various linguistic con-cepts using this approach.
12. They extracted top-20, 5-gram contexts for each neuron based on the magnitude of activations and manually identified the underlying concepts.
13. This manual effort of identifying concepts is cumbersome and requires a human-in-the-loop.
14. Na et al.
15. (2019) addressed this by using lexical concepts of various granularities.
16. Instead of 5-gram contexts, they extracted top-k activating sentences for each neuron.
17. They parsed the sentences to create concepts (words and phrases) using the nodes of the parse trees.
18. They then created synthetic sentences that highlight a concept e.g. a particular word occurring in all synthetic sentences.
19. The neurons that activates largely on these sentences are considered to have learned the concept.
20. This methodology is useful in analyzing neurons that are responsible for multi-word concepts such as phrases and idiomatic collocations.
21. However, the synthetic sentences are often ungrammatical and lead towards a risk of identifying neurons that exhibit arbitrary behavior (like repetition) instead of concept specific behavior.","1. How do corpus-based methods facilitate the discovery of the relationship between neurons and concepts in neural networks?
2. What are their classifications?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s5),"How do Neuron Search methods identify and explain the relationship between specific neurons and concepts in computational neuroscience studies?

1. The second class of corpusbased methods aim to discover neurons for a given concept.
2. The underlying idea is the same i.e. to establish a link between the concept and neurons based on co-occurrences stats, but in the opposite direction.
3. The activation values play a role in weighing these links to obtained a ranked list of neurons against the concept.
4. Mu and Andreas (2020) achieved this by creating a binary mask of a neuron based on a threshold on its activation values for every sentence in the corpus.
5. Similarly, they created a binary mask for every concept based on its presence or absence in a sentence.
6. They then computed the overlap between a given neuron mask vector and a concept mask vector using intersection-over-union (IoU), and use these to generate compositional explanations.
7. Differently from them, Suau et al. (2020) used the values of neuron activations as prediction scores and computed the average precision per neuron and per concept.
8. Finally, Antverg and Belinkov (2022) considered the mean activation values of a neuron with respect to instances that posses the concept of interest.
9. The two methods give an alternative view to neuron interpretation.
10. While Neuron Search methods aim to find the neuron that has learned a concept, Concept Search methods generate explanations for neurons by aligning them with a concept.
11. Limitation The corpus-based methods do not model the selection of group neurons that work together to learn a concept.
12. Concept Search methods consider every neuron independently.
13. Similarly, Neuron Search methods do not find the correlation of a group of neurons with respect to the given concept.","1. How do Neuron Search methods identify the relationship between specific neurons and concepts in computational neuroscience studies?
2. How do they explain the relationship between specific neurons and concepts in computational neuroscience studies?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s7),"How do regularization techniques in linear classifiers influence neuron selection and concept learning, and what are the limitations of probing classifiers?

1. The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed.
2. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept.
3. The regularization of the classifier directly effects the weights and therefore the ranking of neurons.
4. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero.
5. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features.
6. This translates to discovering group neurons that are jointly responsible for a concept.
7. Dalvi et al.
8. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.
9. Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018).
10. Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization.
11. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.
12. suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.
13. Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution.
14. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons.
15. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria.
16. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.
17. Limitation
18. In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.","1. How do regularization techniques in linear classifiers influence neuron selection and concept learning?
2. What are the limitations of probing classifiers?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s8),"How do causation-based methods like ablation and attribution contribute to identifying and understanding the role of neurons in a model's prediction accuracy and concept learning?

1. The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts.
2. They do not inherently reflect their importance towards the model's performance.
3. Causation-based methods identify neurons with respect to model's prediction.
4. Ablation
5. The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value.
6. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance.
7. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised).
8. The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a).
9. The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019).
10. Here, the output class serves as the concept against which we want to find the salient neurons.
11. Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007).
12. Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation .
13. Nevertheless all these approaches are approximations and may incur search errors.
14. Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018).
15. Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact.
16. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact.
17. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.
18. Limitation The attribution-based methods highlight salient neurons with respect to a prediction.
19. What concepts these salient neurons have learned is unknown.
20. Dai et al. (2021) worked around this by limiting their study to model classes where each class serves as a concept.
21. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.","1. How do causation-based methods contribute to identifying and understanding the role of neurons in a model's prediction accuracy?
2. What is the role of ablation in this process?
3. What is the role of attribution in this process?
4. How do these methods assist in concept learning?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s9),"How do Corpus Generation, Matrix Factorization, Clustering Methods, and Multi-model Search improve neuron analysis in NLP, and what are their limitations?

1. In this section, we cover a diverse set of methods that do not fit in the above defined categories.
2. Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations.
3. It is possible that a neuron represents a diverse concept which is not featured in the corpus.
4. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations.
5. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role.
6. Corpus generation has been widely explored in Computer Vision.
7. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron.
8. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs.
9. Poerner et al.
10. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.Limitation
11. Although the corpus generation method has the benefit of generating novel patterns that explain a neuron beyond the space of the underlying corpus, it often generates nonsensical patterns and sentences that are difficult to analyze in isolation.
12. A thorough evaluation is necessary to know its true potential and efficacy in NLP.
13. Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function.
14. Given a model, the activations of an input sentence form a matrix.
15. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept.
16. MF is a local interpretation method.
17. It is commonly used in analyzing vision models (Olah et al., 2018).
18. We could not find any research using MF on the NLP models.
19. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.
20. Limitation Compared to the previously discussed unsupervised methods, MF has an innate benefit of discovering group neurons.
21. However, it is still non-trivial to identify the number of groups (factors) to decompose the activations matrix into.
22. Moreover, the scope of the method is limited to local interpretation.
23. Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion.
24. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster.
25. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.
26. aimed at identifying redundant neurons in the network.
27. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them.
28. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.
29. Limitation Similar to the Matrix Factorization method, the number of clusters is a hyperparameter that needs to be pre-defined or selected empirically.
30. A small number of clusters may result in dissimilar neurons in the same group while a large number of clusters may lead to similar neurons split in different groups.
31. Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it.
32. The search involves identifying neurons that behave similarly across the models.
33. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models.
34. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model.
35. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.
36. Limitation All the methods discussed in this section, require human-in-the-loop to provide explanation for the underlying neurons.
37. They can nevertheless be useful in tandem with the other interpretation methods.
38. For example Dalvi et al. (2019) intersected the neurons discovered via the probing classifier and the multi-model search to describe salient neurons in the NMT models.","1. How do Corpus Generation methods improve neuron analysis in NLP, and what are their limitations?
2. How does Matrix Factorization improve neuron analysis in NLP, and what are its limitations?
3. How do Clustering Methods improve neuron analysis in NLP, and what are their limitations?
4. How does Multi-model Search improve neuron analysis in NLP, and what are its limitations?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s11),"How does ablating top-ranked neurons in a model compare to removing randomly selected ones in terms of impacting performance, according to studies?

1. While ablation has been used to discover salient neurons for the model, it has also been used to evaluate the efficacy of the selected neurons.
2. More concretely, given a ranked list of neurons (e.g. the output of the probing method), we ablate neurons in the model in the order of their importance and measure the effect on the performance.
3. The idea is that removing the top neurons should result in a larger drop in performance compared to randomly selected neurons.
4. Dalvi et al. (2019);  used ablation in the probing classifier to demonstrate correctness of their neuron ranking method.
5. Similarly Bau et al. (2019) showed that ablating the most salient neurons, discovered using multi-model search, in NMT models lead to a much bigger drop in performance as opposed to removing randomly selected neurons.",1. How does ablating top-ranked neurons in a model compare to removing randomly selected ones in terms of impacting performance according to studies?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s12),"How can the effectiveness of salient neurons in representing a concept be evaluated according to the content?

1. Given salient neurons with respect to a concept, a simple method to evaluate their correctness is to train a classifier using them as features and predict the concept of interest.
2. The performance of the classifier relative to a classifier trained using random neurons and least important neurons is used as a metric to gauge the efficacy of the selected salient neurons.
3. However, it is important to ensure that the probe is truly representing the concepts encoded within the learned representations and not memorizing them during classifier training.
4. Hewitt and Liang (2019) introduced Controlled Tasks Selectivity as a measure to gauge this.
5. adapted controlled tasks for neuronprobing to show that their probes indeed reflect the underlying linguistic tasks.",1. How can the effectiveness of salient neurons in representing a concept be evaluated according to the content?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s15),"How has visualization been effectively used to qualitatively evaluate neurons in neural networks, specifically regarding linguistic properties?

1. Visualization has been used as a qualitative measure to evaluate the selected neurons.
2. For example, Dalvi et al. (2019) visualized the top neurons and showed that they focus on very specific linguistic properties.
3. They also visualized top-k activating words for the top neurons per concept to demonstrate the efficacy of their method.
4. Visualization can be a very effective tool to evaluate the interpretations when it works in tandem with other methods e.g. using Concept Search or Probingbased methods to reduce the search space towards only highly activating concepts or the most salient neurons for these concepts respectively.","1. How has visualization been effectively used to qualitatively evaluate neurons in neural networks?
2. How does this specifically relate to linguistic properties?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s18),"How has recent research identified neurons that capture lexical concepts, including their roles in sentiment classification and understanding of related groups of concepts?

1. Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.
2. Visualizations Karpathy et al.
3. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end.
4. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment.
5. For example ""I like this movie a lot"" or ""the movie is incredibly good"".
6. Similarly they discovered neurons that captured ""negation"".
7. Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained.
8. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task.
9. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"".
10. Similarly Na et al.
11. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc.
12. They also found neurons that learn phrasal concepts.
13. Poerner et al.
14. (2018) showed that Concept Search can be enhanced via Corpus Generation.
15. They provided finer interpretation of the neurons by generating synthetic instances.
16. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.","1. How has recent research identified neurons that capture lexical concepts?
2. What are their roles in sentiment classification?
3. How do these neurons contribute to the understanding of related groups of concepts?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s20),"How do neurons in LSTM-based NMT models specialize and exhibit behaviors in capturing linguistic concepts, and what are the implications for machine translation?

1. A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018).
2. § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012).
3. Below we discuss major findings along this line of work:Neurons specialize in core linguistic concepts Dalvi et al.
4. (2019) in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc.
5. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept.
6. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.
7. Neurons exhibit monosemous and polysemous behavior.
8. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis.
9. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc.
10. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"".
11. concepts.
12. Suau et al. (2020) discovered neurons that capture different senses of a word.
13. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.
14. Neurons capture syntactic concepts and complex semantic concepts.
15. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates.
16. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency.
17. Na et al.
18. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases.
19. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.","1. How do neurons in LSTM-based NMT models specialize?
2. How do neurons in LSTM-based NMT models exhibit behaviors in capturing linguistic concepts?
3. What are the implications for machine translation?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s21),"How do researchers identify and evaluate the significance of salient neurons in LSTM-based models and Glove, and what concepts do these neurons most commonly represent?

1. In contrast to analyzing neurons with respect to a pre-defined concept, researchers also interpreted the concepts captured in the most salient neurons of the network.
2. For example, in the analysis of the encoder of LSTM-based models, Bau et al.(2019) used Pearson correlation to discover salient neurons in the network.
3. They found neurons that learn position of a word in the sentence among the most important neurons.
4. Other neurons found included parentheses, punctuation and conjunction neurons.
5. Moreover, Li et al. (2016b) found that the two most salient neurons in Glove were the frequency neurons that play an important role in all predictions.
6. The question of whether core-linguistic concepts are important for the end performance has been a less explored area.
7. Dalvi et al. (2019) compared neurons learning morphological concepts and semantic concepts with unsupervised ranking of neurons with respect to their effect on the end performance.
8. They found that the model is more sensitive to the top neurons obtained using unsupervised ranking compared to linguistic concepts.
9. They showed that the unsupervised ranking of neurons is dominated by position information and other closed class categories such as conjunction and punctuation which according to the ablation experiment are more critical concepts for the end performance than linguistic concepts.","1. How do researchers identify and evaluate the significance of salient neurons in LSTM-based models and Glove?
2. What concepts do these neurons most commonly represent?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s23),"How do neurons in pre-trained language models like LSTM and transformers relate to the hierarchical structure of human languages, according to recent studies?

1. Human languages are hierarchical in structure where morphology and phonology sit at the bottom followed by lexemes, followed by syntactic structures.
2. Concepts such as semantics and pragmatics are placed on the top of the hierarchy.
3. analyzed linguistic hierarchy by studying the spread of neurons across layers in various pre-trained language models.
4. They extracted salient neurons with respect to different linguistic concepts (e.g. morphology and syntax) and found that neurons that capture word morphology were predominantly found in the lower and middle layers and those learning about syntax were found at the higher layers.
5. The observation was found to be true in both LSTMand the transformer-based architectures, and are inline with the findings of representation analysis (Liu et al., 2019;Tenney et al., 2019;Belinkov et al., 2020b).
6. Similarly Suau et al. (2020) analyzed sub-modules within GPT and RoBERTa transformer blocks and showed that lower layers within a transformer block accumulate more salient neurons than higher layers on the tasks of word sense disambiguation or homograph detection.
7. They also found that the neurons that learn homographs are distributed across the network as opposed to sense neurons that were more predominantly found at the lower layers.","1. How do neurons in pre-trained language models like LSTM and transformers relate to the hierarchical structure of human languages?
2. What do recent studies indicate about this relationship?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s24),"How does the inclusion of dropout in training neural networks impact the distribution and redundancy of linguistic information according to recent studies?

1. While it is exciting to see that networks somewhat preserve linguistic hierarchy, many authors found that information is not discretely preserved at any individual layer, but is distributed and is redundantly present in the network.
2. This is an artifact of various training choices such as dropout that encourages the model to distribute knowledge across the network.
3. For example, Li et al. (2016b) found specialized frequency neurons in a GloVe model trained without dropout, as opposed to the variant trained with dropout where the information was more redundantly available.
4. showed that a significant amount of redundancy existed within pre-trained models.
5. They showed that 85% of the neurons across the network are redundant and at least 92% of the neurons can be removed when optimizing towards a downstream task in feature-based transfer learning.","1. How does the inclusion of dropout in training neural networks impact the distribution of linguistic information according to recent studies?
2. How does the inclusion of dropout in training neural networks impact the redundancy of linguistic information according to recent studies?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s25),"How do different neural network architectures compare in terms of neuron distribution and linguistic knowledge representation, according to recent studies?

1. The distribution of neurons across the network has led researchers to draw interesting crossarchitectural comparisons.
2. Wu et al. (2020) performed correlation clustering of neurons across architectures and found that different architectures may have similar representations, but their individual neurons behave differently.
3. Hennigen et al. (2020) compared neurons in contextualized (BERT) embedding with neurons in the static embedding (fastText) and found that fastText required two neurons to capture any morphosyntactic phenomenon as opposed to BERT which required up to 35 neurons to obtain the same performance.
4. showed that the linguistic knowledge in BERT (auto-encoder) is highly distributed across the network as opposed to XLNet (auto-regressive) where neurons from a few layers are mainly responsible for a concept (see Figure 2).
5. Similarly Suau et al. (2020) compared RoBERTa and GPT (auto-encoder vs. generative) models and found differences in the distribution of expert neurons.
6. extended the cross-architectural comparison towards fine-tuned models.
7. They showed that after finetuning on GLUE tasks, the neurons capturing linguistic knowledge are regressed to lower layers in RoBERTa and XLNet as opposed to BERT where it is still retained at the higher layers.","1. How do different neural network architectures compare in terms of neuron distribution?
2. How do these architectures compare in linguistic knowledge representation, according to recent studies?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s26),"What did the survey reveal about how deep NLP models' neurons learn and represent linguistic knowledge, including their structure and distribution across networks?

1. Below is a summary of the key findings that emerged from the work we covered in this survey.
2. Neurons learned within Deep NLP models capture non-trivial linguistic knowledge ranging from lexical phenomenon such as morphemes, words and multi-word expressions to highly complex global phenomenon such as semantic roles and syntactic dependencies.
3. Neuron analysis resonates with the findings of representation analysis (Belinkov et al., 2017a,b;Tenney et al., 2019;Liu et al., 2019) in demonstrating that the networks follow linguistic hierarchy.
4. Linguistic neurons are distributed across the network based on their complexity, with lower layers focused on the lexical concepts and middle and higher layers learning global phenomenon based on long-range contextual dependencies.
5. While the networks preserve linguistic hierarchy, many authors showed that information is not discretely preserved, but is rather distributed and redundantly present in the network.
6. It was also shown that a small optimal subset of neurons w.r.t any concept can be extracted from a network.
7. On another dimension, a few works showed that some concepts are localized to fewer neurons while others are distributed to a large group.
8. Finally, some interesting cross architectural analyses were drawn based on how the neurons are distributed within their layers.","1. What did the survey reveal about how deep NLP models' neurons learn?
2. What did the survey reveal about the representation of linguistic knowledge in deep NLP models?
3. How is the structure of this knowledge distribution across networks detailed in the survey findings?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s28),"How can neurons within NMT models be manipulated to control the model's output, specifically for tense, gender, and number agreement concepts?

1. Once we have identified neurons that capture a certain concept learned in a model, these can be utilized for controlling the model's behavior w.r.t to that concept.
2. Bau et al. (2019) identified Switch Neurons in NMT models that activate positively for the present-tense verbs and negatively for the past-tense verbs.
3. By manipulating the values of these neurons, they were able to successfully change output translations from present to past tense during inference.
4. The authors additionally found neurons that capture gender and number agreement concepts and manipulated them to control the system's output.
5. Another effort along this line was carried by Suau et al.
6. (2020)
7. Controlling model's behavior using neurons en-ables on-the-fly manipulation of output, for example it can be used to debias the output of the model against sensitive attributes like race and gender.","1. How can neurons within NMT models be manipulated to control the model's output?
2. What specific concepts can be controlled through this manipulation, specifically tense, gender, and number agreement?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s29),"How can identifying salient neurons and sub-networks within deep NLP models enhance computational efficiency and maintain performance?

1. Deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments.
2. Identifying salient neurons and sub-networks can be useful for model distillation and efficiency.
3. devised an efficient featurebased transfer learning procedure, stemmed from their redundancy analysis.
4. By exploiting layer and neuron-specific redundancy in the transformer models, they were able to reduce the feature set size to less than 10% neurons for several tasks while maintaining more than 97% of the performance.
5. The procedure achieved a speedup of up to 6.2x in computation time for sequence labeling tasks as opposed to using all the features.","1. How can identifying salient neurons and sub-networks within deep NLP models enhance computational efficiency?
2. How does this identification maintain performance?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s30),"How does the three-step domain adaptation method proposed by Gu et al. (2021) help in preventing catastrophic forgetting and optimizing in-domain performance?

1. Identifying the salient neurons with respect to a domain can be effectively used for domain adaptation and generalization.
2. Gu et al. (2021) proposed a domain adaptation method using neuron pruning to target the problem of catastrophic forgetting of the general domain when fine-tuning a model for a target domain.
3. They introduced a three step adaptation process: i) rank neurons based on their importance, ii) prune the unimportant neurons from the network and retrain with student-teacher framework, iii) expand the network to its original size and fine-tune towards indomain, freezing the salient neurons and adjusting only the unimportant neurons.
4. Using this approach helps in avoiding catastrophic forgetting of the general domain while also obtaining optimal performance on the in-domain data.","1. What is the three-step domain adaptation method proposed by Gu et al. (2021)?
2. How does it help in preventing catastrophic forgetting?
3. How does it optimize in-domain performance?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,(s32),"What are the major open issues and potential future directions in the field of neuron and model interpretation as discussed in recent research?

1. In the following section, we discuss several open issues and limitations related to methods, evaluation and datasets.
2. Moreover, we provide potential future directions vital to the progress of neuron and model interpretation.
3. • DNNs are distributed in nature which encourages groups of neurons to work together to learn a concept.
4. The current analysis methods, at large, ignore interaction between neurons while discovering neurons with respect to a concept.
5. Trying all possible combination of neurons is a computationally intractable problem.
6. A linear classifier using ElasticNet regularization (Dalvi et al., 2019) considers grouping of features during training -however, it's effectiveness in handling grouped neurons has not been empirically validated.
7. Evolutionary algorithms || do not make any assumption of the underline distribution of the features and they have been effectively used for feature selection of multivariate
8. fea-|| https://en.wikipedia.org/wiki/ Evolutionary_algorithm tures.
9. Exploring them for neuron selection is a promising research direction to probe towards latent concepts in these models.
10. •
11. A large number of interpretation studies rely on human-defined linguistic concepts to probe a model.
12. It is possible that the models do not strictly adhere to the humandefined concepts and learn novel concepts about the language.
13. This results in an incorrect or incomplete analysis.
14. Several researchers (Michael et al., 2020; made strides in this direction by analyzing hidden structures in the input representations in an unsupervised manner.
15. They discovered existence of novel structures not captured in the human defined categories.
16. also proposed BERT ConceptNet, a manual annotation of the latent concepts in BERT.
17. Introducing similar datasets across other models enables model-centric interpretation, and is a promising research direction.
18. • While a lot of work has been done on analyzing how knowledge is encoded within the learned representations, the question whether it is used by the model during prediction is a less explored area (Feder et al., 2021;Elazar et al., 2021).
19. Ablation and knowledge attribution methods are two neuron interpretation methods that intrinsically use causal relation to select concept neurons.
20. A few other studies evaluated the causal relation of the selected concept neurons via ablation or by clamping their activation values (Bau et al., 2019;Suau et al., 2020) and observed the change in model's prediction.
21. However, most of the studies do not take into account the causal relation as part of the method or the evaluation of their method.
22. The causal relation with respect to concept neurons is important to understand their importance to overall prediction and it leads way towards practical applications such as debiasing, model distillation and domain adaptation.
23. •
24. The work on neuron interpretation lacks standard evaluation benchmarks, and therefore studies conducted on identical models are not comparable.
25. For example, there exists no gold annotation of neurons with respect to a certain dataset or a class.
26. The curation of standard evaluation benchmarks is an essential step towards improving methods of interpretation of deep neural network models.
27. •
28. The neuron analysis methods vary in their theoretical foundations as well as the perspective they aim to capture with respect to a given concept.
29. This results in a selection of neurons that may not strictly align across all methods.
30. For example, Visualization, Neuron Search and Corpus Search discover neurons that are highly focused on a specific task (like ""less"" suffix or POS ""TO"" concepts), while Probing-based methods discover ranking of neurons that highlight grouping behavior within the neurons targeting broad concepts like POS ""Nouns"".
31. Therefore, the choice of which neuron interpretation method to use is not straightforward and depends on various factors such as the nature of the concept to investigate, the availability of supervised data for the concept of interest etc.
32. Apart from these high-level guiding principles, a thorough comparison of methods with respect to the nature of the concept of interest is needed to fully understand the strengths and weaknesses of each approach.
33. Antverg and Belinkov (2022) is one such effort in this direction that compares three neuron interpretation methods.
34. • Neuron-level interpretation opens door for a number of applications useful for the successful deployment of DNN systems (Section 6).
35. However, most of the research conducted in this direction is preliminary.
36. For example, there are many open research questions in controlling system's behaviour using neurons such as: i) are all concepts manipulatable?
37. ii) how to identify neurons that can be controlled to change the output?
38. iii) is high distributiveness a hindrance for controlling model's behavior?
39. iv) and whether disentangled (Bengio et al., 2012) and sparse models (Frankle and Carbin, 2019) may serve as a better alternate on this front?
40. Addressing these questions will enable a more reliable control of the deep NLP models and entail numerous applications such as removing bias and adapting the system to novel domains.","1. What are the major open issues in the field of neuron and model interpretation as discussed in recent research?
2. What are potential future directions in the field of neuron and model interpretation as discussed in recent research?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s0),"How do large language models (LLMs) transform the field of natural language processing, and what practical advice is provided for their effective and efficient utilization in NLP tasks?

1. In recent years, the rapid development of Large language Models has been revolutionizing the field of natural language processing [12,128,131].
2. These powerful models have shown great potential in addressing a variety of NLP tasks, ranging from natural language understanding (NLU) to generation tasks, even paving the way to Artificial General Intelligence (AGI).
3. However, utilizing these models effectively and efficiently requires a practical understanding of their capabilities and limitations, as well as the data and tasks involved in NLP.
4. To provide a guide for partitioners and end-users, this work focuses on the practical aspects of working with LLMs in downstream NLP tasks.
5. This guide aims to provide practical advice on why or why not to choose LLMs for a given task, as well as guidance on how to select the most suitable LLM, taking into account factors such as model sizes, computational requirements, and the availability of domain-specific pre-trained models.
6. This work offers a thorough understanding of LLMs from a practical perspective, therefore, empowers practitioners and end-users with the practical knowledge needed to successfully leverage the power of LLMs for their own NLP tasks.
7. Our work is structured as follows.
8. First, our work offers a brief introduction to LLMs by discussing the most important models, such as GPT-style and BERT-style architectures.
9. Then, we delve into the critical factors that influence model performance from the data perspective, including pre-training data, training/tuning data, and test data.
10. Last and most importantly, we dive deep into various concrete NLP tasks, offering insights into the applicability of LLMs for knowledge-intensive tasks, traditional NLU tasks, and generation tasks, along with the emergent abilities that these models possess and challenging real-world scenarios.
11. We provide detailed examples to highlight both the successful use cases and the limitations of LLMs in practice.
12. To analyze the abilities of large language models, we compare them with fine-tuned models.
13. As of present, there is no universally recognized definition for LLMs and fine-tuned models.
14. With consideration to practical utility, in our article, the definitions of them are proposed as: LLMs are huge language models pretrained on large amounts of datasets without tuning on data for specific tasks; fine-tuned models are typically smaller language models which are also pretrained and then further tuned on a smaller, task-specific dataset to optimize their performance on that task 1 .
15. This work summarizes the following main practical guides for using LLMs:• Natural language understanding.
16. Employ the exceptional generalization ability of LLMs when facing out-ofdistribution data or with very few training data.
17. • Natural language generation.
18. Utilize LLMs' capabilities to create coherent, contextually relevant, and highquality text for various applications.
19. • Knowledge-intensive tasks.
20. Leverage the extensive knowledge stored in LLMs for tasks requiring domainspecific expertise or general world knowledge.
21. •
22. Reasoning ability.
23. Understand and harness the reasoning capabilities of LLMs to improve decision-making and problem-solving in various contexts.","1. How do large language models (LLMs) transform the field of natural language processing?
2. What practical advice is provided for their effective and efficient utilization in NLP tasks?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s1),"What observations have been made about the evolution of language models, especially concerning the rise of decoder-only models and OpenAI's leadership position?

1. This section provides a brief introduction to state-of-the-art LLMs.
2. These models differ in their training strategies, model architectures, and use cases.
3. To provide a clearer understanding of the LLM landscape, we categorize them into two types: encoder-decoder or encoder-only language models and decoder-only language models.
4. In Figure 1, we show the detailed evolution process of language models.
5. From the evolutionary tree, we make the following interesting observations: a) Decoder-only models have been gradually dominating the development of LLMs.
6. At the early stage of LLMs development, decoder-only models were not as popular as encoder-only and encoder-decoder models.
7. However, after 2021, with the introduction of game-changing LLMs -GPT-3, decoder-only models experienced a significant boom.
8. Meanwhile, after the initial explosive growth brought about by BERT, encoder-only models gradually began to fade away.
9. Fig.
10. 1.
11. The evolutionary tree of modern LLMs traces the development of language models in recent years and highlights some of the most well-known models.
12. Models on the same branch have closer relationships.
13. Transformer-based models are shown in non-grey colors: decoder-only models in the blue branch, encoder-only models in the pink branch, and encoder-decoder models in the green branch.
14. The vertical position of the models on the timeline represents their release dates.
15. Open-source models are represented by solid squares, while closed-source models are represented by hollow ones.
16. The stacked bar plot in the bottom right corner shows the number of models from various companies and institutions.
17. b) OpenAI consistently maintains its leadership position in LLM, both currently and potentially in the future.
18. Other companies and institutions are struggling to catch up with OpenAI in developing models comparable to GPT-3 and the current GPT-4.
19. This leadership position may be attributed to OpenAI's steadfast commitment to its technical path, even when it was not widely acknowledged initially.","1. What observations have been made about the evolution of language models?
2. What does the rise of decoder-only models signify?
3. How is OpenAI's leadership position relevant to the evolution of language models?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s3),"How do BERT-style Masked Language Models (MLMs) utilize unsupervised learning to improve their understanding of natural language relationships and context?

1. As natural language data is readily available and unsupervised training paradigms have been proposed to better utilize extremely large datasets, this motivates the unsupervised learning of natural language.
2. One common approach is to predict masked words in a sentence while considering the surrounding context.
3. This training paradigm is known as the Masked Language Model.
4. This type of training allows the model to develop a deeper understanding of the relationships between words and the context in which they are used.
5. These models are trained on a large corpus of texts using techniques such as the Transformer architecture and have achieved state-of-the-art results in many NLP tasks, such as sentiment analysis and named entity recognition.
6. Notable examples of Masked Language Models include BERT [28], RoBERTa
7. [65], and T5 [84].
8. MLMs have become an important tool in the field of natural language processing due to their success in a wide range of tasks.","1. How do BERT-style Masked Language Models (MLMs) utilize unsupervised learning?
2. How does this improve their understanding of natural language relationships and context?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s4),"What has been shown to improve few-shot and zero-shot performance in autoregressive language models like GPT-3, and how have these models been applied?

1. Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task.
2. Researchers found that scaling up language models significantly improves the few-shot, even zero-shot performance [16].
3. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words.
4. These models have been widely used for downstream tasks such as text generation and question answering.
5. Examples of Autoregressive Language Models include GPT-3
6. [16], OPT
7. [126], PaLM [22], and BLOOM [92].
8. The game changer, GPT-3, for the first time, demonstrated reasonable few-/zero-shot performance via prompting and in-context learning, thus showing the superiority of autoregressive language models.
9. There are also models such as CodeX [2] that are optimized for specific tasks such as code generation, BloombergGPT
10. [117] for the financial domain.
11. The recent breakthrough is ChatGPT, which refines GPT-3 specifically for conversational tasks, resulting in more interactive, coherent, and context-aware conversational for various real-world applications.","1. What has been shown to improve few-shot and zero-shot performance in autoregressive language models like GPT-3?
2. How have these models been applied?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s7),"How do the quality, diversity, and comprehensiveness of pre-training data influence the performance of large language models in various tasks?

1. Pre-training data plays a pivotal role in the development of large language models.
2. As the foundation of remarkable capabilities [5,47] of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance of LLMs significantly
3. [124].
4. The commonly used pretraining data consists of a myriad of text sources, including books, articles, and websites.
5. The data is carefully curated to ensure a comprehensive representation of human knowledge, linguistic nuances, and cultural perspectives.
6. The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses.
7. The diversity of pretraining data also plays a crucial role in shaping the model's performance, and the selection of LLMs highly depends on the components of the pretraining data.
8. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an abundance of multilingual pretraining data.
9. Moreover, PaLM's performance in Question Answering tasks is enhanced by incorporating a considerable amount of social media conversations and Books corpus
10. [22].
11. Likewise, code execution and code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its pretraining dataset.
12. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data.","1. How do the quality, diversity, and comprehensiveness of pre-training data influence the performance of large language models?
2. In what tasks are these influences observed?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s8),"What approaches should be taken when deploying models for downstream tasks with varying amounts of annotated data according to the content provided?

1. When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant.
2. In this section, we provide a succinct overview of the appropriate models to employ for each scenario.
3. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach.
4. LLMs have been shown to outperform previous zero-shot methods [120].
5. Additionally, the absence of a parameter update process ensures that catastrophic forgetting
6. [49] is avoided since the language model parameters remain unaltered.
7. Few annotated data: In this case, the few-shot examples are directly incorporated in the input prompt of LLMs, which is named as in-context learning, and these examples can effectively guide LLMs to generalize to the task.
8. As reported in [16], one-shot and few-shot performance make significant gains, even matching the performance of the SOTAfine-tuned open-domain models.
9. And LLMs' zero/few-shot ability can be improved further by scaling [16].
10. Alternatively, some few-shot learning methods are invented to enhance fine-tuned models, such as meta-learning [56] or transfer learning [88].
11. However, performance might be inferior compared to using LLMs due to fine-tuned models' smaller scale and overfitting.
12. Abundant annotated data: With a substantial amount of annotated data for a particular task available, both fine-tuned models and LLMs can be considered.
13. In most cases, fine-tuning the model can fit the data pretty well.
14. Although, LLMs can be used to meet some constraints such as privacy
15. [99].In this scenario, the choice between using a fine-tuned model or a LLM is task-specific and also depends on many factors, including desired performance, computational resources, and deployment constraints.
16. In a brief summary: LLMs are more versatile w.r.t.
17. the data availability, while fine-tuned models can be considered with abundant annotated data.",1. What approaches should be taken when deploying models for downstream tasks with varying amounts of annotated data?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s9),"How do language models like InstructGPT and ChatGPT maintain effectiveness on downstream tasks despite distributional differences between training and test/user data?

1. When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data.
2. These disparities may encompass domain shifts [132], out-of-distribution variations [31], or even adversarial examples [82].
3. Such challenges significantly hinder fine-tuned modes' effectiveness in real-world applications.
4. They fit into a specific distribution and have a poor ability to generalize to OOD data.
5. However, LLMs perform quite well facing such scenarios because they do not have an explicit fitting process.
6. Moreover, recent advancements have further enhanced the ability of language models in this regard.
7. The Reinforcement Learning from Human Feedback (RLHF) method has notably enhanced LLMs' generalization capabilities [77].
8. For example,InstructGPT demonstrates proficiency in following various instructions for a wide range of tasks and occasionally complying with instructions in different languages, even though such instructions are scarce.
9. Similarly, ChatGPT exhibits consistent advantages on most adversarial and out-of-distribution (OOD) classification and translation tasks [109].
10. Its superiority in understanding dialogue-related texts led to an impressive performance on the DDXPlus dataset [101], a medical diagnosis dataset designed for OOD evaluation.","1. How do language models like InstructGPT and ChatGPT maintain effectiveness on downstream tasks?
2. What mitigates distributional differences between training and test/user data for these models?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s13),"How do fine-tuned models compare to large language models (LLMs) in natural language understanding tasks, and what are some exceptions where LLMs perform better?

1. In most natural language understanding tasks, such as tasks in GLUE [106] and SuperGLUE
2. [105], fine-tuned models still have better performance, if such tasks come with rich well-annotated data and contain very few out-of-distribution examples on test sets.
3. For different tasks and datasets, the gap between small fine-tuned models and LLMs varies.
4. In text classification, on most datasets, LLMs perform slightly worse than fine-tuned models.
5. For sentiment analysis, such as on IMDB [69] and SST
6. [94], fine-tuned models and LLMs perform equally well.
7. For toxicity detection, which is another iconic text classification task, the gap is much larger.
8. All LLMs cannot perform well on this task, and onCivilComments [13] even the best one is only better than random guessing [59].
9. On the other hand, most popular fine-tuned models can obtain much better performance [33].
10. and the Perspective API 3 is still one of the best for detecting toxicity.
11. This API is powered by a multilingual BERT-based model, which is tuned on publicly available toxicity data and several smaller single-language CNNs distilled from this model.
12. This might be due to the fact that toxicity is defined by subtle nuances in linguistic expressions, and large language models are unable to accurately comprehend this task solely based on the provided input.
13. The trend of performance gaps is similar in some other tasks.
14. For natural language inference (NLI) tasks, on most datasets, such as on RTE [106] and SNLI
15. [14], fine-tuned models perform better than LLMs, while on some data such as CB
16. [105], LLMs have obtained comparable performance with fine-tuned models [22].
17. For question answering (QA), on SQuADv2 [86], QuAC
18. [21] and many other datasets, fine-tuned models have superior performance, while on CoQA
19. [87], LLMs perform as well as fine-tuned models [22].
20. In information retrieval (IR) tasks, LLMs are not widely exploited yet.
21. One major reason is that IR tasks are fundamentally different from others.
22. There's no natural way to transform the thousands of candidate texts into a few/zero-shot form which is required by LLMs.
23. The existing evaluation results on MS MARCO(regular/TREC)
24. [73] show that methods based on fine-tuned models have better performance [59].
25. In this evaluation, the LLMs rank passages in an unorthodox way, which requires the LLMs to produce probabilities for passages one by one.
26. For some low-level intermediate tasks, which are not intended for regular users but rather for high level tasks, such as named entity recognition (NER) and dependency parsing, there's not enough result coming from LLMs, because the most current evaluation of LLMs focuses on practical tasks.
27. According to available evaluation results, for the NER task, CoNLL03
28. [89] is still a challenge for LLMs [81], where the performance of fine-tuned models is around as twice as LLMs.
29. These intermediate tasks may vanish soon because LLMs can take over high-level tasks without the help of those intermediate tasks (e.g. dependency parsing for coding tasks; NER for some text generation tasks).
30. In brief, for most traditional NLU tasks, a fine-tuned model is a better choice in terms of the performance on benchmark datasets and the computational cost.
31. The scale of LLMs is usually 10× or even 100× larger than fine-tuned models.
32. One possible cause for the inferior performance of LLMs on certain tasks can be the design of instructions/prompts.
33. Transforming input from tasks like IR and sentence labeling into a few/zero-short instruction form is non-trivial.
34. There may be better ways to adapt language models to traditional NLP tasks in the future.
35. On the other hand, the upper limit of capabilities of fine-tuned models is not reached, and some methods like FLAN-tuning [67] can further boost the performance on NLU tasks.
36. Another interesting finding is that on NLU tasks, after fine-tuning, masked language models, like T5 [85], are better than most auto-regressive language models at the same scale, while some recent results imply that this gap can be bridged by scaling [22].
37. 4.1.2 Use case.
38. However, there are still some NLU tasks suitable for LLMs.
39. One of the representative tasks is miscellaneous text classification [59].
40. In contrast to classic domain-specific text classification tasks such as sentiment analysis, miscellaneous text classification deals with a diverse range of topics and categories that may not have a clear or strong relationship with one another.
41. It's closer to real-world cases and hard to be formatted for using fine-tuned models.
42. Another is the Adversarial NLI (ANLI)
43. [74].
44. It is a difficult dataset composed of adversarially mined natural language inference questions in three rounds (R1, R2, and R3).
45. LLMs have shown superior performance on ANLI, especially on the R3 and R2.
46. Both examples demonstrate the exceptional ability of LLMs to generalize well on out-of-distribution and sparsely annotated data in traditional NLP tasks, surpassing that of fine-tuned models.
47. We've discussed this in the section above 3.3.","1. How do fine-tuned models compare to large language models (LLMs) in natural language understanding tasks?
2. What are some exceptions where LLMs perform better?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s15),"How do large language models (LLMs) perform in tasks such as generation, summarization, machine translation, and code synthesis, according to various evaluations and benchmarks?

1. Due to their strong generation ability and creativity, LLMs show superiority at most generation tasks.
2. 4.2.1 Use case.
3. Generation tasks require models to have a comprehensive understanding of the input contents or requirements and a certain level of creativity.
4. This is what LLMs excel at.
5. For summarization tasks, although LLMs do not have an obvious advantage over fine-tuned models under traditional automatic evaluation metrics, such as ROUGE [60], human evaluation results indicate that humans tend to prefer the results generated by LLMs [38,127] compared to that of fine-tuned models.
6. For example, on CNN/DailyMail [71] and XSUM
7. [72], fine-tuned models like Brio
8. [66] and Pegasus
9. [125] have much better performance than any LLMs w.r.t.ROUGE, but LLMs like OPT
10. [126] perform far better in human evaluation considering all aspects including faithfulness, coherence, and relevance [127].
11. This demonstrates the superiority of LLMs in summarization tasks.
12. On the other hand, it implies that current summarization benchmarks don't contain summaries with high quality or the automatic metrics are not proper for the evaluation of summarization.
13. In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU
14. [78].
15. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16
16. [11], zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model [22].
17. This is mainly due to the fact that English resources compose the main part of the pre-training data.
18. BLOOM
19. [92] is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation.
20. Another interesting finding is that BLOOM achieves good translation quality among Romance languages, even for translation from Galician, which is not included in the pre-training data.
21. One reasonable explanation is that texts from some languages in the same language group can help the LLMs learn more from the similarity.
22. If more multi-lingual texts can be added to the pre-training data, the translation capability may be improved further.
23. Additionally, LLMs are highly skilled in open-ended generations.
24. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans
25. [16].
26. LLMs are remarkably adept at code synthesis as well.
27. Either for text-code generation, such as HumanEval [18] and MBPP
28. [7], or for code repairing, such as DeepFix [39], LLMs can perform pretty well.
29. GPT-4 can even pass 25% problems in Leetcode, which are not trivial for most human coders [76].
30. With training on more code data, the coding capability of LLMs can be improved further
31. [22].
32. While performing well on such tasks, the codes generated by LLMs should be tested carefully to figure out any subtle bugs, which is one of the main challenges for applying LLMs in code synthesis.","1. How do large language models (LLMs) perform in tasks such as generation?
2. How do large language models (LLMs) perform in tasks such as summarization?
3. How do large language models (LLMs) perform in tasks such as machine translation?
4. How do large language models (LLMs) perform in tasks such as code synthesis?
5. According to what evaluations and benchmarks?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s18),"What are the strengths and limitations of Large Language Models (LLMs) in performing knowledge-intensive and contextual tasks, including the impact of retrieval augmentation?

1. (1) LLMs excel at knowledge-intensive tasks due to their massive real-world knowledge.
2. (2) LLMs struggle when the knowledge requirements do not match their learned knowledge, or when they face tasks that only require contextual knowledge, in which case fine-tuned models can work as well as LLMs.
3. 4.3.1 Use case.
4. In general, with billions of training tokens and parameters, LLMs have much more real-world knowledge than fine-tuned models.Closed-book question-answering tasks require the model to answer a given question about factual knowledge without any external information.
5. It does require the memorization of real-world knowledge in the model.
6. LLMs perform better on nearly all datasets, such as on NaturalQuestions [52], WebQuestions [9], and TriviaQA
7. [46].
8. On TriviaQA, even zero-shot LLMs is still much better
9. [22].
10. The massive multitask language understanding (MMLU)
11. [40] is also highly knowledge-intensive.
12. Some tasks only require the model to capture the self-contained knowledge in the contexts.
13. The knowledge in the contexts from the input is enough for the model to make predictions.
14. For these tasks, small fine-tuned models can work pretty well.
15. One such task is machine reading comprehension (MRC).
16. An MRC task provides several paragraphs and requires the model to predict the answer to questions based on these paragraphs.
17. We've discussed MRC in the previous section because it's also a traditional NLU task.
18. Another scenario is that the knowledge within LLMs about real world is useless to the task, or even the required knowledge is counterfactual to the real world.
19. As a result, the LLMs cannot work well on such tasks.
20. In some cases, inconsistent knowledge may even make the LLMs worse than random guessing.
21. For example, in Big-Bench, the Mnist ascii task requires the model to tell the digit represented by an ASCII art.
22. The capability required by this task is nothing about real-world knowledge.
23. Also, in the Inverse Scaling Phenomenon competition [70], the task redefine math redefines a common symbol and requires the model to choose between the original meaning and the meaning derived from the redefinition.
24. What it requires contrasts to the LLMs' knowledge, thus LLMs even perform worse than random guessing.
25. As an alternative to real-world knowledge in LLMs, access to extra knowledge is allowed, and models can thus get enough knowledge for a task via retrieval augmentation.
26. The basic idea of retrieval augmentation is to add an extra information retrieval step prior to making predictions, in which, some useful texts related to the task will be retrieved from a large corpus.
27. Then, the model will make predictions based on both the input contexts and the retrieved texts.
28. With retrieved additional information, the closed-book task can become ""open-book"".
29. In such a scenario, fine-tuned models are pretty good with much smaller sizes, because the required knowledge can be obtained by retrieving.
30. For example, on NaturalQuestions [52], with extra corpus, retrieval augmented models [44,48] are much better than any other methods.","1. What are the strengths of Large Language Models (LLMs) in performing knowledge-intensive and contextual tasks?
2. What are the limitations of Large Language Models (LLMs) in performing knowledge-intensive and contextual tasks?
3. What is the impact of retrieval augmentation on Large Language Models (LLMs)?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s19),"How does scaling up the parameters and computation of LLMs impact their performance and abilities in tasks, particularly in reasoning?

1. Scaling of LLMs (e.g. parameters, training computation, etc.) can greatly empower pretrained language models.
2. With the model scaling up, a model generally becomes more capable in a range of tasks.
3. Reflected in some metrics, the performance shows a power-law relationship with the model scale.
4. For example, the cross-entropy loss which is used to measure the performance for language modeling decreases linearly with the exponential increase in the model scale, which is also called 'scaling-law' [41,47].
5. For some crucial abilities, such as reasoning, scaling the model has gradually transformed these abilities from a very low state to a usable state, and even approaching human capabilities.
6. In this section, we provide an overview of the usage of LLMs in terms of the abilities and behaviors of LLMs along with scaling.","1. How does scaling up the parameters and computation of LLMs impact their performance?
2. How does scaling up the parameters and computation of LLMs impact their abilities in tasks, particularly in reasoning?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s21),"What is the impact of increasing model size on the arithmetic and commonsense reasoning capabilities of Large Language Models, and how does GPT-4's performance compare to other methods?

1. Reasoning, which involves making sense of information, drawing inferences, and making decisions, is one of the essential aspects of human intelligence.
2. It is challenging for NLP.
3. Many existing reasoning tasks can be classified into commonsense reasoning and arithmetic reasoning.
4. Arithmetic reasoning/problem solving.
5. The arithmetic reasoning capability of LLMs benefits greatly from the scaling of model size.
6. For GPT-3, the ability of two-digit addition only becomes apparent when the number of parameters exceeds 13B
7. [16].
8. Tasks to test arithmetic reasoning are trivial for humans and designed to challenge the capability of transferring natural language into mathematical symbols and multi-step inference.
9. On GSM8k
10. [26], SVAMP
11. [79] and AQuA
12. [61], LLMs, as generalists, have competitive performance with most methods which have task-specific designs.
13. And GPT-4 overperforms any other methods [76], even some huge models particularly tuned for arithmetic problems [104].
14. Nevertheless, it should be noted that, without the intervention of external tools, LLMs may occasionally make mistakes in performing basic calculations, although chain-of-thought (CoT) prompting [115] can significantly improve LLMs' ability in calculations.
15. Commonsense reasoning.
16. Commonsense reasoning not only requires LLMs to remember factual knowledge but also requires LLMs to do several inference steps about the facts.
17. Commonsense reasoning increases gradually with the growth of model size.
18. Compared to fine-tuned models, LLMs keep the superiority on most datasets, such as StrategyQA
19. [36] and ARC-C
20. [25].
21. Especially on ARC-C, which contains difficult questions in science exams from grade 3 to grade 9,GPT-4 has been close to the performance of 100% (96.3%)
22. [76].","1. What is the impact of increasing model size on the arithmetic and commonsense reasoning capabilities of Large Language Models?
2. How does GPT-4's performance compare to other methods?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s22),"What are emergent abilities in large language models (LLMs), and can you provide examples of these abilities as the model scales up?

1. Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule.
2. These abilities are called ""emergent ability"".
3. As defined in [113], emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models.
4. This means such abilities cannot be predicted by extrapolating the performance improvements on smaller-scale models and the model suddenly gains good performance on some tasks once the scale exceeds a certain range.
5. The emergent ability is typically unpredictable and surprising, leading to tasks that emerge randomly or unexpectedly.
6. We examine concrete examples of the emergent abilities of LLMs and provide them as an important reference for deciding whether to leverage LLMs' emergent abilities.
7. Handling word manipulation is a typical emergent ability.
8. It refers to the ability to learn symbolic manipulations, such as the reversed words [16], in which the model is given a word spelled backwards, and must output the original word.
9. For example.
10. GPT-3
11. [16] shows the emergent ability for word sorting, and word unscrambling tasks.
12. PaLM [22] exhibits the emergent ability on ASCII word recognition 4 and hyperbaton 5 task.
13. The logical abilities of language models tend to emerge as the model scales up, such as logical deduction, logical sequence, and logic grid puzzles.
14. Additionally, other tasks, such as advanced coding (e.g., auto debugging, code line description), and concept understanding (e.g., novel concepts, simple Turing concepts), are also use cases with the emergent abilities of large language models.","1. What are emergent abilities in large language models (LLMs)?
2. Can you provide examples of these abilities as the model scales up?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s23),"What are the inverse scaling and U-shaped phenomena in LLMs, and why is understanding these crucial for selecting model sizes in specific tasks?

1. Although in most cases, as discussed above, larger models bring better performance, there are still many exceptions that should be considered when choosing the appropriate model.
2. On certain tasks, with the size of LLMs increasing, the performance begins to decrease, such as Redefine-math: tests whether language models are able to work with common symbols when they are redefined to mean something else; Intothe-unknown: requires the model to choose which piece of information would help answer a question; Memo-trap: asks an LM to write a phrase in a way that starts like a famous quote but ends differently 6 .
3. This is also called Inverse Scaling Phenomenon.
4. Another interesting phenomenon observed in the scaling of LLMs is called the U-shaped Phenomenon
5. [114].
6. As the name implies, This phenomenon refers to that as LLM size increases, their performance on certain tasks initially improves but then starts to decline before eventually improving again, such as on: Hindsight-neglect: it tests whether language models are able to assess whether a bet was worth taking based on its expected value; NegationQA: this task takes an existing multiple-choice dataset and negates a part of each question to see if language models are sensitive to negation; Quote-repetition: it asks models to repeat back sentences given in the prompt, with few-shot examples to help it recognize the task.
7. Hence the risk of diminishing performance should be noted and if the task is similar to those we just discussed, careful consideration should be given to whether or not to use huge LLMs.
8. Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field.
9. In a certain sense, the U-shape phenomenon suggests that small-scale models and huge-scale models make predictions with different internal mechanisms.
10. From this perspective, the U-shape phenomenon can be seen as a transformation of the inverse-scaling phenomenon due to some emergent abilities from sufficiently large models [114].
11. GPT-4
12. [76] exhibits a reversal of the inverse scaling phenomenon in some cases, such as on a task called Hindsight Neglect.
13. The explanation for these behaviors of LLMs during scaling is still an open problem.
14. Several hypotheses have been proposed.
15. For emergent abilities, one explanation is that there may be multiple key steps for a task and the LLM cannot handle this task until it's large enough to handle every step, and another explanation is focused on the granularity of evaluation metrics [113].
16. For inverse-scaling phenomenon and u-shape phenomenon, the explanations mainly focus on the model's over-reliance on information from its prior rather than the input prompts, valid but misleading few-shot examples, and distracting easier tasks within a hard task [114].","1. What are the inverse scaling and U-shaped phenomena in LLMs?
2. Why is understanding these crucial for selecting model sizes in specific tasks?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s26),"Why do Large Language Models (LLMs) like ChatGPT struggle with regression tasks compared to models like RoBERTa, and how does this relate to their training objectives?

1. LLMs generally struggle with some tasks due to differences in objectives and training data.
2. Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive.
3. For example, ChatGPT's performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance [130].
4. The Regression tasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs.
5. One primary reason for their subpar performance is the inherent difference between the language modeling objective and the regression task objective.
6. LLMs are designed to predict the next word in a sequence or generate coherent text, with their pre-training focused on capturing linguistic patterns and relationships.
7. Consequently, their internal representations may not be well-suited for modeling continuous numerical outputs.
8. Besides, LLMs have predominantly been trained on text data, focusing on capturing the intricacies of natural language processing.
9. As a result, their performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored.
10. And fine-tuned multimodal models, like BEiT
11. [110] and PaLI
12. [19], still dominate many tasks such as visual question answering (VQA) and image captioning.
13. Nonetheless, the recently introduced GPT-4
14. [76] has taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities.","1. Why do Large Language Models (LLMs) like ChatGPT struggle with regression tasks?
2. How does this struggle compare to models like RoBERTa?
3. How does this relate to their training objectives?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s27),"What roles do large language models (LLMs) play in mimicking human interaction, data annotation, NLG task quality assessment, and how do their capabilities contribute to performance improvement and interpretability?

1. LLMs are particularly suitable for certain tasks.
2. LLMs are very good at mimicking humans, acting as a chatbot, and performing various kinds of tasks.
3. The LLMspowered ChatGPT 7 is surprising for its consistency, reliability, informativeness, and robustness during multiple utterances with humans.
4. The human-feedback procedure plays an important role in acquiring such abilities LLMs can both act as a good annotator and data generator for data augmentation, such as in [27,29,99,121,122].
5. Some LLMs have been found as good as human annotators
6. [37] in some tasks.
7. And the collected texts from GPT-3.5 (text-davinci-003) have been used as human-like instruction-following demonstrations to train other language models [100].
8. LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation.
9. On summarization tasks, GPT-4 as an evaluator achieves a higher correlation with humans than other methods with a large margin [64].
10. Some other evaluators based on LLMs [34,50,64,108] also show good human alignment in more NLG tasks, especially compared with traditional automatic metrics.
11. But the LLM evaluator may have a bias towards the LLM-generated texts [64].
12. Also, as we discussed above, some abilities of LLMs bring bonuses in addition to performance improvement, such as interpretability.
13. The CoT reasoning ability of LLMs can show how an LLM reaches the prediction, which is a good interpretation on the instance level, while it also improves the performance.","1. What roles do large language models (LLMs) play in mimicking human interaction?
2. What roles do LLMs play in data annotation?
3. What roles do LLMs play in NLG task quality assessment?
4. How do their capabilities contribute to performance improvement?
5. How do their capabilities contribute to interpretability?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s28),"What challenges do models face when applied to real-world tasks as opposed to the structured environment of academia?

1. In the last part of this section, we would like to discuss the usage of LLMs and fine-tuned models in real-world ""tasks"".
2. We use the term ""tasks"" loosely, as real-world scenarios often lack well-formatted definitions like those found in academia.
3. Many requests to models even cannot be treated as NLP tasks.
4. Models face challenges in the real world from three perspectives:• Noisy/Unstructured input.
5. Real-world input comes from real-world non-experts.
6. They have little knowledge about how to interact with the model or even cannot use texts fluently.
7. As a result, real-world input data can be messy, containing typos, colloquialisms, and mixed languages, unlike those well-formed data used for pre-training or fine-tuning.
8. •
9. Tasks not formalized by academia.
10. In real-world scenarios, tasks are often ill-defined by academia and much more diverse than those in academic settings.
11. Users frequently present queries or requests that do not fall neatly into predefined categories, and sometimes multiple tasks are in a single query.
12. • Following users' instructions.
13. A user's request may contain multiple implicit intents (e.g. specific requirement to output format), or their desired predictions may be unclear without follow-up questions.
14. Models need to understand user intents and provide outputs that align with those intents.
15. Essentially, these challenges in the real world come from that users' requests deviate significantly from the distribution of any NLP datasets designed for specific tasks.
16. Public NLP datasets are not reflective of how the models are used [77].","1. What challenges do models face when applied to real-world tasks?
2. How do these challenges differ from those in the structured environment of academia?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s29),"Why are large language models (LLMs) considered more suitable for managing real-world scenarios compared to fine-tuned models?

1. LLMs are better suited to handle real-world scenarios compared to fine-tuned models.
2. However, evaluating the effectiveness of models in the real world is still an open problem.
3. Handling such real-world scenarios requires coping with ambiguity, understanding context, and handling noisy input.
4. Compared to fine-tuned models, LLMs are better equipped for this because they have been trained on diverse data sets that encompass various writing styles, languages, and domains.
5. Additionally, LLMs demonstrate a strong ability to generate open-domain responses, making them well-suited for these scenarios.
6. Fine-tuned models, on the other hand, are often tailored to specific, well-defined tasks and may struggle to adapt to new or unexpected user requests.
7. They heavily rely on clear objectives and well-formed training data that specify the types of instructions the models should learn to follow.
8. Fine-tuned models may struggle with noisy input due to their narrower focus on specific distributions and structured data.
9. An additional system is often required as an assistant for fine-tuned models to process unstructured context, determine possible intents, and refine model responses accordingly.
10. Additionally, some mechanics such as instruction tuning [91,112] and human alignment tuning [77] further boost the capabilities of LLMs to better comprehend and follow user instructions.
11. These methods improve the model's ability to generate helpful, harmless, and honest responses while maintaining coherence and consistency [77,91,112].
12. While both methods can make LLMs better generalize to unseen tasks and instructions, it has been noticed that while human labelers prefer models tuned for human alignment [77] to models tuned with instructions from public NLP tasks, such as FLAN [112] and T0 [91].
13. The reason may be similar to reasons for fine-tuned models' inferiority: public NLP tasks/datasets are designed for easy and automatic evaluation, and they can only cover a small part of real-world usage.
14. One of the main issues when it comes to real-world scenarios is how to evaluate whether the model is good or not.
15. Without any formalized tasks or metrics, the evaluation of model effectiveness can only rely on feedback from human labelers.
16. Considering the complexity and cost of human evaluation, there's no massive and systematic comparison between fine-tuned models and LLMs yet.
17. Nevertheless, the huge success and popularity of LLMs such as chatGPT, have confirmed the superiority of LLMs to some extent.","1. Why are large language models (LLMs) considered more suitable for managing real-world scenarios?
2. How does this suitability compare to fine-tuned models?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s31),"What are the considerations for choosing between light, local models and Large Language Models regarding cost, latency, and safety concerns?

1. (1) Light, local, fine-tuned models should be considered rather than LLMs, especially for those who are sensitive to the cost or have strict latency requirements.
2. Parameter-Efficient tuning can be a viable option for model deployment and delivery.
3. (2) The zero-shot approach of LLMs prohibits the learning of shortcuts from task-specific datasets, which is prevalent in fine-tuned models.
4. Nevertheless, LLMs still demonstrate a degree of shortcut learning issues.
5. (3) Safety concerns associated with LLMs should be given utmost importance as the potentially harmful or biased outputs, and hallucinations from LLMs can result in severe consequences.
6. Some methods such as human feedback have shown promise in mitigating these problems.","1. What are the considerations for choosing between light, local models and Large Language Models?
2. What factors related to cost are important in this decision?
3. What factors related to latency should be considered?
4. What safety concerns might influence the choice?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s32),"What challenges and solutions are associated with the cost, latency, and efficiency of training large language models (LLMs), including the role of Parameter-Efficient Tuning?

1. In real-world deployment, performance, cost, and latency are all important considerations, not just the performance of the models.
2. While some parameter-efficient methods have been developed, practitioners must balance efficiency with effectiveness in the practice.
3. Cost.
4. LLMs have grown increasingly larger in recent years, with models such as GPT-1, GPT-2, and GPT-3 featuring 117 million, 1.5 billion, and 175 billion parameters, respectively.
5. The cost of training an LLM is heavily influenced by its size, with estimates suggesting that training the 11B parameter variant of T5 costs well over $1.3 million for a single run, while a single training run of GPT-3 175B requires $4.6 million [3].
6. The energy consumption for training large models is equally impressive.
7. The total energy consumption for training a transformer model with 6B parameters to completion is estimated to be around 103.5 MWh
8. [30].
9. Google reports that training PaLM consumed about 3.4 GWh in about two months [6].
10. Furthermore, the dataset size also scales rapidly with the size of the model, with GPT-3 175B trained on 499 billion tokens [16].
11. Another key metric that reflects the computing cost is Flops, with GPT-3 175B requiring 3.14 × 10 23Flops, while a T5 11B model only requires 3.30 × 10 22 , which is 10 times less.
12. In addition to these costs, hardware requirements are also substantial.
13. OpenAI has collaborated with Microsoft on a supercomputer hosted in the Microsoft Azure cloud, consisting of 285k CPU cores and 10k high-end GPUs to support the training of large models.
14. For users of the OpenAI API, pricing varies based on the model and usage, with options such as GPT-3.5-turbo charging $0.002 per 1k tokens for chat service.
15. However, for users who require custom models, training costs $0.03 per 1k tokens, while usage costs $0.12 per 1k tokens
16. [4].
17. Therefore, for users who cannot afford such a large cost, such as small startups, individual users, etc., a small, fine-tuned model is a better and more reasonable choice.Latency.
18. Latency is a crucial factor to consider in real-world applications of LLMs.
19. Inference time is a commonly used metric to measure latency, which is highly dependent on the model size, architecture, and token size.
20. For instance, the inference time for the GPT-J 6B model is 0.077s, 0.203s, and 0.707s when the max token size is set to 2, 8, and 32, respectively.
21. Additionally, when the max token size is fixed at 32, the inference time for the InstructGPT model (davinci v2) is 1.969s.
22. As LLMs are often too large to be run on a single user's machine, companies provide LLM services via APIs.
23. The API latency can vary depending on the user's location, and the average latency of the OpenAI API service for a single request can range from a few hundred milliseconds to several seconds.
24. In scenarios where high latency is not acceptable, large LLMs may not be appropriate.
25. For example, scalability is critical in many information retrieval applications.
26. To deploy information retrieval systems on the web, search engines require very efficient inference for systems to be useful.
27. The idealized denoised inference time for the InstructGPT davinci v2 (175B*) model is 0.21s per request (i.e., a query-passage pair to be scored), which is too slow for web search engines.
28. Parameter-Efficient Tuning.
29. In practice, we may tune the model on some specific datasets.
30. Parameter-Efficient Tuning (PET) is an efficient technique to tune a small portation of model parameters (or extra parameters) while freezing most parameters of the pre-trained LLMs.
31. The main goal of PEFT is to greatly decrease the computational and storage costs while keeping the performance of the original models.
32. The common techniques for PET are LoRA
33. [42]
34. , Prefix Tuning [58], P-Tuning [62,63].
35. As an illustration, the LoRA method maintains the weights of the pre-trained model and incorporates low-rank matrices into every layer of the Transformer architecture.
36. This approach considerably minimizes the number of parameters that require training for subsequent tasks, thereby increasing overall efficiency.
37. Alpaca-LoRA 8 proposes integrating Low-Rank Adaptation (LoRA) into LLaMA-Alpaca, which enables runs LLaMA within hours on a single RTX 4090.
38. All these PFT methods can be helpful either for fine-tuning a model to a specific task or tuning LLMs to meet special requirements like human alignment.","1. What challenges are associated with the cost, latency, and efficiency of training large language models (LLMs)?
2. What solutions are associated with the cost, latency, and efficiency of training large language models (LLMs)?
3. What is the role of Parameter-Efficient Tuning in training large language models (LLMs)?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s33),"How do the issues of robustness, calibration, fairness, bias, and spurious biases in large language models (LLMs) impact their trustworthiness in sensitive applications like healthcare and finance?

1. Given that LLMs are now involved in sensitive areas such as healthcare, finance, and law, it is crucial to ensure that they are trustworthy and capable of producing reliable output.
2. Robustness and Calibration.
3. The accuracy and robustness of the LLMs are shown to have a very strong correlation [59].
4. The models that have high accuracy on the scenario also have good robustness.
5. However, the robustness of the zero-shot becomes worse after being tuned on extra application-specific tasks data [116].
6. This may due to overfitting, which leads to poor generalizability due to the extremely high complexity of the model and the limited training samples from downstream tasks [43].
7. In a similar vein, it has been observed that fine-tuning a model can result in significant miscalibrations, owing to over-parameterization [51].
8. Therefore, fine-tuned models may not be an optimal choice when robustness and calibration are critical considerations.
9. However, human alignment has been found as a potential solution for enhancing model robustness.
10. InstructGPT davinci v2 (175B*) has been shown to outperform other models in terms of robustness.
11. On the other hand, achieving optimal calibration of the model depends on the scenario and adaptation procedure employed.
12. Fairness and Bias.
13. LLMs have been shown to exhibit disparate treatment and impact, perpetuating societal biases and potentially leading to discrimination [10,17].
14. To ensure fairness and equity for all users, it is crucial to address these issues in the development and deployment of NLP models.
15. Disparities in performance between demographic groups can serve as an indicator of fairness problems.
16. LLMs are particularly susceptible to fairness issues, as significant performance disparities have been observed across demographic categories such as dialect, religion, gender, and race
17. [59].
18. However, research has shown that aligning models with human instructions can improve LLM performance regardless of their size, with the InstructGPTmodel (davinci v2) exhibiting smaller performance disparities than other LLMs [23].Spurious Biases.
19. The shortcut learning problem has been observed in various natural language understanding tasks under the pretraining and fine-tuning paradigm, where models heavily rely on spurious correlations between input and labels in the fine-tuning data for prediction [31,35,98].
20. For example, in reading comprehension tasks, fine-tuned models tend to focus on the lexical matching of words between the question and the original passage, neglecting the intended reading comprehension task itself [53].
21. In contrast, large language models are not directly trained on fine-tuned datasets, which makes it less likely for them to learn shortcut features present in the fine-tuned dataset, thereby enhancing the model's generalization capabilities.
22. However, LLMs are not infallible and may exhibit some shortcut learning during in-context learning.
23. For example, recent preliminary studies have begun investigating the robustness of prompt-based methods in large-scale language models [111,129].
24. One such study evaluates the few-shot learning performance of GPT-3 on text classification and information extraction tasks [129].
25. and reveal that the examined LLMs are susceptible to majority label bias and position bias, where they tend to predict answers based on the frequency or position of the answers in the training data.
26. Moreover, these LLMs exhibit common token bias, favoring answers that are prevalent in their pre-training corpus.
27. Recent studies show that this positional bias can be mitigated by selecting proper prompts [68].
28. In summary, while LLMs significantly reduce the shortcut learning problem prevalent in fine-tuned models, they still exhibit some shortcut learning issues and should be approached with caution when deploying them in downstream applications.","1. What are the issues of robustness in large language models (LLMs)?
2. What are the issues of calibration in LLMs?
3. What are the issues of fairness in LLMs?
4. What are the biases present in LLMs?
5. What are spurious biases in LLMs?
6. How do these issues impact their trustworthiness in sensitive applications like healthcare and finance?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s34),"What are the new safety challenges associated with the increasing capabilities of Large Language Models (LLMs) in influencing opinions, and what measures can mitigate these issues?

1. LLMs have demonstrated their extremely strong capabilities in many areas such as reasoning, knowledge retention, and coding.
2. As they become more powerful and human-like, their potential to influence people's opinions and actions in significant ways grows.
3. As a result, some new safety challenges to our society should be considered and have caught lots of attention in recent works
4. [75,76].Hallucinations.
5. The potential for LLMs to ""hallucinate,"" or generate nonsensical or untruthful content, can have significant negative impacts on the quality and reliability of information in various applications.
6. As LLMs become increasingly convincing and believable, users may develop an overreliance on them and trust them to provide accurate information in areas with which they are somewhat familiar.
7. This can be particularly dangerous if the model produces content that is entirely false or misleading, leading to incorrect decisions or actions taken based on that information.
8. Such outcomes can have serious consequences in many domains, such as healthcare, finance, or public policy, where the accuracy and reliability of information are critical.
9. To mitigate these issues, reinforcement learning from human feedback (RLHF) is widely used [75,77] and LLMs themselves have been integrated into the loop
10. [75].
11. Harmful content.
12. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack.
13. The implementation of safeguards to detect and correct those contents can be mitigation [97].
14. These LLMs can also have dual-use potential by providing required illicit information, leading to risks such as the proliferation of weapons [75] and even terrorism attack planning.
15. It is crucial to ensure using these LLMs responsibly, with safeguards in place to prevent harm.
16. Also, in existing work, feedback from humans plays an important role in getting rid of harmful outputs.Privacy.
17. LLMs can face serious security issues.
18. An example is the issue of user privacy.
19. It is reported that Samsung employees were using ChatGPT to process their work when they inadvertently leaked top-secret data, including the source code proper of the new program, internal meeting minutes related to the hardware, etc.
20. The Italian data protection agency declared that OpenAI, the developer of ChatGPT, illicitly gathered personal user data, leading Italy to become the first government to prohibit ChatGPT over privacy concerns [1].","1. What are the new safety challenges associated with the increasing capabilities of Large Language Models (LLMs) in influencing opinions?
2. What measures can mitigate these issues?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Computer Science, Linguistics",(s35),"What are the future challenges in developing large language models for natural language processing as outlined in the conclusion of a scientific study?

1. Recent advances in large language models have been revolutionizing the field of natural language processing.
2. Effectively using LLMs requires understanding their capabilities, and limitations for various NLP tasks.
3. This work presents a practical guide to working with LLMs for downstream NLP tasks.
4. We first discuss prominent models like GPT-style and BERT-style architectures and the factors influencing their performance.
5. We then explore using LLMs for downstream tasks, including knowledge-intensive tasks, NLU, and NLG tasks, as well as providing concrete examples of successes and limitations.
6. This practical guide offers insights into LLMs and best practices for harnessing LLMs across NLP tasks.
7. We hope it would enable researchers and practitioners to leverage their potential and drive innovation in language technologies.
8. In the following, we figure out the future challenges of the LLMs:
9. • Evaluation of proposed models on real-world ""datasets"".
10. While existing deep learning models are primarily evaluated on standard academic datasets, such as ImageNet, which have been milestones in deep learning development.
11. However, the limitations of standard academic datasets can not exactly reflect real-world performance.
12. As models advance, it is crucial to assess them on more diverse, complex, and realistic data that reflect real-world needs.
13. Evaluating models on real-world ""datasets"", in addition to academic ones, will provide a more rigorous test of their capabilities, as well as a better understanding of their effectiveness in real-world applications.
14. This ensures that the models are capable of addressing real-world challenges and delivering practical solutions.
15. • Model Alignment.
16. Ensuring that increasingly powerful and autonomous models align with human values and priorities is essential.
17. Methods must be developed to guarantee that these models behave as intended and do not optimize for undesirable outcomes.
18. It is crucial to integrate alignment techniques from the start of the model development process.
19. Model transparency and interpretability are also important factors for evaluating and ensuring alignment.
20. Additionally, as we look toward the future, an even more daunting challenge looms: aligning superhuman systems.
21. While this task is currently beyond our demands, it is important to consider and prepare for the potential implications of aligning such advanced systems, as they may present unique complexities and ethical concerns [8,15].
22. • Safety Alignment.
23. While discussion of AI existential risks is important, concrete research is needed to guarantee the safe development of advanced AI.
24. This includes techniques for interpretability, scalable oversight and governance, and formal verification of model properties.
25. Safety should be considered not just an add-on but an integral part of the model-building process.
26. • Performance Prediction with Scaling.
27. It is difficult to anticipate how model performance will change as model size and complexity increases dramatically.
28. Developing methods to better predict model performance after scaling up or as new architectures are developed would allow for more efficient use of resources and accelerated progress.
29. Some possibilities include: training a smaller 'seed' model and extrapolating its growth, simulating the effects of increased scale or model tweaks, and benchmarking iterations of the model at different scales to build scaling laws.
30. These could provide insight into the performance of models even before they are built.",1. What are the future challenges in developing large language models for natural language processing as outlined in the conclusion of a scientific study?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s0),"What are the main focuses and contributions of current research on multi-hop Machine Reading Comprehension (MRC) as discussed in the provided paper?

1. Machine reading comprehension (MRC) is one of the most important and long-standing topics in the Natural Language Machine reading comprehension (MRC) is one of the most important and long-standing topics in Natural Language Processing (NLP).
2. MRC provides a way to evaluate an NLP system's capability for natural language understanding.
3. An MRC task, in brief, refers to the ability of a computer to read and understand natural language context and then find the answer to questions about that context.
4. The emergence of large-scale single-document MRC datasets, such as SQuAD [1], CNN/Daily mail
5. [2], has led to increased attention to this topic and different models have been proposed to address the MRC problem, such as (Chen, Bolton, and Manning, 2016)
6. Adriana Trigiani is an Italian American best-selling author of sixteen books, television writer, film director, and entrepreneur based in Greenwich Village, New York City.
7. (2) Based on Trigiani's 2000 best-selling novel of the same name, the story is set in the actual Virginia town of Big Stone Gap circa 1970s.
8. The first attempt to improve the simple single-hop MRC task happened with emerging of some datasets like TriviaQA
9. [15] and NarrativeQA
10. [16].
11. These datasets addressed more challenges by introducing multiple passages per each question and also presenting a more complex kind of questions that couldn't be answered with one single sentence.
12. Although this kind of question was more complex than single-hop questions, they still could be answered by a few nearby sentences within one passage, which means they mostly do not need multi-hop reasoning.
13. They are generally known as the multi-passage or multi-document dataset thatis closer to open-domain Question Answering or retrieving-reading problems, which means models have to focus on retrieving the most related passage and then answer the question based on that passage instead of reasoning over information from multiple passages.
14. HotpotQA
15. [11] and WikiHop
16. [17] can be mentioned as the first and most popular multi-hop datasets which in addition to providing multiple passages per each question, ensure that the question can only be answered by reasoning over disjoint pieces of information across different passages.
17. It has been shown that the models with successful results in single-hop MRC datasets have limited success on these datasets [18].
18. Recently, a lot of studies have been done in the field of multi-hop MRC, they focus on different aspects of the task.
19. One of the most basic aspects is to propose a model for solving the multi-hop MRC problem, which has received great attention in recent years.
20. Due to the importance of this task, and also the high speed of presenting new models, it is necessary to present a comprehensive investigation of current models.
21. It would clarify the advantages and disadvantages of existing solutions and help improve future models.
22. To have an accurate view of the growing trend of multi-hop MRC, Figure 2 has been prepared.
23. In 2017, several datasets were introduced (like TriviaQA
24. [15]) that are not considered multi-hop datasets, but they attracted attention to this task by proposing more complicated questions than single-hop questions.
25. Although proposing multi-hop models has been beginning from 2018, there was a serious shortage of multi-hop datasets, so in 2018 some MRC datasets were been proposed with a main focus on the multihop challenges.
26. These datasets made a proper situation to present the multi-hop MRC models, and as you can see a significant number of multi-hop models were been proposed in 2019.
27. The trend of proposing new multi-hop datasets and models has been continued in 2020.
28. It can be concluded from Figure 2 that after proposing each new dataset, some models will be proposed as well to address the new challenges of that dataset, as you can see the number of models in 2019 because of the large number of datasets in 2018.
29. Besides, the small number of researches in 2022 is not due to the lack of attention to this field, but it is because the studies in 2022 have not yet been fully published.
30. There are some review papers on the MRC task that we will mention here to explain the difference and the innovation aspect of our paper.
31. Liu et al.
32. [19] reviewed 85 studies from 2015 to 2018 with a focus on neural network solutions for the MRC problem to investigate the neural network methods in the MRC task.
33. Baradaran, Ghiasi, and Amirkhani
34. [20] presented a survey on the MRC field based on 124 reviewed papers from 2016 to 2018 with a focus on presenting a comprehensive survey on different aspects of machine reading comprehension systems, including their approaches, structures, input/outputs, and research novelties.
35. Thayaparan, Valentino, and Freitas
36. [20] proposed a systematic review of explainable MRC, from 2014 to 2020 with a focus on the explainable feature of the recent MRC methods.
37. Zhang, Zhao, and Wang
38. [22] presented a survey on the role of contextualized language models (CLMs) on MRC from 2015 to 2019.
39. Bai and Wang
40. [23] presented a survey on textual question answering with a focus on datasets and metrics, they investigate 47 datasets and 8 metrics.
41. Although the above studies investigate different aspects of MRC/QA, none of them have focused on the multi-hop challenges.
42. Due to the importance and increasing attention to multi-hop MRC it is necessary to investigate the multi-hop MRC studies separately.
43. Our contribution in this paper is to focus on the multi-hop MRC models and techniques.
44. In this regard, we first proposed the problem definition of the multi-hop MRC task, then categorize 31 models from 2018 to 2022 based on their main techniques and also investigate each model in detail.
45. Also, a comprehensive comparison of the models and techniques will be presented.
46. Finally, open issues in this field have been discussed.
47. It is important to note that since there is a close relationship between MRC and Question Answering, most of the existing machine reading comprehension tasks are in the form of textual question answering [24], also MRC is known as a basic task of textual question answering [19].
48. Thus, we consider cloze domain textual question answering as a typical MRC task in this paper.
49. Our resources were Google, Google Scholar, IEEE Explore, and Elsevier.
50. We searched for papers with these keywords: ""multihop machine reading comprehension"", ""multi-hop question answering"", ""reading comprehension over multiple documents"", ""reading comprehension over multiple passages"", ""question answering over multiple documents"", and ""question answering over multiple passages"".
51. The rest of this paper is organized as follows:
52. In Section 2, the definition of the multi-hop MRC problem is explained.
53. In section 3, besides categorizing 31 models based on their main techniques, each model will be reviewed in detail with a focus on its main idea and superiority.
54. Next, a comprehensive comparison of the models and techniques will be done by presenting some figures and tables in section 4.
55. Then, open issues are expressed in Section 5, and finally, Section 6 concludes the paper.","1. What are the main focuses of current research on multi-hop Machine Reading Comprehension (MRC)?
2. What are the contributions of current research on multi-hop Machine Reading Comprehension (MRC) as discussed in the provided paper?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s6),"What are the three main categories of techniques used in multi-hop MRC, and what does each aim to address in the comprehension process?

1. In this paper, 31 studies have been investigated, which propose a model for multi-hop MRC based on the presented problem in section 2.
2. It is important to note that since there is a close relationship between MRC and Question Answering, most of the existing machine reading comprehension tasks are in the form of textual question answering [24], and also MRC is known as a basic task of textual question answering [19].
3. Thus, we consider cloze domain textual question answering as a typical MRC task in this paper .
4. Existing studies for multi-hop MRC can be mainly divided into three categories: decomposition, recurrent reasoning based on memory retrieval, and multi-step reasoning based on graph neural networks.
5. For each category, after explaining the main technique, the multi-hop MRC models will be reviewed in detail; beside reviewing the detail architectures of each model, we also focus on the superiority and the motivation of them.
6. Also, the disadvantages of each technique will be discussed at the end.
7. In the next section (4) a comprehensive comparison of the techniques and models will be presented.
8. The techniques do not have a specific order, because all three techniques have been used by models from 2018 to 2022 ( Figure   40), but as much as possible, the studies within the techniques have been according to the order of published time.","1. What are the three main categories of techniques used in multi-hop MRC?
2. What does each aim to address in the comprehension process?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s7),"How does the decomposition technique simplify the process of answering complex multi-hop MRC questions according to the paper?

1. Complicated question is a basic challenge of multi-hop MRC, unlike the single-hop questions, they cannot be answered easily and require complicated reasoning.
2. Since the human reasoning about complex questions is done by decomposition, answering subquestions, summarizing, and comparing [26], then this technique has focused on simplifying the problem by decomposition of a complex question into multiple simple sub-questions.
3. It means it reduces multi-hop MRC to multiple single-hop MRC.
4. This technique mostly uses the single-hop MRC models to find the answers to sub-questions and then combine the answers to obtain the final answer.
5. In the following, the models which use this technique for multi-hop MRC will be reviewed in detail.
6. Self-assembling MNM: Jiang and Bansal
7. [18] focused on identifying the sub-questions in the correct reasoning order and presented an interpretable and controller-based self-assembling neural modular network for the multi-hop reasoning process which includes two main parts, Modular Network with a Controller (top) and the Dynamically-assembled Modular Network (bottom) that can be seen in Figure 10.
8. The main idea of the model to handle multi-hop questions is done with Controller that computes an attention distribution over all question words at every reasoning step, which finds the sub-question that should be answered at the current step.
9. In summary, Controller reads the question and predicts a series of modules that could be executed in order to answer the given question.
10. Each module deals with a single-hop sub-question, then they will be chained together according to the predicated order by controller to get the final answer.
11. The mentioned modules are described as follows: All modules take the question representation , context representation ℎ, and sub-question vector as input.",1. How does the decomposition technique simplify the process of answering complex multi-hop MRC questions according to the paper?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s11),"How do recurrent reasoning-based techniques enhance multi-hop MRC tasks using state representations and advanced neural concepts?

1. The sequence models have been first used for single-hop MRC tasks, and most of them are based on Recurrent Neural Networks (RNNs), some studies focus on using them in multi-hop MRC.
2. It can be called state-based reasoning models and they are closer to a standard attention-based RC model with an additional ""state"" representation that is iteratively updated.
3. The changing state representation results in the model focusing on different parts of the passage during each iteration, allowing it to combine information from different parts of the passage [28].
4. Most models, presented in this section, have used advanced neural network concepts, such as attention mechanism and network memory for multi-hop reasoning.
5. In the following the models which use this technique will be reviewed in detail including the architecture alongside the superiority and the motivation of them.","1. How do recurrent reasoning-based techniques enhance multi-hop Machine Reading Comprehension (MRC) tasks?
2. What role do state representations play in this enhancement?
3. How do advanced neural concepts contribute to the process?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s14),"What is the significance of path-based models in multi-hop MRC, and how does the EPAr model utilize a reasoning tree for answer extraction?

1. In the following, we review models that focus on finding the right path in information to find the answer.
2. As multi-hop MRC faces more information and complex questions, finding the right path has become more important and difficult, so many models in this technique are path-based.
3. One of the important advantages of path-based models is that they are interpretable because they can provide the evidence chain to the final answer.
4. EPAr: Jiang et al.
5. [33] proposed an interpretable model named Explore-Propose-Assemble reader (EPAr) to mimic the coarseto-fine-grained reasoning behavior of humans when facing multiple long documents.
6. The main idea is to construct a reasoning tree according to the documents like a hierarchical memory network and use the path in this tree to extract the final answer.
7. This model has three components as shown in Figure 14:•The T-hop Document Explorer (DE) module constructs the reasoning tree like a hierarchical memory network.
8. In each step, it selects one document, updates the memory cell using the selected document, and iteratively selects the next related document.","1. What is the significance of path-based models in multi-hop MRC?
2. How does the EPAr model utilize a reasoning tree for answer extraction?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s15),"How do the Answer Proposer and Evidence Assembler modules function in the context of multi-hop machine reading comprehension to predict final answers?

1. The Answer Proposer (AP) module uses the constructed reasoning tree to predict an answer from every root-to-leaf path.
2. •The Evidence Assembler (EA) module extracts a key sentence containing the proposed answer from every path and combines them to predict the final answer.
3. Figure 14: Architecture of EPAr [33] PathNet:
4. Kundu et al.
5. [34] proposed a path-based reasoning approach for multi-hop MRC which first extracts all paths in the passages based on implicit relations between entities, and then composes the passage representations along each path to compute a passage-based representation.
6. In other words, the passages representation is achieved by considering the paths.
7. They first find all possible path from passages.
8. It starts with selecting a passage that contains a head entity from the question, and then finds all entities and noun phrases from the same sentence.
9. Afterward, it selects the next passage that contains the potential intermediate entity identified above.
10. Finally, it is checked whether the next passage contains any of the candidate answer choices or not.
11. The resulting will be a set of entity sequences.
12. After obtaining all potential paths, it is time to score each path using the PathNet model based on two perspectives: 1) Context-based Path Scoring, which is based on the interaction with the question encoding, and 2) Passage-based Path Scoring, which is based on the interaction between the passage-based path encoding vector and the candidate encoding.
13. There is an example of the process in Figure 15 which In the Rank-1 path, the model composes the implicit located in relations between (Zoo lake, Johannesburg) and (Johannesburg, Gauteng).
14. However, this method extracts many invalid paths, then causes wasting the computing resources [35].Question: (zoo lake, located in the administrative territorial entity, ?)
15. Answer: gauteng Rank-1
16. Path: (zoo lake, Johannesburg, gauteng) Passage1: ...
17. Zoo Lake is a popular lake and public park in Johannesburg, South Africa.
18. It is part of the Hermann Eckstein Park and is ...
19. Passage2: ...
20. Johannesburg (also known as Jozi, Joburg and Egoli) is the largest city in South Africa and is one of the 50 largest urban areas in the world.
21. It is the provincial capital of Gauteng, which is ...","1. How do the Answer Proposer module function in the context of multi-hop machine reading comprehension?
2. How does the Evidence Assembler module function in the context of multi-hop machine reading comprehension?
3. How do these modules collaborate to predict final answers?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s16),"How do recent advancements in multi-hop machine reading comprehension (MRC) models improve the process of knowledge extraction and reasoning from text?

1. (zoo lake, South Africa, gauteng) Passage1: ...
2. Zoo Lake is a popular lake and public park in Johannesburg, South Africa.
3. It is ...
4. Passage2: ...
5. aka The Reef, is a 56-kilometre -long north -facing scarp in the Gauteng Province of South Africa.
6. It consists of a ...
7. [36] focused on the inherent sequential of the multi-hop MRC, which means the system must decide where to look next, based on the current state.
8. Then they proposed a model named Deep Reinforcement Learning-based where the knowledge extraction phase is explicitly decoupled from the question answering phase.
9. This model uses the current information in the knowledge chain to inform which information should be achieved in the next step.
10. The model consists of two main components:1) a Linker to construct a sentence-level chain form the sentences of supporting documents that allow movement between documents, and 2) an Extractor which learns where to look based on the current question and already-visited sentences.
11. As Figure 16 shows, after embedding sentences into a matrix and constructing a graph, the policy is run to select the next sentence.
12. In this regard, the convolution-based policy network is used to learn a traversal policy from current options, previously accepted sentences, and the current question.
13. For example, in this figure, the policy has learned to select the correct next sentence (ct) from the previous sentence (ct-1).
14. After this action, the answer sentence (o1) can be selected during the next step.
15. SMR: Huo, Ge and Zhao
16. [37] proposed a Sentence-level Multi-hop Reasoning approach named SMR while most existing approaches only use document-level or entity-level inferences.
17. In this regard, an initial sentence is first found based on the main entity in the question, and that sentence is then used to find more relevant sentences to create multiple sentence-based reasoning chains as a memory network.
18. Besides, some sentences are concatenated to prevent the mistakes of Co-Reference resolution methods and to reduce the number of hops.
19. An example of such a concatenation is shown in Figure 17.
20. There are two phases in this model: 1) the Selecting phase to select the most relevant sentence to the network memory state as the initial sentence of the current hop, and 2) the Establishing phase to prepare to go to the next hop by updating the network memory state.
21. Finally, the information of the reasoning chains is used to predict the final answer.
22. Figure 17: Representation enrichment via concatenation of adjacent sentences [37] ChainEx: Chen, Lin and Durrett
23. [38] proposed a sentence-based model that does not rely on gold annotated chains or supporting facts at the training and test phases, instead, pseudo-gold reasoning chains are derived using some heuristics based on named entity recognition and coreference resolution during the training time, and it learns to extract chains from raw texts at the test time.
24. They first extract a reasoning chain over the text for a multi-hop reasoning task, and then apply a BERT-based QA system to find the answer by learning from these chains.
25. To construct the reasoning chain, each sentence is considered as a node in the chain, and there is an edge between two sentences if they have the same entity.
26. Besides, there are edges between all sentences from the same paragraph.
27. The model starts from the question and finds all possible reasoning chains.
28. The chain extractor is a neural network with two main components: Sentence Encoding and Chain Prediction.
29. In the sentence encoding component, the BERT-Para model provides a representation from each paragraph jointly with the question.
30. In the chain prediction component, an LSTM-based pointer network is used to extract the reasoning chain.
31. The output of the chain extractor is a variable-length sequence of sentences.
32. Finally, a BERT-based QA system is applied to the extracted chains to find the final answer.
33. Figure 18 show an example with two possible ""reasoning chains"".
34. The first chain is most appropriate, while the second requires a less well-supported inferential leap.
35. Figure 18: A multi-hop example in ChainEx
36. [38] SCR: Huo and Zhao
37. [35] proposed another sentence-level model, since using entity and document makes the chain too coarse and ambiguous or too subtle, limited, and less accurate.
38. This study believes that using sentences as inference nodes is more reasonable than using documents or entities.
39. To explain the problem, you can see an example in Figure 19, as it is clear, with using documents, although the path is more complete, there is a lot of unrelated information, which can produce redundancy.
40. On the other hand, with using entities, although the path is concise it is too subtle and limited, and much information will be lost.
41. However, if the path uses sentences as nodes, it is not limited anymore and with less information redundancy.
42. Besides a sentence-based path can explain the reasoning process better.
43. Then they proposed a Sentence-based Circular Reasoning (SCR) approach, and it consists of three modules: Sentence Encoder (SE) to obtain the sentence representation, Path Generator (PG) to iteratively infer among sentences of multiple documents according to the question, and Path Evaluator (PE) to evaluate the obtained path and predicts an answer ( Figure 20).
44. Unlike the decomposition technique, this technique focuses on multi-hop reasoning instead of simplifying the multi-hop MRC problem and has achieved great attention among studies.
45. The main disadvantage of this technique is that they are expensive and time-consuming approaches, and require a large amount of data to train.","1. How do recent advancements in multi-hop machine reading comprehension (MRC) models improve the process of knowledge extraction?
2. How do they improve the process of reasoning from text?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s17),"What is the main idea behind graph-based techniques in multihop machine reading comprehension (MRC), and what challenges do they address?

1. Graph-based techniques because of their natural language relationship representation ability [39] has attracted attention in multihop MRC.
2. It is natural to model natural language context into graph structure and the process of multi-hop reasoning as moving among nodes [14].
3. The main idea of the Graph-based technique is to construct a graph based on the context and question, and then the reasoning is performed by message passing over this structure using graph neural networks.
4. The process of constructing the graph from large textual data and reasoning over it are challenging tasks.
5. There are a lot of studies that focus on these challenges which are explained in this subsection in detail.
6. Constructing graphs from input data is one of the basic parts of this technique.
7. Some studies construct an entity graph from the input data, which means the nodes of the graph nodes are the entities of the context and question.
8. A lot of studies work on this kind of graph which will be reviewed in the following.","1. What is the main idea behind graph-based techniques in multihop machine reading comprehension (MRC)?
2. What challenges do graph-based techniques address in multihop machine reading comprehension (MRC)?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s19),"How do Song et al. and Xiao et al. contribute to enhancing multi-hop reading comprehension and reasoning through their respective models, focusing on global context inference and dynamic entity graph construction?

1. Song et al.
2. [40] focused on inferring global context as an important key in multi-hop reading comprehension, while previous studies approximate global evidence with local coreference information with DAG-styled GRU.
3. They proposed a model for better connecting global evidence, with a more complex graph compared to DAGs.
4. They construct an entity graph with three types of edges: the edge between the same entity within a passage, the edges between two mentions of different entities within a context window, and coreference-typed edges.
5. The graph might also have cycles which makes it difficult to apply a DAG network to it.
6. (A graph with three types of edges and a DAG graph are shown in Figure 21).
7. For inferring the global context, the related information of the constructed graph has been merged.
8. In this study, two recent graph neural networks have been applied to this graph: graph convolutional network (GCN) and graph recurrent network (GRN) for evidence aggregation.
9. Afterward, an attention mechanism is applied in order to match the hidden states at each graph encoding step with the question representation.
10. Finally, a probability distribution is calculated from the matching results.
11. The architecture of this model is shown in Figure 22.
12. However, this model still only implicitly combines knowledge from all passages, and are therefore unable to provide explicit reasoning paths [28].
13. DFGN:
14. Xiao et al.
15. [41] proposed a model to improve the interaction between the information of documents and the entity graph.
16. They proposed a fusion process of Doc2Graph and Graph2Doc for multi-hop reasoning that leads to a less noisy entity graph and more accurate answers.
17. The process of constructing dynamic entity graph iterates in multiple rounds to achieve multi-hop reasoning.
18. In each round, DFGN generates and reasons on a dynamic graph, where irrelevant entities are masked out while only reasoning sources are preserved, via a mask prediction module.
19. Then the fusion block not only aggregates information from documents to the entity graph (doc2graph) but also propagate the information of the entity graph back to document representations (graph2doc).
20. Figure 23 illustrates the Fusion block in DFGN which consists of:","1. How do Song et al. contribute to enhancing multi-hop reading comprehension and reasoning?
    1.1. What focus does their model have on global context inference?
2. How do Xiao et al. contribute to enhancing multi-hop reading comprehension and reasoning?
    2.1. What focus does their model have on dynamic entity graph construction?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s23),"What advancements and challenges are associated with the latest graph-based models for multi-hop machine reading comprehension (MRC)?

1. The Pairwise Bi-Linear layer to find gold documents accurately.
2. Figure 32: Architecture of the Selection module [47] HGN: Fang et al.
3. [48] presented a method named Hierarchical Graph Network (HGN) which uses different levels of granularity to construct the graph.
4. In this graph, there are four types of nodes: questions, paragraphs, sentences, and entities to cover the different structures of the problem input.
5. Besides, there are seven types of bidirectional edges including edges between the question node and paragraph nodes, edges between the question node and its entity nodes, edges between a paragraph node and its sentence nodes, edges between sentence nodes and their linked paragraph nodes, edges between a sentence node and its entities nodes, edges between paragraph nodes, and edges between sentence nodes from the same paragraph.
6. As you can see in Figure 33 after constructing the hierarchical Graph, a graph-attention-based message passing algorithm is used for reasoning over the graph and finally, the multi-task prediction module is used for paragraph selection, supporting facts prediction, entity prediction, and answer span extraction.
7. However, it uses link information from Wikipedia pages, which makes the graph inflexible for general use [14].
8. Figure 33: Architecture of HGN
9. [48] HGNN: Wang et al.
10. [49] proposed a hierarchical graph neural network with a focus on compositional QA.
11. In this study, the answers are derived from multiple but discontinuous segments in the documents.
12. The nodes can be normal tokens, question tokens, sentence tokens, and special html image tokens.
13. As you can see in figure (left), the input of the BERT sequence encoder is a question , two sentences ( 1 + 2 ) and a special image node ℎ 1 (some special tokens is used to indicate the question <SEP>, sentence <EOS> and special html image element <html>.
14. The hierarchical graph neural networks blocks are shown in Figure 34 (middle).
15. The attention-based Hierarchical Graph Neural Network (HGNN) uses three types of connections: 1) intra-sentence connection which means the connection of words within a sentence, 2) inter-sentence connection which means the connection of common tokens (e.g., sentence tokens or question tokens), and 3) global connection which means the connection between the question tokens and all the words in the document.
16. The final prediction is made based on the sentence nodes and special html nodes.
17. Finally, the connection mask matrix is used to connect the different tokens in the graph and predict the final answer.
18. Figure 34: Architecture of HGNN [49] AMGN:
19. Li et al.
20. [39] proposed a model named Asynchronous Multi-grained Graph Network (AMGN) for multi-hop MRC has been proposed.
21. Previous studies perform message passing synchronously at each step of the graph update and ignore that differentlevel relationships have different priorities and the reasoning has to be done in an ordered logic.
22. First, a multi-grained graph is constructed using the entity and sentence to reflect the relation level of the information.
23. Second, an algorithm for asynchronous message propagation according to the relationship levels (e.g., entity-entity!
24. entity-sentence!
25. sentence-sentence) to update the graph to mimic human multi-hop reading logic is proposed.
26. Besides, a question reformulation mechanism (RNN-based) is proposed to iteratively update the latent question representation with sentence nodes.
27. These sentence nodes are directly used for supporting fact prediction.
28. As it has been shown in Figure 35, the whole model consists of four main components: Paragraph-selector for reducing search space, Encoder to encode the context and question, Construction & Reasoning for multi-grained graph construction and multi-step asynchronous node update, and Multi-task Prediction to predict the final answer.
29. Figure 35: Architecture of AMGN [39] ClueReader:
30. Feng et al.
31. [13] Proposes a model with a heterogenous graph and also based on the grandmother cells concept [9] to imitate the process of human brain.
32. This concept explains that to answer a question, we generally recall a mountain of related evidence whatever the form it is (such as a paragraph, a short sentence, or a phrase), and coordinate theirs inter relationships before we carry out the final results.
33. However, most of the studies on multi-hop MRC cannot gather the semantic features in multi-angle representations, which causes incomplete conclusions.
34. Inspired by the concept of the Grandmother Cells in cognitive neuroscience, a spatial graph attention framework named ClueReader has been proposed.
35. This model is designed to assemble the semantic features in multi-angle representations and automatically concentrate or alleviate the information for reasoning.
36. The name ""ClueReader"" is a metaphor for the pattern of the model: assume the subjects of queries are the start points of clues, the reasoning entities are bridge points, and the latent candidate entities are the grandmother cells.
37. the model has to achieve candidate entities from the clues.
38. As you can see in Figure 36 after constructing a graph from different kinds of nodes, a GAT layer performs the message passing to find the final answer.
39. Figure 36: Architecture of ClueReader [13] IP-LQR: Tang et al.
40. [50] proposed a model named the latent query reformulation method (IP-LQR) to consider the phrase as nodes to construct the graph.
41. As you can see in the example in Figure 37 when ""Soviet statesman"" is used as the reasoning starting point, there are three viable subsequent entities to follow, namely, ""Mikhail Gorbachev"", ""Nikolai Viktorovich Podgorny""' and ""Andrei Pavlovich Kirilenko"".
42. It can be non-trivial to choose from the three candidates to ensure that the reasoning process will eventually lead to the right answer.
43. In contrast, if ""former Soviet statesman"" has been considered as the starting point, it will be much easier to locate the right subsequent entity ""Mikhail Gorbachev"" to update the query.
44. Then they proposed the latent query reformulation method (IP-LQR), which incorporates phrases in the latent query reformulation to improve the cognitive ability of the system.
45. They also design a semantic-augmented fusion method based on the phrase graph, which is then used to propagate the information (Figure 38).
46. After encoding the context and question into the high vector space and acquiring the representations of phrases, sentences, and paragraphs via the mean-pooling layer.
47. Then, a similarity evaluation strategy is designed to calculate the weights of edges in the graph, also the fusion layer is used as an information aggregation to latently update the original question's representation.
48. Finally, a re-attention mechanism is used to help locate the gold answer based on the new representation.
49. Recently, graph-based models have become very popular and recognized as the main solution for multi-hop MRC because of the nature of modeling such a process into graph structure and the good results.
50. But there is some drawback to this technique, the first one is the expensive computational process of the graph-based methods [52], and the second problem is that the graph-based models often can't cover all the inherent structure of documents and loss valuable structural information by modeling documents into graphs [12].","1. What advancements are associated with the latest graph-based models for multi-hop machine reading comprehension (MRC)?
2. What challenges are associated with the latest graph-based models for multi-hop machine reading comprehension (MRC)?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s24),"How do graph-free techniques in multi-hop MRC compare with traditional graph-based methods in terms of necessity and effectiveness?

1. While graph-based methods were being dominated in multi-hop MRC, a question arose that whether the use of a graph is necessary despite the mentioned problem?
2. Several studies addressed this important question, which we named the Graph-free technique because the main aim of them is to prove that graph is not necessary for multi-hop MRC.
3. They may use other techniques to prove this fact (decomposition, recurrent reasoning or any other new techniques).
4. Because of the importance of this question, we will review these studies in this section with a focus on the main idea of them.
5. GF:
6. Shao et al.
7. [53] attempted to answer this question: How much does graph structure contribute to answer a multi-hop question.
8. They reimplement a graph-based model-Dynamically Fused Graph Network
9. [54]-and claimed that the graph structure can play an important role only if the pre-trained models are used in a feature-based manner, while if the pre-trained models are used in the fine-tuning approach, the graph structure may not be helpful.
10. Then the adjacency matrix based on manually defined rules and the graph structure can be regarded as prior knowledge, which could be learned by self-attention or Transformers.
11. They proved that when texts have been modeled as an entity graph, both graph-attention and self-attention can achieve comparable results, but when texts have been modeled as a sequence structure, only a 2-layer Transformer could achieve similar results as DFGN.
12. As you can see in Figure 40 when the entity graph is fully connected, a graph-attention layer will degenerate into a selfattention layer.
13. However, this study as the first attempt of a graph-free model suffers from huge performance gap compared to the state-art-of the graph based-models [14].
14. Figure 40: An example of the relation between the graph-attention network and the self-attention network [53] AMS:
15. Yuntao et al.
16. [52] proposed another non-graph model with a focus on the document filters step to denoise irrelevant documents, and proves that if this step has been done properly, even a single-hop model can be used for multi-hop.
17. They investigate that promising performance of the filter from Hierarchical Graph Network (HGN) (Fang et al., 2020) and showed that for 2paragraph selection, both precision and recall can achieve around 95%, and for 4-paragraph selection, recall will be nearly 99%.
18. Then they proposed Answer Multi-hop questions by Single-hop QA (AMS) models and used a single-hop QA models based on the attention mechanism with the HGN's document filter.
19. As you can see in Figure 41 after the Document denoise layer, they used an Attention-based single-hop layer.
20. AMS could achieve a comparable result with stat-of-the-art graph-bade models but still couldn't improve them.
21. Figure 41: Overall architecture of AMS
22. [52] S2G: Wu, Zhang, and Zhao
23. [14] investigated whether graph modeling is necessary for multi-hop.
24. In this regard, they first proved that the retrieval stage is the most important module, while the existing studies focus on the reader module by graph modeling.
25. This study presents a graph-free alternative named select-to-guide (S2G) to retrieve evidence paragraphs in a coarse-tofine manner, incorporated with two attention mechanisms, which shows conforming to the nature of multi-hop reasoning.
26. For the paragraph retrieval module, this study introduced a cascaded paragraph retrieval module that retrieves the evidence paragraphs in an explicit coarse-to-fine manner, and in multi-task module there are one shared encoder module alongside with two interdependent modules with an attention mechanism ( Figure 42).
27. However Concrete error analysis on S2G shows that there is still room for improvement on the multi-hop retriever module design.
28. Figure 42: The multi-task module of S2G
29. [14] As the last part of this section, Figure 43 has been prepared to summarize the techniques and models.","1. How do graph-free techniques in multi-hop MRC compare with traditional graph-based methods in terms of necessity?
2. How do graph-free techniques in multi-hop MRC compare with traditional graph-based methods in terms of effectiveness?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s27),"What analysis of the popularity trends of graph-based versus recurrent reasoning-based techniques in multi-hop MRC studies from 2018 to 2022 can tell us about their future use?

1. The frequency of each technique among reviewed studies is shown in Figure 44.
2. As you can see, the number of studies of the Graph-based techniques is the most, and after that the Recurrent reasoning-based technique has achieved good attention.
3. But the number of studies cannot be enough to have a proper investigation, and it is needed to show the growth trend of each technique in different years.
4. In this regard, Figure 45 shows the growth trend of each technique from 2018 to 2022.
5. The graph-based technique has achieved the most attention in 2019, 2021, and 2022 that proves that popularity trend of this technique in different years as well.
6. The first graph-free study has been proposed in 2020 and immediately this question was raised that whether the graph was really necessary due to the expensive computational?
7. After that, some other studies followed this question and it can be said that can affect the popularity trend of the graph-based technique in future.
8. However, graph-based technique still can be considered the most popular technique in multi-hop MRC.","1. What analysis can tell us about the popularity trends of graph-based versus recurrent reasoning-based techniques in multi-hop MRC studies from 2018 to 2022?
2. What can those trends indicate about their future use?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s28),"How does analyzing the performance of models using the HotpotQA and Wikihop datasets contribute to multi-hop MRC techniques' improvement?

1. In this section we will show the performance of the models.
2. This investigation is helpful in several ways; it will determine the stat-of-the-result, and also shows which models and techniques has achieved the best result.
3. On the other hand, it can show the overall performance and effectiveness of each technique in multi-hop MRC To evaluate the results of the models we need to use the evaluation metrics of the datasets.
4. HotpotQA
5. [11] and Wikihop
6. [55], are two populare datasets among the reviewed studies as it has been clear in Figure 46 in which shows the percentage of use of two datasets among the reviewed models from 2018 to 2022.
7. Then they provide a proper situation for evaluating the model's performance.",1. How does analyzing the performance of models using the HotpotQA and Wikihop datasets contribute to multi-hop MRC techniques' improvement?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Computer Science, Linguistics",(s30),"What does the WikiHop dataset comprise, and how is accuracy used to evaluate the performance in its tasks?

1. The papers that have used the WikiHop [55] dataset is investigated in this section.
2. WikiHop consists of 51k questions, answers, and context where each context consists of several documents from Wikipedia .Each question in WikiHop is a tuple, which denotes two entities, and their relationship, then the answers in the WikiHop dataset are a single entity.
3. Accuracy is a popular and fairly common metric to evaluate the performance of multiple-choice and Cloze-style MRC tasks.
4. In the multiple-choice task, it is required to check whether the correct answer has been selected from the candidate answers.
5. In contrast, in the Cloze-style task, it is required to check whether the correct words have been selected for the missing wordsSince the answer type of this model is multiple-choice then accuracy is the evaluation metric on this dataset which is obtained for both the test and development set.
6. For each paper, the year of publication, the technique along with the results are shown in Table 3.
7. The best result is for ChainEX
8. [38] which has used the recurrent reasoning technique.
9. Besides that, the graph-based models could achieve the good result in this dataset too.","1. What does the WikiHop dataset comprise?
2. How is accuracy used to evaluate the performance in its tasks?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s0),"How have recent advancements in NLP models addressed the challenges of data scarcity, high computational requirements, and the need for linguistic expertise in annotators?

1. In recent years, data-driven neural models have achieved great success in machine learning problems.
2. In the field of Natural Language Processing (NLP), the introduction of transformers [120] and pre-trained language models such as BERT [25] has lead to a huge leap of the performance in multiple downstream tasks.
3. However, such data-driven models usually require a large amount of labeled training samples, which is often expensive for NLP tasks where linguistic knowledge is expected from annotators.
4. Training deep neural networks on a large dataset also asks for immense computing power as well as huge time and storage budget.
5. To further improve the model performance, combat the data scarcity problem, and facilitate cost-efficient machine learning, researchers have adopted Multi-Task Learning (MTL)
6. [7,150] for NLP tasks.
7. MTL trains machine learning models from multiple related tasks simultaneously or enhances the model for a specific task using auxiliary tasks.
8. Learning from multiple tasks makes it possible for learning models to capture generalized and complementary knowledge from the tasks at hand besides task-specific features.
9. Tasks in MTL can be tasks with assumed relatedness [20,23,40,56,121], tasks with different styles of supervision (e.g., supervised and unsupervised tasks
10. [41,64,73]), tasks with different types of goals (e.g., classification and generation [85]), tasks with different levels of features (e.g., token-level and sentence-level features [57,109]), and even tasks in different modalities (e.g., text and image data [66,115]).
11. Alternatively, we can treat the same task in multiple hierarchical architecture models the hierarchical relationships between tasks.
12. Such architecture can hierarchically combine features from different tasks, take the output of one task as the input of another task, or explicitly model the interaction between tasks.
13. The modular architecture decomposes the whole model into shared components and task-specific components that learn task-invariant and task-specific features, respectively.
14. Different from the above three architectures, the generative adversarial architecture borrows the idea of the generative adversarial network [35] to improve capabilities of existing models.
15. Note that the boundaries between different categories are not always solid and hence a specific model may fit into multiple classes.
16. Still, we believe that this taxonomy could illustrate important ideas behind the design of MTL architectures.
17. Before introducing MTL architectures, we would like to clarify the definitions of hard and soft parameter sharing.
18. In this paper, hard parameter sharing refers to sharing the same model parameters among multiple tasks, and it is the most widely used approach in multi-task learning models.
19. Soft parameter sharing, on the other hand, constrains a distance metric between the intended parameters, such as the Euclidean distance [38] and correlation matrix penalty
20. [41], to force certain parameters of models for different tasks to be similar.
21. Alternatively, [58] adds a regularization term to ensure the outputs of encoders of each task to be close for similar input instances.
22. Differently, some researchers use hard parameter sharing to indicate a multi-task learning model that shares all the hidden layers except the final task-specific output layers and use soft parameter sharing to describe a multi-task model that partially shares hidden layers
23. [22] such as embedding layers and low-level encoders.
24. In this paper, such models fall into the 'parallel architecture' category.","1. How have recent advancements in NLP models addressed the challenge of data scarcity?
2. How have recent advancements in NLP models addressed the challenge of high computational requirements?
3. How have recent advancements in NLP models addressed the need for linguistic expertise in annotators?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s5),"How do hierarchical interactive multi-task learning (MTL) models improve predictions compared to single-pass machine learning models?

1. As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers.
2. In this case, there is no dependency other than layer sharing among tasks.
3. Therefore, there is no constraint on the order of training samples from each task.
4. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks.
5. Fig. 1 illustrates different forms of parallel architectures.
6. 2.1.1 Vanilla Tree-like Architectures.
7. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches).
8. A shallow trunk can be simply the word representation layer
9. [108] while a deep trunk can be the entire model except output layers.
10. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153].
11. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.
12. The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task.
13. A solution is to equip the shared trunk with task-specific encoders [47,58,137].
14. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages.
15. Another way is to make different groups of tasks share different parts of the trunk [37,79,89].
16. Moreover, this idea can be applied to the decoder.
17. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.
18. 2.1.2 Parallel Feature Fusion.
19. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task.
20. As shown in Fig.
21. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations).
22. The green blocks represent shared parameters and the orange blocks are task-specific parameters.
23. Red circles represent feature fusion mechanism .to produce shared representations that can be used as additional features for each task-specific model [69].
24. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].
25. However, different parts of the shared features are not equally important to each task.
26. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks.
27. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate.
28. Similarly, GIRN
29. [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks.
30. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism.
31. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S =
32. [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | ·
33. | compute the absolute value in an element-wise manner.
34. After that, S is concatenated with task-specific representations to form the input to the output layer.
35. Some models directly integrate features from different tasks.
36. A straightforward solution is to compute a weighted average of feature representations.
37. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules.
38. In addition to weighted average, we can aggregate features via more sophisticated mechanisms.
39. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism.
40. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task.
41. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks.
42. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates.
43. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph .
44. The update gate z determines how much information should be maintained from task and then it emits final outputh for task .
45. Mathematically, the two gates can be formulated aswhere (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function.
46. When considering all pairwise directions, the output for each task is given by the sum of each row inTask routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task.
47. Given tasks, the routing network
48. [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight.
49. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task.
50. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .","1. How do hierarchical interactive multi-task learning (MTL) models improve predictions?
2. How does this improvement compare to single-pass machine learning models?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s7),"How do Generative Adversarial Networks (GANs) enhance generative tasks in computer vision and NLP, and what additional benefits do they offer in utilizing unlabeled data?

1. Feature Levels.
2. Models using the parallel architecture handle multiple tasks in parallel.
3. These tasks may concern features at different abstraction levels.
4. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level.
5. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels
6. [20,80,102,109] as illustrated in Fig. 1c.
7. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers.
8. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features.
9. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified.
10. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.
11. In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful.
12. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection
13. [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation
14. [154].
15. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task.
16. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks.
17. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.","1. How do Generative Adversarial Networks (GANs) enhance generative tasks in computer vision and NLP?
2. What additional benefits do GANs offer in utilizing unlabeled data?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s15),"How does joint multi-task learning (MTL) differ from auxiliary MTL, and what are its applications across various fields including natural language processing and sentiment analysis?

1. Interactive Architecture.
2. Different from most machine learning models that give predictions in a single pass, hierarchical interactive MTL explicitly models the interactions between tasks via a multi-turn prediction mechanism which allows a model to refine its predictions over multiple steps with the help of the previous outputs from other tasks in a way similar to recurrent neural networks.
3. [44] maintains a shared latent representation which is updated by iterations.
4. Multi-step attention network
5. [51] refines its prediction by attending to input representations in previous steps.
6. In cyclic MTL
7. [146], the output of one task is used as an extra input to its successive lower-level task and the output of the last task is fed to the first one, forming a loop.
8. Most hierarchical interactive MTL models as introduced above report that performance converges quickly at = 2 steps, showing the benefit and efficiency of doing multi-step prediction.","1. How does joint multi-task learning (MTL) differ from auxiliary MTL?
2. What are its applications across various fields including natural language processing and sentiment analysis?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s16),"How does multi-lingual multi-task learning (MTL) enhance machine learning models for natural language processing, and what are some key methods and aims in its development?

1. The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules.
2. The shared modules learn shared features from multiple tasks.
3. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios.
4. On the other hand, task-specific modules learn features that are specific to a certain task.
5. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.
6. The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1.
7. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did.
8. [1] shares word and character embedding matrices and combines them differently for different tasks.
9. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT).
10. Modular designs are also widely used in multi-lingual tasks.
11. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks.
12. Shared embeddings can be used alongside task-specific embeddings [59,138] as well.
13. In addition to word embeddings, [147] shares label embeddings between tasks.
14. Researchers have also developed modular architectures at a finer granularity.
15. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation.
16. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer.
17. [92] creates encoder modules on different levels, including task level, task group level, and universal level.
18. When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task.
19. While this approach usually attains good performance, it poses heavy computational and storage costs.
20. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model.
21. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters).
22. [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model.
23. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed.
24. In Multiple ADapters for Cross-lingual transfer (MAD-X)
25. [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings.
26. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch.
27. Further, task adaptation modules can also be dynamically generated by a meta-network.
28. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors aswhere L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix.
29. Differently, Conditionally Adaptive MTL (CA-MTL)
30. [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as) is a diagonal block matrix consisting of learnable linear transformations over z .
31. Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism.
32. Similar adaptation operations are used in the input alignment and layer normalization as well.
33. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.","1. How does multi-lingual multi-task learning (MTL) enhance machine learning models for natural language processing?
2. What are some key methods in the development of multi-lingual multi-task learning (MTL)?
3. What are the aims in the development of multi-lingual multi-task learning (MTL)?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s17),"What does multimodal MTL contribute to machine learning, and how does it utilize auditory, visual, and cognitive features for cross-modal tasks?

1. Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision.
2. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones.
3. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images.
4. A similar idea can be used in MTL for NLP tasks.
5. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model.
6. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.
7. An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized.
8. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents.
9. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach.
10. First, a discriminator that rates the quality of candidate answers is trained on labeled samples.
11. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.","1. What does multimodal MTL contribute to machine learning?
2. How does it utilize auditory, visual, and cognitive features for cross-modal tasks?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Computer Science, Linguistics",(s18),"What factors determine the suitability of tasks for multi-task learning (MTL) in natural language processing, and how can MTL performance be enhanced?

1. The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function.
2. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation.
3. Different tasks may use different types of loss functions.
4. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning.
5. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined aswhere L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights.
6. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.
7. In this case, an important question is how to assign a proper weight to each task.
8. The simplest way is to set them equally [91,126,156], i.e., = 1 .
9. As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155].
10. For example, to prevent large datasets from dominating training, [93] sets the weights aswhere |D | denotes the size of the training dataset for task .
11. The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages.
12. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks aswhere measures the variance of the training loss for task .
13. In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label.
14. To ensure that a student model could receive enough supervision during knowledge distillation, BAM!
15. [19] combines the supervised loss L with the distillation loss L aswhere increases linearly from 0 to 1 in the training process.
16. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks.
17. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set aswhere is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller.
18. [85] guides the model to focus on easy tasks by setting weights aswhere denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature.
19. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models.
20. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input.
21. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features.
22. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks.
23. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD).
24. Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores.
25. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously.
26. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex.
27. To address this issue, Tchebycheff loss
28. [75] optimizes an MTL model by an ∞ objective, which is formulated aswhere L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 .
29. The Tchebycheff loss can be combined with adversarial MTL as in [70].
30. Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients.
31. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance.
32. As a remedy, PCGrad [142] directly projects conflicting gradients.
33. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g asBased on the observation that gradient similarity correlates well with language similarity and model performance, GradVac
34. [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients.
35. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g .
36. Notice that PCGrad is a special case of GradVac when = 0.
37. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.","1. What factors determine the suitability of tasks for multi-task learning (MTL) in natural language processing?
2. How can MTL performance be enhanced?"
231603122,Persuasive Natural Language Generation -A Literature Review,"Computer Science, Linguistics, Business",(s0),"How does 'The Social Dilemma' illustrate the impact of social media on society and relate it to broader concepts of persuasion and surveillance capitalism?

1. The movie 'The Social Dilemma' by Jeff Orlowski (2020) explores the rise of social media and the damage it has caused to society.
2. With a rather negative connotation, the directors address the topic of digital platforms and how their users are influenced and persuaded in surveillance capitalism (Economist 2019).
3. Persuasion is an activity that involves one party, the persuader , trying to induce another party, the persuadee , to believe or disbelieve something or to do something (Iyer & Sycara 2019).
4. The Economist (2019) claims that as a central tenet of surveillance capitalism, and persuasion is, furthermore, important in many aspects of daily life.
5. Consider, for example, an employee demanding an increase in compensation, a physician trying to get a patient to enter a slimming programme, a charity volunteer trying to raise funds for a school project (Hunter et al. 2019), or a government advisor trying to get people to take a vaccination in the midst of a pandemic for the greater good.
6. A persuasive Natural Language Generation (NLG) artificial intelligence (AI) is a system that can create communications aimed at a user (the persuadee) in order to persuade her to accept a specific argument through persuasive messages.
7. he persuadee benefits from eating vegetables to improve their health but is also confronted with opposing arguments to erase misunderstandings in the persuader's point of view.
8. To do this, a persuasive NLG AI aims to use convincing arguments in order to persuade the persuadee.
9. With recent advances in natural language processing and its subfield of natural language generation (NLG), it was demonstrated that pretrained language models (e.g., GPT-3) can achieve state-of-the-art results on a wide range of NLP tasks (Economist 2020).
10. Such models allow for writing human-like texts through NLG, and can be fine-tuned for persuasion .
11. In the research of NLP and persuasion, Atalay et al. (2019) focus on syntax and persuasion, while Li et al.
12. (2020) identify persuasion with NLP in online debates or in the news , and Rocklage et al.
13. (2018) identify the relationship between psychological factors (e.g. emotions) and persuasion.
14. In their seminal work, Iyer & Sycara (2019, p. 4) confer that ""working with [subsequent uptake by the persuadee]"" is an additional step.
15. To explore this step, we conducted a structured literature review to identify whether the above research streams (natural language processing & persuasion) may fit in the following research question:","1. How does 'The Social Dilemma' illustrate the impact of social media on society?
2. How does it relate to broader concepts of persuasion?
3. How does it relate to surveillance capitalism?"
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",(s0),"What is the fundamental inspiration behind Artificial Intelligence, and what is essential for creating AI systems that achieve human-level perception abilities?

1. The initial inspiration of Artificial Intelligence (AI) is to imitate human perception, e.g., seeing, hearing, touching, smelling.
2. In general, a modality is often associated with a specific sensor that creates a unique communication channel, such as vision and language [1].
3. In humans, a fundamental mechanism in our sensory perception is the ability to leverage multiple modalities of perception data collectively in order to engage ourselves properly with the world under dynamic unconstrained circumstances, with each modality serving as a distinct information source characterized by different statistical properties.
4. For example, an image gives the visual appearance of an ""elephants playing in water"" scene via thousands of pixels, whilst the corresponding text describes this moment with a sentence using discrete words.
5. Fundamentally, a multimodal AI system needs to ingest, interpret, and reason about multimodal information sources to realize similar human level perception abilities.
6. Multimodal learning (MML) is a general approach to building AI models that can extract and relate information from multimodal data [1].
7. This survey focuses on multimodal learning with Transformers [2] (as demonstrated in Figure 1), inspired by their intrinsic advantages and scalability in modelling different modalities (e.g., language, visual, auditory) and tasks (e.g., language translation, image recognition, speech recognition) with fewer modality-specific architectural assumptions (e.g., translation invariance and local grid attention bias in vision)
8. [3].
9. Concretely, the input to a Transformer could encompass one or multiple sequences of tokens, and each sequence's attribute (e.g., the modality label, the sequential order), naturally allowing for MML without architectural modification [4].
10. Further, learning per-modal specificity and inter-modal •
11. This paper is accepted by IEEE TPAMI.
12. •
13. Peng Xu is with Tsinghua University.
14. Xiatian Zhu is with the University of Surrey.
15. David A. Clifton is with the University of Oxford, UK, and also with Oxford Suzhou Centre for Advanced Research, Suzhou, PRC. •
16. Corresponding author: David A. Clifton correlation can be simply realized by controlling the input pattern of self-attention.
17. Critically, there is a recent surge of research attempts and activities across distinct disciplines exploring the Transformer architectures, resulting in a large number of novel MML methods being developed in recent years, along with significant and diverse advances in various areas [4], [5], [6], [7], [8].
18. This calls for a timely review and summary of representative methods to enable researchers to understand the global picture of the MML field across related disciplines and more importantly to capture a holistic structured picture of current achievements as well as major challenges.
19. Taxonomy For better readability and reachability from and across different disciplines, we adopt a two-tier structured taxonomy based on the application and challenge dimensions respectively.
20. This has several benefits: (1) Researchers with expertise in specific applications can find those applications appropriate to their own research domain before connecting to other related domains.
21. (2) Similar model designs and architectures developed in different domains can be summarized in an abstract, formula-driven perspective so that the mathematical ideas of various models formed in different applications can be correlated and contrasted on common ground, crossing domain-specific restrictions.
22. Crucially, our taxonomy offers an interesting stereo-view of individual works with the insights in both application specificity and formulation generality.
23. It is hoped that this can help to break down domain boundaries and foster more effective idea communication and exchange across modalities.
24. By using the prompt modelling strategy [9], [10] as a basis for investigation, we also include the classical classification problem (e.g., image classification) -usually regarded as a single modality learning application in conventional MML surveys
25. [1], [11], [12] -as a special MML application.
26. This has the potential to significantly enrich MML, as the classification problem is an AI topic amongst the most extensive studies in the literature [13].
27. Scope
28. This survey will discuss the multimodality specific designs of Transformer architecture including, but not limited to, the following modalities: RGB image
29. [5], depth image
30. [14], multispectral image
31. [15], video
32. [7], audio/speech/music [14], [16], [17], table
33. [18], scene graph/layout
34. [19], [20], [21], [22], pose skeleton
35. [23], SQL [24], [25], recipe [26], programming language
36. [27], sign language
37. [28], [29], [30], point cloud
38. [31], symbolic knowledge (graph) [32], [33], multimodal knowledge graph [34], sketch drawing [35], [36], [37], [38], 3D object/scene
39. [39], [40], [41], document
40. [42], [43], programming code
41. [44] and Abstract Syntax Tree (AST) -a kind of graph
42. [45], optical flow
43. [46], medical knowledge (e.g., diagnosis code ontology [47]).
44. Note that this survey will not discuss the multimodal papers where Transformer is used simply as the feature extractor without multimodal designs.","1. What is the fundamental inspiration behind Artificial Intelligence?
2. What is essential for creating AI systems that achieve human-level perception abilities?"
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",(s10),"How do position embeddings contribute to Transformers' ability to understand temporal, spatial, and structural information in data such as text and graphs?

1. We relate this paper to existing surveys of the two specific dimensions MML and Transformers.
2. There exist a few MML surveys
3. [1], [11], [12].
4. In particular, [1] proposed a structured, acknowledged taxonomy by five challenges, which we also adopt as part of our structure.
5. Unlike [1], [11], and [12], which review general machine learning models, we instead focus on Transformer architectures and their self-attention mechanisms.
6. Several surveys dedicated to Transformers have been recently introduced, with a range of emphases including general Transformers [48], efficient designs [49], visualization
7. [50], computer vision tasks [51], [52], [53], [54], medical imaging
8. [55], video tasks [56], and vision language pretraining [57].
9. While [51], [53], [54], [55] consider MML, their reviews are somewhat limited in the scope, taxonomy, and coverage.
10. To our knowledge, only a few surveys on video-language pretraining (VLP)
11. [57], [58], [59] are relevant to MML.
12. However, VLP is only a subdomain of MML.
13. In this survey, we focus solely on the intersection of multimodal learning and Transformers.
14. Features To our knowledge, this paper is the first comprehensive review of the state of Transformer based multimodal machine learning.
15. The major features of this survey include (1) We highlight that Transformers have the advantage that they can work in a modality-agnostic way.
16. Thus, they are compatible with various modalities (and combinations of modalities).
17. To support this view, we, for the first time, offer an understanding of the intrinsic traits of Transformers in a multimodal context from a geometrically topological perspective.
18. We suggest that self-attention be treated as a graph style modelling, which models the input sequence (both uni-modal and multimodal) as a fully-connected graph.
19. Specifically, self-attention models the embedding of arbitrary tokens from an arbitrary modality as a graph node.
20. (2) We discuss the key components of Transformers in a multimodal context as mathematically as possible.
21. (3) Based on Transformers, cross-modal interactions (e.g., fusion, alignment) are essentially processed by self-attention and its variants.
22. In this paper, we extract the mathematical essence and formulations of Transformer based MML practices, from the perspective of self-attention designs.
23. Contributions Having presented our review of the landscape of multimodal learning, Transformer ecosystem, and multimodal big data era in Section 2, we summarize our main contributions as the follows.
24. (1) In Section 3, we present a systematic reviewing of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective.
25. (2) We contribute a taxonomy for Transformer based MML from two complementary perspectives, i.e., application based and challenge based.
26. In Section 4, we provide a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks.
27. In Section 5, we summarize the common challenges and designs shared by the various multimodal Transformer models and applications.
28. (3) In Section 6, we discuss current bottlenecks, existing problems, and potential research directions for Transformer based MML.","1. How do position embeddings contribute to Transformers' ability to understand temporal information in data?
2. How do position embeddings contribute to Transformers' ability to understand spatial information in data?
3. How do position embeddings contribute to Transformers' ability to understand structural information in data such as text and graphs?"
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",(s30),"What are the main challenges in applying multimodal pretraining Transformer methods, especially BERT-style models, to generative tasks according to recent discussions?

1. MML
2. [1], [60], [61] has been an important research area in recent decades; an early multimodal application -audiovisual speech recognition was studied in 1980s
3. [62].
4. MML is key to human societies.
5. The world we humans live in is a multimodal environment, thus both our observations and behaviours are multimodal
6. [63].
7. For instance, an AI navigation robot needs multimodal sensors to perceive the real-world environment [64], [65], [66], e.g., camera, LiDAR, radar, ultrasonic, GNSS, HD Map, odometer.
8. Furthermore, human behaviours, emotions, events, actions, and humour are multimodal, thus various human-centred MML tasks are widely studied, including multimodal emotion recognition [67], multimodal event representation
9. [68], understanding multimodal humor [69], face-body-voice based video person-clustering [70], etc.
10. Thanks to the development of the internet and a wide variety of intelligent devices in recent years, increasing amounts of multimodal data are being transmitted over the internet, thus an increasing number of multimodal application scenarios are emerging.
11. In modern life, we can see various multimodal applications, including commercial services (e.g., e-commerce/commodity retrieval [71], visionand-language navigation (VLN)
12. [72], [73], [74], [75], [76]), communication (e.g., lip reading [77], sign language translation [28], [29]), human-computer interaction
13. [78], healthcare AI
14. [79], [80], surveillance AI
15. [81], etc.
16. Moreover, in the era of Deep Learning, deep neural networks greatly promote the development of MML, and Transformers
17. [2] are a highly competitive architecture family, bringing new challenges and opportunities to MML.
18. In particular, the recent success of large language models and their multimodal derivatives
19. [82], [83], [84], [85], [86] further demonstrates the potential of Transformers in multimodal foundation models.","1. What are the main challenges in applying multimodal pretraining Transformer methods to generative tasks?
2. Specifically, what challenges do BERT-style models face in these applications according to recent discussions?"
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",(s35),"How do MML Transformers achieve fusion across multiple modalities, and what are the implications for multimodal contextual representation learning?

1. Transformers are emerging as promising learners.
2. Vanilla Transformer [2] benefits from a self-attention mechanism, and is a breakthrough model for sequence-specific representation learning that was originally proposed for NLP, achieving the state-of-the-art on various NLP tasks.
3. Following the great success of Vanilla Transformer, a lot of derivative models have been proposed, e.g., BERT
4. [4], BART
5. [87], GPT
6. [88], Longformer
7. [43], Transformer-XL
8. [89], XLNet
9. [90].
10. Transformers currently stand at the dominant position in NLP domains, and this motivates researchers try to apply Transformers to other modalities, such as visual domains.
11. In early attempts for visual domain, the general pipeline is ""CNN features + standard Transformer encoder"", and researchers achieved BERT-style pretraining, via preprocessing raw images by resizing to a low resolution and reshaping into a 1D sequence [91].Vision Transformer (ViT)
12. [5] is a seminal work that contributes an end-to-end solution by applying the encoder of Transformer to images.
13. Both ViT and its variants have been widely applied to various computer vision tasks, including low-level tasks [92], recognition
14. [93], detection
15. [94], segmentation [95], etc, and also work well for both supervised [93] and self-supervised [96], [97], [98] visual learning.
16. Moreover, some recently-released works provide further theoretical understanding for ViT, e.g., its internal representation robustness
17. [99], the continuous behaviour of its latent representation propagation [100], [101].
18. Motivated by the great success of Transformer, VideoBERT [7] is a breakthrough work that is the first work to extend Transformer to the multimodal tasks.
19. VideoBERT demonstrates the great potential of Transformer in multimodal context.
20. Following VideoBERT, a lot of Transformer based multimodal pretraining models (e.g., ViLBERT
21. [102], LXMERT
22. [103], VisualBERT
23. [104], VL-BERT [105], UNITER [106], CBT
24. [107], Unicoder-VL
25. [108], B2T2
26. [109], VLP
27. [110], 12-in-1 [111], Oscar
28. [112], Pixel-BERT [113], ActBERT
29. [114], ImageBERT
30. [115], HERO
31. [116], UniVL
32. [117]) have become research topics of increasing interest in the field of machine learning.
33. In 2021, CLIP [9] was proposed.
34. It is a new milestone that uses multimodal pretraining to convert classification as a retrieval task that enables the pretrained models to tackle zero-shot recognition.
35. Thus, CLIP is a successful practice that makes full use of large-scale multimodal pretraining to enable zero-shot learning.
36. Recently, the idea of CLIP is further studied, e.g., CLIP pretrained model based zero-shot semantic segmentation [118], ALIGN
37. [119], CLIP-TD
38. [120], ALBEF
39. [121], and CoCa [122].","1. How do MML Transformers achieve fusion across multiple modalities?
2. What are the implications for multimodal contextual representation learning?"
249642175,Multimodal Learning with Transformers: A Survey,"Computer Science, Medicine",(s42),"What are the challenges and recent advancements in designing task-agnostic multimodal machine learning (MML) architectures, and how might future research direction be oriented?

1. In the past decade, with the rapid development of internet applications such as social media and online retail, massive multimodal datasets have been proposed, e.g., Conceptual Captions [123], COCO
2. [124], VQA
3. [125],
4. Visual Genome
5. [126], SBU Captions
6. [127], Cooking312K
7. [7], LAIT
8. [115], e-SNLI-VE
9. [128], ARCH
10. [129], Adversarial VQA
11. [130], OTT-QA [18], MULTIMODALQA (MMQA) [131], VALUE [132], Fashion IQ
12. [133], LRS2-BBC
13. [134], ActivityNet
14. [135], VisDial [136].
15. Some emergent new trends among the recently released multimodal datasets are:(1) Data scales are larger.
16. Various recently released datasets are million-scale, e.g., Product1M
17. [137], Conceptual 12M
18. [138], RUC-CAS-WenLan [139] (30M), HowToVQA69M
19. [140], HowTo100M [141], ALT200M [142], LAION-400M
20. [143].
21. (2) More modalities.
22. In addition to the general modalities of vision, text, and audio, further diverse modalities are emerging, e.g., Pano-AVQA
23. [144] -the first large-scale spatial and audio-visual question answering dataset on 360 • videos, YouTube-360 (YT-360)
24. [145] (360 • videos), AIST++ [146] (a new multimodal dataset of 3D dance motion and music),
25. Artemis [147] (affective language for visual arts).
26. In particular, MultiBench [148] provides a dataset including 10 modalities.
27. (3) More scenarios.
28. In addition to common caption and QA datasets, more applications and scenarios have been studied, e.g., CIRR [149] (real-life images), Product1M [137], Bed and Breakfast (BnB)
29. [150] (vision-and-language navigation), M3A
30. [151] (financial dataset), X-World
31. [152] (autonomous drive).
32. (4) Tasks are more difficult.
33. Beyond the straightforward tasks, more abstract multimodal tasks are proposed, e.g., MultiMET
34. [153] (a multimodal dataset for metaphor understanding), Hateful Memes
35. [154] (hate speech in multimodal memes).
36. (5) Instructional videos have become increasingly popular, e.g., cooking video YouCookII
37. [155].
38. Aligning a sequence of instructions to a video of someone carrying out a task is an example of a powerful pretraining pretext task [7], [156].
39. Pretext tasks are pre-designed problems to force the models to learn representation by solving them.
40. Similar to other deep neural network architectures, Transformers are also data hungry.
41. Therefore, their highcapacity models and multimodal big data basis co-create the prosperity of the Transformer based multimodal machine learning.
42. For instance, big data bring zero-shot learning capability to VLP Transformer models.","1. What are the challenges in designing task-agnostic multimodal machine learning (MML) architectures?
2. What are the recent advancements in designing task-agnostic multimodal machine learning (MML) architectures?
3. How might future research direction be oriented?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",(s2),"What are the core components of transformer-based PLMs like BERT and RoBERTa, and how do their embedding layers function?

1. T RANSFORMER
2. [1] based PLMs like BERT
3. [2], RoBERTa
4. [3], T5
5. [4] have started a new era in modern NLP.
6. These models combine the power of transformers, transfer learning, and self-supervised learning.
7. Transformers use self-attention which can be run in parallel and can model long-range relationships with ease.
8. In transfer learning [5], knowledge gained by the model in the source task is transferred to the target task.
9. For example, computer vision models are trained over large labeled datasets, and then these pretrained models are used in similar tasks where the labeled datasets are small [6], [7].
10. The main advantages of pretrained models are a) they learn language representations that are useful across tasks and b) no need to train the downstream models from scratch.
11. However, in NLP, it is quite expensive and difficult to obtain such large, annotated datasets.
12. So, transformerbased PLMs are pretrained over large unlabeled text data using self-supervised learning.
13. Self-supervised learning is in between supervised and unsupervised learning.
14. Supervised learning requires human-annotated instances while unsupervised learning does not require any labeled instances.
15. Self-supervised learning relies on labels like supervised and semi-supervised learning.
16. However, these labels are not human assigned but created automatically by using the relationships between various sections of the input data.
17. Once the model is pre-trained over large volumes of text, it can be used in various downstream tasks by fine-tuning after adding task-specific layers [2].
18. In the initial days, NLP systems are mostly rulebased.
19. The development of rule-based systems is quite difficult as it requires significant human intervention in the form of domain expertise to frame the rules.
20. It is required to reframe the rules with even with a small change in the input data which makes it expensive and laborious.
21. Machine learning systems to some extent brought flexibility in developing NLP systems.
22. Machine learning systems learn the rules during training and thereby avoids the laborious process of manual rule framing.
23. However, the main drawback in machine learning models is the requirement of feature engineering which again requires domain expertise.
24. With the development of various deep learning models like convolutional neural networks (CNNs) and recurrent neural networks (RNNs) which can learn features automatically and better hardware like GPUs, NLP researchers shifted to deep learning models with dense word vectors as input [8], [9].
25. Traditional text representation methods like tf-idf and one-hot vectors are high-dimensional which demand more computational resources.
26. Moreover, these representations are unable to encode syntactic and sematic information.
27. This requirement of low-dimensional text vectors which can also encode language information leads to the development of embedding models like Word2Vec
28. [10], Glove
29. [11].
30. As these models cannot encode sub-word information and suffer from the out of vocabulary (OOV) problem, FastText [12] is proposed.
31. Some of the drawbacks of using CNN or RNN with dense word vectors as input are a) Embeddings models like Word2Vec, Glove, and FastText are based on shallow neural networks.
32. Shallow neural networks with only two or three layers are unable to capture more language information into word vectors.
33. Being context insensitive further limits the quality of these word vectors.
34. b)
35. Even though word embeddings are pre-trained on text corpus, the parameters of models like CNN and RNN are randomly initialized and learned during model training.
36. Learning model parameters from scratch requires a large number of training instances.Self-attention computes the representation of every token in the input based on its interaction with every token in the input.
37. As a result, the self-attention mechanism can better handle long distance word relationships compared to CNN and RNN [1], [13], [14].
38. Moreover, transformers can learn complex language information by applying self-attention layers iteratively i.e., by using a stack of self-attention layers.
39. Transformers with self-attention as the core component have become the primary choice of architecture for pretrained language models in NLP.
40. Transformer-based PLMs like BERT
41. [2], RoBERTa
42. [3], ALBERT
43. [15], T5
44. [4] achieved tremendous success in many of the NLP tasks.
45. These models eliminate the requirement of training a downstream model from scratch.
46. With the success of these models, pretraining the model on large volumes of text and then fine-tuning it on task-specific datasets has become a standard approach in modern NLP.
47. Following the suc-  [16], ClinicalBERT
48. [17], and BlueBERT [18].
49. All these models are obtained by further pretraining general BERT on biomedical texts except ClinicalBERT which is initialized from BioBERT.
50. Lee et al.
51. [16] proposed BioBERT in January 2019
52. and it is the first transformer-based BPLM.
53. After that, number of models are proposed like ClinicalBERT
54. [17], ClinicalXLNet [19], BlueBERT
55. [18], PubMedBERT
56. [20], ouBioBERT [21].
57. Since BioBERT, around 40+ BPLMs are proposed to push the state-of-the-art in various biomedical NLP tasks.
58. Figure 1 summarizes key milestones in transformer-based BPLMs.
59. Transformer-based BPLMs have become the first choice for any task in biomedical NLP.
60. However, there is no survey paper that presents the recent trends in the transformer-based PLMs in biomedical NLP.
61. Currently, there are three survey papers that provide a comprehensive review of embeddings in the biomedical domain and three survey papers that provide a comprehensive review of transformer-based PLMs in the general domain.
62. The survey paper written by Kalyan and Sangeetha [22] is the first comprehensive survey on embeddings in biomedical NLP.
63. This paper a) classify and compare various biomedical corpora b) present a brief overview of various context insensitive embedding models and compare them c) classify and explain various biomedical embeddings d) present solutions to various challenges in biomedical embeddings.
64. The survey papers written by Chiu and Baker [23], Khattak et al.
65. [24] also present the same contents differently.
66. All these three survey papers provide information mostly on context insensitive biomedical embeddings with very little emphasis on transformer-based BPLMs.
67. The paper by Wang et al.
68. [25] provides empirical evaluation of word embeddings trained from various corpora.
69. The survey papers written by Qiu et al.
70. [13], Liu et al.
71. [26] and Kalyan et al.
72. [14] present a review of various transformer-based PLMs in the general domain only.
73. So, we strongly believe there is a need for a survey paper that presents the recent trends related to transformer-based BPLMs (T-BPLMs).
74. Figure  2 summarizes the contents of this survey paper.
75. BioBERT was released in January 2019.
76. So, we gathered articles published in between January 2019 and July 2021.
77. For the literature search, we initially used keywords like ""biomedical pretrained models"", ""clinical pretrained models"", ""BioBERT"", ""PubMedBERT"", ""BlueBERT"", ""ClinicalBERT"", ""transformer-based language models"" and ""in-domain pretrained models"".
78. We iteratively added new keywords from the gathered articles and finally arrived at this list of keywords ""biomedical pretrained models"", ""clinical pretrained models"", ""BioBERT"", ""PubMedBERT"", ""BlueBERT"", ""Clin-icalBERT"", ""transformer-based language models"", ""indomain pretrained models"", ""BioELECTRA"", ""BioAL-BERT"", ""BLUE benchmark"", ""BLURB benchmark"", ""transformers"", ""domain-specific pretrained models"", ""medical language models"", ""multi-modal pretrained models"".
79. Finally, we collected around 4567 articles out of which 1246 articles were duplicate.
80. After excluding the duplicate and irrelevant articles, there were 121 articles.
81. We considered an article as irrelevant based on the following (a) article is not related to natural language processing (321 articles)
82. (b) article is related to natural language processing but not related to biomedical domain (2509 articles) (c) article is related to biomedical domain but the approach is mainly based on context insensitive embeddings models and cited T-BPLMs papers in future work (155 articles).
83. (d) article is related to biomedical domain and approach is based on T-BPLMs but the approach involves mere application of T-BPLMs without much novelty (215 articles).","1. What are the core components of transformer-based PLMs like BERT and RoBERTa?
2. How do their embedding layers function?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",(s19),"How do auxiliary pretraining tasks, like triple classification and multi-similarity Loss, improve in-domain models by using UMLS knowledge?

1. SA is a much better alternative compared to convolution and recurrent layers to encode global contextual information.
2. For a sequence of input tokens, SA updates each input token vector by encoding global contextual information i.e., it expresses each token vector as a weighted sum of all the token vectors where the weights are given by attention scores.
3. The final input representation matrix X is transformed into Query (Q ∈ R n × q ), Key (K ∈ R n × k ) and Value (V ∈ R n × v ) matrices using three weight matrices W Q ∈ R e × q , W K ∈ R e × k
4. andHere h represents the number of self-attention heads.
5. The output of SA layer is computed as 1) Compute similarity matrix ( S ∈ R n × n ) as Q.K T .
6. 2) To obtain stable gradients, scale the similarity matrix values using √ q and then use softmax to convert similarity scores to probability values to get matrix P ∈ R n × n .
7. Formally, P = Sof tmax((Q.K T )/
8. √ q)3) Compute the final weighted values matrix Z ∈ R n × v as P.V","1. How do auxiliary pretraining tasks improve in-domain models?
2. How does triple classification contribute to the improvement?
3. How does multi-similarity Loss contribute to the improvement?
4. How is UMLS knowledge used in these tasks?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",(s25),"What are auxiliary embeddings, and how do they enhance the model's learning ability by providing additional information?

1. With only one self-attention layer, the meaning of a word may largely depend on the same word itself.
2. To avoid this, SA is applied multiple times in parallel each with different weight matrices.
3. Thus, MHSA allows the transformer to attend to multiple positions while encoding a word.
4. Let Z 1 , Z 2 , Z 3 ,..,Z h represent the weighted values matrices of h self-attention heads.
5. Then the final weighted value matrix is obtained by concatenating all these individual weight matrices and then projecting it.","1. What are auxiliary embeddings?
2. How do they enhance the model's learning ability by providing additional information?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",(s31),"What makes biomedical text mining challenging, and how have researchers addressed these challenges with specialized pre-trained language models?

1. Two linear layers with a non-linear activation constitutes the PFN.
2. PFN is applied to every input token vector.
3. Models like BERT uses Gelu
4. [33] activation function.
5. Here the parameters of PFNs applied on each of the token vectors are the same.
6. Formally,","1. What makes biomedical text mining challenging?
2. How have researchers addressed these challenges with specialized pre-trained language models?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",(s35),"What advancements have been made in biomedical NLP tasks using models like BioBERT, and how do newer models like Clinical Kb-BERT improve upon these through further pretraining?

1. Add represents residual connection while Norm represents layer normalization.
2. Add and Norm is applied on both MHSA and PFN of transformer encoder to stay away from vanishing and exploding gradients.
3. In general, a transformed-based PLM consists of a sequence of transformer encoder layers after the embedding layer.
4. Each transformer encoder layer updates the input token vectors by encoding global contextual information.
5. By updating the input token vector using a sequence of transformer encoders help the model to encode more language information.
6. Formally,Here LN represents Layer Normalization,Ê m−1 represents the output after applying Add and Norm over the output of MHSA and E m represents the output after applying Add and Norm over the output of PFN in m th encoder layer.
7. Overall, E m represents the output of m th encoder layer with E m−1 as input.
8. Here the input for the first transformer encoder layer is, E 0 = X.","1. What advancements have been made in biomedical NLP tasks using models like BioBERT?
2. How do newer models like Clinical Kb-BERT improve upon these through further pretraining?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",(s36),"What are ""Green Models"" in the context of adapting Transformer-based Pretrained Language Models for the biomedical domain, and how do they achieve a balance between performance and environmental friendliness?

1. Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade.
2. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions.
3. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances.
4. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data.
5. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples.
6. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].
7. Robotics is the first AI field to use self-supervised learning methods [34].
8. Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38].
9. SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods.
10. SSL is similar to unsupervised learning as it does not depend on human-labeled instances.
11. It is also similar to supervised learning as it learns using supervision.
12. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data.
13. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks.
14. The pseudo labels are generated depending on the definitions of pre-training tasks.
15. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34].
16. In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling).
17. In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization).
18. In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection).
19. For more details about different SSL methods, please refer to the survey paper written by Liu et al.
20. [34].","1. What are ""Green Models"" in the context of adapting Transformer-based Pretrained Language Models for the biomedical domain?
2. How do they achieve a balance between performance and environmental friendliness?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",(s38),"What advancements have been made in developing multi-modal models for specific NLP tasks, particularly in the biomedical domain?

1. Mixed domain pretraining involves training the model using both general and in-domain text.
2. Depending on whether the pretraining is done simultaneously or not, mixed domain pretraining can be classified into a) Continual pretraining -initially the model is pre-trained over general domain text and then adapted to the biomedical domain
3. [16] and b) Simultaneous pretraining -the model is pre-trained over the combined corpora having both general and in-domain text where the indomain text is up sampled to ensure balanced pretraining [21].
4. Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs.
5. It is also referred to as further pretraining.
6. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7).
7. For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles
8. [16].
9. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].
10. Simultaneous Pretraining (SPT) :
11. Continual pretraining achieved good results by adapting general models to the biomedical domain [16], [18], [19], [39], [40].
12. However, it requires large volumes of in-domain text.
13. Otherwise, CPT may result in suboptimal performance.
14. Simultaneous pretraining comes to the rescue when only a small amount of in-domain text is available.
15. Here, the pretraining corpora consist of both in-domain and general domain text where the in-domain text is up sampled to ensure a balanced pretraining (refer Figure  8).
16. For example, BERT (jpCR+jpW)
17. [41] is developed by simultaneous pretraining over a small amount of Japanese clinical text and a large amount of Japanese Wikipedia text.
18. This model outperformed UTH-BERT in clinical text classification.
19. UTH-BERT [42] is trained from scratch over Japanese clinical text.","1. What advancements have been made in developing multi-modal models for specific NLP tasks?
2. How do these advancements specifically apply to the biomedical domain?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",(s49),"What are the challenges and costs associated with adapting general BERT to specific domains like the biomedical field using popular approaches like MDPT and DSPT?

1. The main drawback in continual pretraining is the general domain vocabulary.
2. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2].
3. As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning.
4. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords.
5. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10).
6. For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles
7. [20].
8. PubMed achieved state-of-the-art results in the BLURB benchmark.
9. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes
10. [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11).
11. In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained.
12. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences.","1. What are the challenges associated with adapting general BERT to specific domains like the biomedical field?
2. What are the costs associated with adapting general BERT to specific domains using popular approaches like MDPT and DSPT?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",(s56),"What are two options for representing in-domain words meaningfully in continually pretrained language models adapted to specific domains?

1. During pretraining, the language models learn language representations based on the supervision provided by one or more pretraining tasks.
2. A pretraining task is a pseudo-supervised task whose labels are generated automatically.
3. A pretraining task can be main or auxiliary.
4. The main pretraining tasks allow the model to learn language representations while auxiliary pretraining tasks allow the model to gain knowledge from human-curated sources like Ontology
5. [34], [45]- [47].
6. The classification of pretraining tasks is given in Figure 12 and a brief summary of various pretraining tasks is presented in Table 1.",1. What are two options for representing in-domain words meaningfully in continually pretrained language models adapted to specific domains?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",(s58),"How does the utilization of all final hidden vectors through methods like max-pooling and attention improve final sequence representation in text classification tasks?

1. The main pretraining tasks allow the model to learn language representations.
2. Some of the commonly used main pretraining tasks are masked language modelling (MLM)
3. [2], replaced token detection (RTD)
4. [50], sentence boundary objective (SBO)
5. [49], next sentence prediction (NSP)
6. [2] and sentence order prediction (SOP)
7. [15].
8. Masked Language Modeling (MLM).
9. It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens
10. [2].
11. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens.
12. However, the meaning of a word depends on both the left and right contexts.
13. Devlin et al.
14. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model.
15. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI.
16. Less challenging as it involves topic prediction which is a relatively easy task.","1. How does the utilization of all final hidden vectors improve final sequence representation in text classification tasks?
2. What methods, like max-pooling and attention, are used to improve final sequence representation?
3. How do these methods improve text classification tasks?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",(s62),"What are the main drawbacks of the standard and low-cost domain adaptation approaches in developing Biomedical Language Models (BPLMs), and how might further research address these issues?

1. SOP sentence-level Allows the model to learn sentence-level reasoning skills by modeling intersentence coherence.
2. More challenging compared to NSP as SOP involves only sentence coherence.
3. ALBERT
4. [15] SBO phrase-level Model predicts the masked tokens in a span based on boundary token representations and position embeddings.
5. SpanBERT [49] RTD word-level Model checks every token whether it is replaced or not.
6. More efficient compared to MLM as it involves all the tokens in the input.
7. ELECTRA
8. [50]  {x 1 , x 2 , . . .
9. , x m }, a subset of tokens is randomly chosen and these tokens are replaced.
10. The authors replaced tokens, 80% of the time with a special token '[MASK]', 10% of the time with a random token, and 10% of the time with the same token.
11. This is done to handle the mismatch between pretraining and fine-tuning phases.
12. Formally,wherex is the masked version of x and m(x) represents the set of masked token positions.
13. Some of the improvements like dynamic masking [3], whole word masking [2], [20], [51], whole entity masking [48], [52], and whole span masking [48] are introduced in MLM to further improve its efficiency as a pretraining task.
14. Delvin et al.
15. [2] used static masking to replace the tokens i.e., the input sentences are masked once during pre-processing and the model predicts the same masked tokens in the input sentences for every epoch during pretraining.
16. In the case of dynamic masking [3], different tokens are masked in the input sentence for different epochs which prevents the model from predicting the same masked tokens in every epoch and hence it learns more.
17. Whole word masking is much more challenging as the model has to predict the entire word rather than part of a word.
18. In the case of the whole entity and span maskings, in-domain entities and phrases in the input sentences are identified and then masked rather than masking the randomly chosen tokens.
19. As a result, the model learns entity-centric and in-domain linguistic knowledge during pretraining which enhances the performance of the model in downstream tasks [48], [52].
20. For example, Zhang et al.
21. [48] trained MC-BERT using NSP and MLM with whole entity and span maskings.
22. Michalopoulos et al.
23. [46] used novel multi-label loss-based MLM along with NSP to further pretrain ClinicalBERT on MIMIC-III clinical notes to get UmlsBERT.
24. Novel multilabel lossbased MLM allows the model to connect all the words under the same concept.
25. Sequence-to-Sequence MLM (Seq2SeqLM) is an extension of MLM to models based on encoder-decoder architecture.
26. Models like T5 [4] are pretrained using Seq2SeqLM pretraining task.
27. Replaced Token Detection (RTD)
28. [50].
29. It is a novel pretraining task that involves verifying whether each token in the input is replaced or not.
30. Initially, some of the tokens in the input sentences are replaced with words predicted by a small generator network, and then the model (discriminator) is asked to predict the status of each word as replaced or not.
31. The two advantages of RTD over MLM are a) RTD provides more training signal compared to MLM as RTD involves checking the status of every token in the input rather than a subset of randomly chosen tokens like MLM.
32. and b) Unlike MLM, RTD does not use any special tokens like '[MASK]' to corrupt the input.
33. So, it avoids the mismatch problem that the special token '[MASK]' is seen only during pretraining but not during fine-tuning.
34. Formally,wherex is the corrupted version of x and t = 1 when the token is not a replaced one.
35. Span Boundary Objective (SBO)
36. [49].
37. It is a novel pretraining task that involves predicting the entire masked span based on the context.
38. Initially, a contiguous span of tokens is randomly chosen and masked and then the model is asked to predict the masked tokens in the span based on the token representations at the boundary.
39. In the case of MLM, the model predicts the masked token based on the final hidden vector of the masked token.
40. However, in the case of SBO, the model predicts the masked token in the span based on the final hidden vectors of the boundary tokens and the position embedding of the masked token.
41. SBO is more challenging as it is difficult to predict the entire span ""frequent bathroom runs"" than predicting ""frequent"" when the model already sees ""bathroom runs"".
42. SBO helps the model to achieve better results in span extraction-based tasks like entity extraction and question answering [49], [53].
43. Let s and e represent the start and end indices of the span in the input sequence.
44. Then, each token x i in the span is predicted based on the final hidden vectors of the boundary tokens x s−1 , x e+1 and its position embedding p i−s+1 .
45. Thenwhere y i = g(x s−1 , x e+1 , p i−s+1 ), g() represents feedforward network of two layers and S represents the positions of tokens in contiguous span.
46. Next Sentence Prediction (NSP)
47. [2].
48. NSP is a sentence-level pretraining task that involves predicting whether given two sentences appear consecutively or not .
49. It is basically a two-way sentence pair classification task.
50. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsN ext, IsN otN
51. ext} depending on whether the two sentences are consecutive or not.
52. NSP helps the model to learn sentence-level reasoning skills which are useful in downstream tasks involving sentence pairs like natural language inference, text similarity, and question answering [2].
53. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are positive and the rest negative.
54. Let z represents aggregate vector representation of the sentence pair (x, y).
55. Then,where t = 1 when the two sentences x and y are consecutive.
56. Sentence Order Prediction (SOP)
57. [15].
58. SOP is a novel sentence-level pretraining task which models intersentence coherence.
59. Like NSP, SOP is a two-way sentence pair classification.
60. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsSwapped, IsN otSwapped} depending on whether the sentences are swapped or not.
61. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are swapped and the rest are not swapped.
62. Unlike NSP which involves the prediction of both topic and coherence, SOP involves only sentence coherence prediction [15].
63. Topic prediction is comparatively easier which questions the effectiveness of NSP as a pretraining task [3], [15], [49].
64. Let z represent aggregate vector representation of the sentence pair (x, y).
65. Then,where t = 1 when the two sentences x and y are not swapped.","1. What are the main drawbacks of the standard domain adaptation approaches in developing Biomedical Language Models (BPLMs)?
2. What are the main drawbacks of the low-cost domain adaptation approaches in developing Biomedical Language Models (BPLMs)?
3. How might further research address these issues?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",(s64),"What is the purpose of benchmarks in evaluating NLP models, and how do they differ across general and biomedical domains?

1. Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS
2. [54] into indomain models to further enhance them.
3. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45].
4. This auxiliary task is used by Hao et al.
5. [45] to inject UMLS relation knowledge into in-domain models.
6. Yuan et al.
7. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.
8. Similarly, Liu et al.
9. [34] used multi-similarity loss-based pretraining task to inject UMLS synonym knowledge into PubMedBERT.","1. What is the purpose of benchmarks in evaluating NLP models?
2. How do benchmarks differ across general and biomedical domains?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",(s66),"How does the advancement in efficient models like ConvBERT and DeBERTa aim to address the challenges of pretraining's high computational cost and large data requirements in NLP?

1. IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets.
2. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains.
3. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]-
4. [58].
5. For example, Cengiz et al.
6. [55] fine-tuned indomain model on general NLI datasets like SNLI
7. [59] and MNLI
8. [60] before fine-tuning on MedNLI
9. [61].
10. Same Task Same Domain -Here, the source and target datasets are from the same task and domain.
11. But the source dataset is a more generic one while the target dataset is more specific [62], [63].
12. For example, Gao et al.
13. [63] fine-tuned BlueBERT on a large general biomedical NER corpus like MedMentions
14. [64] or Semantic Medline before fine-tuning on the small target NER corpus.
15. Different Task Same Domain -Here, the source and target datasets are from different tasks but the same domain.
16. Fine-tuning on source dataset which is from the same domain allows the model to gain more domainspecific knowledge which improves the performance of the model on the same domain target task
17. [65].
18. McCreery et al.
19. [65] fine-tuned the model on the medical questionanswer pairs dataset to enhance its performance on the medical question similarity dataset.
20. Different Task Different Domain -Here, the source and target datasets are from different tasks and different domains.
21. For example, Jeong et al.
22. [66] fine-tuned BioBERT on a general MultiNLI dataset to improve the performance of the model in biomedical QA.
23. Here the model learns sentence level reasoning skills which are useful in biomedical QA.","1. How does the advancement in efficient models like ConvBERT and DeBERTa aim to address the challenges of pretraining's high computational cost?
2. How does the advancement in efficient models like ConvBERT and DeBERTa aim to address the challenges of large data requirements in NLP?"
