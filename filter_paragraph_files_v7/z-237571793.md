# Multi-Task Learning in Natural Language Processing: An Overview

CorpusID: 237571793 - [https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a](https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a)

Fields: Linguistics, Computer Science

## (s0) INTRODUCTION
Number of References: 4

(p0.0) In recent years, data-driven neural models have achieved great success in machine learning problems. In the field of Natural Language Processing (NLP), the introduction of transformers [120] and pre-trained language models such as BERT [25] has lead to a huge leap of the performance in multiple downstream tasks. However, such data-driven models usually require a large amount of labeled training samples, which is often expensive for NLP tasks where linguistic knowledge is expected from annotators. Training deep neural networks on a large dataset also asks for immense computing power as well as huge time and storage budget. To further improve the model performance, combat the data scarcity problem, and facilitate cost-efficient machine learning, researchers have adopted Multi-Task Learning (MTL) [7,150] for NLP tasks.
## (s5) Hierarchical
Number of References: 3

(p5.0) Interactive Architecture. Different from most machine learning models that give predictions in a single pass, hierarchical interactive MTL explicitly models the interactions between tasks via a multi-turn prediction mechanism which allows a model to refine its predictions over multiple steps with the help of the previous outputs from other tasks in a way similar to recurrent neural networks. [44] maintains a shared latent representation which is updated by iterations. Multi-step attention network [51] refines its prediction by attending to input representations in previous steps. In cyclic MTL [146], the output of one task is used as an extra input to its successive lower-level task and the output of the last task is fed to the first one, forming a loop. Most hierarchical interactive MTL models as introduced above report that performance converges quickly at = 2 steps, showing the benefit and efficiency of doing multi-step prediction.
## (s7) Generative Adversarial Architectures
Number of References: 7

(p7.0) Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

(p7.1) An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.
## (s11) Task Scheduling
Number of References: 5

(p11.0) Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.
## (s14) Primary
Number of References: 23

(p14.0) Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

(p14.1) [62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.
## (s15) Joint MTL
Number of References: 24

(p15.0) Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

(p15.1) Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type.
## (s16) Multi-lingual MTL
Number of References: 8

(p16.0) Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

(p16.1) Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

(p16.2) Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.
## (s17) Multimodal MTL
Number of References: 2

(p17.0) As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.
## (s18) Task Relatedness in MTL
Number of References: 9

(p18.0) A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

(p18.1) As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.
