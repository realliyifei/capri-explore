# A Survey on Metric Learning for Feature Vectors and Structured Data

CorpusID: 168956 - [https://www.semanticscholar.org/paper/6f0cde3fcab0044f386b5b8a4244c371507bec15](https://www.semanticscholar.org/paper/6f0cde3fcab0044f386b5b8a4244c371507bec15)

Fields: Mathematics, Computer Science

## (s0) Introduction
Number of References: 2

(p0.0) The notion of pairwise metric-used throughout this survey as a generic term for distance, similarity or dissimilarity function-between data points plays an important role in many machine learning, pattern recognition and data mining techniques. 1 For instance, in classification, the k-Nearest Neighbor classifier (Cover and Hart, 1967) uses a metric to identify the nearest neighbors; many clustering algorithms, such as the prominent K-Means (Lloyd, 1982), rely on distance measurements between data points; in information retrieval, doc-uments are often ranked according to their relevance to a given query based on similarity scores. Clearly, the performance of these methods depends on the quality of the metric: as in the saying "birds of a feather flock together", we hope that it identifies as similar (resp. dissimilar) the pairs of instances that are indeed semantically close (resp. different). General-purpose metrics exist (e.g., the Euclidean distance and the cosine similarity for feature vectors or the Levenshtein distance for strings) but they often fail to capture the idiosyncrasies of the data of interest. Improved results are expected when the metric is designed specifically for the task at hand. Since manual tuning is difficult and tedious, a lot of effort has gone into metric learning, the research topic devoted to automatically learning metrics from data.
## (s19) Online Approaches
Number of References: 2

(p19.0) In online learning (Littlestone, 1988), the algorithm receives training instances one at a time and updates at each step the current hypothesis. Although the performance of online algorithms is typically inferior to batch algorithms, they are very useful to tackle large-scale problems that batch methods fail to address due to time and space complexity issues. Online learning methods often come with regret bounds, stating that the accumulated loss suffered along the way is not much worse than that of the best hypothesis chosen in hindsight. 22 Shalev-Shwartz et al., 2004), for Pseudo-metric Online Learning Algorithm, is the first online Mahalanobis distance learning approach and learns the matrix M as well as a threshold b ≥ 1. At each step t, POLA receives a pair
## (s61) Oncina and Sebban
Number of References: 3

(p61.0) The work of Oncina and Sebban (2006) describes three levels of bias induced by the use of generative models: (i) dependence between edit operations, (ii) dependence between the costs and the prior distribution of strings p e (x), and (iii) the fact that to obtain the posterior probability one must divide by the empirical estimate of p e (x). These biases are highlighted by empirical experiments conducted with the method of Ristad and Yianilos (1998). To address these limitations, they propose the use of a conditional transducer as a discriminative model that directly models the posterior probability p(x ′ |x) that an input string x is turned into an output string x ′ using edit operations. 46 Parameter estimation is also done with EM where the maximization step differs from that of Ristad and Yianilos (1998) as shown below:
