# MIXED-PRECISION NEURAL NETWORKS: A SURVEY A PREPRINT

CorpusID: 251554723 - [https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152](https://www.semanticscholar.org/paper/30fa61a9c1db9527987fc25a91f26b9f23aa5152)

Fields: Computer Science

## (s18) ZeroQ: A Novel Zero Shot Quantization Framework (ZeroQ) 3
Number of References: 3

(p18.0) ZeroQ, proposed in [35], is a framework that optimizes, based on a proposed loss function and the batch normalization layers of the full precision model, an engineered distilled dataset to enable per-layer bitwidth allocation for weights and activations via Pareto Frontier without a need to access the training or validation data. The quantization technique is post-training, deterministic rounding.
## (s19) HAWQ-V3: Dyadic Neural Network Quantization 4
Number of References: 3

(p19.0) In [51], the authors propose HAWQ-V3, a hardware-aware fixed low-precision and MXPDNN framework (for weights and activations) with integer-only inference by solving a constrained Integer Linear Programming (ILP) formulation. It relies on uniform quantization (with rounding). Also, channel-wise symmetric quantization is used for weights, layer-wise asymmetric quantization is utilized for activations, and static quantization for all the scaling factors. Results are presented with/without distillation, so this falls in the retraining/post-training category respectively.
## (s20) Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization (MPQNNCO)
Number of References: 3

(p20.0) The authors in [92] formulate the mixed-precision problem (for weights and activations) as a discrete constrained optimization problem which they solved by relying on second-order Taylor expansion, Hessian matrix computations, and a greedy search algorithm. In particular, the optimization problem is reformulated as a Multiple Knapsack problem which is solved by the proposed greedy search algorithm efficiently. This framework relies on deterministic rounding quantization for weights and activations (where the step size is found via solving an optimization problem). Since the framework starts with a pre-trained network and relies on fine-tuning after quantization, the technique is categorized as retraining.
## (s25) APQ: Joint Search for Network Architecture, Pruning and Quantization Policy (APQ) 5
Number of References: 63

(p25.0) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p25.1) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p25.2) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.

(p25.3) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p25.4) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p25.5) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.

(p25.6) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p25.7) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p25.8) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.
## (s27) HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs (HMQ)
Number of References: 6

(p27.0) HMQ [87] is a mixed-precision quantization block that enables an efficient search for quantization parameters by utilizing the Gumbel-Softmax estimator [105]. In this framework, an optimization method based on the HMQ blocks is used to search for the bitwidths (of weights and activations) and the threshold of each quantizer at the same time. Deterministic rounding is used for quantization after finding the optimal bitwidths. Since the optimization technique starts from a pre-trained full precision model then utilizes fine-tuning after quantization, the quantization technique is retraining.
## (s28) Rethinking Differentiable Search for Mixed-Precision Neural Networks (EdMIPS) 6
Number of References: 6

(p28.0) EdMIPS is proposed [37] as an efficient differentiable "mixed-precision network search" (MPS) that aims to find optimal per-layer precision of weights and activations without a proxy task, by solving a constrained optimization problem. The quantization technique is similar to that of HWGQ [82]. As such, the quantization technique is deterministic rounding. Since it trains from scratch, it is categorized as training-aware.
## (s33) AutoQ: Automated Kernel-Wise Neural Network Quantization
Number of References: 6

(p33.0) AutoQ is proposed as an MXPDNN framework that relies on hierarchical deep reinforcement learning (DRL) to automatically and rapidly assign mixed-precision for weights kernels and activations across the layers of the DNN [143]. Starting from a pre-trained network with full precision, AutoQ quantizes the weights (per kernel) and activations (per layer) based on the work done in [29]. The quantization technique used is classified as training-aware, deterministic rounding.
## (s39) Comparison Against Binary Neural Networks
Number of References: 9

(p39.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p39.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s67) ZeroQ: A Novel Zero Shot Quantization Framework (ZeroQ) 3
Number of References: 3

(p67.0) ZeroQ, proposed in [35], is a framework that optimizes, based on a proposed loss function and the batch normalization layers of the full precision model, an engineered distilled dataset to enable per-layer bitwidth allocation for weights and activations via Pareto Frontier without a need to access the training or validation data. The quantization technique is post-training, deterministic rounding.
## (s68) HAWQ-V3: Dyadic Neural Network Quantization 4
Number of References: 3

(p68.0) In [51], the authors propose HAWQ-V3, a hardware-aware fixed low-precision and MXPDNN framework (for weights and activations) with integer-only inference by solving a constrained Integer Linear Programming (ILP) formulation. It relies on uniform quantization (with rounding). Also, channel-wise symmetric quantization is used for weights, layer-wise asymmetric quantization is utilized for activations, and static quantization for all the scaling factors. Results are presented with/without distillation, so this falls in the retraining/post-training category respectively.
## (s69) Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization (MPQNNCO)
Number of References: 3

(p69.0) The authors in [92] formulate the mixed-precision problem (for weights and activations) as a discrete constrained optimization problem which they solved by relying on second-order Taylor expansion, Hessian matrix computations, and a greedy search algorithm. In particular, the optimization problem is reformulated as a Multiple Knapsack problem which is solved by the proposed greedy search algorithm efficiently. This framework relies on deterministic rounding quantization for weights and activations (where the step size is found via solving an optimization problem). Since the framework starts with a pre-trained network and relies on fine-tuning after quantization, the technique is categorized as retraining.
## (s74) APQ: Joint Search for Network Architecture, Pruning and Quantization Policy (APQ) 5
Number of References: 63

(p74.0) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p74.1) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p74.2) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.

(p74.3) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p74.4) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p74.5) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.

(p74.6) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p74.7) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p74.8) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.
## (s76) HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs (HMQ)
Number of References: 6

(p76.0) HMQ [87] is a mixed-precision quantization block that enables an efficient search for quantization parameters by utilizing the Gumbel-Softmax estimator [105]. In this framework, an optimization method based on the HMQ blocks is used to search for the bitwidths (of weights and activations) and the threshold of each quantizer at the same time. Deterministic rounding is used for quantization after finding the optimal bitwidths. Since the optimization technique starts from a pre-trained full precision model then utilizes fine-tuning after quantization, the quantization technique is retraining.
## (s77) Rethinking Differentiable Search for Mixed-Precision Neural Networks (EdMIPS) 6
Number of References: 6

(p77.0) EdMIPS is proposed [37] as an efficient differentiable "mixed-precision network search" (MPS) that aims to find optimal per-layer precision of weights and activations without a proxy task, by solving a constrained optimization problem. The quantization technique is similar to that of HWGQ [82]. As such, the quantization technique is deterministic rounding. Since it trains from scratch, it is categorized as training-aware.
## (s82) AutoQ: Automated Kernel-Wise Neural Network Quantization
Number of References: 6

(p82.0) AutoQ is proposed as an MXPDNN framework that relies on hierarchical deep reinforcement learning (DRL) to automatically and rapidly assign mixed-precision for weights kernels and activations across the layers of the DNN [143]. Starting from a pre-trained network with full precision, AutoQ quantizes the weights (per kernel) and activations (per layer) based on the work done in [29]. The quantization technique used is classified as training-aware, deterministic rounding.
## (s88) Comparison Against Binary Neural Networks
Number of References: 9

(p88.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p88.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
## (s116) ZeroQ: A Novel Zero Shot Quantization Framework (ZeroQ) 3
Number of References: 3

(p116.0) ZeroQ, proposed in [35], is a framework that optimizes, based on a proposed loss function and the batch normalization layers of the full precision model, an engineered distilled dataset to enable per-layer bitwidth allocation for weights and activations via Pareto Frontier without a need to access the training or validation data. The quantization technique is post-training, deterministic rounding.
## (s117) HAWQ-V3: Dyadic Neural Network Quantization 4
Number of References: 3

(p117.0) In [51], the authors propose HAWQ-V3, a hardware-aware fixed low-precision and MXPDNN framework (for weights and activations) with integer-only inference by solving a constrained Integer Linear Programming (ILP) formulation. It relies on uniform quantization (with rounding). Also, channel-wise symmetric quantization is used for weights, layer-wise asymmetric quantization is utilized for activations, and static quantization for all the scaling factors. Results are presented with/without distillation, so this falls in the retraining/post-training category respectively.
## (s118) Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization (MPQNNCO)
Number of References: 3

(p118.0) The authors in [92] formulate the mixed-precision problem (for weights and activations) as a discrete constrained optimization problem which they solved by relying on second-order Taylor expansion, Hessian matrix computations, and a greedy search algorithm. In particular, the optimization problem is reformulated as a Multiple Knapsack problem which is solved by the proposed greedy search algorithm efficiently. This framework relies on deterministic rounding quantization for weights and activations (where the step size is found via solving an optimization problem). Since the framework starts with a pre-trained network and relies on fine-tuning after quantization, the technique is categorized as retraining.
## (s123) APQ: Joint Search for Network Architecture, Pruning and Quantization Policy (APQ) 5
Number of References: 63

(p123.0) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p123.1) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p123.2) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.

(p123.3) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p123.4) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p123.5) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.

(p123.6) APQ [89] is a framework that jointly optimizes the DNN architecture, pruning policy, and mixed-precision policy (for weights and activations) by relying on a pre-trained once-for-all network that generates DNN architectures and a quantization-aware predictor that demolishes the quantized data collection time. The quantization technique used by APQ is retraining, deterministic, and rounding.

(p123.7) To efficiently implement deep learning on a target hardware with some resource constraints, works in literature have proposed techniques to optimize the model architecture and other techniques to optimize model compression (i.e. pruning and quantization). The sequential optimization of these stages (architecture, pruning, quantization) has been shown to be efficient [46,159], but poses a challenge with hyper-parameter tuning as the number of hyper-parameters grows exponentially [160]. To mitigate the sub-optimality resulting due to optimizing each one of these stages separately, APQ reorganizes the pipeline of the three stages: architecture, pruning, and quantization into two stages: architecture search (coarse-grain architecture search and fine-grain channel search for channel-wise pruning) and mixed-precision search. The joint optimization objective of these two stages is formulated as follows. F * = arg max (F,w,P,Q) ACC val (Q(P (F, w)))

(p123.8) where ACC val is a function that searches for the DNN architecture, F , with the best validation accuracy on the dataset, Q is the mixed-precision quantization function, and P is the function that prunes the channels. To solve this objective function, first (1) a flexible once-for-all network that supports an extremely large and fine-grained search space is trained -via Progressive Shrinking (PS) algorithm [161] -to support both operator change and a number of channel changes (PS is viewed as a generalized pruning method which shrinks multiple dimensions; depth, width, kernel size, and resolution of the full network) , so as any sub-network can be extracted from it and its accuracy can be approximately evaluated directly without re-training. MobileNet-V2 is used as the backbone block that builds the once-for-all network. Then, (2) a quantization-aware predictor is built to predict the accuracy of models with different architectures and different bitwidths after quantization without having to fine-tune the models (which is usually time and resource-consuming). The predictor built is a 3-layer feed-forward DNN. Since collecting data to train this predictor is prohibitively time-consuming, a predictor-transfer technique is proposed: First, a full-precision accuracy predictor is trained on cheap data points collected by evaluating the once-for-all network, and then the quantization-aware predictor is transferred from the full-precision predictor. The input to this quantization-aware predictor is the encoded network architecture, pruning strategy, and quantization policy. The network architecture is encoded block by block, where each block is the concatenation of the encoded one-hot vectors for kernel size, channel numbers, and weight/activation bitwidths. The quantization policy is also encoded as one-hot vectors. Having trained the quantization-aware predictor, the search is then deemed ultra-fast, and this predictor is used to estimate the predicted accuracy instead of the measured accuracy. Moreover, this predictor can be utilized for new hardware platforms and deployment scenarios without training the model again. After training the quantization-aware predictor, (3) a latency/energy lookup table is constructed to do a resource-constrained evolution search [162] (whose evaluation process is replaced by the quantization-aware predictor) since evaluating each candidate policy on an actual hardware platform is costly. BitFusion [31] is used to measure the resource consumption for the mixed-precision model and fill the values in the lookup The reported results show that one model found by APQ (whose architecture description is not shown or reported) with transfer outperforms all the other baselines in terms of accuracy (in particular it has a 0.5% higher accuracy than the best performing baseline in term of accuracy: Single Path One-Shot), another model with transfer has the lowest latency, and a third model without transfer has the lowest BitOps. Moreover, the marginal carbon dioxide emission and the marginal cloud compute cost of the reported APQ models are way less than those reported for the baselines. Moreover, with decreasing the energy (latency) constraint, the accuracy of APQ's model decreases but remains in all cases higher than the top-1% accuracy of HAQ with MobileNet-V2 and the fixed 6-bit (4-bit) precision MobileNet-V2 models. In particular, for the tightest latency/energy constraint, APQ's model achieves a higher top-1% accuracy (+10.5%/+11.3%) compared to the MobileNet-V2 baseline. For the experiment that varies the BitOps, even with a tight BitOps constraint, APQ's model improves the top-1% accuracy by more than 2% compared with the searched model using Single Path One-Shot. In conclusion, APQ is a framework that jointly optimizes architecture, pruning, and mixed quantization by starting from a pre-trained once-for-all network and relying on a quantization-aware predictor along with a lookup table to conduct a budget-constrained evolutionary search efficiently. The reported results for APQ searched models are competitive with other top-performing baselines.
## (s125) HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs (HMQ)
Number of References: 6

(p125.0) HMQ [87] is a mixed-precision quantization block that enables an efficient search for quantization parameters by utilizing the Gumbel-Softmax estimator [105]. In this framework, an optimization method based on the HMQ blocks is used to search for the bitwidths (of weights and activations) and the threshold of each quantizer at the same time. Deterministic rounding is used for quantization after finding the optimal bitwidths. Since the optimization technique starts from a pre-trained full precision model then utilizes fine-tuning after quantization, the quantization technique is retraining.
## (s126) Rethinking Differentiable Search for Mixed-Precision Neural Networks (EdMIPS) 6
Number of References: 6

(p126.0) EdMIPS is proposed [37] as an efficient differentiable "mixed-precision network search" (MPS) that aims to find optimal per-layer precision of weights and activations without a proxy task, by solving a constrained optimization problem. The quantization technique is similar to that of HWGQ [82]. As such, the quantization technique is deterministic rounding. Since it trains from scratch, it is categorized as training-aware.
## (s131) AutoQ: Automated Kernel-Wise Neural Network Quantization
Number of References: 6

(p131.0) AutoQ is proposed as an MXPDNN framework that relies on hierarchical deep reinforcement learning (DRL) to automatically and rapidly assign mixed-precision for weights kernels and activations across the layers of the DNN [143]. Starting from a pre-trained network with full precision, AutoQ quantizes the weights (per kernel) and activations (per layer) based on the work done in [29]. The quantization technique used is classified as training-aware, deterministic rounding.
## (s137) Comparison Against Binary Neural Networks
Number of References: 9

(p137.0) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.1) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 

(p137.2) The main motivation behind the MXPDNN frameworks is their promise of high energy efficiency and throughput. Logically, the Binary Neural Networks (BNNs) offers the optimal results in both energy efficiency and throughput when both weights and activations are quantized using a bitwidth of one. However, due to the extremely low precision, a higher loss in accuracy is incurred by such BNNs. Hence, hereon we juxtapose the state-of-art BNN accuracy results (adapted from [38], and where both weights and activations are quantized with a bitwidth of one) against the best accuracy results reported by the MXPDNN frameworks (where both weights and activations are quantized in a mixed precision manner). Table 5 summarizes the comparison results on ImageNet and Cifar-10 datasets. As one can observe from the Table, the accuracy achieved by mixed precision frameworks is slightly higher than that of the BNNs for both datasets and across all models, with the exception of quantizing AlexNet on ImageNet. TSQ a notably higher accuracy than that achieved by SLWP when AlexNet is quantized on ImageNet. In general, the difference in accuracies reported between BNNs and mixed precision frameworks is not very high, which calls for more effort in better optimizing the mixed precision frameworks. 
