# Graph learning in robotics: a survey

CorpusID: 263830775 - [https://www.semanticscholar.org/paper/a25b6ec7300fa7b569c64fbf05d7546a5569a64c](https://www.semanticscholar.org/paper/a25b6ec7300fa7b569c64fbf05d7546a5569a64c)

Fields: Engineering, Computer Science

## (s4) II. PRELIMINARIES ON GRAPH NEURAL NETWORKS
Number of References: 19

(p4.0) In the last few years, learning on graph structures has gained significant relevance in the community, thanks to their intrinsic capability to easily process data lying on irregular domains, such as three-dimensional point clouds, spatiotemporal and functional relationships [14]- [17].Similarly to what Convolutional Neural Networks (CNNs) did for standard images, the community has devoted significant effort in defining convolution-based aggregation mechanisms for graph structures.

(p4.1) A graph convolution operation can be formulated over two different domains, spectral or spatial.The first family of methods [18]- [22] usually exploits graph Fourier transform, eventually complemented with polynomial approximations to reduce the computational burden [20], [21].Among these, it is worth mentioning the Graph Convolutional Network (GCN) [21], which demonstrated notable results for semisupervised problems, such as semi-supervised node or multiclass classification.However, a critical limitation of this formulation is the inability to generalise the learned filters, computed over the spectrum of the graph Laplacian, to a variable graph structure.

(p4.2) The second class of approaches defines the graphconvolution operation in the spatial domain.In this scenario, the graph convolution is defined as a local, i.e. computed over a neighbourhood, weighted aggregation of signals.Since it is defined at the neighbourhood level, this formulation is suitable for any type of signal that can be defined over a graph, even with a variable graph structure.Several definitions are present in the literature [5], [6], [24]- [30].These often differ on the computation of the weights used in the aggregation.For example, many formulations use scalar weights [24], [25], [27] or matrices [6], [26], [28], [30] independent from the input data.On the other side, [5] proposes edgedependent matrices as weights, providing an operation with more representational power.For a compact overview, the reader could refer to Fig. 3 for different graph convolution formulations and to Tab. 1 for a comparison between implementations.
## (s6) 1) Spectral Graph Convolutions
Number of References: 5

(p6.0) In the spectral domain, the filtering operation corresponds to a multiplication between the graph signal x âˆˆ R M and the Graph convolutional operations with different information sharing mechanism.MPNN [23] implements message passing conditioned by edge attributes.GCN [21] is reported in its equivalent spatial formulation where it implements a simple aggregation of the neighbouring nodes.With a similar operation, GAT [24] exploits self-attention to weight the contributions of the nearest neighbours.Instead ECC [5] implements an edge-depended weighting function to weight the contribution of each node in the neighbourhood.DGCNN [6] aggregate the information of the neighbourhood by means of learned edge features.
