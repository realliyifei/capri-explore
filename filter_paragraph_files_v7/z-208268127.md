# Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms

CorpusID: 208268127 - [https://www.semanticscholar.org/paper/54d4a221db5a91a2487b1610374843fafff5a23d](https://www.semanticscholar.org/paper/54d4a221db5a91a2487b1610374843fafff5a23d)

Fields: Mathematics, Computer Science

## (s2) Value-Based Methods
Number of References: 2

(p2.0) Value-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q π * . The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate. One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a). When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) ← (1 − α)Q(s, a) + α r + γ max a Q (s , a ) ,
## (s19) Multi-Agent MDP & Markov Teams
Number of References: 2

(p19.0) Consider a Markov game as in Definition 2.2 with R 1 = R 2 = · · · = R N = R, where the reward R : S × A × S → R is influenced by the joint action in A = A 1 × · · · × A N . As a result, the Q-function is identical for all agents. Hence, a straightforward algorithm proceeds by performing the standard Q-learning update (2.1) at each agent, but taking the max over the joint action space a ∈ A. Convergence to the optimal/equilibrium Q-function has been established in Szepesvári and Littman (1999); , when both state and action spaces are finite.
## (s26) Partially Observed Model
Number of References: 4

(p26.0) We complete the overview for cooperative settings by briefly introducing a class of significant but challenging models where agents are faced with partial observability. Though common in practice, theoretical analysis of algorithms in this setting is still in its infancy, in contrast to the aforementioned fully observed settings. In general, this setting can be modeled by a decentralized POMDP (Dec-POMDP) (Oliehoek and Amato, 2016), which shares almost all elements such as the reward function and the transition model, as the MMDP/Markov team model in §2.2.1, except that each agent now only has its local observations of the system state s. With no accessibility to other agents' observations, an individual agent cannot maintain a global belief state, the sufficient statistic for decision making in single-agent POMDPs. Hence, Dec-POMDPs have been known to be NEXPhard (Bernstein et al., 2002), requiring super-exponential time to solve in the worst case.
## (s32) Policy-Based Methods
Number of References: 14

(p32.0) For continuous games, due to the general negative results therein, Mazumdar and Ratliff (2018) introduces a new class of games, Morse-Smale games, for which the gradient dynamics correspond to gradient-like flows. Then, definitive statements on almost sure convergence of PG methods to either limit cycles, Nash equilibria, or non-Nash fixed points can be made, using tools from dynamical systems theory. Moreover, Balduzzi et al. (2018); Letcher et al. (2019) have studied the second-order structure of game dynamics, by decomposing it into two components.The first one, named symmetric component, relates to potential games, which yields gradient descent on some implicit function; the second one, named antisymmetric component, relates to Hamiltonian games that follows some conservation law, motivated by classical mechanical systems analysis. The fact that gradient descent converges to the Nash equilibrium of both types of games motivates the development of the Symplectic Gradient Adjustment (SGA) algorithm that finds stable fixed points of the game, which constitute all local Nash equilibria for zero-sum games, and only a subset of local NE for general-sum games. Chasnov et al. (2019) provides finitetime local convergence guarantees to a neighborhood of a stable local Nash equilibrium of continuous games, in both deterministic setting, with exact PG, and stochastic setting, with unbiased PG estimates. Additionally, Chasnov et al. (2019) has also explored the effects of non-uniform learning rates on the learning dynamics and convergence rates. Fiez et al. (2019) has also considered general-sum Stackelberg games, and shown that the same two-timescale algorithm update as in the zero-sum case now converges almost surely to the stable attractors only. It has also established finite-time performance for local convergence to a neighborhood of a stable Stackelberg equilibrium. In complete analogy to the zero-sum class, these convergence results for continuous games do not apply to MARL in Markov games directly, as they are built upon the differentiability/smoothness of the long-term return, which may not hold for general MGs, for example, LQ games (Mazum-dar et al., 2019).
## (s40) Other Applications
Number of References: 12

(p40.0) Furthermore, another popular testbed of MARL is the StarCraft II (Vinyals et al., 2017), which is an immensely popular multi-player real-strategy computer game. This game can be formulated as a multi-agent Markov game with partial observation, where each player has only limited information of the game state. Designing reinforcement learning systems for StarCraft II is extremely challenging due to the needs to make decisions under uncertainty and incomplete information, to consider the optimal strategy in the long-run, and to design good reward functions that elicits learning. Since released, both the full-game and sub-game version of StarCraft II have gained tremendous research interest. A breakthrough in this game was achieved by AlphaStar, recently proposed in , which has demonstrated superhuman performances in zero-sum two-player full-game StarCraft II. Its reinforcement learning algorithm combines LSTM for the parametrization of policy and value functions, asynchronous actor-critic (Mnih et al., 2016) for policy updates, and neural fictitious self-play (Heinrich and Silver, 2016) for equilibrium finding.

(p40.1) Furthermore, another popular testbed of MARL is the StarCraft II (Vinyals et al., 2017), which is an immensely popular multi-player real-strategy computer game. This game can be formulated as a multi-agent Markov game with partial observation, where each player has only limited information of the game state. Designing reinforcement learning systems for StarCraft II is extremely challenging due to the needs to make decisions under uncertainty and incomplete information, to consider the optimal strategy in the long-run, and to design good reward functions that elicits learning. Since released, both the full-game and sub-game version of StarCraft II have gained tremendous research interest. A breakthrough in this game was achieved by AlphaStar, recently proposed in , which has demonstrated superhuman performances in zero-sum two-player full-game StarCraft II. Its reinforcement learning algorithm combines LSTM for the parametrization of policy and value functions, asynchronous actor-critic (Mnih et al., 2016) for policy updates, and neural fictitious self-play (Heinrich and Silver, 2016) for equilibrium finding.
## (s41) Mixed Settings
Number of References: 6

(p41.0) Compared to the cooperative and competitive settings, research on MARL under the mixed setting is rather less explored. One application in this setting is multi-player poker. As we have mentioned in §5.2, Pluribus introduced in Brown and Sandholm (2019) has demonstrated superhuman performances in six-player no-limit Texas hold'em. In addition, as an extension of the problem of learning to communicate, introduced in §5.1, there is a line of research that aims to apply MARL to tackle learning social dilemmas, which is usually formulated as a multi-agent stochastic game with partial information. Thus, most of the algorithms proposed under these settings incorporate RNN or LSTM for learning representations of the histories experience by the agent, and the performances of these algorithms are usually exhibited using experimental results; see, e.g., Leibo et al. (2017); Lerer and Peysakhovich (2017); Hughes et al. (2018), and the references therein.
## (s47) Value-Based Methods
Number of References: 2

(p47.0) Value-based RL methods are devised to find a good estimate of the state-action value function, namely, the optimal Q-function Q π * . The (approximate) optimal policy can then be extracted by taking the greedy action of the Q-function estimate. One of the most popular value-based algorithms is Q-learning (Watkins and Dayan, 1992), where the agent maintains an estimate of the Q-value functionQ(s, a). When transitioning from stateaction pair (s, a) to next state s , the agent receives a payoff r and updates the Q-function according to:Q (s, a) ← (1 − α)Q(s, a) + α r + γ max a Q (s , a ) ,
## (s64) Multi-Agent MDP & Markov Teams
Number of References: 2

(p64.0) Consider a Markov game as in Definition 2.2 with R 1 = R 2 = · · · = R N = R, where the reward R : S × A × S → R is influenced by the joint action in A = A 1 × · · · × A N . As a result, the Q-function is identical for all agents. Hence, a straightforward algorithm proceeds by performing the standard Q-learning update (2.1) at each agent, but taking the max over the joint action space a ∈ A. Convergence to the optimal/equilibrium Q-function has been established in Szepesvári and Littman (1999); , when both state and action spaces are finite.
## (s71) Partially Observed Model
Number of References: 4

(p71.0) We complete the overview for cooperative settings by briefly introducing a class of significant but challenging models where agents are faced with partial observability. Though common in practice, theoretical analysis of algorithms in this setting is still in its infancy, in contrast to the aforementioned fully observed settings. In general, this setting can be modeled by a decentralized POMDP (Dec-POMDP) (Oliehoek and Amato, 2016), which shares almost all elements such as the reward function and the transition model, as the MMDP/Markov team model in §2.2.1, except that each agent now only has its local observations of the system state s. With no accessibility to other agents' observations, an individual agent cannot maintain a global belief state, the sufficient statistic for decision making in single-agent POMDPs. Hence, Dec-POMDPs have been known to be NEXPhard (Bernstein et al., 2002), requiring super-exponential time to solve in the worst case.
## (s77) Policy-Based Methods
Number of References: 14

(p77.0) For continuous games, due to the general negative results therein, Mazumdar and Ratliff (2018) introduces a new class of games, Morse-Smale games, for which the gradient dynamics correspond to gradient-like flows. Then, definitive statements on almost sure convergence of PG methods to either limit cycles, Nash equilibria, or non-Nash fixed points can be made, using tools from dynamical systems theory. Moreover, Balduzzi et al. (2018); Letcher et al. (2019) have studied the second-order structure of game dynamics, by decomposing it into two components.The first one, named symmetric component, relates to potential games, which yields gradient descent on some implicit function; the second one, named antisymmetric component, relates to Hamiltonian games that follows some conservation law, motivated by classical mechanical systems analysis. The fact that gradient descent converges to the Nash equilibrium of both types of games motivates the development of the Symplectic Gradient Adjustment (SGA) algorithm that finds stable fixed points of the game, which constitute all local Nash equilibria for zero-sum games, and only a subset of local NE for general-sum games. Chasnov et al. (2019) provides finitetime local convergence guarantees to a neighborhood of a stable local Nash equilibrium of continuous games, in both deterministic setting, with exact PG, and stochastic setting, with unbiased PG estimates. Additionally, Chasnov et al. (2019) has also explored the effects of non-uniform learning rates on the learning dynamics and convergence rates. Fiez et al. (2019) has also considered general-sum Stackelberg games, and shown that the same two-timescale algorithm update as in the zero-sum case now converges almost surely to the stable attractors only. It has also established finite-time performance for local convergence to a neighborhood of a stable Stackelberg equilibrium. In complete analogy to the zero-sum class, these convergence results for continuous games do not apply to MARL in Markov games directly, as they are built upon the differentiability/smoothness of the long-term return, which may not hold for general MGs, for example, LQ games (Mazum-dar et al., 2019).
## (s85) Other Applications
Number of References: 12

(p85.0) Furthermore, another popular testbed of MARL is the StarCraft II (Vinyals et al., 2017), which is an immensely popular multi-player real-strategy computer game. This game can be formulated as a multi-agent Markov game with partial observation, where each player has only limited information of the game state. Designing reinforcement learning systems for StarCraft II is extremely challenging due to the needs to make decisions under uncertainty and incomplete information, to consider the optimal strategy in the long-run, and to design good reward functions that elicits learning. Since released, both the full-game and sub-game version of StarCraft II have gained tremendous research interest. A breakthrough in this game was achieved by AlphaStar, recently proposed in , which has demonstrated superhuman performances in zero-sum two-player full-game StarCraft II. Its reinforcement learning algorithm combines LSTM for the parametrization of policy and value functions, asynchronous actor-critic (Mnih et al., 2016) for policy updates, and neural fictitious self-play (Heinrich and Silver, 2016) for equilibrium finding.

(p85.1) Furthermore, another popular testbed of MARL is the StarCraft II (Vinyals et al., 2017), which is an immensely popular multi-player real-strategy computer game. This game can be formulated as a multi-agent Markov game with partial observation, where each player has only limited information of the game state. Designing reinforcement learning systems for StarCraft II is extremely challenging due to the needs to make decisions under uncertainty and incomplete information, to consider the optimal strategy in the long-run, and to design good reward functions that elicits learning. Since released, both the full-game and sub-game version of StarCraft II have gained tremendous research interest. A breakthrough in this game was achieved by AlphaStar, recently proposed in , which has demonstrated superhuman performances in zero-sum two-player full-game StarCraft II. Its reinforcement learning algorithm combines LSTM for the parametrization of policy and value functions, asynchronous actor-critic (Mnih et al., 2016) for policy updates, and neural fictitious self-play (Heinrich and Silver, 2016) for equilibrium finding.
## (s86) Mixed Settings
Number of References: 6

(p86.0) Compared to the cooperative and competitive settings, research on MARL under the mixed setting is rather less explored. One application in this setting is multi-player poker. As we have mentioned in §5.2, Pluribus introduced in Brown and Sandholm (2019) has demonstrated superhuman performances in six-player no-limit Texas hold'em. In addition, as an extension of the problem of learning to communicate, introduced in §5.1, there is a line of research that aims to apply MARL to tackle learning social dilemmas, which is usually formulated as a multi-agent stochastic game with partial information. Thus, most of the algorithms proposed under these settings incorporate RNN or LSTM for learning representations of the histories experience by the agent, and the performances of these algorithms are usually exhibited using experimental results; see, e.g., Leibo et al. (2017); Lerer and Peysakhovich (2017); Hughes et al. (2018), and the references therein.
