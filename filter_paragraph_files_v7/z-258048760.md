# 11 A Survey on Recent Teacher-student Learning Studies

CorpusID: 258048760 - [https://www.semanticscholar.org/paper/099aacd1c1e08a24de71e9390784812ec0957bd2](https://www.semanticscholar.org/paper/099aacd1c1e08a24de71e9390784812ec0957bd2)

Fields: Mathematics, Education, Computer Science

## (s8) Summary
Number of References: 4

(p8.0) In this paper, we reviewed recent developments in knowledge distillation [13,15,19,22], a technique for compressing complex deep neural networks (DNNs) into smaller and faster DNNs while preserving accuracy. We discussed four variants of knowledge distillation, including teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation. Teaching assistant distillation introduces an intermediate model between the teacher and the student, while curriculum distillation designs the learning process to follow a curriculum. Mask distillation focuses on transferring the attention mechanism learned by the teacher, and decoupling distillation decouples the distillation loss from the task loss. These variants of knowledge distillation have shown promise in improving the performance of knowledge distillation, making it a valuable tool for model compression and acceleration.
