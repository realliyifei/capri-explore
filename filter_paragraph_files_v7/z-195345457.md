# Emotionally-Aware Chatbots: A Survey

CorpusID: 195345457 - [https://www.semanticscholar.org/paper/c131665638feb8c11f936989ffc6187317593b41](https://www.semanticscholar.org/paper/c131665638feb8c11f936989ffc6187317593b41)

Fields: Computer Science

## (s10) Automatic Evaluation.
Number of References: 10

(p10.0) In automatic evaluation, some studies focus on evaluating the system at emotion level [23,65]. Therefore, some common metrics such as precision, recall, and accuracy are used to measure system performance, compared to the gold label. This evaluation is similar to emotion classification tasks such as previous SemEval 2018 [34] and SemEval 2019 13 . Other studies also proposed to use perplexity to evaluate the model at the content level (to determine whether the content is relevant and grammatical) [23,26,45]. This evaluation metric is widely used to evaluate dialogue-based systems which rely on probabilistic approach [48]. Another work by [45] used BLEU to evaluate the machine response and compare against the gold response (the actual response), although using BLEU to measure conversation generation task is not recommended by [25] due to its low correlation with human judgment.
