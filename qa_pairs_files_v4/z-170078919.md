# A survey of Object Classification and Detection based on 2D/3D data

CorpusID: 170078919 - [https://www.semanticscholar.org/paper/b2c768b7ddc9a038111a1361ef25828adfdcbdf8](https://www.semanticscholar.org/paper/b2c768b7ddc9a038111a1361ef25828adfdcbdf8)

Fields: Engineering, Computer Science

## (s3) Object detection
(p3.0) In the object detection research area, just like in the image classification where the ImageNet dataset is available to train the algorithm, in the object detection, COCO(Common Objects in Context) [4] and VOC dataset can be used to train the algorithms. For the COCO dataset, the bounding box and the mask of the objects are provided. The first important contribution of using deep neural networks to solve the object detection problem is the R-CNN [20]. The region of interest(ROI) of an image is proposed by the selective search(SS) [21] algorithm, and the ROI is cropped to feed a CNN to do the detection. However, as every proposed interesting area will be calculated to predict whether that specified region is an object, the speed of this algorithm is very slow. In order to address this problem, a fast R-CNN algorithm is proposed in [22] by improving the feature map generation efficiency. In another paper which is short for faster RCNN [23], instead of using the SS algorithm to generate ROI, the ROI is proposed by using deep neural network structure. By the combination, the performance of the algorithm can also be improved itself. Figure 11: The 2D center offset box encoding method.
## (s4) Bounding box encoding method
(p4.0) In the object detection task, one import part is estimating the bounding box of the object. The 2D bounding box estimation only focus on axis-aligned boxes. The box encoding method is simply based on a simple center coordinates of the bounding box (x, y) and the offset of the bounding box: width: W and height: H. We are calling this method as 2D center offset box encoding method and it is shown in Figure 11. The RCNN, Fast RCNN, Faster RCNN, YOLO, YOLOv2 and Mask R-CNN are using this kind of bounding box encoding with a slightly difference on the loss calculation. [20], Fast RCNN [22] and Faster RCNN RCNN is an important framework in 2D image detection task which mainly uses the CNN to extract the features for each cropped interesting region. After that the extracted features are fed to a SVM to do the classification and a bounding box regression is followed to improve the bounding box prediction based on the method from [24]. It mainly has two stages: region proposal and detection. As the detection process is computation expensive, the region proposals can make the detection step mainly focus on limited interesting regions (about 2000 regions for a typical image) which greatly reduces the complexity of the whole system and achieves a good performance on the 2D image detection task. This two-stage detection framework is becoming a classical model in both 2D image based object detection and 3D image based object systems. The frame work of RCNN is shown in Figure 12. Figure 13: Fast R-CNN [22] architecture. An input image and multiple regions of interest (RoIs) are input into a fully convolutional network. Each RoI is pooled into a fixed-size feature map and then mapped to a feature vector by fully connected layers (FCs). The network has two output vectors per RoI: softmax probabilities and per-class bounding-box regression offsets. The architecture is trained end-to-end with a multi-task loss.  Fast R-CNN improves the RCNN mainly with respect to three aspects: First, instead of doing convolution operations separately for each proposed region, the Fast RCNN does the convolution operations for the whole image firstly and then uses region proposals from the feature map directly to do the further detection. The feature map level region proposals are projected from the region proposals based on the original image. Second, using the softmax layer to replace the SVM classifier to make the detection under one deep learning framework. Finally, Fast R-CNN is using the Multi-task loss to do the object classification and the bounding box regression. The Fast RCNN framework is shown in Figure 13. In order to have a same size of feature vectors from different size proposed regions, the ROI pooling is used and the ROI pooling is demonstrated in Figure 14. Faster RCNN Figure 15: The three main steps for the Faster RCNN [23], system: head(backbone network), RPN and detection network.
## (s16) Novel view point models
(p16.0) RotationNet is an extension of MVCNN [60]. In this paper, multiple views from different angles are explored. Three models of camera views are proposed as shown in Figure 37. The performance of case(i) (the same view points model as MVCNN [60]) and case(ii) are compared. Case (ii) achieves a better performance based on the ModelNet40 task. For the ModelNet40, the case(iii) model is not used.
## (s27) The 3D bounding box encoding methods
(p27.0) In the following section we will focus on the 3D only or 2D+3D detection systems.  A comprehensive comparison of the input data, the feature representation of input data and the bounding box encoding methods for both the proposals and the final 3D bounding box detection is provided in Table 11. DeepSliding [71] and VoxelNet [74] are using the 3D convolutional neural network to do the proposals generation and the bounding box detection. MV3D [72], AVOD [73] are projecting the depth or LiDAR data to a 2D similar images and are using a 2D CNN to do the proposal generation and bounding box detection. The relationship between MV3D and AVOD is explained later. F-PointNet [75] is using 2D RGB images to help generate the proposals and it is a special framework. At the same time, the different proposal generation method will have an influence on the application scenario of those frameworks, to be discussed later.
## (s34) BEV features for MV3D
(p34.0) The bird's eye view representation is encoded by height, intensity and density. The point cloud is projected into a 2D grid with resolution of 0.1m. For each cell, the height feature is computed as the maximum height of the points in the cell. To encode more detailed height information, the point cloud is divided equally into M slices. A height map is computed for each slice. The intensity feature is the reflectance value of the point which has the maximum height in each cell. The point cloud density indicates the number of points in each cell. So it has (M + 2) channel features [72] . MV3D uses point cloud in the range of [0, 70.4] × [− 40,40] meters in the X and Y dimensions. The size of the input features is 704 × 800 × (M + 2). The value of M is not provided in [72]. The length of Z dimension is also not provided. If we suppose that for the length of Z dimension is 2.5 meters as in AVOD [73] and the resolution of the Z dimension is also 0.1 meters, then the size of the input feature for the BEV will be 704 × 800 × 27.
## (s35) FV features for MV3D
(p35.0) MV3D projects the FV into a cylinder plane to generate a dense front view map as in VeloFCN [84]. The front view map is encoded with threechannel features, which are height, distance and intensity as shown in Figure  64. Since KITTI uses a 64-beam Velodyne laser scanner, the size of map for the front view is 64 × 512.  [72] The performance of MV3D is evaluated based on the outdoor KITTI dataset. The performance of 3D object detection based on the test set can be found from the leaderboard. The performance of 3D object detection based on validation dataset is shown in Figures 65 and 66. It only provides the car detection results. Detection results for the pedestrians and cyclists are not provided.   The framework of AVOD is shown in Figure 68. AVOD is using the same encoding method as MV3D for the BEV. In AVOD, the value of M is set as 5 and the range of the LiDAR is [0, 70] × [−40, 40] × [0, 2.5] meters. So the size of the input feature for the BEV is 700 × 800 × 7. AVOD is using both the BEV and image to do the region proposals which is the main difference to the MV3D work.  VoxelNet architecture is shown in Figure 71. The feature learning network takes a raw point cloud as input, partitions the space into voxels, and transforms points within each voxel to a vector representation characterizing the shape information. The space is represented as a sparse 4D tensor. The convolutional middle layers processes the 4D tensor to aggregate spatial context. Finally, a RPN generates the 3D detection. A fixed number, T , of points from voxels containing more than T points are randomly sampled. For each point, a 7-feature is used which is (x, y, z, r, x− v x , y − v y , z − v z ) where x, y, z are the XY Z coordinates for each point. r is the received reflectance and (v x , v y , v z ) is the centroid of points in the voxel. Voxel Feature Encoding is proposed in VoxelNet. The 7-feature for each point is fed into the Voxel feature encoding layer as shown in Figure 72. Fully connected networks are used in the VFE network with element-wise MaxPooling for each point and concatenation between each point and the element-wise MaxPooling output. The input of the VFE is T × 7 and the output will be T × C where C depends on the FC layers of the VFE itself and depends on the whole VFE layers network used. Finally, an element-wise MaxPooling is used again and change the dimension of the output to 1 × C. Then for each voxel we have a one vector with C elements as shown in Figure 71. For the whole framework, we will have an input data with shape of C ×D ×H ×W .
