# A Comprehensive Review of Various Diabetic Prediction Models: A Literature Survey

CorpusID: 248173948 - [https://www.semanticscholar.org/paper/bf1a3b9a295dc31c9d71cad4ab29ca115415f037](https://www.semanticscholar.org/paper/bf1a3b9a295dc31c9d71cad4ab29ca115415f037)

Fields: Medicine, Computer Science

## (s7) V. Jackins Method.
(p7.0) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.
## (s16) Conclusion
(p16.0) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p16.1) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p16.2) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.
## (s26) V. Jackins Method.
(p26.0) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.
## (s35) Conclusion
(p35.0) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p35.1) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p35.2) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.
## (s45) V. Jackins Method.
(p45.0) In this article, the authors have worked on three datasets together, diabetes, cancer, and heart disease. A two-step method is used by the authors in which the data in the dataset undergoes a cleaning process, and then the machine learning classifiers are applied to the datasets. Data preprocessing [18] includes replacing the missing values of the dataset with the null values and then checking the correlation between the features of the dataset. Correlation [27] helps in determining important features from the dataset. If two features are highly correlated, then one of the attributes from the dataset can be skipped while the other can be used for prediction purposes. In the next step, data is split into training and testing. 70% of the dataset is used for training purposes by the author, and the remaining 30% is used for testing purposes. en, the naïve Bayes and random forest classifier are applied to the filtered dataset, and the evaluation parameters are compared. For diabetes, the authors have made use of the Pima Indians Diabetes Dataset. Dataset is analysed through Anaconda 4.1.
## (s54) Conclusion
(p54.0) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p54.1) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.

(p54.2) Based upon the comparative analysis and the above discussion, it can be concluded that Kamrul Hasan et al.'s [36] method is by far the best approach for diabetes prediction, as it is ranking features, selecting predominant features, filling missing values by median, and then tuning the hyperparameters as well. All the experiments were conducted using python. Although the accuracy achieved is only 78.9% by ensembling of AdaBoost and gradient boost, the AUC achieved is approximately 95%. A comparison of the threefeature selection technique and six machine learning classifiers has been made, and the ensembling of AdaBoost and gradient boost gave the best results.
