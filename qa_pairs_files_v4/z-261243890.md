# Computation-efficient Deep Learning for Computer Vision: A Survey

CorpusID: 261243890 - [https://www.semanticscholar.org/paper/a87947f88519dba980d0f16cdfb78ed09a8e02f0](https://www.semanticscholar.org/paper/a87947f88519dba980d0f16cdfb78ed09a8e02f0)

Fields: Engineering, Computer Science

## (s7) 1) Marrying Convolution and Attention Modules.
(p7.0) Convolution and self-attention are both important modules with their own strengths. A considerable amount of literature has been published to study how to combine them for a higher overall computational efficiency. At the per-layer level, convolution can be leveraged to generate the inputs of self-attention, e.g., queries/keys/values [75,76] or position embeddings [119]. In addition, some works simultaneously utilize self-attention and a convolutional layer, and fuse their outputs [120,121], which facilitates the learning of local features. Another promising idea is to integrate convolution into the feed-forward network after the self-attention module [76,122,123].
## (s8) 3) Model Scaling.
(p8.0) On top of designing a single efficient model, it is also important to obtain a family of models that can adapt to varying computational budgets. An important principle for addressing this issue is compound scaling [29,82], which indicates that simultaneously increasing the depth, width and input resolution of a given base model will yield a family of efficient network architectures. Doll√°r et al. [137] further study how to design a proper model scaling rule in terms of the actual runtime. In addition, TinyNets [138] extend this idea to the shrinking of the model size.
## (s11) Latency-aware Neural Architecture Search
(p11.0) From the lens of practical efficiency, an important challenge faced by NAS is the inference speed on real hardware (e.g., GPUs or CPUs). Since NAS usually leads to irregular network architectures, the obtained model with low theoretical computational cost may not be efficient in practice. To address this issue, recent NAS methods explicitly incorporate real latency into the optimization objective to achieve a good trade-off between real speed and accuracy [53,54,159]. As representative examples, MobileNetV3 [28] leverages hardware-aware NAS to obtain the basic architecture, and modifies it manually. Once-for-all [24] proposes to train a shared general super-nets, and perform NAS on top of it conditioned on the specific hardwares, yielding a state-of-the-art efficiency.
## (s14) 1) Marrying 2D and 3D Convolution.
(p14.0) A basic idea is to avoid designing a pure 3D ConvNets, i.e., most of the feature extraction process may be accomplished by the efficient 2D convolution, while 3D convolution is only introduced at several particular positions. From the lens of macro-architecture, this goal can be attained by sequentially mixing 2D and 3D blocks, either first using 3D and later 2D or first 2D and later 3D [162,163]. At the micro-architecture level, the group-wise or depth-width 3D convolution can be integrated in to the transform module of 2D split-transform-merge architecture (Eq. (2)) [164,165].

(p14.1) 2) (2+1)D Networks. Another elegant idea is to decompose 3D convolution into two components: a 2D convolution that extract representation from video frames, and a temporal operation that only focuses on learning the temporal relationships. The former can directly adopt 2D neural operators, while the latter can be implemented using 1D temporal convolution [166,167,168], adaptive 1D convolution [169], and MLPs [170].

(p14.2) 3) 2D Networks. In addition to the aforementioned approaches, the models with only 2D convolution may also be able to model temporal relationships. This is typically achieved by designing zero-parameter operations. For example, subtracting the features of adjacent frames to extract the motion information [171,172]. The temporal-shift-based models [173,174,175] propose to shift part of the channels of 2D features along the temporal dimension, performing information exchange among neighboring frames efficiently.
## (s18) Point-based Models
(p18.0) A fundamental type of 3D geometric data structure is the cloud of 3D points, where each point is represented by its three coordinates. PointNet [186] is the pioneering work that leveraging deep learning to process 3D point clouds. It adopts point-wise feature extraction with shared MLPs to maintain the permutation invariance. PointNet++ [187] improves PointNet by facilitating capturing local geometric structures. On top of them, a number of works focus on how to aggregating local information effectively without increasing computational cost significantly. Representative approaches include introducing graph neural networks [188,189], projecting 3D points to regular grids to perform convolution [190,191,192,193], aggregating the features of adjacent points using the weights determined by the local geometric structure [194,195,196], and self-attention [197,198]. In particular, recent works have revealed that point-based models can achieve state-of-the-art computational efficiency with proper training and model scaling techniques [199].
## (s25) Dynamic Width
(p25.0) Instead of skipping an entire layer, a less aggressive approach is adjusting the network width to different inputs. In this direction, the most popular implementation is dynamically skipping the channels in convolutional blocks via a gating module [35,225,226,227] (Figure 4). Specifically, a gating module is first executed before conducting a convolution operation. The output of this gating module is a C-dimensional binary vector that decides whether to compute each channel, where C is the output channel number. This implementation is similar to that in the aforementioned layer-skipping scheme. The most prominent difference is that the output of the gating module in layer skipping is a scalar, and the gating module in channel-skipping is required to output a vector controlling the computation of different channels. Apart from convolution layers, the same idea can also be applied in vision Transformers to dynamically skip channels in multi-layer perceptron (MLP) blocks [224].
## (s28) Pixel-level Dynamic Networks
(p28.0) A typical approach to spatial-wise adaptive inference is dynamically deciding whether to compute each pixel in a convolution block based on a binary mask [235,236,237]. This form is similar to that in layer skipping and channel skipping (Sec. 3.1), except that the gating module is required to output a spatial mask. Each element of this spatial mask determines the computation of a feature pixel. In this way, the mask generators learn to locate the most discriminative regions in image features, and redundant computation on less informative pixels can be skipped.
## (s30) Resolution-level Dynamic Networks
(p30.0) Most existing vision models process different images with the same resolution. However, the input complexity could vary, and not all images require a high-resolution representation. Ideally, low-resolution representations should be sufficient for those "easy" samples with large objects and canonical features. The early work [249] proposes to adaptively zoom input images in the face detection task. The recent resolution adaptive network (RANet) [217] builds a multi-scale architecture, in which inputs are first processed with a low resolution and a small sub-network. Large sub-networks and high-resolution representations are conditionally activated based on early predictions. Instead of using a specialized structure, dynamic resolution network [250] rescales each image with the resolution predicted by a small model and feeds the rescaled image to common CNNs.
## (s32) Dynamic Recurrent Models
(p32.0) Different video frames are unequally informative. To this end, extensive studies propose to dynamically activate computation when updating the hidden state in recurrent models. For example, LiteEval [251] establishes two different sized LSTM [252]. In each time step, a gating module is used to decide which LSTM should be executed for processing the current frame. AdaFuse [253] dynamically skips the computation of some convolution channels, and these channels are filled with the hidden state from the previous step. Moreover, the numerical precision [254] and image resolution [255] of different frames can also be dynamically decided.

(p32.1) The aforementioned works generally require a ConvNet for encoding each input frame before updating the hidden state. A more flexible solution is allowing the network to learn "where to see". In other words, networks can directly jump to an arbitrary temporal location in the video [37,256,257] or perform early exiting [258,259,260] instead of "watch" the entire video frame by frame.
## (s44) Two-stage Approaches
(p44.0) From the lens of efficiency, a notable milestone of deep-learning-based instance segmentation is the proposing of Mask R-CNN [318]. Mask R-CNN is developed by introducing mask segmentation branches on the basis of Faster R-CNN [272]. It enjoys high computational efficiency by directly obtaining the regions of interest from the feature maps. In contrast, MaskLab [319] improved Faster R-CNN by adding the semantic segmentation and direction prediction paths. To improve the accuracy of Mask R-CNN, MS R-CNN [320] predicts the quality of the predicted instance masks and prioritizes more accurate mask predictions during validation. PANet [321] introduces a path augmentation mechanism to facilitate the bottom-up information interaction of feature maps. HTC [322] proposes a hybrid task cascade framework to learn more discriminative features progressively while integrating complementary features in the meantime.
## (s58) Hardware-aware Model Design
(p58.0) As the practical latency of models can be influenced by many factors other than theoretical computation, the commonly used FLOPs is an inaccurate proxy for network efficiency. Ideally, one should develop efficient models based on specific hardware properties. However, hand-designing networks for different hardware devices can be laborious. Therefore, automatically searching for efficient architectures is emerging as a promising direction. Compared to the traditional NAS methods [31,414], this line of works can generate appropriate models which satisfy different hardware constraints and gain realistic efficiency in practice. For example, ProxylessNAS [54] establishes a latency prediction function based on realistic tests on targeted hardware, and the predicted latency is then directly used as a regularization item in the NAS objective. A similar idea is also implemented by MnasNet [53] to search for efficient models on mobile devices. The following works FBNet [159], FBNet-v2 [415] and OFA [416] have improved NAS techniques.
## (s63) Developing Task-specialized Models
(p63.0) In addition to the architectural advancements in backbone models, tailoring deep learning methodologies to specific computer vision tasks of interest has been demonstrated as crucial. Two research challenges of particular significance in this domain can be identified. Firstly, the exploitation of representations extracted by backbones to efficiently obtain task-specific features is essential, for example, multi-scale features for object detection and multi-path fused features for semantic segmentation. A potential solution to this challenge could involve designing specialized, efficient decoders (e.g., utilizing NAS [311,437]). Secondly, it is important to streamline the multi-stage design of visual tasks (e.g., two-stage object detection [273] and instance segmentation [318] algorithms) to achieve end-to-end paradigms with minimal performance compromises. Additionally, the removal of time-consuming components, such as non-maximum suppression (NMS) [8], is crucial. A promising area for future research may involve the development of an efficient, unified, and end-to-end learnable interface for a majority of prevalent computer vision tasks [438].
## (s65) Leveraging Large-scale Training Data
(p65.0) Contemporary large visual backbone models have exhibited remarkable scalability in response to the increasing volumes of training data [6], that is, the model's performance consistently enhances as more train-ing data becomes accessible. However, it is generally arduous for computationally efficient models with a reduced number of parameters to capitalize on this high-data regime to the same extent as their larger counterparts. For example, the improvements attained by pre-training light-weighted models on expansive ImageNet-22K/JFT datasets are typically inferior to those observed in larger models [6,7,74]. This challenge is similarly experienced by self-supervised learning algorithms, where the methods effective for larger models frequently produce limited gains for smaller models [440,441]. As a result, a propitious avenue of research involves the exploration of effective scalable supervised and unsupervised learning algorithms for light-weighted models, allowing them to reap the benefits of an unlimited amount of data without incurring the expense of acquiring annotations. Some recent works on novel training algorithms have started to preliminarily explore this direction [82,442,443,444,445].
