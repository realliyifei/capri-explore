# Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond

CorpusID: 258331833 - [https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a](https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a)

Fields: Linguistics, Computer Science

## (s8) Finetuning data
(p8.0) When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant. In this section, we provide a succinct overview of the appropriate models to employ for each scenario. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach. LLMs have been shown to outperform previous zero-shot methods [120]. Additionally, the absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model parameters remain unaltered.
