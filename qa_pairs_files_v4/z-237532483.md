# A Survey on Temporal Sentence Grounding in Videos

CorpusID: 237532483 - [https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453](https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453)

Fields: Computer Science

## (s17) Spatio-temporal localization.
(p17.0) Spatial-temporal sentence grounding in videos is another extension from TSGV which mainly localizes the referring object/instance as a continuing spatialtemporal tube (i.e., a sequence of bounding boxes) extracted from an untrimmed video via a natural language description. Since fine-grained labeling process of localizing a tube (i.e., annotate a spatial region for each frame in videos) for STSGV is labor-intensive and complicated, Chen et al. [13] propose to solve this task in a weakly-supervised manner which only needs video-level descriptions, with a newly-constructed VID-sentence dataset. Besides, VOGNet [50] commits to address the task of video object grounding, which grounds objects in videos referred to the natural language descriptions, and constructs a new dataset called ActivityNet-SRL. Tang et al. [56] employ visual transformer to solve a similar task which aims to localize a spatio-temporal tube of the target person from an untrimmed video based on a given textural description with a newly-constructed HC-STVG dataset.
