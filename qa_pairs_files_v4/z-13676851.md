# Computer-Based Interventions for Problematic Alcohol Use: a Review of Systematic Reviews

CorpusID: 13676851 - [https://www.semanticscholar.org/paper/e4ae452a3041e9e05cc0802a6721209a98779183](https://www.semanticscholar.org/paper/e4ae452a3041e9e05cc0802a6721209a98779183)

Fields: Medicine, Computer Science

## (s11) Length of Intervention-Outcome
(p11.0) Eight reviews (57 %) addressed the association between length of intervention and outcome. Three of these addressed this theme quantitatively [15,19,20].  [20], no significant associations were found in a meta-regression between number of sessions and effect size (b = −0.0001, 95 % CI −0.004 to 0.003). White et al. 2010 noted that the prepost differential effect size for brief personalized feedback programs (d = 0.39) was somewhat smaller than the effect size for the multisession modularized programs (d = 0.56) [17]. Some reviews addressed the theme narratively; Dedert et al. 2015 stated that variability in treatment intensity was insufficient to formally test its association with outcomes [24], while Bhochhibhoya et al. 2015 concluded that more prolonged, multi-session interventions seem to be more effective than one-time interventions [23].
## (s13) Trial Engagement-Outcome
(p13.0) Three reviews mentioned the possible association between trial engagement (or its reverse, drop-out) and outcome (21 %) with one of them presenting a quantitative analysis on this theme. Leeman et al. 2015 did not find differences in effect sizes between studies with retention rates at follow-up of more than 70 % versus less than 70 % [25]. White et al. 2010 reported retention in the intervention groups of the included trials ranging from 38.9 to 100 %, with a median of 74.5 % at 6 months; retention in control groups were quite similar with a range of 33.4 to 100 % and a median of 74.9 % at 6 months [17]. Dotson et al. 2015 reported drop-out rates in the included studies, ranging from 4.2 to 21 % [26]. This review in addition contended that researchers should not only measure drop-out rates but also find a way to measure whether participants actually pay attention to the intervention content as there are indications that some participants may engage in other activities while completing Internet interventions [27] in [26]].
## (s16) Strengths and Limitations
(p16.0) A significant strength of this review is that it followed a robust methodology and answered two specific questions determined a priori, with findings synthesized narratively in response to these questions. However, some limitations should be mentioned: Whilst we had minimum quality criteria for the inclusion of systematic reviews in our review, we did not undertake a quality assessment of the reviews. For reviews published until the end of 2014, however, critical summaries are available from the Database of Abstracts of Reviews of Effects (DARE) [13]. Furthermore, discussion of moderators was not based on experimental data but was limited to meta-regression (i.e. observational data) in the included reviews. There is a growing literature that uses experimental design to explore the impact of some of these moderators [34,[40][41][42]. Finally, some individual RCT's are included in several reviews; the results of the reviews are thus not fully independent of each other.
