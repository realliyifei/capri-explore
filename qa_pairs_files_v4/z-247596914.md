# Visuo-Haptic Object Perception for Robots: An Overview

CorpusID: 247596914 - [https://www.semanticscholar.org/paper/49579115424853a2e479678023ef592fddaca003](https://www.semanticscholar.org/paper/49579115424853a2e479678023ef592fddaca003)

Fields: Engineering, Computer Science

## (s1) Neural Basis of Visuo-Haptic Object Perception
(p1.0) The fact that there is no learning algorithm yet that reaches the level of proficiency of the human brain when it comes to recognizing objects illustrates how complex this cognitive task actually is (Smith et al, 2018;Krüger et al, 2013;James et al, 2007). The human brain is capable of performing it both quickly and accurately, even when the visual information available is incomplete or ambiguous. One reason might be that the brain can complement that 'picture' with information from other sensory modalities at will; usually, it does this with haptics. However, it is also because the learning machinery in the human brain seems to be suited to learn from drastically different frequency distributions than those used in machine learning, as described by Smith et al (2018). In particular, infants seem to use curriculum learning constrained by their developing sensorimotor abilities and actions. However, what is in strong contrast with machine learning algorithms is that the learning machinery, at least in infants, is particularly effective in learning from extremely skewed frequency distributions, i.e., a very small number of instances are highly frequent while most other instances are encountered very rarely. For instance, in very young infants, more than 80% of faces they are exposed to are from 2-3 individuals (Smith et al, 2018). We argue that taking inspiration from the complementary nature of the sensory modalities as well as processes in the brain that are involved in fusing the information they provide during object perception, might help build better robotic systems. While this topic is an active area of research and considerable new insights have been gained, there are still many aspects about the inner workings of the human brain during object perception that are not fully understood.
## (s13) Tactile Sensors
(p13.0) Tactile sensors are mostly designed to mimic mechanoreceptors, particularly to detect mechanical pressure. The main objectives of tactile sensors are to determine the location, shape and intensity of contacts. These properties are determined by measuring the instantaneous pressure or force applied to the sensor's surface on multiple contact points. Also, the contact's late effects, i.e., body-borne vibrations, may carry relevant information. Body-borne vibrations are not as commonly measured or exploited as part of haptic sensing; however, there are some examples, e.g., Syrymova et al (2020); Toprak et al (2018), including sensors that are inspired by hair follicle receptors or ciliary structure (Alfadhel and Kosel, 2015;Ribeiro et al, 2017;Kamat et al, 2019) and that have been proven very effective in obtaining information about the texture of objects (Ribeiro et al, 2020b,a).

(p13.1) Thermoceptors, although an integral part of human haptic perception, are typically not classified as tactile sensors within robotic applications. However, they are sometimes included because they might help compensate for thermal effects (Tomo et al, 2016), thus helping to obtain a more robust electronic signal related to pressure or vibrations, or because they might help to classify the material of the object in contact (Wade et al, 2017). In contrast, nociceptors have not yet been developed as part of haptic or tactile sensing per se but can be and have been implemented in software based on the limitations of robots (e.g., Navarro-Guerrero et al, 2017b,a).
## (s28) Transfer Learning
(p28.0) One of the challenges of transfer learning (colearning) is that machine learning models are based on the assumption that both training and test data are drawn from the same distribution. However, such an assumption does not hold when transferring knowledge between different robots or sensor modalities. A possible solution is domain adaptation, a.k.a. transfer learning, (e.g., Daumé III and Marcu, 2006;Wang and Deng, 2018). Here, training samples from a source dataset are adapted to fit a target distribution.
## (s40) Multimodal Signal Processing and Applications
(p40.0) With regards to signal processing and applications, even though multimodal visuo-haptic approaches for grasping show better results and have the potential to handle use-cases where visual information alone is insufficient, vision-only grasping approaches (e.g., Levine et al, 2018;Mahler et al, 2017;Bousmalis et al, 2018;James et al, 2019) are still more popular. Some reasons for this popularity are that the availability, durability and understanding of vision sensors are better than tactile ones. Moreover, the simulation of vision sensors is easier and more realistic, and the collection, processing and interpretation of visual information are easier than tactile sensor readings. On the other side of the spectrum, there are also recent grasping approaches (e.g., Murali et al, 2020;Hogan et al, 2018) that only use tactile information, but such approaches are usually only suitable for limited scenarios or parts of the grasping process. Thus, future efforts should be concentrated on multimodal approaches. However, as discussed by Xia et al (2022), the main challenge is ensuring safety during the physical contact between the object and the robot necessary for tactile sensing. To avoid the hardware dependencies and the safety risks, simulations are a promising alternative to real-world training and data collection for learningbased grasping approaches. However, due to the inaccurate nature of simulations, they cannot completely replace, but they can significantly reduce, the amount of real-world data needed. Finally, finetuning on the real system or sim2real techniques (e.g., Ding et al, 2020;Narang et al, 2021) can help to bridge the simulation-to-reality gap.

(p40.1) Another major problem of data-driven and endto-end learning grasping approaches is that they require a vast amount of training data, in contrast to humans, who learn and generalize from very few examples. In this regard, future work should concentrate on improving the sample efficiency of the algorithms. One option is to include priors in the learning process, e.g., meaningful relations between tactile sensing regions can be incorporated into the model through graph-like structures, e.g., Garcia-Garcia et al (2019). Another option is combining model-based and model-free techniques for grasping or developing hierarchical and multi-stage approaches. An added benefit of such approaches is that they provide better control over the grasping process and increased interpretability of the model's behaviour, which is crucial for applications in industrial or collaborative environments alongside humans. Safety is of utmost importance in such environments, and integrating tactile sensors like robotic skin (Pang et al, 2021) can help improve tasks like grasping, prevent injuries, and enable compliant robot control.
