# Placental Vessel Segmentation and Registration in Fetoscopy: Literature Review and MICCAI FetReg2021 Challenge Findings

CorpusID: 257220323 - [https://www.semanticscholar.org/paper/9774087e7ffba606d4d70f080fb29763e56bbd02](https://www.semanticscholar.org/paper/9774087e7ffba606d4d70f080fb29763e56bbd02)

Fields: Engineering, Medicine, Computer Science

## (s17) Frame Registration and Mosaicking Evaluation
(p17.0) For evaluating homographies and mosaics (Task 2), we use the evaluation metric presented by Bano et al. (2020a) in the absence of groundtruth. The metric that we referred as -frame structural similarity index measure (SSIM) aims to evaluate the consistency in the adjacent frames. A visual illustration of the -frame SSIM metric is presented in Fig. 6. Given consecutive frames and a set of − 1 homographies { 1 , 2 , ..., −1 }, we evaluate the consistency between them. The ultimate clinical goal of Figure 6: Illustration of the N-frame SSIM evaluation metric from Bano et al. (2020a) fetoscopic registration is to generate consistent, comprehensible and complete mosaics that map the placental surface and guide the surgeon. Considering adjacent frames will have a large overlap along them, we evaluate the registration consistency between pairs of non-consecutive frames frames apart that have a large overlap in the FoV and present a clear view of the placental surface. Consider a source image , a target image + , and a homography transformation → + between them, we define the consistency between these two images as:
## (s21) BioPolimi
(p21.0) The team BioPolmini from Politecnico di Milano (Italy) are Chiara Lena, Ilaria Anita Cintorrino, Gaia Romana De Paolis and Jessica Biagioli. The model proposed by BioPolimi has a ResNet50 (He et al., 2016) backbone followed by the U-Net (Ronneberger et al., 2015) decoder for segmentation. The model is trained for 700 epochs with 6-fold cross-validation, using learning rate and batch size of 10 −3 and 32, respectively. To be consistent with the FetReg Challenge baseline, training images are resized to 448 × 448 pixels. Data augmentation, consisting of random crop with size 256 × 256 pixels, random rotation (in range (−45 • , +45 • )), horizontal and vertical flip and random variation in brightness (in range (−20%, +20%)), is applied to the training data. During inference, testing images are cropped in patches of dimension 256×256 pixels. The final prediction is obtained by overlapping the prediction obtained for each patch with a stride equal to 8.
## (s41) Baseline
(p41.0) As the baseline model, we trained a U-Net (Ronneberger et al., 2015) with ResNet50 (He et al., 2016) backbone as described in Bano et al. (2020a). Softmax activation is used at the final layer. Cross-entropy loss is computed and back propagated during training. Before training, the images are first resized to 448×448 pixels. To perform data augmentation, at each iteration step, a patch of 256 × 256 pixels is extracted at a random position in the image. Each of the extracted patches is augmented by applying a random rotation in range (−45 • , +45 • ), horizontal and vertical flip, scaling with a factor in the range of (−20%, +20%) and random variation in brightness (−20%, +20%) and contrast (−10%, +10%). Segmentation results are obtained by inference using 448 × 446 pixels resized input image. The baseline model is trained for 300 epochs on the training dataset. We create 6 folds, where each fold contains 3 procedures, to preserve as much variability as possible while keeping the number of samples in each fold approximately balanced. The final model is trained on the entire dataset, splitting videos in 80% for training and 20% for validation. The data is distributed to represent the same amount of variability in both subsets.
