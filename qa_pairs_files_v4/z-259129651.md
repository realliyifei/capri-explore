# How Can Recommender Systems Benefit from Large Language Models: A Survey

CorpusID: 259129651 - [https://www.semanticscholar.org/paper/bac54736112098616f0e1c90435888ef3e119d32](https://www.semanticscholar.org/paper/bac54736112098616f0e1c90435888ef3e119d32)

Fields: Linguistics, Computer Science

## (s11) Tune LLM; Infer with CRM (Quadrant 1)
(p11.0) Existing works in quadrant 1 mainly focus on applying relatively smaller pretrained language models (e.g., BERT) to the field of news recommendation [Zhang et al., 2021a;Yu et al., 2022b] and ecommercial advertisement [Muhamed et al., 2021;Li et al., 2023e]. As discussed in Section 2.5, the primary roles of these small-scale language models are only to serve as feature encoders for semantic representation enhancement. Consequently, a conventional recommendation model (CRM) is required to make the final recommendation, with generated textual embeddings as auxiliary inputs. Additionally, the small model size makes it affordable to fully finetune the language model during the training phase. TransRec [Fu et al., 2023a] proposes layerwise adaptor tuning over BERT and ViT models to ensure the training efficiency and multi-modality enhanced representations. As shown in Figure 3, since CRM is involved and LLM is tunable, the research works in quadrant 1 could better align to the data distribution of recommender systems and thus all achieve satisfying performance. However, they only leverage small-scale language models as feature encoders, and thus the key capacities (e.g., reasoning, instruction following) of large foundation models still remain underexplored in this quadrant.
