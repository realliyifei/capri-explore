# A Primer in BERTology: What we know about how BERT works

CorpusID: 211532403 - [https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0](https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0)

Fields: Computer Science, Linguistics

## (s1) BERT embeddings
### Question: What do studies reveal about BERT's embeddings and their characteristics in representation space?

(p1.1) In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).

## (s2) What knowledge does BERT have?
### Question: What methods and findings characterize studies on knowledge encoded in BERT's weights?

(p2.0) A number of studies have looked at the types of knowledge encoded in BERT's weights. The popular approaches include fill-in-the-gap probes of BERT's MLM, analysis of self-attention weights, and probing classifiers using different BERT representations as inputs.  showed that BERT representations are hierarchical rather than linear, i.e. there is something akin to syntactic tree structure in addition to the word order information. Tenney et al. (2019b) and  also showed that BERT embeddings encode information about parts of speech, syntactic chunks and roles. However, BERT's knowledge of syntax is partial, since probing classifiers could not recover the labels of distant parent nodes in the syntactic tree .

## (s3) Syntactic knowledge
### Question: How does BERT handle syntactic competence, subject-predicate agreement, and negation according to recent studies?

(p3.2) Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. "ever") and the words that allow their use (e.g. "whether") than scope violations. The above evidence of syntactic knowledge is belied by the fact that BERT does not "understand" negation and is insensitive to malformed input. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019). This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.

## (s4) Semantic knowledge
### Question: What does research reveal about BERT's understanding of semantic roles and structures?

(p4.0) To date, more studies were devoted to BERT's knowledge of syntactic rather than semantic phenomena. However, we do have evidence from an MLM probing study that BERT has some knowledge for semantic roles (Ettinger, 2019). BERT is even able to prefer the incorrect fillers for semantic roles that are semantically related to the correct ones, to those that are unrelated (e.g. "to tip a chef" should be better than "to tip a robin", but worse than "to tip a waiter"). Tenney et al. (2019b) showed that BERT encodes information about entity types, relations, semantic roles, and proto-roles, since this information can be detected with probing classifiers.

## (s5) World knowledge
### Question: How does BERT perform in knowledge induction, extraction, and reasoning?

(p5.0) MLM component of BERT is easy to adapt for knowledge induction by filling in the blanks (e.g. "Cats like to chase [ ]"). There is at least one probing study of world knowledge in BERT (Ettinger, 2019), but the bulk of evidence comes from  (Petroni et al., 2019) numerous practitioners using BERT to extract such knowledge. Petroni et al. (2019) showed that, for some relation types, vanilla BERT is competitive with methods relying on knowledge bases (Figure 3). Davison et al. (2019) suggest that it generalizes better to unseen data. However, to retrieve BERT's knowledge we need good template sentences, and there is work on their automatic extraction and augmentation (Bouraoui et al., 2019; However, BERT cannot reason based on its world knowledge. Forbes et al. (2019) show that BERT can "guess" the affordances and properties of many objects, but does not have the information about their interactions (e.g. it "knows" that people can walk into houses, and that houses are big, but it cannot infer that houses are bigger than people.)  and  also show that the performance drops with the number of necessary inference steps. At the same time, Poerner et al. (2019) show that some of BERT's success in factoid knowledge retrieval comes from learning stereotypical character combinations, e.g. it would predict that a person with an Italian-sounding name is Italian, even when it is factually incorrect.

## (s7) Self-attention heads
### Question: What are the findings on BERT heads' roles in coreference resolution and task performance?

(p7.9) Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system,. Kovaleva et al. (2019) showed that even when attention heads specialize in tracking semantic relations, they do not necessarily contribute to BERT's performance on relevant tasks. Kovaleva et al. (2019) identified two heads of base BERT, in which self-attention maps were closely aligned with annotations of core frame semantic relations (Baker et al., 1998). Although such relations should have been instrumental to tasks such as inference, a head ablation study showed that these heads were not essential for BERT's success on GLUE tasks.

## (s8) BERT layers
### Question: What do the initial layers of BERT represent, and how does this representation evolve?

(p8.0) The first layer of BERT receives as input representations that are a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most linear word order information.  report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the index of a token, the main auxiliary verb and the sentence subject.

### Question: How do BERT's layer performances differ in handling syntactic and semantic information?

(p8.2) The prominence of syntactic information in the middle BERT layers must be related to  observation that the middle layers of Transformers are overall the best-performing and the most transferable across tasks (see Figure 5). There is conflicting evidence about syntactic chunks. Tenney et al. (2019a) conclude that "the basic syntactic information appears earlier in the network while high-level semantic features appears at the higher layers", drawing parallels between this order and the order of components in a typical NLP pipeline -from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by  find the opposite: both POS-3 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of  tagging and chunking were also performed best at the middle layers, in both BERT-base and BERTlarge.

### Question: What explains the variability in BERT's layer functionality and its impact on model performance?

(p8.3) The final layers of BERT are the most taskspecific. In pre-training, this means specificity to the MLM task, which would explain why the middle layers are more transferable . In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019). At the same time, Hao et al. (2019) report that if the weights of lower layers of the fine-tuned BERT are restored to their original values, it does not dramatically hurt the model performance. Tenney et al. (2019a) suggest that while most of syntactic information can be localized in a few layers, semantics is spread across the entire model, which would explain why certain non-trivial examples get solved incorrectly at first but correctly at higher layers. This is rather to be expected: semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166-182). But this raises the question of what stacking more Transformer layers actually achieves in BERT in terms of the spread of semantic knowledge, and whether that is beneficial. The authors' comparison between base and large BERTs shows that the overall pattern of cumulative score gains is the same, only more spread out in the large BERT.

## (s10) Pre-training BERT
### Question: How can external knowledge integration and pre-training impact BERT model performance and robustness?

(p10.9) Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (Poerner et al., 2019) and ERNIE . Alternatively, SemBERT  integrates semantic role information with BERT representations. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides. Hao et al. (2019) conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 6). However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019).

## (s11) Model architecture choices
### Question: Who conducted the most systematic study on BERT's architecture and what were the findings?

(p11.0) To date, the most systematic study of BERT architecture was performed by . They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance. They also publish their recommendations for other model parameters. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.  observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.

## (s12) Fine-tuning BERT
### Question: How does fine-tuning affect BERT's attention to linguistic patterns in GLUE tasks?

(p12.0) Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to • Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).

### Question: How does initialization affect NLP model training and reporting practices?

(p12.5) Initialization can have a dramatic effect on the training process (Petrov, 2010). However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018). Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.

## (s13) Overparametrization
### Question: How do studies suggest optimizing Transformer and BERT models for enhanced performance?

(p13.1) Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019) showed that all but a few Transformer heads could be pruned without significant losses in performance. For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers).

## (s15) Multilingual BERT
### Question: What is mBERT and how does it perform across different language processing tasks?

(p15.0) Multilingual BERT (mBERT 6 ) is a version of BERT that was trained on Wikipedia in 104 languages (110K wordpiece vocabulary). Languages with a lot of data were subsampled, and some were super-sampled using exponential smoothing. mBERT performs surprisingly well in zero-shot transfer on many tasks (Wu and Dredze, 2019;Pires et al., 2019), although not in language generation (Rönnqvist et al., 2019). The model seems to naturally learn high-quality cross-lingual word alignments (Libovický et al., 2019), with caveats for open-class parts of speech (Cao et al., 2019). Adding more languages does not seem to harm the quality of representations (Artetxe et al., 2019).

### Question: What syntactic properties and crosslingual capabilities does mBERT exhibit across languages?

(p15.3) At least some of the syntactic properties of English BERT hold for mBERT: its MLM is aware of 4 types of agreement in 26 languages (Bacon and Regier, 2019), and main auxiliary of the sentence can be detected in German and Nordic languages Rönnqvist et al. (2019). Pires et al. (2019) and Wu and Dredze (2019) hypothesize that shared word-pieces help mBERT, based on experiments where the task performance correlated with the amount of shared vocabulary between languages. However,  dispute this account, showing that bilingual BERT models are not hampered by the lack of shared vocabulary. Artetxe et al. (2019) also show crosslingual transfer is possible by swapping the model Figure 7: Language centroids of the mean-pooled mBERT representations (Libovický et al., 2019) vocabulary, without any shared word-pieces.

