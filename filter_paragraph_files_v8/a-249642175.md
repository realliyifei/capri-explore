# Multimodal Learning with Transformers: A Survey

CorpusID: 249642175 - [https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7](https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7)

Fields: Medicine, Computer Science

## (s0) INTRODUCTION
### Question: What is the primary goal of multimodal AI systems in imitating human perception?

(p0.0) The initial inspiration of Artificial Intelligence (AI) is to imitate human perception, e.g., seeing, hearing, touching, smelling. In general, a modality is often associated with a specific sensor that creates a unique communication channel, such as vision and language [1]. In humans, a fundamental mechanism in our sensory perception is the ability to leverage multiple modalities of perception data collectively in order to engage ourselves properly with the world under dynamic unconstrained circumstances, with each modality serving as a distinct information source characterized by different statistical properties. For example, an image gives the visual appearance of an "elephants playing in water" scene via thousands of pixels, whilst the corresponding text describes this moment with a sentence using discrete words. Fundamentally, a multimodal AI system needs to ingest, interpret, and reason about multimodal information sources to realize similar human level perception abilities. Multimodal learning (MML) is a general approach to building AI models that can extract and relate information from multimodal data [1].

### Question: What are the advantages and applications of Transformers in multimodal learning?

(p0.1) This survey focuses on multimodal learning with Transformers [2] (as demonstrated in Figure 1), inspired by their intrinsic advantages and scalability in modelling different modalities (e.g., language, visual, auditory) and tasks (e.g., language translation, image recognition, speech recognition) with fewer modality-specific architectural assumptions (e.g., translation invariance and local grid attention bias in vision) [3]. Concretely, the input to a Transformer could encompass one or multiple sequences of tokens, and each sequence's attribute (e.g., the modality label, the sequential order), naturally allowing for MML without architectural modification [4]. Further, learning per-modal specificity and inter-modal • This paper is accepted by IEEE TPAMI. • Peng Xu is with Tsinghua University. Xiatian Zhu is with the University of Surrey. David A. Clifton is with the University of Oxford, UK, and also with Oxford Suzhou Centre for Advanced Research, Suzhou, PRC. • Corresponding author: David A. Clifton correlation can be simply realized by controlling the input pattern of self-attention. Critically, there is a recent surge of research attempts and activities across distinct disciplines exploring the Transformer architectures, resulting in a large number of novel MML methods being developed in recent years, along with significant and diverse advances in various areas [4], [5], [6], [7], [8]. This calls for a timely review and summary of representative methods to enable researchers to understand the global picture of the MML field across related disciplines and more importantly to capture a holistic structured picture of current achievements as well as major challenges.

