# Neuron-level Interpretation of Deep NLP Models: A Survey

CorpusID: 237353268 - [https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd](https://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd)

Fields: Computer Science

## (s0) Introduction
### Question: What is the focus of research in analyzing Deep Neural Networks' opaqueness?

(p0.1) This opaqueness of Deep Neural Networks has spurred a new area of research to analyze and understand these models. A plethora of papers have * The authors contributed equally † The work was done while the author was at QCRI been written in the past five years on interpreting deep NLP models and to answer one question in particular: What knowledge is learned within representations? We term this work as the Representation Analysis.

### Question: What is the focus of Representation Analysis in NLP according to recent studies?

(p0.2) Representation Analysis thrives on post-hoc decomposability, where we analyze the embeddings to uncover linguistic (and non-linguistic) concepts * that are captured as the network is trained towards an NLP task (Adi et al., 2016;Belinkov et al., 2017a;Conneau et al., 2018;Liu et al., 2019;Tenney et al., 2019). A majority of the work on Representation Analysis has focused on a holistic view of the representations i.e. how much knowledge of a certain concept is learned within representations as a whole (See Belinkov et al. (2020a) for a survey done on this line of work). Recently, a more fine-grained neuron interpretation has started to gain attention. In addition to the holistic view of the representation, Neuron Analysis provides insight into a fundamental question: How is knowledge structured within these representations? In particular, it targets questions such as:

### Question: What are the main focuses of current neuron analysis research?

(p0.7) The work on neuron analysis has explored various directions such as: proposing novel methods to discover concept neurons (Mu and Andreas, 2020;Hennigen et al., 2020), analyzing and comparing architectures using neuron distributions (Wu et al., 2020;Suau et al., 2020;, and enabling applications of neuron analysis (Bau et al., 2019;Dai et al., 2021). In this survey, we aim to provide a broad perspective of the field with an in-depth coverage of each of these directions. We propose a matrix of seven attributes to compare various neuron analysis methods. Moreover, we discuss the open issues and promising future directions in this area.

## (s4) Corpus-based Methods
### Question: How do corpus-based methods facilitate understanding neuron roles and functions?

(p4.0) Corpus-based methods discover the role of neurons by aggregating statistics over data activations. They establish a connection between a neuron and a concept using co-occurrence between a neuron's activation values and existence of the concept in ‡ Saturated neurons have a gradient value of zero. Dead neurons have an activation value of zero.  the underlying input instances (e.g. word, phrases or the entire sentence). Corpus-based methods are global interpretation methods as they interpret the role of a neuron over a set of inputs. They can be effectively used in combination with the visualization method to reduce the search space for finding the most relevant portions of data that activates a neuron, thus significantly reducing the humanin-the-loop effort. Corpus-based methods can be broadly classified into two sets: i) the methods that take a neuron as an input and identify the concept the neuron has learned (Concept Search), ii) and others that take a concept as input and identify the neurons learning the concept (Neuron Search).

## (s7) Linear Classifiers
### Question: How does regularization type affect neuron ranking in concept learning models?

(p7.0) The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept. The regularization of the classifier directly effects the weights and therefore the ranking of neurons. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features. This translates to discovering group neurons that are jointly responsible for a concept. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.

### Question: What method mitigates neuron probe memorization in classifier analysis?

(p7.1) Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018). Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.  suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.

## (s8) Causation-based methods
### Question: What is the purpose of ablation in neural network analysis?

(p8.1) Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised). The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a). The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019). Here, the output class serves as the concept against which we want to find the salient neurons.

### Question: What do attribution-based methods in AI research help discover?

(p8.3) Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018). Dai et al. (2021) used an attribution-based method to identify salient neurons with respect to a relational fact. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (Sundararajan et al., 2017) to identify top neu-rons that express a relational fact. The work of Dai et al. (2021) shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.

## (s9) Miscellaneous Methods
### Question: What method generates sentences to reveal hidden neuron information and surpasses Concept Search?

(p9.1) Corpus Generation A large body of neuron analysis methods identify neurons with respect to pre-defined concepts and the scope of search is only limited to the corpus used to extract the activations. It is possible that a neuron represents a diverse concept which is not featured in the corpus. The Corpus Generation method addresses this problem by generating novel sentences that maximize a neuron's activations. These sentences unravel hidden information about a neuron, facilitating the annotator to better describe its role. Corpus generation has been widely explored in Computer Vision. For example, Erhan et al. (2009) used gradient ascent to generate synthetic input images that maximize the activations of a neuron. However, a gradient ascent can not be directly applied in NLP, because of the discrete inputs. Poerner et al. (2018) worked around this problem by using Gumble Softmax and showed their method to surpass Concept Search method (Kádár et al., 2017) in interpreting neurons.

### Question: What is Matrix Factorization and how is it applied in NLP?

(p9.3) Matrix Factorization Matrix Factorization (MF) method decomposes a large matrix into a product of smaller matrices of factors, where each factor represents a group of elements performing a similar function. Given a model, the activations of an input sentence form a matrix. MF can be effectively applied to decompose the activation matrix into smaller matrices of factors where each factor consists of a set of neurons that learn a concept. MF is a local interpretation method. It is commonly used in analyzing vision models (Olah et al., 2018). We could not find any research using MF on the NLP models. To the best of our knowledge, Alammar (2020) is the only blog post that introduced them in the NLP domain.

### Question: What technique groups neurons with similar activations to identify redundancy in networks?

(p9.5) Clustering Methods Clustering is another effective way to analyze groups of neurons in an unsupervised fashion. The intuition is that if a group of neurons learns a specific concept, then their activations would form a cluster. Meyes et al. (2020) used UMAP (Mclnnes et al., 2020) to project activations to a low dimensional space and performed K-means clustering to group neurons.  aimed at identifying redundant neurons in the network. They first computed correlation between neuron activation pairs and used hierarchical clustering to group them. The neurons with highly correlated behavior are clustered together and are considered redundant in the network.

### Question: What are the methods used in multi-model search for task-based neuron similarity?

(p9.7) Multi-model Search Multi-model search is based on the intuition that salient information is shared across the models trained towards a task i.e. if a concept is important for a task then all models optimized for the task should learn it. The search involves identifying neurons that behave similarly across the models. Bau et al. (2019) used Pearson correlation to compute a similarity score of each neuron of a model with respect to the neu-rons of other models. They aggregated the correlations for each neuron using several methods with the aim of highlighting different aspects of the model. More specifically, they used Max Correlation to capture concepts that emerge strongly in multiple models, Min Correlation to select neurons that are correlated with many models though they are not among the top correlated neurons, Regression Ranking to find individual neurons whose information is distributed among multiple neurons of other models, and SVCCA (Raghu et al., 2017) to capture information that may be distributed in fewer dimensions than the whole representation.

## (s10) Evaluation
### Question: What are the challenges in evaluating correctness of neuron analysis methods?

(p10.0) In this section, we survey the evaluation methods used to measure the correctness of the neuron analysis methods. Due to the absence of interpretation benchmarks, it is difficult to precisely define "correctness". Evaluation methods in interpretation mostly resonate with the underlying method to discovered salient neurons. For example visualization methods often require qualitative evaluation via human in the loop, probing methods claim correctness of their rankings using classifier accuracy as a proxy. Antverg and Belinkov (2022) highlighted this discrepancy and suggested to disentangle the analysis methodology from the evaluation framework, for example by using a principally different evaluation method compared to the underlying neuron analysis method. In the following, we summarize various evaluation methods and their usage in the literature.

## (s11) Ablation
### Question: How does ablating top versus random neurons affect model performance?

(p11.0) While ablation has been used to discover salient neurons for the model, it has also been used to evaluate the efficacy of the selected neurons. More concretely, given a ranked list of neurons (e.g. the output of the probing method), we ablate neurons in the model in the order of their importance and measure the effect on the performance. The idea is that removing the top neurons should result in a larger drop in performance compared to randomly selected neurons. Dalvi et al. (2019);  used ablation in the probing classifier to demonstrate correctness of their neuron ranking method. Similarly Bau et al. (2019) showed that ablating the most salient neurons, discovered using multi-model search, in NMT models lead to a much bigger drop in performance as opposed to removing randomly selected neurons.

## (s15) Qualitative Evaluation
### Question: How is visualization used to evaluate neurons in linguistic studies?

(p15.0) Visualization has been used as a qualitative measure to evaluate the selected neurons. For example, Dalvi et al. (2019) visualized the top neurons and showed that they focus on very specific linguistic properties. They also visualized top-k activating words for the top neurons per concept to demonstrate the efficacy of their method. Visualization can be a very effective tool to evaluate the interpretations when it works in tandem with other methods e.g. using Concept Search or Probingbased methods to reduce the search space towards only highly activating concepts or the most salient neurons for these concepts respectively.

## (s18) Lexical Concepts
### Question: What do neurons learn in different NLP and multi-modal tasks according to various studies?

(p18.1) Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment. For example "I like this movie a lot" or "the movie is incredibly good". Similarly they discovered neurons that captured "negation". Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task. For example, they discovered neurons that learn electronic items "camera, laptop, cables" and salad items "broccoli, noodles, carrots etc". Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. "law, legal" etc. They also found neurons that learn phrasal concepts. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation. They provided finer interpretation of the neurons by generating synthetic instances. For example, they showed that a "horse racing" neuron identified via concept search method was in fact a general "racing" neuron by generating novel contexts against this neuron.

## (s20) Linguistic Concepts
### Question: What distinguishes monosemous from polysemous neurons in language processing?

(p20.2) Neurons exhibit monosemous and polysemous behavior. Xin et al. (2019) found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example "chillax" a verb formed blending "chill" and "relax". concepts. Suau et al. (2020) discovered neurons that capture different senses of a word. Similarly, Bau et al. (2019) found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.

### Question: How do neurons understand syntax and semantics according to various studies?

(p20.3) Neurons capture syntactic concepts and complex semantic concepts. Lakretz et al. (2019) discovered neurons that capture subject-verb agreement within LSTM gates. Karpathy et al. (2015) also found neurons that activate within quotes and brackets capturing long-range dependency. Na et al. (2019) aligned neurons with syntactic parses to show that neurons learn syntactic phrases. Seyffarth et al. (2021) analyzed complex semantic properties underlying a given sentence.

## (s21) Salient Neurons for Models
### Question: What are the most important neurons in NLP models according to Pearson correlation studies?

(p21.1) (2019) used Pearson correlation to discover salient neurons in the network. They found neurons that learn position of a word in the sentence among the most important neurons. Other neurons found included parentheses, punctuation and conjunction neurons. Moreover, Li et al. (2016b) found that the two most salient neurons in Glove were the frequency neurons that play an important role in all predictions.

### Question: How do core-linguistic concepts affect end performance compared to unsupervised neuron ranking?

(p21.2) The question of whether core-linguistic concepts are important for the end performance has been a less explored area. Dalvi et al. (2019) compared neurons learning morphological concepts and semantic concepts with unsupervised ranking of neurons with respect to their effect on the end performance. They found that the model is more sensitive to the top neurons obtained using unsupervised ranking compared to linguistic concepts. They showed that the unsupervised ranking of neurons is dominated by position information and other closed class categories such as conjunction and punctuation which according to the ablation experiment are more critical concepts for the end performance than linguistic concepts.

## (s25) Comparing Architectures
### Question: What findings do studies reveal about neuron behavior and distribution across different AI architectures?

(p25.0) The distribution of neurons across the network has led researchers to draw interesting crossarchitectural comparisons. Wu et al. (2020) performed correlation clustering of neurons across architectures and found that different architectures may have similar representations, but their individual neurons behave differently. Hennigen et al. (2020) compared neurons in contextualized (BERT) embedding with neurons in the static embedding (fastText) and found that fastText required two neurons to capture any morphosyntactic phenomenon as opposed to BERT which required up to 35 neurons to obtain the same performance.  showed that the linguistic knowledge in BERT (auto-encoder) is highly distributed across the network as opposed to XLNet (auto-regressive) where neurons from a few layers are mainly responsible for a concept (see Figure 2). Similarly Suau et al. (2020) compared RoBERTa and GPT (auto-encoder vs. generative) models and found differences in the distribution of expert neurons.  extended the cross-architectural comparison towards fine-tuned models. They showed that after finetuning on GLUE tasks, the neurons capturing linguistic knowledge are regressed to lower layers in RoBERTa and XLNet as opposed to BERT where it is still retained at the higher layers.

## (s26) Summary of Findings
### Question: What linguistic knowledge do neurons in Deep NLP models capture and organize?

(p26.0) Below is a summary of the key findings that emerged from the work we covered in this survey. Neurons learned within Deep NLP models capture non-trivial linguistic knowledge ranging from lexical phenomenon such as morphemes, words and multi-word expressions to highly complex global phenomenon such as semantic roles and syntactic dependencies. Neuron analysis resonates with the findings of representation analysis (Belinkov et al., 2017a,b;Tenney et al., 2019;Liu et al., 2019) in demonstrating that the networks follow linguistic hierarchy. Linguistic neurons are distributed across the network based on their complexity, with lower layers focused on the lexical concepts and middle and higher layers learning global phenomenon based on long-range contextual dependencies. While the networks preserve linguistic hierarchy, many authors showed that information is not discretely preserved, but is rather distributed and redundantly present in the network. It was also shown that a small optimal subset of neurons w.r.t any concept can be extracted from a network. On another dimension, a few works showed that some concepts are localized to fewer neurons while others are distributed to a large group. Finally, some interesting cross architectural analyses were drawn based on how the neurons are distributed within their layers.

## (s28) Controlling Model's Behavior
### Question: How do neurons control model behavior by capturing specific concepts?

(p28.0) Once we have identified neurons that capture a certain concept learned in a model, these can be utilized for controlling the model's behavior w.r.t to that concept. Bau et al. (2019) identified Switch Neurons in NMT models that activate positively for the present-tense verbs and negatively for the past-tense verbs. By manipulating the values of these neurons, they were able to successfully change output translations from present to past tense during inference. The authors additionally found neurons that capture gender and number agreement concepts and manipulated them to control the system's output. Another effort along this line was carried by Suau et al. (2020) Controlling model's behavior using neurons en-ables on-the-fly manipulation of output, for example it can be used to debias the output of the model against sensitive attributes like race and gender.

## (s30) Domain Adaptation
### Question: How does neuron pruning help in domain adaptation and prevent catastrophic forgetting?

(p30.0) Identifying the salient neurons with respect to a domain can be effectively used for domain adaptation and generalization. Gu et al. (2021) proposed a domain adaptation method using neuron pruning to target the problem of catastrophic forgetting of the general domain when fine-tuning a model for a target domain. They introduced a three step adaptation process: i) rank neurons based on their importance, ii) prune the unimportant neurons from the network and retrain with student-teacher framework, iii) expand the network to its original size and fine-tune towards indomain, freezing the salient neurons and adjusting only the unimportant neurons. Using this approach helps in avoiding catastrophic forgetting of the general domain while also obtaining optimal performance on the in-domain data. 

## (s31) Compositional Explanations
### Question: How do neurons associated with concepts explain model predictions, as illustrated by Mu and Andreas (2020)?

(p31.0) Knowing the association of a neuron with a concept enables explanation of model's output. Mu and Andreas (2020) identified neurons that learn certain concepts in vision and NLP models. Using a composition of logical operators, they provided an explanation of model's prediction. Figure 3 presents an explanation using a gender-sensitive neuron. The neuron activates for contradiction when the premise contains the word man. Such explanations provide a way to generate adversarial examples that change model's predictions.

## (s32) Open Issues and Future Directions
### Question: What challenge do interpretation studies face in analyzing language models according to (p32.2)?

(p32.2) • A large number of interpretation studies rely on human-defined linguistic concepts to probe a model. It is possible that the models do not strictly adhere to the humandefined concepts and learn novel concepts about the language. This results in an incorrect or incomplete analysis. Several researchers (Michael et al., 2020; made strides in this direction by analyzing hidden structures in the input representations in an unsupervised manner. They discovered existence of novel structures not captured in the human defined categories.  also proposed BERT ConceptNet, a manual annotation of the latent concepts in BERT. Introducing similar datasets across other models enables model-centric interpretation, and is a promising research direction.

### Question: How do neuron interpretation methods like ablation study concept neurons' importance in model predictions?

(p32.3) • While a lot of work has been done on analyzing how knowledge is encoded within the learned representations, the question whether it is used by the model during prediction is a less explored area (Feder et al., 2021;Elazar et al., 2021). Ablation and knowledge attribution methods are two neuron interpretation methods that intrinsically use causal relation to select concept neurons. A few other studies evaluated the causal relation of the selected concept neurons via ablation or by clamping their activation values (Bau et al., 2019;Suau et al., 2020) and observed the change in model's prediction. However, most of the studies do not take into account the causal relation as part of the method or the evaluation of their method. The causal relation with respect to concept neurons is important to understand their importance to overall prediction and it leads way towards practical applications such as debiasing, model distillation and domain adaptation.

### Question: What factors influence the choice of neuron analysis methods in scientific studies?

(p32.5) • The neuron analysis methods vary in their theoretical foundations as well as the perspective they aim to capture with respect to a given concept. This results in a selection of neurons that may not strictly align across all methods. For example, Visualization, Neuron Search and Corpus Search discover neurons that are highly focused on a specific task (like "less" suffix or POS "TO" concepts), while Probing-based methods discover ranking of neurons that highlight grouping behavior within the neurons targeting broad concepts like POS "Nouns". Therefore, the choice of which neuron interpretation method to use is not straightforward and depends on various factors such as the nature of the concept to investigate, the availability of supervised data for the concept of interest etc. Apart from these high-level guiding principles, a thorough comparison of methods with respect to the nature of the concept of interest is needed to fully understand the strengths and weaknesses of each approach. Antverg and Belinkov (2022) is one such effort in this direction that compares three neuron interpretation methods.

