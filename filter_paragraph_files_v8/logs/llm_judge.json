{"211532403": {"(s1)": {"(p1.1)": "What do studies reveal about BERT's embeddings and their characteristics in representation space?"}, "(s2)": {"(p2.0)": "What methods and findings characterize studies on knowledge encoded in BERT's weights?"}, "(s3)": {"(p3.2)": "How does BERT handle syntactic competence, subject-predicate agreement, and negation according to recent studies?"}, "(s4)": {"(p4.0)": "What does research reveal about BERT's understanding of semantic roles and structures?"}, "(s5)": {"(p5.0)": "How does BERT perform in knowledge induction, extraction, and reasoning?"}, "(s7)": {"(p7.9)": "What are the findings on BERT heads' roles in coreference resolution and task performance?"}, "(s8)": {"(p8.0)": "What do the initial layers of BERT represent, and how does this representation evolve?", "(p8.2)": "How do BERT's layer performances differ in handling syntactic and semantic information?", "(p8.3)": "What explains the variability in BERT's layer functionality and its impact on model performance?"}, "(s10)": {"(p10.9)": "How can external knowledge integration and pre-training impact BERT model performance and robustness?"}, "(s11)": {"(p11.0)": "Who conducted the most systematic study on BERT's architecture and what were the findings?"}, "(s12)": {"(p12.0)": "How does fine-tuning affect BERT's attention to linguistic patterns in GLUE tasks?", "(p12.5)": "How does initialization affect NLP model training and reporting practices?"}, "(s13)": {"(p13.1)": "How do studies suggest optimizing Transformer and BERT models for enhanced performance?"}, "(s15)": {"(p15.0)": "What is mBERT and how does it perform across different language processing tasks?", "(p15.3)": "What syntactic properties and crosslingual capabilities does mBERT exhibit across languages?"}}, "237353268": {"(s1)": {"(p1.2)": "What is a concept and its types according to Stock (2010)?"}, "(s3)": {"(p3.0)": "What are the limitations of using visualization to understand neuron roles in deep NLP models?"}, "(s4)": {"(p4.1)": "What methods help identify concepts learned by neurons in AI research?"}, "(s5)": {"(p5.0)": "What methods do corpus-based approaches use to link concepts with neurons?"}, "(s6)": {"(p6.0)": "What are probing-based methods and how do they work in neural network interpretation?"}, "(s7)": {"(p7.0)": "How do different regularization techniques affect neuron importance in concept learning models?", "(p7.1)": "What are the limitations and solutions in the accuracy of neuron probing classifiers?"}, "(s8)": {"(p8.1)": "What is ablation in neural networks and how is it used to identify salient neurons?", "(p8.3)": "How do attribution-based methods discover causal neurons in Transformer models?", "(p8.4)": "How do attribution-based methods and neuron analysis improve understanding of learned concepts in AI models?"}, "(s9)": {"(p9.1)": "How does corpus generation help in understanding neuron activations in NLP models?", "(p9.3)": "What is Matrix Factorization and its application in analyzing vision and NLP models?", "(p9.5)": "How do clustering methods analyze and identify redundant neurons in neural networks?"}, "(s15)": {"(p15.0)": "How is visualization utilized to evaluate neurons in linguistic models?"}, "(s18)": {"(p18.1)": "What do studies reveal about neurons' roles in processing language and concepts in AI models?"}, "(s20)": {"(p20.2)": "How do neurons exhibit varying roles in language processing according to recent studies?", "(p20.3)": "How do neurons process syntax and semantics according to various studies?"}, "(s23)": {"(p23.0)": "How do pre-trained language models reflect the hierarchical structure of human languages?"}, "(s24)": {"(p24.0)": "How do training choices like dropout affect information distribution in neural networks?"}, "(s25)": {"(p25.0)": "How do neural network architectures differ in neuron behavior and representation distribution?"}, "(s26)": {"(p26.0)": "What do neurons learn in Deep NLP models and how is linguistic information structured?"}, "(s28)": {"(p28.0)": "How can identifying and manipulating specific neurons control a model's output, such as tense and bias?"}, "(s31)": {"(p31.0)": "How do neurons associated with concepts explain model predictions and generate adversarial examples?"}}, "258331833": {"(s3)": {"(p3.0)": "What motivates unsupervised learning of natural language and how do Masked Language Models work?"}, "(s4)": {"(p4.0)": "How do autoregressive language models enhance few-shot and zero-shot learning performance?", "(p4.1)": "What are examples and advancements in autoregressive language models?"}, "(s7)": {"(p7.0)": "How does pre-training data influence large language model performance and capabilities?"}, "(s8)": {"(p8.0)": "What models are best for zero, few, and abundant annotated data scenarios?"}, "(s9)": {"(p9.0)": "What challenges do LLMs face when applied to real-world downstream tasks?"}, "(s13)": {"(p13.2)": "What is the effectiveness of different models in detecting online toxicity, including CivilComments and Perspective API 3?", "(p13.4)": "Why are LLMs not widely used in information retrieval tasks?", "(p13.8)": "What are the challenges and insights in adapting language models to NLP tasks?", "(p13.10)": "What are examples of tasks showing LLMs' superior generalization in NLP?"}, "(s15)": {"(p15.4)": "How do LLMs compare in machine translation performance to commercial tools and in low-resource languages?", "(p15.6)": "What are the capabilities of LLMs in generating content and coding?"}, "(s18)": {"(p18.3)": "What makes LLMs excel in closed-book question-answering tasks across various datasets?", "(p18.4)": "Why is machine reading comprehension considered a knowledge-intensive but manageable task for MMLU?"}, "(s19)": {"(p19.0)": "How does scaling impact the capabilities and performance of large language models?"}, "(s22)": {"(p22.0)": "What are emergent abilities in large-scale models, and why are they significant?", "(p22.2)": "What emergent abilities do large language models like GPT-3 and PaLM exhibit?"}, "(s23)": {"(p23.3)": "What explains the behavior of large language models scaling, including emergent abilities and U-shape phenomenon?"}, "(s26)": {"(p26.1)": "Why do LLMs underperform in regression and multimodal tasks despite their NLP success?"}, "(s27)": {"(p27.3)": "How do LLMs perform in NLG task quality assessment compared to traditional metrics?"}, "(s29)": {"(p29.2)": "What methods enhance LLMs' abilities to understand instructions and generate better responses?"}, "(s32)": {"(p32.2)": "What are the computational and financial costs of large model training and using OpenAI's API?", "(p32.4)": "What is Parameter-Efficient Tuning and its common techniques in model optimization?"}, "(s33)": {"(p33.2)": "What affects the robustness and calibration of fine-tuned models, and how can these be enhanced?", "(p33.3)": "What challenges do LLMs face in terms of fairness and bias, and how can they be addressed?", "(p33.4)": "How do LLMs compare to fine-tuned models in managing shortcut learning in NLU tasks?"}, "(s34)": {"(p34.3)": "What are the risks and necessary safeguards associated with using Large Language Models (LLMs)?", "(p34.4)": "What privacy issues have LLMs faced, illustrated by Samsung and OpenAI incidents?"}}, "254408864": {"(s7)": {"(p7.0)": "How does decomposition simplify multi-hop MRC into single-hop problems?", "(p7.1)": "What is the self-assembling neural modular network for multi-hop reasoning described by Jiang and Bansal?"}, "(s11)": {"(p11.0)": "What are state-based reasoning models in multi-hop MRC and their key features?"}, "(s15)": {"(p15.2)": "What approach does PathNet use for multi-hop MRC passage representation?"}, "(s16)": {"(p16.1)": "How does the SMR approach utilize sentence-level multi-hop reasoning for text comprehension?", "(p16.2)": "What are the phases and methods in ChainEx's sentence-based model for multi-hop reasoning tasks?"}, "(s17)": {"(p17.0)": "Why have graph-based techniques become attractive in multihop machine reading comprehension (MRC)?"}, "(s19)": {"(p19.0)": "What model did Song et al. propose for enhancing multi-hop reading comprehension's global context inference?"}, "(s22)": {"(p22.0)": "What is the structure and purpose of the Heterogeneous Document-Entity graph proposed by Tu et al.?", "(p22.2)": "What does the SAE system by Tu et al. propose for enhancing model interpretability in multi-hop reasoning?"}, "(s24)": {"(p24.1)": "What findings did studies by Shao et al. and Yuntao et al. reveal about graph structure and document filters in multi-hop question answering?"}, "(s29)": {"(p29.0)": "How are model performances on HotpotQA evaluated using EM, F1, and various metrics sets?"}}, "237571793": {"(s1)": {"(p1.2)": "What solution improves the tree-like architecture's expressive power for tasks in models?", "(p1.5)": "How do models control information flow in multi-task learning to reduce inter-task interference?", "(p1.8)": "How does task routing contribute to feature fusion in neural network models?"}, "(s2)": {"(p2.0)": "How do models in parallel architecture manage tasks across different feature abstraction levels?", "(p2.1)": "How can MTL's performance be enhanced by incorporating auxiliary tasks at various levels?"}, "(s4)": {"(p4.1)": "What principle underlies the hierarchical feature pipeline in multi-task models?", "(p4.2)": "How do hierarchical feature pipelines enhance multi-task learning in natural language processing tasks?", "(p4.4)": "How are outputs in hierarchical signal pipelines used to enhance task performance?"}, "(s6)": {"(p6.1)": "What are common practices and examples of modular architectures in multi-task and multi-lingual tasks?"}, "(s7)": {"(p7.0)": "How do GANs improve performance in computer vision and MTL for NLP tasks?", "(p7.1)": "How do generative adversarial architectures enhance document representation and machine reading comprehension?"}, "(s9)": {"(p9.10)": "How does GradVac optimize multi-lingual model performance through gradient manipulation?"}, "(s11)": {"(p11.0)": "How does task scheduling influence multi-task learning model training methods?", "(p11.5)": "How do models learn multiple tasks sequentially while addressing dependency and difficulty levels?", "(p11.6)": "What is the pre-train then fine-tune methodology used in auxiliary MTL?"}, "(s13)": {"(p13.1)": "How do recent studies incorporate auxiliary tasks to enhance performance in various NLP challenges?"}, "(s14)": {"(p14.0)": "How is auxiliary multi-task learning applied to various classification and detection tasks?", "(p14.2)": "How does multi-task learning (MTL) enhance various text generation tasks?"}, "(s15)": {"(p15.0)": "How do joint multi-task learning models differ from and relate to auxiliary MTL in performance optimization?", "(p15.2)": "What are applications and benefits of joint MTL in multi-domain and multi-formalism NLP tasks?"}, "(s16)": {"(p16.0)": "How do multi-lingual machine learning models benefit from multi-task learning and adversarial training?", "(p16.1)": "What are the goals and outcomes of employing multi-lingual MTL in language translation and evaluation?", "(p16.2)": "How do multi-lingual representation models facilitate cross-lingual knowledge transfer and enhance performance?"}, "(s17)": {"(p17.0)": "What does multimodal learning involve in NLP research and its application in speech translation?"}, "(s18)": {"(p18.0)": "What factors influence the performance of multi-task learning in NLP?", "(p18.1)": "How does Multi-Task Learning (MTL) effectiveness relate to task relatedness and selection?"}, "(s21)": {"(p21.1)": "How can multi-label datasets be created from existing data with examples?", "(p21.2)": "How can self-supervised multi-label datasets be automatically created for various AI tasks?"}}, "231603122": {"(s2)": {"(p2.1)": "What is the focus and methodology of this literature review on persuasion and NLG?"}, "(s5)": {"(p5.1)": "What are the determinants of linguistic appropriacy in persuasive AI messaging?"}, "(s10)": {"(p10.0)": "What are the tools and datasets used for persuasion analysis in NLP/NLG studies?"}}, "249642175": {"(s1)": {"(p1.0)": "What sets this survey apart in discussing multimodal learning and Transformers?"}, "(s4)": {"(p4.2)": "What are the applications and advancements of Vision Transformer in computer vision and multimodal tasks?"}, "(s10)": {"(p10.0)": "How do position embeddings enhance Transformer models in processing different data structures?"}, "(s18)": {"(p18.0)": "How do users prepare multimodal inputs for Transformers through tokenization and embedding?"}, "(s21)": {"(p21.12)": "How does VideoBERT fuse video and text modalities, and what are its limitations?"}, "(s29)": {"(p29.0)": "What are common pretext tasks in Transformer based multimodal pretraining?"}, "(s30)": {"(p30.0)": "What are the current bottlenecks in multimodal pretraining Transformer methods?"}, "(s31)": {"(p31.0)": "Why is task-specific pretraining often chosen over universal in multimodal Transformers?"}, "(s35)": {"(p35.0)": "How do MML Transformers integrate multimodal information, and what are the trends in their fusion methods?"}, "(s36)": {"(p36.1)": "What challenges and solutions exist for fine-grained alignment in multimodal learning models?"}, "(s37)": {"(p37.4)": "What are the key challenges and solutions in transferring multimodal pretrained models to real-world applications?"}, "(s41)": {"(p41.0)": "How do Transformers excel in multimodal learning according to recent studies?"}, "(s42)": {"(p42.0)": "What are the challenges and potentials in designing universal MML models for diverse tasks?", "(p42.1)": "What strategies and challenges exist in achieving fine-grained MML through latent semantic alignments?"}}, "233481730": {"(s2)": {"(p2.0)": "What are the core components and their functions in transformer-based PLMs like BERT and RoBERTa?"}, "(s10)": {"(p10.1)": "What are the characteristics and categories of self-supervised learning in AI?"}, "(s13)": {"(p13.1)": "What is Continual Pretraining in biomedical NLP research and how is it implemented?"}, "(s14)": {"(p14.0)": "What is the main drawback of continual pretraining in natural language processing models?"}, "(s16)": {"(p16.0)": "What are pretraining tasks in language models and their classifications?"}, "(s19)": {"(p19.0)": "How do auxiliary pretraining tasks utilize human-curated sources like UMLS to improve in-domain models?"}, "(s21)": {"(p21.0)": "What are the methods and benefits of Incremental Fine-Tuning (IFT) on datasets?"}, "(s22)": {"(p22.0)": "What are the benefits and limitations of multi-task fine-tuning in model training?"}, "(s24)": {"(p24.1)": "What are character embeddings, their advantages, and limitations in model pretraining?", "(p24.2)": "What are subword embeddings and how are their vocabularies generated?", "(p24.5)": "How does SentencePiece address BPE and WordPiece's space assumption issue in different languages?", "(p24.7)": "What are code embeddings and their application in models like BERT-EHR, MedBERT, and BEHRT?"}, "(s25)": {"(p25.0)": "What is the purpose and types of auxiliary embeddings in model training?"}, "(s29)": {"(p29.0)": "What is the RadCore dataset and its contribution to radiology report analysis?"}, "(s30)": {"(p30.0)": "How have social media platform health discussions influenced health-related research and technology?"}, "(s31)": {"(p31.0)": "Why has biomedical text mining become increasingly important in recent research?", "(p31.1)": "How are biomedical pre-trained language models developed from general BERT models?"}, "(s32)": {"(p32.0)": "What challenges exist in collecting in-domain text for pretraining transformer-based PLMs?"}, "(s34)": {"(p34.0)": "How have researchers expanded T-BPLMs to non-English languages following BioBERT's success?"}, "(s36)": {"(p36.0)": "How do CPT and DSPT/SPT adapt T-PLMs to in-domain tasks, and what are their limitations?", "(p36.1)": "What are the features and benefits of GreenBioBERT and exBERT in biomedical research?"}, "(s40)": {"(p40.0)": "What is Natural Language Inference and how is it applied in NLP and biomedical domains?"}, "(s41)": {"(p41.1)": "How have advancements in BERT models impacted entity extraction techniques and performance?"}, "(s42)": {"(p42.0)": "What distinguishes Semantic Textual Similarity from Natural Language Inference in evaluating sentence semantics?", "(p42.1)": "How have recent advancements improved clinical STS task performance?"}, "(s43)": {"(p43.0)": "What is the role of relation extraction in transforming unstructured text and its advancements?"}, "(s44)": {"(p44.1)": "How do modifications enhance BERT models for clinical text classification tasks?"}, "(s45)": {"(p45.0)": "What are key challenges and advancements in biomedical question answering systems development?"}, "(s46)": {"(p46.0)": "Why is automatic biomedical text summarization important for researchers?", "(p46.2)": "How do BioBERT, BioBERTSum, and AlphaBERT contribute to biomedical text summarization?"}, "(s53)": {"(p53.0)": "How can ontology knowledge injection enhance BioBERT and PubMedBERT models for better results?"}, "(s54)": {"(p54.1)": "What is multi-task fine-tuning and why is it beneficial in the biomedical domain?", "(p54.2)": "What are the top techniques and examples of data augmentation in machine learning?"}, "(s55)": {"(p55.0)": "How do transformed-based PLMs perform on noisy instances, and what are proposed solutions?"}, "(s56)": {"(p56.0)": "How can T-PLMs adapt to in-domain text while effectively representing in-domain words?"}, "(s58)": {"(p58.0)": "What method did Devlin et al. suggest for sequence representation in classification tasks?"}, "(s60)": {"(p60.0)": "What challenges do deep learning models face in ensuring fair decisions across diverse groups?"}, "(s61)": {"(p61.0)": "What are the risks and solutions related to data leakage in biomedical language models?"}, "(s62)": {"(p62.0)": "What are the challenges and solutions in adapting PLMs for the biomedical domain?"}, "(s64)": {"(p64.0)": "Why are benchmarks necessary for evaluating pre-trained language models in NLP tasks?"}, "(s65)": {"(p65.0)": "What do intrinsic probes reveal about PLMs during pretraining in NLP research?"}, "(s66)": {"(p66.0)": "How do ConvBERT and DeBERTa improve efficiency in NLP pretraining compared to traditional models?"}}}