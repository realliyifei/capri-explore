# Multi-Task Learning in Natural Language Processing: An Overview

CorpusID: 237571793
 
tags: #Linguistics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a](https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

Multi-Task Learning in Natural Language Processing: An Overview
2021. September 2021

Shijie Chen 
Y U Zhang 
Shijie Chen 
Yu Zhang 
Qiang Yang 

QIANG YANG
Southern University of Science and Technology and Peng Cheng Laboratory
The Ohio State University
HongUSA, China


Kong University of Science and Technology
China

Multi-Task Learning in Natural Language Processing: An Overview
11302021. September 202110.1145/nnnnnnn.nnnnnnn
Deep learning approaches have achieved great success in the field of Natural Language Processing (NLP). However, deep neural models often suffer from overfitting and data scarcity problems that are pervasive in NLP tasks. In recent years, Multi-Task Learning (MTL), which can leverage useful information of related tasks to achieve simultaneous performance improvement on multiple related tasks, has been used to handle these problems. In this paper, we give an overview of the use of MTL in NLP tasks. We first review MTL architectures used in NLP tasks and categorize them into four classes, including the parallel architecture, hierarchical architecture, modular architecture, and generative adversarial architecture. Then we present optimization techniques on loss construction, data sampling, and task scheduling to properly train a multi-task model. After presenting applications of MTL in a variety of NLP tasks, we introduce some benchmark datasets. Finally, we make a conclusion and discuss several possible research directions in this field.ACM Reference Format:domains or languages as multiple tasks, which is also known as multi-domain learning[140]in some literature, and learn an MTL model from them.MTL naturally aggregates training samples from datasets of multiple tasks. The benefit is escalated when unsupervised or self-supervised tasks, such as language modeling, are included. This is especially meaningful for low-resource tasks and languages whose labeled dataset is sometimes too small to sufficiently train a model. In most cases, the enlarged training dataset alleviates the risk of the overfitting and leads to more robust models. From this perspective, MTL acts similarly to data augmentation techniques[37]. However, MTL provides additional performance gain compared to data augmentation approaches, due to the learned shared knowledge.While the thirst for better performance has driven people to build increasingly large models, developing more compact and efficient models with competitive performance has also received a growing interest in recent years. Through implicit knowledge sharing during the training process, MTL models could match or even exceed the performance of their single-task counterparts using much less training samples[27,108]. Besides, multi-task adapters[94,113]transfer large pre-trained models to new tasks and languages by adding a modest amount of task-specific weights. In this way, the costly fine-tuning of the entire model is avoided, which is important for realworld applications such as mobile computing and latency-sensitive services. Many NLP models leverage additional features, including hand-crafted features and output of automatic NLP tools. Through MTL on different linguistic tasks, such as chunking, Part-Of-Speech (POS) tagging, Named Entity Recognition (NER), and dependency parsing, we can reduce the reliance on external knowledge and prevent error propagation, which results in simpler models with potentially better performance[72,102,111,154].This paper reviews the use of MTL in recent NLP research. We focus on the ways in which researchers apply MTL to NLP tasks, including model architecture, training process, and data source. While most pre-trained language models take advantage of MTL during pre-training, they are not designed for specific down-stream tasks and thus they are not in the focus of this paper. Depending on the objective of applying MTL, we denote by auxiliary MTL the case where auxiliary tasks are introduced to improve the performance of one or more primary tasks and by joint MTL the case where multiple tasks are equally important. In this paper, we first introduce popular MTL architectures used in NLP tasks and categorize them into four classes, including the parallel architecture, hierarchical architecture, modular architecture, and generative adversarial architecture. Then we review optimization techniques of MTL for NLP tasks in terms of the loss construction, data sampling, and task scheduling. After that, we present applications of MTL, which include auxiliary MTL and joint MTL as two main classes, in a variety of NLP tasks, and introduce some MTL benchmark datasets used in NLP. Finally, we conclude the whole paper and discuss several possible research topics in this field.The rest of this paper is organized as follows. Section 2 presents the architecture of MTL models in NLP and Section 3 presents techniques for training MTL models. The applications of MTL in different NLP tasks are summarized in Section 4. At last, we introduce MTL benchmark datasets for the NLP field in Section 5 before making a conclusion in Section 6. Notations. In this paper, we use lowercase letters, such as , to denote scalers and use lowercase letters in boldface, such as x, to denote vectors. Uppercase letters, such as and , are used for constants and uppercase letters in boldface are used to represent matrices, including feature matrices like X and weight matrices like W. In general, a multi-task learning model, parametrized by , handles tasks on a dataset D with a loss function L.MTL ARCHITECTURES FOR NLP TASKSMost research literatures focus on the design of MTL architectures for NLP tasks. Based on how the relatedness between tasks are utilized, we categorize MTL architectures into the following classes: parallel architecture, hierarchical architecture, modular architecture, and generative adversarial architecture. The parallel architecture shares the bulk of the model among multiple tasks while each task has its own task-specific output layer. The

# INTRODUCTION

In recent years, data-driven neural models have achieved great success in machine learning problems. In the field of Natural Language Processing (NLP), the introduction of transformers [120] and pre-trained language models such as BERT [25] has lead to a huge leap of the performance in multiple downstream tasks. However, such data-driven models usually require a large amount of labeled training samples, which is often expensive for NLP tasks where linguistic knowledge is expected from annotators. Training deep neural networks on a large dataset also asks for immense computing power as well as huge time and storage budget. To further improve the model performance, combat the data scarcity problem, and facilitate cost-efficient machine learning, researchers have adopted Multi-Task Learning (MTL) [7,150] for NLP tasks.

MTL trains machine learning models from multiple related tasks simultaneously or enhances the model for a specific task using auxiliary tasks. Learning from multiple tasks makes it possible for learning models to capture generalized and complementary knowledge from the tasks at hand besides task-specific features. Tasks in MTL can be tasks with assumed relatedness [20,23,40,56,121], tasks with different styles of supervision (e.g., supervised and unsupervised tasks [41,64,73]), tasks with different types of goals (e.g., classification and generation [85]), tasks with different levels of features (e.g., token-level and sentence-level features [57,109]), and even tasks in different modalities (e.g., text and image data [66,115]). Alternatively, we can treat the same task in multiple hierarchical architecture models the hierarchical relationships between tasks. Such architecture can hierarchically combine features from different tasks, take the output of one task as the input of another task, or explicitly model the interaction between tasks. The modular architecture decomposes the whole model into shared components and task-specific components that learn task-invariant and task-specific features, respectively. Different from the above three architectures, the generative adversarial architecture borrows the idea of the generative adversarial network [35] to improve capabilities of existing models. Note that the boundaries between different categories are not always solid and hence a specific model may fit into multiple classes. Still, we believe that this taxonomy could illustrate important ideas behind the design of MTL architectures.

Before introducing MTL architectures, we would like to clarify the definitions of hard and soft parameter sharing. In this paper, hard parameter sharing refers to sharing the same model parameters among multiple tasks, and it is the most widely used approach in multi-task learning models. Soft parameter sharing, on the other hand, constrains a distance metric between the intended parameters, such as the Euclidean distance [38] and correlation matrix penalty [41], to force certain parameters of models for different tasks to be similar. Alternatively, [58] adds a regularization term to ensure the outputs of encoders of each task to be close for similar input instances. Differently, some researchers use hard parameter sharing to indicate a multi-task learning model that shares all the hidden layers except the final task-specific output layers and use soft parameter sharing to describe a multi-task model that partially shares hidden layers [22] such as embedding layers and low-level encoders. In this paper, such models fall into the 'parallel architecture' category.


## Parallel Architectures

As its name suggests, the model for each task run in parallel under the parallel architecture, which is implemented by sharing certain intermediate layers. In this case, there is no dependency other than layer sharing among tasks. Therefore, there is no constraint on the order of training samples from each task. During training, the shared parameters receive gradients from samples of each task, enabling knowledge sharing among tasks. Fig. 1 illustrates different forms of parallel architectures.

2.1.1 Vanilla Tree-like Architectures. The simplest form of parallel architecture is the tree-like architecture (refer to Fig. 1a), where the models for different tasks share a base feature extractor (i.e., the trunk) followed by task-specific encoders and output layers (i.e., the branches). A shallow trunk can be simply the word representation layer [108] while a deep trunk can be the entire model except output layers. The tree-like architecture was proposed by [7] and has been used in [2, 3, 5, 8, 9, 12, 15, 21, 29, 30, 38, 40, 43, 51, 53, 54, 60, 67, 69, 71-73, 79, 84, 85, 90, 91, 96, 106, 107, 111, 116, 121, 125-127, 130, 131, 141, 143, 149, 151-153]. Such tree-like architecture is also known as hard sharing architecture or multi-head architecture, where each head corresponds to the combination of a task-specific encoder and the corresponding output layer or just a branch, in some literatures.

The tree-like architecture uses a single trunk to force all tasks to share the same low-level feature representation, which may limit the expressive power of the model for each task. A solution is to equip the shared trunk with task-specific encoders [47,58,137]. For example, [65] combines a shared character embedding layer and languagespecific word embedding layers for different languages. Another way is to make different groups of tasks share different parts of the trunk [37,79,89]. Moreover, this idea can be applied to the decoder. For instance, [128] shares the trunk encoder with a source-side language model and shares the decoder with a target-side denoising autoencoder.

2.1.2 Parallel Feature Fusion. Different from learning shared features implicitly by sharing model parameters in the trunk, MTL models can actively combine features from different tasks, including shared and task-specific features, to form representations for each task. As shown in Fig. 1b, such models can use a globally shared encoder represents the corresponding label (ℎ are shared latent representations). The green blocks represent shared parameters and the orange blocks are task-specific parameters. Red circles represent feature fusion mechanism .

to produce shared representations that can be used as additional features for each task-specific model [69]. The shared representations can also be used indirectly as the key for attention layers in each task-specific model [118].

However, different parts of the shared features are not equally important to each task. To selectively choose features from the shared features and alleviate inter-task interference, several models have been devised to control information flow between tasks. For example, [69] adapts LSTMs to the MTL setting by adding a global memory module and pairwise local shared memory modules, and fuses shared features into hidden states of each LSTM by a read gate and a write gate. Similarly, GIRN [39] interleaves hidden states of LSTMs and [148] extends this idea to use pairwise coupling and local fusion layers with a global fusion layer to share information between tasks. The sifted multi-task learning method [133] filters useless features by per-task gating and selects useful information from the shared memory by a multi-head attention mechanism. The gated shared feature G and attention result A are combined, as in [82], to form the entire shared feature representation S = [G; |G − A|; G ⊙ A; A] for each task, where ⊙ represents the element-wise multiplication and | · | compute the absolute value in an element-wise manner. After that, S is concatenated with task-specific representations to form the input to the output layer.

Some models directly integrate features from different tasks. A straightforward solution is to compute a weighted average of feature representations. Here the weights can be computed separately according to the input [61] or by an attention mechanism as in [153] that learns task representations as query keys in attention modules. In addition to weighted average, we can aggregate features via more sophisticated mechanisms. For example, based on the cross-stitch network [81], [22] proposes a gated network where shared and task-specific features are combined by a gating mechanism. A similar gating mechanism is used in [56] to combine features from the primary and the auxiliary task. The SLUICE network [100] learns a task relatedness matrix that controls the range and extent of parameter sharing between tasks. While the relatedness matrix in the SLUICE network is fixed, a leaky unit is proposed in [135] to dynamically control pairwise feature flow based on input features, and similar to RNN cells, it modulates information flow by two gates. Specifically, given two tasks and , the leaky gate r determines how much knowledge should be transferred from task to task and then it emits a feature maph . The update gate z determines how much information should be maintained from task and then it emits final outputh for task . Mathematically, the two gates can be formulated as
r = (W · [h , h ]) h = tanh(U · h + W · (r ⊙ h )) z = (W · [h , h ]) h = z · h + (1 − z ) ·h ,
where (·) denotes the sigmoid function and tanh(·) denotes the hyperbolic tangent function. When considering all pairwise directions, the output for each task is given by the sum of each row in
         =1 z 1 (1 − z 12 ) · · · (1 − z 1 ) (1 − z 21 ) =1 z 2 · 1 − z 2 ) . . . . . . . . . . . . (1 − z 1 ) (1 − z 2 ) · · · =1 z          ·          h 1 h 12 · · · h 1 h 21 h 2 · · ·h 2 . . . . . . . . . . . . h 1h 2 · · ·h          / .
Task routing is another way to achieve feature fusion, where the paths that samples go through in the model differ by their task. Given tasks, the routing network [144] splits RNN cells into several shared blocks with task-specific blocks (one for each task) and then modulates the input to as well as output from each RNN block by a learned weight. MCapsNet [136], which brings CapsNet [101] to NLP tasks, replaces dynamic routing in CapsNet with task routing to build different feature spaces for each task. In MCapsNet, similar to dynamic routing, task routing computes task coupling coefficients ( ) for capsule in the current layer and capsule in the next layer for task .


### Supervision at Different

Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.


## Hierarchical Architectures

The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.

2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.


### Hierarchical

Pipeline. Instead of aggregating features from different tasks as in feature fusion architectures, pipeline architectures treat the output of a task as an extra input of another task and form a hierarchical pipeline between tasks. The extra input can be used directly as input features or used indirectly as control signals to enhance the performance of other tasks. In this section, we refer to output as the final result from a task, including the final output distribution and hidden states before the last output layer. We further divide hierarchical pipeline architectures into hierarchical feature pipeline and hierarchical signal pipeline.

In hierarchical feature pipeline, the output of one task is used as extra features for another task. The tasks are assumed to be directly related so that outputs instead of hidden feature representations are helpful to other tasks. For example, [14] feeds the output of a question-review pair recognition model to the question answering model. [44] feeds the output of aspect term extraction to aspect-term sentiment classification. Targeting community question answering, [139] uses the result of question category prediction to enhance document representations. [110] feeds the result of morphological tagging to a POS tagging model and the two models are further tied by skip connections. To enable asynchronous training of multi-task models, [152] initializes input from other tasks by a uniform distribution across labels.

Hierarchical feature pipeline is especially useful for tasks with hierarchical relationships. [30] uses the output of neighboring word semantic type prediction as extra features for neighboring word prediction. [43] uses skip connections to forward predictions of lower-level POS tagging, chunking, and dependency parsing tasks to higherlevel entailment and relatedness classification tasks. In addition, deep cascade MTL [33] adds both residual connections and cascade connections to a single-trunk parallel MTL model with supervision at different levels, where residual connections forward hidden representations and cascade connections forward output distributions of a task to the prediction layer of another task. [112] includes the output of the low-level discourse element identification task in the organization grid, which consists of sentence-level, phrase-level, and document-level features of an essay, for the primary essay organization evaluation task. In [107], the word predominant sense prediction task and the text categorization task share a transformer-based embedding layer and embeddings of certain words in the text categorization task could be replaced by prediction results of the predominant sense prediction task.

The direction of hierarchical pipelines is not necessarily always from low-level tasks to high-level tasks. For example, in [1], the outputs of word-level tasks are fed to the char-level primary task. [99] feeds the output of more general classification models to more specific classification models during training, and the more general classification results are used to optimize beam search of more specific models at test time.

In hierarchical signal pipeline, the outputs of tasks are used indirectly as external signals to help improve the performance of other tasks. For example, the predicted probability of the sentence extraction task is used to weigh sentence embeddings for a document-level classification task [50]. For the hashtag segmentation task, [74] first predicts the probability of a hashtag being single-token or multi-token as an auxiliary task and further uses the output to combine single-token and multi-token features. In [106], the output of an auxiliary entity type prediction task is used to disambiguate candidate entities for logical form prediction. The outputs of a task can also be used for post-processing. For instance, [145] uses the output of NER to help extract multi-token entities.


### Hierarchical

Interactive Architecture. Different from most machine learning models that give predictions in a single pass, hierarchical interactive MTL explicitly models the interactions between tasks via a multi-turn prediction mechanism which allows a model to refine its predictions over multiple steps with the help of the previous outputs from other tasks in a way similar to recurrent neural networks. [44] maintains a shared latent representation which is updated by iterations. Multi-step attention network [51] refines its prediction by attending to input representations in previous steps. In cyclic MTL [146], the output of one task is used as an extra input to its successive lower-level task and the output of the last task is fed to the first one, forming a loop. Most hierarchical interactive MTL models as introduced above report that performance converges quickly at = 2 steps, showing the benefit and efficiency of doing multi-step prediction.


## Modular Architectures

The idea behind the modular MTL architecture is simple: breaking an MTL model into shared modules and task-specific modules. The shared modules learn shared features from multiple tasks. Since the shared modules can learn from many tasks, they can be sufficiently trained and can generalize better, which is particularly meaningful for low-resource scenarios. On the other hand, task-specific modules learn features that are specific to a certain task. Compared with shared modules, task-specific modules are usually much smaller and thus less likely to suffer from overfitting caused by insufficient training data.

The simplest form of modular architectures is a single shared module coupled with task-specific modules as in tree-like architectures described in Section 2.1.1. Besides, another common practice is to share the first embedding layers across tasks as [58,156] did. [1] shares word and character embedding matrices and combines them differently for different tasks. [103] shares two encoding layers and a vocabulary lookup table between the primary neural machine translation task and the Relevance-based Auxiliary Task (RAT). Modular designs are also widely used in multi-lingual tasks. Unicoder [49] shares vocabulary and the entire transformer architecture among different languages and tasks. Shared embeddings can be used alongside task-specific embeddings [59,138] as well. In addition to word embeddings, [147] shares label embeddings between tasks. Researchers have also developed modular architectures at a finer granularity. For example, [119] splits the model into task-specific encoders and language-specific encoders for multi-lingual dialogue evaluation. In [24], each task has its own encoder and decoder, while all tasks share a representation learning layer and a joint encoding layer. [92] creates encoder modules on different levels, including task level, task group level, and universal level.

When adapting large pre-trained models to down-stream tasks, a common practice is to fine-tune a separate model for each task. While this approach usually attains good performance, it poses heavy computational and storage costs. A more cost-efficient way is to add task-specific modules into a single shared pre-trained model. As an example, multi-task adapters adapt single-task models to multiple tasks by adding extra task-specific parameters (adapters). [113] adds task-specific Projected Attention Layers (PALs) in parallel with self-attention operatioins in a pre-trained BERT model. Here PALs in different layers share the same parameters to reduce model capacity and improve training speed. In Multiple ADapters for Cross-lingual transfer (MAD-X) [94], the model is decomposed into PALs (a) Bert and PALs [113] Embd Inv Adapt Enc Lang Adapt four types of adapters: language adapters, task adapters, invertible adapters, and its counterpart inversed adapters, where language adapters learn language-specific task-invariant features, task adapters learn language-invariant task-specific features, invertible adapters conversely map input embeddings from different tasks into a shared feature space, and inversed adapters map hidden states into domain-specific embeddings. MAD-X can perform quick domain adaptation by directly switching corresponding language and task adapters instead of training new models from the scratch. Further, task adaptation modules can also be dynamically generated by a meta-network. Hypergrid transformer [117] scales the weight matrix of the second feed forward layer in each transformer block by the multiplication of two vectors as
Task Adapt Embd Inv Adapt -1 (b) MAD-X [94]H(x) = ( ((L · x) (L · x))) ⊙ W,
where L and L are either globally shared task feature vectors or local instance-wise feature vectors, is a scaling operation, x is an input vector, and W is a learnable weight matrix. Differently, Conditionally Adaptive MTL (CA-MTL) [95] implements task adaptors in the self-attention operation of each transformer block based on task representations {z } as
Attention (Q, K, V, z ) = softmax M (z ) + QK √ V where M(z ) = diag(A ′ 1 (z ), . . . , A ′ (z )
) is a diagonal block matrix consisting of learnable linear transformations over z . Therefore, M(z ) injects task-specific bias into the attention map in the self-attention mechanism. Similar adaptation operations are used in the input alignment and layer normalization as well. Impressively, a single jointly trained Hypergrid transformer or CA-MTL model could match or outperform single-task fine-tuned models on multi-task benchmark datasets while only adding a negligible amount of parameters.


## Generative Adversarial Architectures

Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.
L = min max ∑︁ =1 | D | ∑︁ =1 [ ( (X))],,
An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.


# OPTIMIZATION FOR MTL MODELS

Optimization techniques of training MTL models are equally as important as the design of model architectures. In this section, we summarize optimization techniques for MTL models used in recent research literatures targeting NLP tasks, including loss construction, data sampling, and task scheduling.


## Loss Construction

The most common approach to train an MTL model is to linearly combine loss functions of different tasks into a single global loss function. In this way, the entire objective function of the MTL model can be optimized through conventional learning techniques such as stochastic gradient descent with back-propagation. Different tasks may use different types of loss functions. For example, in [141], the cross-entropy loss for the relation identification task and the ranking loss for the relation classification task are linearly combined, which performs better than single-task learning. Specifically, given tasks each associated with a loss function L and a weight , the overall loss L is defined as
L = ∑︁ =1 L + ∑︁ L + ∑︁ L ,
where L , L , and L denotes loss functions of different tasks, adaptive losses, and regularization terms, respectively, with , , and being their respective weights. For cases where the tasks are optimized in turns as in [114] instead of being optimized jointly, is equivalent to the sampling weight for task , which will be discussed in Section 3.2.

In this case, an important question is how to assign a proper weight to each task. The simplest way is to set them equally [91,126,156], i.e., = 1 . As a generalization, the weights are usually viewed as hyper-parameters and set based on experience or through grid search [9, 13, 15, 22, 24, 28, 39, 56, 67-69, 72, 74, 84, 98, 103, 105, 106, 111, 124, 133, 134, 138, 145, 146, 146, 147, 149, 151, 154, 155]. For example, to prevent large datasets from dominating training, [93] sets the weights as
∝ 1 |D | ,
where |D | denotes the size of the training dataset for task . The weights can also be adjusted dynamically during the training process certain metrics and through adjusting weights, we can purposely emphasize different tasks in different training stages. For instance, since dynamically assigning smaller weights to more uncertain tasks usually leads to good performance for MTL according to [17], [57] assigns weights based on the homoscedasticity of training losses from different tasks as
= 1 2 2 ,
where measures the variance of the training loss for task . In [64], the weight of an unsupervised task is set to a confidence score that measures how much a prediction resembles the corresponding self-supervised label. To ensure that a student model could receive enough supervision during knowledge distillation, BAM! [19] combines the supervised loss L with the distillation loss L as
L = L + (1 − )L ,
where increases linearly from 0 to 1 in the training process. In [112], three tasks are jointly optimized, including the primary essay organization evaluation (OE) task and the auxiliary sentence function identification (SFI) and paragraph function identification (PFI) tasks. The two lower-level auxiliary tasks are assumed to be equally important with weights set to 1 (i.e., = = 1) and the weight of the OE task is set as
= max min L L · , 1 , 0.01 ,
where is initialized to 0.1 and then dynamically updated so that the model focuses on the lower-level tasks at first before becomes larger when L gets relatively smaller. [85] guides the model to focus on easy tasks by setting weights as
( ) = 1 + exp(( ′ − )/ ) ,(1)
where denotes the number of epochs, and ′ are hyperparameters for each task, and denotes temperature. In addition to combining loss functions from different tasks, researchers also use additional adaptive loss functions L to enhance MTL models. In [62], the alignment between an attention vector and a hand-crafted lexicon feature vector is normalized to encourage the model to attend to important words in the input. [14] penalizes the similarity between attention vectors from two tasks and the Euclidean distance between the resulting feature representations to enforce the models to focus on different task-specific features. To learn domain-invariant features, [137] minimizes a distance function (·) between a pair of learned representations from different tasks. Candidates of (·) include the KL divergence, maximum mean discrepancy (MMD), and central moment discrepancy (CMD). Extensive experiments report that KL divergence gives overall stable improvements on all experiments while CMD hits more best scores. The 1 metric linearly combines different loss functions and optimizes all tasks simultaneously. However, when we view multi-task learning as a multi-objective optimization problem, this type of objective functions cannot guarantee optimality in obtaining Pareto-optimal models when each loss function is non-convex. To address this issue, Tchebycheff loss [75] optimizes an MTL model by an ∞ objective, which is formulated as
L ℎ = max 1 L 1 ℎ , 1 , . . . , L ℎ ,
where L denotes the training loss for task , ℎ denotes the shared model parameters, denotes task-specific model parameters for task , denotes the empirical loss of task , and = 1 =1 1 . The Tchebycheff loss can be combined with adversarial MTL as in [70].

Aside from studying how to combine loss functions of different tasks, some studies optimize the training process by manipulating gradients. When jointly learning multiple tasks, the gradients from different tasks may be in conflict with each other, causing negative inter-task interference that harms performance. As a remedy, PCGrad [142] directly projects conflicting gradients. Specifically, given two conflicting gradients g and g from tasks and , respectively, PCGrad projects g onto the normal plane of g as
g ′ = g − g · g g 2 g .
Based on the observation that gradient similarity correlates well with language similarity and model performance, GradVac [129], which targets at optimization of multi-lingual models, regulates parameter updates according to geometry similarities between gradients. That is, GradVac alters both the direction and magnitude of gradients so that they are aligned with the cosine similarity between gradient vectors by modifying g as 1] is the cosine distance between gradients g and g . Notice that PCGrad is a special case of GradVac when = 0. While PCGrad does nothing for gradients from positively associated tasks, GradVac aligns both positively and negatively associated gradients, leading to a consist performance improvement for multi-lingual models.
g ′ = g + ∥g ∥ √︃ 1 − 2 − √︂ 1 − 2 g √︂ 1 − 2 · g where ∈ [−1,

## Data Sampling

Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.

A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.

(2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,
∝ 1 .
As a generalization of proportional sampling in Eq. (2), for task can take the following form as
∝ |D |
where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to
( ) = 1 − 0.8( − 1) − 1 .
In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as
( ) = 1 min , ( − 1) − 0 + 0 ,
where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.


## Task Scheduling

Task scheduling determines the order of tasks in which an MTL model is trained. A naive way is to train all tasks together. For example, [148] takes this way to train an MTL model, where data batches are organized as four-dimensional tensors of size × × × , where denotes the number of samples, denotes the number of tasks, denotes sequence length, and represents embedding dimensions. Similarly, [143] puts labeled data and unlabeled data together to form a batch and [134] learns the dependency parsing and semantic role labeling tasks together. In the case of auxiliary MTL, [3] trains the primary task and one of the auxiliary tasks together at each step. Conversely, [111] trains one of the primary tasks and the auxiliary task together and shuffles between the two primary tasks.

Alternatively, we can train an MTL model on different tasks at different steps. Similar to the data sampling in Section 3.2, we can assign a task sampling weight for task , which is also called mixing ratio, to control the frequency of data batches from task . The most common task scheduling technique is to shuffle between different tasks [5,20,30,33,38,44,51,71,73,79,80,89,93,99,102,108,109,114,118], either randomly or according to a pre-defined schedule. While random shuffling is widely adopted, introducing more heuristics into scheduling could help further improving the performance of MTL models. For example, according to the similarity between each task and the primary task in a multi-lingual multi-task scenario, [65] defines as
= |D | 1 2 ,
where or is set to 1 if the corresponding task or language is the same as the primary task and 0.1 otherwise. Instead of using a fixed mixing ratio designed by hand, some researchers explore using a dynamic mixing ratio during the training process. [40] schedules tasks by a state machine that switches between the two tasks and updates learning rate when validation loss rises. [37] develops a controller meta-network that dynamically schedules tasks based on multi-armed bandits. The controller has arms and optimizes a control policy for arm (task) at step based on an estimated action value , defined as
( ) = exp( , / )/ ∑︁ =1 exp( , / ) , = (1 − ) 0, + ∑︁ =1 (1 − ) −
where denotes temperature, is decay rate, and is the observed reward at step that is defined as the negative validation loss of the primary task. Besides probabilistic approaches, task scheduling could also use heuristics based on certain performance metrics. By optimizing the Tchebycheff loss, [75] learns from the task which has the worst validation performance at each step. The CA-MTL model [95] introduces an uncertainty-based sampling strategy based on Shannon entropy. Specifically, given a batch size and tasks, a pool of × samples are first sampled. Then, the uncertainty measure U ( ) for a sample x from task is defined as
U (x) = (x) × ′
where denotes the Shannon entropy of the model's prediction on x,ˆis the model's maximum average entropy over samples from each task, and ′ denotes the entropy of a uniform distribution and is used to normalize the variance of the number of classes in each task. At last, samples with the highest uncertainty measures are used for training at the current step. Experiments show that this uncertainty-based sampling strategy could effectively avoid catastrophic forgetting when jointly learning multiple tasks and outperforms the aforementioned annealed sampling [113].

In some cases, multiple tasks are learned sequentially. Such tasks usually form a clear dependency relationship or are of different difficulty levels. For instance, [50] uses a baby step curriculum learning approach [18] and trains different tasks in the order of increasing difficulties. Similarly, [43] trains a multi-task model in the order of low-level tasks, high-level tasks, and at last mixed-level batches. [85] focuses on learning easy tasks at the beginning and then shifts its focus to more difficult tasks through loss scheduling as described in Eq. (1). Unicoder [49] trains its five pre-training objectives sequentially in each step. [90] applies sequential training for a multi-task neural architecture search algorithm to maintain performance on previously learned tasks while maximizing performance in the current domain. [94] first pre-trains language and invertible adapters on language modeling before training task adapters on different down-stream tasks, where the language and invertible adapters can also receive gradient when training task adapters. To stabilize the training process when alternating between tasks, successive regularization [30,43] is added to loss functions as a regularization term, which is defined as L = − ′ 2 , where and ′ are model parameters before and after the update in the previous training step and is a hyperparameter.

For auxiliary MTL, some researchers adopt a pre-train then fine-tune methodology, which trains auxiliary tasks first before fine-tuning on the down-stream primary tasks. For example, [55] trains auxiliary POS tagging and domain prediction tasks first before the news headline popularity prediction task. [44] trains document-level tasks first and then the aspect-level primary task by fixing document-level model parameters so that parameters for domain-level tasks are not affected by the small amount of aspect-level data. [125] first pre-trains on self-supervised tasks and then fine-tunes on the smaller disfluency detection data. Similarly, [14] first pre-trains on the abundant question-answer pair data before fine-tuning on the limited question-review answer identification data.


# APPLICATION PROBLEMS

In this section, we summarize the application of multi-task learning in NLP tasks, including applying MTL to optimize certain primary tasks (corresponding to the auxiliary MTL setting), to jointly learn multiple tasks (corresponding to the joint MTL setting), and to improve the performance in multi-lingual multi-task and multimodal scenarios. Existing research works have also explored different ways to improve the performance and efficiency of MTL models, as well as using MTL to study the relatedness of different tasks.


## Auxiliary MTL

Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.

Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.


## Primary

Researchers have also applied auxiliary MTL to classification tasks, such as explicit [71] and implicit [56] discourse relation classification. To improve automatic rumor identification, [53] jointly trains on the stance classification and veracity prediction tasks. [55] learns a headline popularity prediction model with the help of POS tagging and domain prediction. [59] enhances a rumor detection model with user credibility features. [28] adds a low-level grammatical role prediction task into a discourse coherence assessment model to help improve its performance. [74] enhances the hashtag segmentation task by introducing an auxiliary task which predicts whether a given hashtag is single-token or multi-token. In [107], text classification is boosted by learning the predominant sense of words. [133] assists the fake news detection task by stance classification. [14] jointly learns the answer identification task with an auxiliary question answering task. To improve slot filling performance for online shopping assistants, [33] adds NER and segment tagging tasks as auxiliary tasks. In [112], the organization evaluation for student essays is learned together with the sentence and paragraph discourse element identification tasks.

[62] models the stance detection task with the help of the sentiment classification and self-supervised stance lexicon tasks. Generative adversarial MTL architectures are used to improve classification tasks as well. Targeting pharmacovigilance mining, [138] treats mining on different data sources as different tasks and applies self-supervised adversarial training as an auxiliary task to help the model combat the variation of data sources and produce more generalized features. Differently, [98] enhances a feature extractor through unsupervised adversarial training with a discriminator that is pre-trained with supervised data. Sentiment classification models can be enhanced by POS tagging and gaze prediction [80], label distribution learning [149], unsupervised topic modeling [124], or domain adversarial training [127]. In [132], besides the shared base model, a separate model is built for each Microblog user as an auxiliary task. [96] estimates causality scores via Naranjo questionnaire, consisting of 10 multiple-choice questions, with sentence relevance classification as an auxiliary task. [67] introduces an auxiliary task of selecting the passages containing the answers to assist a multi-answer question answering task. [139] improves a community question answering model with an auxiliary question category classification task. To counter data scarcity in the multi-choice question answering task, [51] proposes a multi-stage MTL model that is first coarsely pre-trained using a large out-of-domain natural language inference dataset and then fine-tuned on an in-domain dataset.

For text generation tasks, MTL is brought in to improve the quality of the generated text. It is observed in [27] that adding a target-side language modeling task on the decoder of a neural machine translation (NMT) model brings moderate but consistent performance gain. [73] learns a multi-lingual NMT model with constituency parsing and image caption generation as two auxiliary tasks. Similarly, [144] learns an NMT model together with the help of NER, syntactic parsing, and semantic parsing tasks. To make an NMT model aware of the vocabulary distribution of the retrieval corpus for query translation, [103] adds an unsupervised auxiliary task that learns continuous bag-of-words embeddings on the retrieval corpus in addition to the sentence-level parallel data. Recently, [128] builds a multi-lingual NMT system with source-side language modeling and target-side denoising autoencoder. For the sentence simplification task, [37] uses paraphrase generation and entailment generation as two auxiliary tasks. [38] builds an abstractive summarization model with the question and entailment generation tasks as auxiliary tasks. By improving a language modeling task through MTL, we can generate more natural and coherent text for question generation [154] or task-oriented dialogue generation [155]. [105] implements a semantic parser that jointly learns question type classification, entity mention detection, as well as a weakly supervised objective via question paraphrasing. [9] enhances a text-to-SQL semantic parser by adding explicit condition value detection and value-column mapping as auxiliary tasks. [99] views hierarchical text classification, where each text may have several labels on different levels, as a generation tasks by generating from more general labels to more specific ones, and an auxiliary task of generating in the opposite order is introduced to guide the model to treat high-level and low-level labels more equally and therefore learn more robust representations.

Besides tackling specific tasks, some researchers aim at building general-purpose text representations for future use in downstream tasks. For example, [114] learns sentence representations through multiple weakly related tasks, including learning skip-thought vectors, neural machine translation, constituency parsing, and natural language inference tasks. [126] trains multi-role dialogue representations via unsupervised multi-task pre-training on reference prediction, word prediction, role prediction, and sentence generation. As existing pre-trained models impose huge storage cost for the deployment, PinText [156] learns user profile representations through learning custom word embeddings, which are obtained by minimizing the distance between positive engagement pairs based on user behaviors, including homefeed, related pins, and search queries, by sharing the embedding lookup table.


## Joint MTL

Different from auxiliary MTL, joint MTL models optimize its performance on several tasks simultaneously. Similar to auxiliary MTL, tasks in joint MTL are usually related to or complementary to each other. Table 2 gives an overview of task combinations used in joint MTL models. In certain scenarios, we can even convert models following the traditional pipeline architecture as in single-task learning to joint MTL models so that different tasks can adapt to each other. For example, [93] converts the parsing of Alexa meaning representation language into three independent tagging tasks for intents, types, and properties, respectively. [110] transforms the pipeline relation between POS tagging and morphological tagging into a parallel relation and further builds a joint MTL model. Joint MTL has been proven to be an effective way to improve the performance of standard NLP tasks. For instance, [43] trains six tasks of different levels jointly, including POS tagging, chunking, dependency parsing, relatedness classification, and entailment classification. [148] applies parallel feature fusion to learn multiple classification tasks, including sentiment classification on movie and product reviews. Different from traditional pipeline methods, [72] jointly learns identification and classification of entities, relations, and coreference clusters in scientific literatures. [102] optimizes four semantic tasks together, including NER, entity mention detection (EMD), coreference resolution (CR), and relation extraction (RE) tasks. [40,141,145] learn entity extraction alongside relation extraction. For sentiment analysis tasks, [8] jointly learns dialogue act and sentiment recognition using the tree-like parallel MTL architecture. [44] learns the aspect term extraction and aspect sentiment classification tasks jointly to facilitate aspect-based sentiment analysis. [151] builds a joint aspect term, opinion term, and aspect-opinion pair extraction model through MTL and shows that the joint model outperforms single-task and pipeline baselines by a large margin.

Besides well-studied NLP tasks, joint MTL is also widely applied in various downstream tasks. One major problem of such tasks is the lack of sufficient labeled data. Through joint MTL, one could take advantage of data-rich domains via implicit knowledge sharing. In addition, abundant unlabeled data could be utilized via unsupervised learning techniques. [152] develops a joint MTL model for the NER and entity name normalization tasks in the medical field. [68,146] use MTL to perform simile detection, which includes simile sentence classification and simile component extraction. To analyze Twitter demographic data, [121] jointly learns classification models for genders, ages, political orientations, and locations. The SLUICE network [100] is used to learn four different non-literal language detection tasks in English and German [26]. [86] jointly trains a monolingual formality transfer model and a formality sensitive machine translation model between English and French. For community question answering, [52] builds an MTL model that extracts existing questions related to the current one and looks for question-comment threads that could answer the question at the same time. To analyze the argumentative structure of scientific publications, [57] optimizes argumentative component identification, discourse role classification, citation context identification, subjective aspect classification, and summary relevance classification together with a dynamic weighting mechanism. Considering the connection between sentence emotions and the use of the metaphor, [22] jointly trains a metaphor identification model with an emotion detection model. To ensure the consistency between generated key phrases (short text) and headlines (long text), [85] trains the two generative models jointly with a document category classification model and adds a hierarchical consistency loss based on the attention mechanism. An MTL model is proposed in [111] to jointly perform zero pronoun detection, recovery, and resolution, and unlike existing works, it does not require external syntactic parsing tools. Table 2. A summary of joint MTL studies according to types of tasks involved. 'W', 'S', 'D', and 'O' in the four rightmost columns represent the word-level, sentence-level, and document-level tasks, and tasks of other abstract levels such as RE, respectively. A single checkmark could mean joint learning of multiple tasks of the same type.
Reference W S D O Tagging Generation Classification Classification Classification [93] ✓ [110] ✓ [43] ✓ ✓ ✓ [148] ✓ ✓ [72] ✓ ✓ [102] ✓ ✓ [40] ✓ ✓ [145] ✓ ✓ [141] ✓ ✓ [36] ✓ [8] ✓ [44] ✓ ✓ [151] ✓ ✓ [152] ✓ [68] ✓ ✓ [146] ✓ ✓ [121] ✓ [26] ✓ [86] ✓ [57] ✓ ✓ [22] ✓ ✓ [85] ✓ ✓ [111] ✓ ✓
Moreover, joint MTL is suitable for multi-domain or multi-formalism NLP tasks. Multi-domain tasks share the same problem definition and label space among tasks, but have different data distributions. Applications in multi-domain NLP tasks include sentiment classification [60,131], dialog state tracking [83], essay scoring [21], deceptive review detection [41], multi-genre emotion detection and classification [116], RST discourse parsing [6], historical spelling normalization [5], and document classification [118]. Multi-formalism tasks have the same problem definition but may have different while structurally similar label spaces. [54,91] model three different formalisms of semantic dependency parsing (i.e., DELPH-IN MRS (DM) [32], Predicate-Argument Structures (PAS) [76], and Prague Semantic Dependencies (PSD) [42]) jointly. In [47], a transition-based semantic parsing system is trained jointly on different parsing tasks, including Abstract Meaning Representation (AMR) [4], Semantic Dependency Parsing (SDP) [88], and Universal Dependencies (UD) [87], and it shows that joint training improves performance on the testing UCCA dataset. [71] jointly models discourse relation classification on two distinct datasets: PDTB and RST-DT. [29] shows the dual annotation and joint learning of two distinct sets of relations for noun-noun compounds could improve the performance of both tasks. In [143], an adversarial MTL model is proposed for morphological modeling for high-resource modern standard Arabic and its low-resource dialect Egyptian Arabic, to enable knowledge between the two domains.


## Multi-lingual MTL

Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.


## Multimodal MTL

As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

[89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.

Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.

In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.


## Task Relatedness in MTL

A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.


# DATA SOURCE AND BENCHMARKS FOR MULTI-TASK LEARNING

In this section, we introduce the ways of preparing datasets for training MTL models and some benchmark datasets.


## Data Source

Given tasks with corresponding datasets D = {X , Y }, = 1, . . . , , where X denotes the set of data instances in task and Y denotes the corresponding labels, we denote the entire dataset for the tasks by D = {X, Y}. We describe different forms of D in the following sections.

5.1.1 Disjoint Datasets. In most multi-task learning literature, the training sets of different tasks have distinct label spaces, i.e. ∀ ≠ , Y ∩ Y = ∅. In this case, D = {D 1 , . . . , D } The most popular way to train MTL models on such tasks is to alternate between different tasks [5,20,27,30,40,43,69,71,73,79,89,96,109,136,153], either randomly or by a pre-defined order. Thus the model handles data from different datasets in turns as discussed in Section 3.


### Multi-label Datasets.

Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,
D = {X,Ŷ} whereŶ = ∪ =1 Y .
Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.


## Multi-task Benchmark Datasets

As summarized in Table 3, we list a few public multi-task benchmark datasets for NLP tasks.

• GLUE [123] is a benchmark dataset for evaluating natural language understanding (NLU) models. The main benchmark consists of 8 sentence and sentence-pair classification tasks as well as a regression task. The tasks cover a diverse range of genres, dataset sizes, and difficulties. Besides, a diagnostic dataset is provided to evaluate the ability of NLU models on capturing a pre-defined set of language phenomena. • SuperGLUE [122] is a generalization of GLUE. As the performance of state-of-the-art models has exceeded non-expert human baselines on GLUE, SuperGLUE contains a set of 8 more challenging NLU tasks along with comprehensive human baselines. Besides retaining the two hardest tasks in GLUE, 6 tasks are added with two new question formats: coreference resolution and question answering (QA).  [46] is a multi-task few-shot learning dataset for world knowledge and problem solving abilities of language processing models. This dataset covers 57 subjects including 19 in STEM, 13 in humanities, 12 in social sciences, and 13 in other subjects. This dataset is split into a few-shot development set that has 5 questions for each subject, a validation set for tuning hyper-parameters containing 1540 questions, and a test set with 14079 questions. • Xtreme [48] is a multi-task benchmark dataset for evaluating cross-lingual generalization capabilities of multi-lingual representations covering 9 tasks in 40 languages. The tasks include 2 classification tasks, 2 structure prediction tasks, 3 question answering tasks, and 2 sentence retrieval tasks. Out of the 40 languages involved, 19 languages appear in at least 3 datasets and the rest 21 languages appear in at least one dataset. • XGLUE [63] is a benchmark dataset that supports the development and evaluation of large cross-lingual pre-trained language models. The XGLUE dataset includes 11 downstream tasks, including 3 single-input understanding tasks, 6 pair-input understanding tasks, and 2 generation tasks. The pre-training corpus consists of a small corpus that includes a 101G multi-lingual corpus covering 100 languages and a 146G bilingual corpus covering 27 languages, and a large corpus with 2,500G multi-lingual data covering 89 languages. • LSParD [105] is a multi-task semantic parsing dataset with 3 tasks, including question type classification, entity mention detection, and question semantic parsing. Each logical form is associated with a question and multiple human annotated paraphrases. This dataset contains 51,164 questions in 9 categories, 3361 logical form patterns, and 23,144 entities. • ECSA [33] is a dataset for slot filling, named entity recognition, and segmentation to evaluate online shopping assistant systems in Chinese. The training part contains 24,892 pairs of input utterances and their corresponding slot labels, named entity labels, and segment labels. The testing part includes 2,723 such pairs with an Out-of-Vocabulary (OOV) rate of 85.3%, which is much higher than the ATIS dataset [45] whose OOV is smaller than 1%. • ABC [34], the Anti-reflexive Bias Challenge, is a multi-task benchmark dataset designed for evaluating gender assumptions in NLP models. ABC consists of 4 tasks, including language modeling, natural language inference (NLI), coreference resolution, and machine translation. A total of 4,560 samples are collected by a template-based method. The language modeling task is to predict the pronoun of a sentence. For NLI and coreference resolution, three variations of each sentence are used to construct entailment pairs. For machine translation, sentences with two variations of third-person pronouns in English are used as source sentences. • CompGuessWhat?! [115] is a dataset for grounded language learning with 65,700 collected dialogues. It is an instance of the Grounded Language Learning with Attributes (GROLLA) framework. The evaluation process includes three parts: goal-oriented evaluation (e.g., Visual QA and Visual NLI), object attribute prediction, and zero-shot evaluation. • SCIERC [72] is a multi-label dataset for identifying entities, relations, and cross-sentence coreference clusters from abstracts of research papers. SCIERC contains 500 scientific abstracts collected from proceedings in 12 conferences and workshops in artificial intelligence.


# CONCLUSION AND DISCUSSIONS

In this paper, we give an overview of the application of multi-task learning in recent natural language processing research, focusing on deep learning approaches. We first present different architectures of MTL used in recent research literature, including parallel architecture, hierarchical architecture, modular architecture, and generative adversarial architectures. After that, optimization techniques, including loss construction, data sampling, and task scheduling are discussed. After briefly summarizing the application of MTL in different down-stream tasks, we describe the ways to manage data sources in MTL as well as some MTL benchmark datasets for NLP research.

There are several directions worth further investigations for future studies. Firstly, given multiple NLP tasks, how to find a set of tasks that could take advantage of MTL remains a challenge. Besides improving performance of MTL models, a deeper understanding of task relatedness could also help expanding the application of MTL to more tasks. Though there are some works studying this issue, as discussed in Section 4.5, they are far from mature.

Secondly, current NLP models often rely on a large or even huge amount of labeled data. However, in many real-world applications, where large-scale data annotation is costly, this requirement cannot be easily satisfied. In this case, we may consider to leverage abundant unlabeled data in MTL by using self-supervised or unsupervised learning techniques.

Thirdly, we are curious about whether we can create more powerful Pre-trained Language Models (PLMs) via more advanced MTL techniques. PLMs have become an essential part of NLP pipeline. Though most PLMs are trained on multiple tasks, the MTL architectures used are mostly simple tree-like architectures. A better MTL architecture might be the key for the next breakthrough for PLMs.

At last, it would be interesting to extend the use of MTL to more NLP tasks. Though there are many NLP tasks that can be jointly learned by MTL, most NLP tasks are well-studied tasks, such as classification, sequence labeling, and text generation, as shown in Tables 1 and 2. We would like to see how MTL could benefit more challenging NLP tasks, such as building dialogue systems and multi-modal learning tasks.

## Fig. 1 .
1Illustration for parallel architectures. For task , ℎ ( ) represents the latent representation at the -th layer and

## Fig. 2 .
2Illustration for hierarchical architectures. ℎ represents different hidden states andˆrepresents the predicted output distribution for task . Red boxes stand for hierarchical feature fusion mechanisms. The purple block and blue circle in (b) stand for hierarchical feature and signal pipeline unit respectively.

## Fig. 3 .
3Illustration for multi-task adapters.


Vol. 1, No. 1, Article . Publication date: September 2021.

## Table 3 .
3Statistics of multi-task benchmark datasets for NLP tasks.• Measuring Massive Multitask Language Understanding (MMMLU)Dataset 
# Tasks # Languages # Samples 
Topic 
GLUE [123] 
9 
1 (en) 
2157k 
Language Understanding 
Super GLUE [122] 
8 
1 (en) 
160k 
Language Understanding 
MMMLU [46] 
57 
1 (en) 
-
Language Understanding 
Xtreme [48] 
9 
40 
597k 
Multi-lingual Learning 
XGLUE [63] 
11 
100 
2747G 
Cross-lingual Pre-training 
LSParD [105] 
3 
1 (en) 
51k 
Semantic Parsing 
ECSA [33] 
3 
1 (cn) 
28k 
Language Processing 
ABC [34] 
4 
1 (en) 
5k 
Anti-reflexive Gender Bias Detection 
CompGuessWhat?! [115] 
4 
1 (en) 
66k 
Grounded Language Learning 
SCIERC [72] 
3 
1 (en) 
500 
Scientific Literature Understanding 


, Vol. 1, No. 1, Article . Publication date: September 2021.
ACKNOWLEDGEMENTSThis work is supported by NSFC 62076118.
A Multitask Learning Approach for Diacritic Restoration. Sawsan Alqahtani, Ajay Mishra, Mona Diab, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsSawsan Alqahtani, Ajay Mishra, and Mona Diab. 2020. A Multitask Learning Approach for Diacritic Restoration. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 8238-8247.

Mutlitask Learning for Cross-Lingual Transfer of Semantic Dependencies. Maryam Aminian, Mohammad Sadegh Rasooli, Mona Diab, arXiv:2004.14961Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingMaryam Aminian, Mohammad Sadegh Rasooli, and Mona Diab. 2020. Mutlitask Learning for Cross-Lingual Transfer of Semantic Dependencies. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. arXiv:2004.14961

Multi-Task Learning of Keyphrase Boundary Classification. Isabelle Augenstein, Anders Søgaard, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2Short Papers)Isabelle Augenstein and Anders Søgaard. 2017. Multi-Task Learning of Keyphrase Boundary Classification. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, 341-346.

Abstract Meaning Representation for Sembanking. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. the 7th Linguistic Annotation Workshop and Interoperability with DiscourseAssociation for Computational LinguisticsLaura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for Sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. Association for Computational Linguistics, 178-186.

Improving Historical Spelling Normalization with Bi-Directional LSTMs and Multi-Task Learning. Marcel Bollmann, Anders Søgaard, Proceedings of the 26th International Conference on Computational Linguistics. The COLING 2016 Organizing Committee. the 26th International Conference on Computational Linguistics. The COLING 2016 Organizing CommitteeMarcel Bollmann and Anders Søgaard. 2016. Improving Historical Spelling Normalization with Bi-Directional LSTMs and Multi-Task Learning. In Proceedings of the 26th International Conference on Computational Linguistics. The COLING 2016 Organizing Committee, 131-139.

Multi-View and Multi-Task Training of RST Discourse Parsers. Chloé Braud, Barbara Plank, Anders Søgaard, Proceedings of the 26th International Conference on Computational Linguistics. the 26th International Conference on Computational LinguisticsChloé Braud, Barbara Plank, and Anders Søgaard. 2016. Multi-View and Multi-Task Training of RST Discourse Parsers. In Proceedings of the 26th International Conference on Computational Linguistics. 1903-1913.

Multitask Learning. Rich Caruana, Machine Learning. 28Rich Caruana. 1997. Multitask Learning. Machine Learning 28, 1 (1997), 41-75.

Multi-Task Dialog Act and Sentiment Recognition on Mastodon. Christophe Cerisara, Somayeh Jafaritazehjani, Adedayo Oluokun, Hoa T Le, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsAssociation for Computational LinguisticsChristophe Cerisara, Somayeh Jafaritazehjani, Adedayo Oluokun, and Hoa T. Le. 2018. Multi-Task Dialog Act and Sentiment Recognition on Mastodon. In Proceedings of the 27th International Conference on Computational Linguistics. Association for Computational Linguistics, 745-754.

Zero-Shot Text-to-SQL Learning with Auxiliary Task. Shuaichen Chang, Pengfei Liu, Yun Tang, Jing Huang, Xiaodong He, Bowen Zhou, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Shuaichen Chang, Pengfei Liu, Yun Tang, Jing Huang, Xiaodong He, and Bowen Zhou. 2020. Zero-Shot Text-to-SQL Learning with Auxiliary Task. Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (April 2020), 7488-7495.

Multi-Task Learning for Sequence Tagging: An Empirical Study. Soravit Changpinyo, Hexiang Hu, Fei Sha, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsAssociation for Computational LinguisticsSoravit Changpinyo, Hexiang Hu, and Fei Sha. 2018. Multi-Task Learning for Sequence Tagging: An Empirical Study. In Proceedings of the 27th International Conference on Computational Linguistics. Association for Computational Linguistics, 2965-2977.

Embodied Multimodal Multitask Learning. Devendra Singh Chaplot, Lisa Lee, Ruslan Salakhutdinov, Devi Parikh, Dhruv Batra, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Ninth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence OrganizationDevendra Singh Chaplot, Lisa Lee, Ruslan Salakhutdinov, Devi Parikh, and Dhruv Batra. 2020. Embodied Multimodal Multitask Learning. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, 2442-2448.

Sentiment and Emotion Help Sarcasm? A Multi-Task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis. Dushyant Singh Chauhan, S R Dhanush, Asif Ekbal, Pushpak Bhattacharyya, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsDushyant Singh Chauhan, Dhanush S R, Asif Ekbal, and Pushpak Bhattacharyya. 2020. Sentiment and Emotion Help Sarcasm? A Multi-Task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 4351-4360.

Meta Multi-Task Learning for Sequence Modeling. Junkun Chen, Xipeng Qiu, Pengfei Liu, Xuanjing Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceJunkun Chen, Xipeng Qiu, Pengfei Liu, and Xuanjing Huang. 2018. Meta Multi-Task Learning for Sequence Modeling. In Proceedings of the AAAI Conference on Artificial Intelligence.

Answer Identification from Product Reviews for User Questions by Multi-Task Attentive Networks. Long Chen, Ziyu Guan, Wei Zhao, Wanqing Zhao, Xiaopeng Wang, Zhou Zhao, Huan Sun, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Long Chen, Ziyu Guan, Wei Zhao, Wanqing Zhao, Xiaopeng Wang, Zhou Zhao, and Huan Sun. 2019. Answer Identification from Product Reviews for User Questions by Multi-Task Attentive Networks. Proceedings of the AAAI Conference on Artificial Intelligence 33 (July 2019), 45-52.

APE: Argument Pair Extraction from Peer Review and Rebuttal via Multi-Task Learning. Liying Cheng, Lidong Bing, Qian Yu, Wei Lu, Luo Si, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsLiying Cheng, Lidong Bing, Qian Yu, Wei Lu, and Luo Si. 2020. APE: Argument Pair Extraction from Peer Review and Rebuttal via Multi-Task Learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 7000-7011.

Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation. Tzu-Wei Shun-Po Chuang, Alexander H Sung, Hung-Yi Liu, Lee, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsShun-Po Chuang, Tzu-Wei Sung, Alexander H. Liu, and Hung-yi Lee. 2020. Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 5998-6003.

Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics. Roberto Cipolla, Yarin Gal, Alex Kendall, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEERoberto Cipolla, Yarin Gal, and Alex Kendall. 2018. Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 7482-7491.

Volkan Cirik, csEduard Hovy, csLouis-Philippe Morency, csarXiv:1611.06204arXiv:1611.06204Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks. csVolkan Cirik, Eduard Hovy, and Louis-Philippe Morency. 2016. Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks. arXiv:1611.06204 [cs] (Nov. 2016). arXiv:1611.06204 [cs]

BAM! Born-Again Multi-Task Networks for Natural Language Understanding. Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning, V Quoc, Le, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsKevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V. Le. 2019. BAM! Born-Again Multi-Task Networks for Natural Language Understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 5931-5937.

A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. Ronan Collobert, Jason Weston, Proceedings of the 25th International Conference on Machine Learning (ICML '08). the 25th International Conference on Machine Learning (ICML '08)Association for Computing MachineryRonan Collobert and Jason Weston. 2008. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In Proceedings of the 25th International Conference on Machine Learning (ICML '08). Association for Computing Machinery, 160-167.

Constrained Multi-Task Learning for Automated Essay Scoring. Ronan Cummins, Meng Zhang, Ted Briscoe, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1Long Papers)Ronan Cummins, Meng Zhang, and Ted Briscoe. 2016. Constrained Multi-Task Learning for Automated Essay Scoring. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 789-799.

Modelling the Interplay of Metaphor and Emotion through Multitask Learning. Verna Dankers, Marek Rei, Martha Lewis, Ekaterina Shutova, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational LinguisticsVerna Dankers, Marek Rei, Martha Lewis, and Ekaterina Shutova. 2019. Modelling the Interplay of Metaphor and Emotion through Multitask Learning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 2218- 2229.

Online Multitask Learning for Machine Translation Quality Estimation. G C José, Matteo De Souza, Elisa Negri, Marco Ricci, Turchi, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Long Papers)José G. C. de Souza, Matteo Negri, Elisa Ricci, and Marco Turchi. 2015. Online Multitask Learning for Machine Translation Quality Estimation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, 219-228.

Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge Base Question Answering. Yang Deng, Yuexiang Xie, Yaliang Li, Min Yang, Nan Du, Wei Fan, Kai Lei, Ying Shen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Yang Deng, Yuexiang Xie, Yaliang Li, Min Yang, Nan Du, Wei Fan, Kai Lei, and Ying Shen. 2019. Multi-Task Learning with Multi-View Attention for Answer Selection and Knowledge Base Question Answering. Proceedings of the AAAI Conference on Artificial Intelligence 33 (July 2019), 6318-6325.

BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, 4171-4186.

Killing Four Birds with Two Stones: Multi-Task Learning for Non-Literal Language Detection. Erik-Lân Do Dinh, Steffen Eger, Iryna Gurevych, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsAssociation for Computational LinguisticsErik-Lân Do Dinh, Steffen Eger, and Iryna Gurevych. 2018. Killing Four Birds with Two Stones: Multi-Task Learning for Non-Literal Language Detection. In Proceedings of the 27th International Conference on Computational Linguistics. Association for Computational Linguistics, 1558-1569.

Using Target-Side Monolingual Data for Neural Machine Translation through Multi-Task Learning. Tobias Domhan, Felix Hieber, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsTobias Domhan and Felix Hieber. 2017. Using Target-Side Monolingual Data for Neural Machine Translation through Multi-Task Learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 1500-1505.

Multi-Task Learning for Coherence Modeling. Youmna Farag, Helen Yannakoudakis, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsYoumna Farag and Helen Yannakoudakis. 2019. Multi-Task Learning for Coherence Modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 629-639.

Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation. Murhaf Fares, Stephan Oepen, Erik Velldal, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsMurhaf Fares, Stephan Oepen, and Erik Velldal. 2018. Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 1488-1498.

Hierarchical Multi-Task Word Embedding Learning for Synonym Prediction. Hongliang Fei, Shulong Tan, Ping Li, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningACMHongliang Fei, Shulong Tan, and Ping Li. 2019. Hierarchical Multi-Task Word Embedding Learning for Synonym Prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 834-842.

Christiane Fellbaum, Theory and Applications of Ontology: Computer Applications. SpringerChristiane Fellbaum. 2010. WordNet. In Theory and Applications of Ontology: Computer Applications. Springer, 231-243.

DeepBank. A Dynamically Annotated Treebank of the Wall Street Journal. Dan Flickinger, Yi Zhang, Valia Kordoni, Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories. the 11th International Workshop on Treebanks and Linguistic TheoriesDan Flickinger, Yi Zhang, and Valia Kordoni. 2012. DeepBank. A Dynamically Annotated Treebank of the Wall Street Journal. In Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories. 85-96.

Deep Cascade Multi-Task Learning for Slot Filling in Online Shopping Assistant. Yu Gong, Xusheng Luo, Yu Zhu, Wenwu Ou, Zhao Li, Muhua Zhu, Kenny Q Zhu, Lu Duan, Xi Chen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Yu Gong, Xusheng Luo, Yu Zhu, Wenwu Ou, Zhao Li, Muhua Zhu, Kenny Q. Zhu, Lu Duan, and Xi Chen. 2019. Deep Cascade Multi-Task Learning for Slot Filling in Online Shopping Assistant. Proceedings of the AAAI Conference on Artificial Intelligence 33 (July 2019), 6465-6472.

Type B Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias. Ana Valeria Gonzalez, Maria Barrett, Rasmus Hvingelby, Kellie Webster, Anders Søgaard, arXiv:2009.11982Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingAna Valeria Gonzalez, Maria Barrett, Rasmus Hvingelby, Kellie Webster, and Anders Søgaard. 2020. Type B Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. arXiv:2009.11982

Generative Adversarial Nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems. Curran Associates, Inc27Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. In Advances in Neural Information Processing Systems, Vol. 27. Curran Associates, Inc.

Dynamic Sampling Strategies for Multi-Task Reading Comprehension. Ananth Gottumukkala, Dheeru Dua, Sameer Singh, Matt Gardner, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsAnanth Gottumukkala, Dheeru Dua, Sameer Singh, and Matt Gardner. 2020. Dynamic Sampling Strategies for Multi-Task Reading Com- prehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 920-924.

Dynamic Multi-Level Multi-Task Learning for Sentence Simplification. Han Guo, Ramakanth Pasunuru, Mohit Bansal, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsAssociation for Computational LinguisticsHan Guo, Ramakanth Pasunuru, and Mohit Bansal. 2018. Dynamic Multi-Level Multi-Task Learning for Sentence Simplification. In Proceedings of the 27th International Conference on Computational Linguistics. Association for Computational Linguistics, 462-476.

Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation. Han Guo, Ramakanth Pasunuru, Mohit Bansal, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1Long Papers)Han Guo, Ramakanth Pasunuru, and Mohit Bansal. 2018. Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 687-697.

GIRNet: Interleaved Multi-Task Recurrent State Sequence Models. Divam Gupta, Tanmoy Chakraborty, Soumen Chakrabarti, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Divam Gupta, Tanmoy Chakraborty, and Soumen Chakrabarti. 2019. GIRNet: Interleaved Multi-Task Recurrent State Sequence Models. Proceedings of the AAAI Conference on Artificial Intelligence 33 (July 2019), 6497-6504.

Table Filling Multi-Task Recurrent Neural Network for Joint Entity and Relation Extraction. Pankaj Gupta, Hinrich Schütze, Bernt Andrassy, Proceedings of the 26th International Conference on Computational Linguistics. The COLING 2016 Organizing Committee. the 26th International Conference on Computational Linguistics. The COLING 2016 Organizing CommitteePankaj Gupta, Hinrich Schütze, and Bernt Andrassy. 2016. Table Filling Multi-Task Recurrent Neural Network for Joint Entity and Relation Extraction. In Proceedings of the 26th International Conference on Computational Linguistics. The COLING 2016 Organizing Committee, 2537-2547.

Deceptive Review Spam Detection via Exploiting Task Relatedness and Unlabeled Data. Zhen Hai, Peilin Zhao, Peng Cheng, Peng Yang, Xiao-Li Li, Guangxia Li, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsZhen Hai, Peilin Zhao, Peng Cheng, Peng Yang, Xiao-Li Li, and Guangxia Li. 2016. Deceptive Review Spam Detection via Exploiting Task Relatedness and Unlabeled Data. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 1817-1826.

Announcing Prague Czech-English Dependency Treebank 2.0. Jan Hajic, Eva Hajicová, Jarmila Panevová, Petr Sgall, Ondrej Bojar, Silvie Cinková, Eva Fucíková, Marie Mikulová, Petr Pajas, Jan Popelka, LREC. Jan Hajic, Eva Hajicová, Jarmila Panevová, Petr Sgall, Ondrej Bojar, Silvie Cinková, Eva Fucíková, Marie Mikulová, Petr Pajas, Jan Popelka, et al. 2012. Announcing Prague Czech-English Dependency Treebank 2.0.. In LREC. 3153-3160.

A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks. Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, Richard Socher, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsKazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. 2017. A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 1923-1933.

An Interactive Multi-Task Learning Network for End-to-End Aspect-Based Sentiment Analysis. Ruidan He, Hwee Tou Wee Sun Lee, Daniel Ng, Dahlmeier, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2019. An Interactive Multi-Task Learning Network for End-to- End Aspect-Based Sentiment Analysis. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 504-515.

The ATIS Spoken Language Systems Pilot Corpus. Charles T Hemphill, John J Godfrey, George R Doddington, Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley. PennsylvaniaCharles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990. The ATIS Spoken Language Systems Pilot Corpus. In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24-27,1990.

Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring Massive Multitask Language Understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, International Conference on Learning Representations. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring Massive Multitask Language Understanding. In International Conference on Learning Representations.

Multitask Parsing Across Semantic Representations. Daniel Hershcovich, Omri Abend, Ari Rappoport, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1Long Papers)Daniel Hershcovich, Omri Abend, and Ari Rappoport. 2018. Multitask Parsing Across Semantic Representations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 373-385.

XTREME: A Massively Multilingual Multi-Task Benchmark for Evaluating Cross-Lingual Generalization. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, Melvin Johnson, arXiv:2003.11080Proceedings of the 37th International Conference on Machine Learning (ICML). the 37th International Conference on Machine Learning (ICML)Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. XTREME: A Massively Multilingual Multi-Task Benchmark for Evaluating Cross-Lingual Generalization. In Proceedings of the 37th International Conference on Machine Learning (ICML). July 2020 (July 2020). arXiv:2003.11080

Unicoder: A Universal Language Encoder by Pre-Training with Multiple Cross-Lingual Tasks. Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Ming Zhou, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational LinguisticsHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. 2019. Unicoder: A Universal Language Encoder by Pre-Training with Multiple Cross-Lingual Tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 2485-2494.

Extractive Summarization Using Multi-Task Learning with Document Classification. Masaru Isonuma, Toru Fujino, Junichiro Mori, Yutaka Matsuo, Ichiro Sakata, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsMasaru Isonuma, Toru Fujino, Junichiro Mori, Yutaka Matsuo, and Ichiro Sakata. 2017. Extractive Summarization Using Multi-Task Learning with Document Classification. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2101-2110.

MMM: Multi-Stage Multi-Task Learning for Multi-Choice Reading Comprehension. Di Jin, Shuyang Gao, Jiun-Yu Kao, Tagyoung Chung, Dilek Hakkani-Tur, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Di Jin, Shuyang Gao, Jiun-Yu Kao, Tagyoung Chung, and Dilek Hakkani-tur. 2020. MMM: Multi-Stage Multi-Task Learning for Multi-Choice Reading Comprehension. Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (April 2020), 8010-8017.

Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings. Shafiq Joty, Lluís Màrquez, Preslav Nakov, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsShafiq Joty, Lluís Màrquez, and Preslav Nakov. 2018. Joint Multitask Learning for Community Question Answering Using Task- Specific Embeddings. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 4196-4207.

All-in-One: Multi-Task Learning for Rumour Verification. Elena Kochkina, Maria Liakata, Arkaitz Zubiaga, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsAssociation for Computational LinguisticsElena Kochkina, Maria Liakata, and Arkaitz Zubiaga. 2018. All-in-One: Multi-Task Learning for Rumour Verification. In Proceedings of the 27th International Conference on Computational Linguistics. Association for Computational Linguistics, 3402-3413.

Multi-Task Semantic Dependency Parsing with Policy Gradient for Learning Easy-First Strategies. Shuhei Kurita, Anders Søgaard, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsShuhei Kurita and Anders Søgaard. 2019. Multi-Task Semantic Dependency Parsing with Policy Gradient for Learning Easy-First Strategies. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2420-2430.

Predicting News Headline Popularity with Syntactic and Semantic Knowledge Using Multi-Task Learning. Sotiris Lamprinidis, Daniel Hardt, Dirk Hovy, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsSotiris Lamprinidis, Daniel Hardt, and Dirk Hovy. 2018. Predicting News Headline Popularity with Syntactic and Semantic Knowledge Using Multi-Task Learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 659-664.

Multi-Task Attention-Based Neural Networks for Implicit Discourse Relationship Representation and Identification. Man Lan, Jianxiang Wang, Yuanbin Wu, Zheng-Yu Niu, Haifeng Wang, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsMan Lan, Jianxiang Wang, Yuanbin Wu, Zheng-Yu Niu, and Haifeng Wang. 2017. Multi-Task Attention-Based Neural Networks for Implicit Discourse Relationship Representation and Identification. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 1299-1308.

Investigating the Role of Argumentation in the Rhetorical Analysis of Scientific Publications with Neural Multi-Task Learning Models. Anne Lauscher, Goran Glavaš, Simone Paolo Ponzetto, Kai Eckert, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsAnne Lauscher, Goran Glavaš, Simone Paolo Ponzetto, and Kai Eckert. 2018. Investigating the Role of Argumentation in the Rhetorical Analysis of Scientific Publications with Neural Multi-Task Learning Models. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 3326-3338.

Multi-Task Learning for Metaphor Detection with Graph Convolutional Neural Networks and Word Sense Disambiguation. Duong Le, My Thai, Thien Nguyen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Duong Le, My Thai, and Thien Nguyen. 2020. Multi-Task Learning for Metaphor Detection with Graph Convolutional Neural Networks and Word Sense Disambiguation. Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (April 2020), 8139-8146.

Rumor Detection by Exploiting User Credibility Information, Attention and Multi-Task Learning. Quanzhi Li, Qiong Zhang, Luo Si, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsQuanzhi Li, Qiong Zhang, and Luo Si. 2019. Rumor Detection by Exploiting User Credibility Information, Attention and Multi-Task Learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 1173-1179.

Multi-Domain Sentiment Classification. Shoushan Li, Chengqing Zong, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics. the 46th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsShoushan Li and Chengqing Zong. 2008. Multi-Domain Sentiment Classification. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 257-260.

Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction. Xin Li, Wai Lam, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsXin Li and Wai Lam. 2017. Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2886-2892.

Multi-Task Stance Detection with Sentiment and Stance Lexicons. Yingjie Li, Cornelia Caragea, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational LinguisticsYingjie Li and Cornelia Caragea. 2019. Multi-Task Stance Detection with Sentiment and Stance Lexicons. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 6299-6305.

XGLUE: A New Benchmark Datasetfor Cross-Lingual Pre-Training, Understanding and Generation. Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, Ming Zhou, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang, Rahul Agrawal, Edward Cui, Sining Wei, Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu, Shuguang Liu, Fan Yang, Daniel Campos, Rangan Majumder, and Ming Zhou. 2020. XGLUE: A New Benchmark Datasetfor Cross- Lingual Pre-Training, Understanding and Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 6008-6018.

Semi-Supervised Learning on Meta Structure: Multi-Task Tagging and Parsing in Low-Resource Scenarios. Kyungtae Lim, Jay Yoon Lee, Jaime Carbonell, Thierry Poibeau, Proceedings of the AAAI Conference on Artificial Intelligence, Association for the Advancement of Artificial Intelligence. the AAAI Conference on Artificial Intelligence, Association for the Advancement of Artificial IntelligenceAssociation for the Advancement of Artificial IntelligenceKyungTae Lim, Jay Yoon Lee, Jaime Carbonell, and Thierry Poibeau. 2020. Semi-Supervised Learning on Meta Structure: Multi-Task Tagging and Parsing in Low-Resource Scenarios. In Proceedings of the AAAI Conference on Artificial Intelligence, Association for the Advancement of Artificial Intelligence (Ed.). Association for the Advancement of Artificial Intelligence.

A Multi-Lingual Multi-Task Architecture for Low-Resource Sequence Labeling. Ying Lin, Shengqi Yang, Veselin Stoyanov, Heng Ji, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1Long Papers)Ying Lin, Shengqi Yang, Veselin Stoyanov, and Heng Ji. 2018. A Multi-Lingual Multi-Task Architecture for Low-Resource Sequence Labeling. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 799-809.

Jointly Learning Grounded Task Structures from Language Instruction and Visual Demonstration. Changsong Liu, Shaohua Yang, Sari Saba-Sadiya, Nishant Shukla, Yunzhong He, Song-Chun, Joyce Zhu, Chai, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsChangsong Liu, Shaohua Yang, Sari Saba-Sadiya, Nishant Shukla, Yunzhong He, Song-Chun Zhu, and Joyce Chai. 2016. Jointly Learning Grounded Task Structures from Language Instruction and Visual Demonstration. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 1482-1492.

A Multi-Answer Multi-Task Framework for Real-World Machine Reading Comprehension. Jiahua Liu, Wan Wei, Maosong Sun, Hao Chen, Yantao Du, Dekang Lin, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsJiahua Liu, Wan Wei, Maosong Sun, Hao Chen, Yantao Du, and Dekang Lin. 2018. A Multi-Answer Multi-Task Framework for Real-World Machine Reading Comprehension. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2109-2118.

Neural Multitask Learning for Simile Recognition. Lizhen Liu, Xiao Hu, Wei Song, Ruiji Fu, Ting Liu, Guoping Hu, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsLizhen Liu, Xiao Hu, Wei Song, Ruiji Fu, Ting Liu, and Guoping Hu. 2018. Neural Multitask Learning for Simile Recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 1543-1553.

Deep Multi-Task Learning with Shared Memory for Text Classification. Pengfei Liu, Xipeng Qiu, Xuanjing Huang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsPengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016. Deep Multi-Task Learning with Shared Memory for Text Classification. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 118-127.

Adversarial Multi-Task Learning for Text Classification. Pengfei Liu, Xipeng Qiu, Xuanjing Huang, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational Linguistics1Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017. Adversarial Multi-Task Learning for Text Classification. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 1-10.

Implicit Discourse Relation Classification via Multi-Task Neural Networks. Yang Liu, Sujian Li, Xiaodong Zhang, Zhifang Sui, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceYang Liu, Sujian Li, Xiaodong Zhang, and Zhifang Sui. 2016. Implicit Discourse Relation Classification via Multi-Task Neural Networks. In Proceedings of the AAAI Conference on Artificial Intelligence.

Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction. Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsYi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 3219-3232.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser, arXiv:1511.06114Multi-Task Sequence to Sequence Learning. International Conference on Learning Representations. Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2016. Multi-Task Sequence to Sequence Learning. International Conference on Learning Representations 2016 (March 2016). arXiv:1511.06114

Multi-Task Pairwise Neural Ranking for Hashtag Segmentation. Mounica Maddela, Wei Xu, Daniel Preoţiuc-Pietro, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsMounica Maddela, Wei Xu, and Daniel Preoţiuc-Pietro. 2019. Multi-Task Pairwise Neural Ranking for Hashtag Segmentation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2538-2549.

Tchebycheff Procedure for Multi-Task Text Classification. Yuren Mao, Shuang Yun, Weiwei Liu, Bo Du, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsYuren Mao, Shuang Yun, Weiwei Liu, and Bo Du. 2020. Tchebycheff Procedure for Multi-Task Text Classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 4217-4226.

The Penn Treebank: Annotating Predicate Argument Structure. Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert Macintyre, Ann Bies, Mark Ferguson, Karen Katz, Britta Schasberger, Human Language Technology: Proceedings of a Workshop Held at. Plainsboro, New JerseyMitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating Predicate Argument Structure. In Human Language Technology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 8-11, 1994.

When Is Multitask Learning Effective? Semantic Sequence Prediction under Varying Data Conditions. Alonso Héctor Martínez, Barbara Plank, Proceedings of the 15th Conference of the European Chapter. the 15th Conference of the European ChapterAssociation for Computational Linguistics1Héctor Martínez Alonso and Barbara Plank. 2017. When Is Multitask Learning Effective? Semantic Sequence Prediction under Varying Data Conditions. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. Association for Computational Linguistics, 44-53.

Adversarial Training for Multi-Task and Multi-Lingual Joint Modeling of Utterance Intent Classification. Ryo Masumura, Yusuke Shinohara, Ryuichiro Higashinaka, Yushi Aono, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsRyo Masumura, Yusuke Shinohara, Ryuichiro Higashinaka, and Yushi Aono. 2018. Adversarial Training for Multi-Task and Multi- Lingual Joint Modeling of Utterance Intent Classification. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 633-639.

Multi-Task and Multi-Lingual Joint Learning of Neural Lexical Utterance Classification Based on Partially-Shared Modeling. Ryo Masumura, Tomohiro Tanaka, Ryuichiro Higashinaka, Hirokazu Masataki, Yushi Aono, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsAssociation for Computational LinguisticsRyo Masumura, Tomohiro Tanaka, Ryuichiro Higashinaka, Hirokazu Masataki, and Yushi Aono. 2018. Multi-Task and Multi-Lingual Joint Learning of Neural Lexical Utterance Classification Based on Partially-Shared Modeling. In Proceedings of the 27th International Conference on Computational Linguistics. Association for Computational Linguistics, 3586-3596.

Cognition-Cognizant Sentiment Analysis With Multitask Subjectivity Summarization Based on Annotators' Gaze Behavior. Abhijit Mishra, Srikanth Tamilselvam, Riddhiman Dasgupta, Seema Nagar, Kuntal Dey, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceAbhijit Mishra, Srikanth Tamilselvam, Riddhiman Dasgupta, Seema Nagar, and Kuntal Dey. 2018. Cognition-Cognizant Sentiment Analysis With Multitask Subjectivity Summarization Based on Annotators' Gaze Behavior. In Proceedings of the AAAI Conference on Artificial Intelligence.

Cross-Stitch Networks for Multi-Task Learning. Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, Martial Hebert, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEEIshan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. 2016. Cross-Stitch Networks for Multi-Task Learning. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 3994-4003.

Natural Language Inference by Tree-Based Convolution and Heuristic Matching. Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, Zhi Jin, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2Short Papers)Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. 2016. Natural Language Inference by Tree-Based Convolution and Heuristic Matching. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, 130-136.

Multi-Domain Dialog State Tracking Using Recurrent Neural Networks. Nikola Mrkšić, Ó Diarmuid, Blaise Séaghdha, Milica Thomson, Pei-Hao Gašić, David Su, Tsung-Hsien Vandyke, Steve Wen, Young, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics2Short Papers)Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thomson, Milica Gašić, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen, and Steve Young. 2015. Multi-Domain Dialog State Tracking Using Recurrent Neural Networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). Association for Computational Linguistics, 794-799.

Answering While Summarizing: Multi-Task Learning for Multi-Hop QA with Evidence Extraction. Kosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito, Hisako Asano, Junji Tomita, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsKosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito, Hisako Asano, and Junji Tomita. 2019. Answering While Summarizing: Multi-Task Learning for Multi-Hop QA with Evidence Extraction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2335-2345.

Keeping Consistency of Sentence Generation and Document Classification with Multi-Task Learning. Toru Nishino, Shotaro Misawa, Ryuji Kano, Tomoki Taniguchi, Yasuhide Miura, Tomoko Ohkuma, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational LinguisticsToru Nishino, Shotaro Misawa, Ryuji Kano, Tomoki Taniguchi, Yasuhide Miura, and Tomoko Ohkuma. 2019. Keeping Consistency of Sentence Generation and Document Classification with Multi-Task Learning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 3195-3205.

Multi-Task Neural Models for Translating Between Styles Within and Across Languages. Xing Niu, Sudha Rao, Marine Carpuat, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsAssociation for Computational LinguisticsXing Niu, Sudha Rao, and Marine Carpuat. 2018. Multi-Task Neural Models for Translating Between Styles Within and Across Languages. In Proceedings of the 27th International Conference on Computational Linguistics. Association for Computational Linguistics, 1008- 1021.

Universal Dependencies v1: A Multilingual Treebank Collection. Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajič, Christopher D Manning, Ryan Mcdonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Reut Tsarfaty, Daniel Zeman, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). the Tenth International Conference on Language Resources and Evaluation (LREC'16)European Language Resources Association (ELRAJoakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajič, Christopher D. Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Reut Tsarfaty, and Daniel Zeman. 2016. Universal Dependencies v1: A Multilingual Treebank Collection. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). European Language Resources Association (ELRA), 1659-1666.

Towards Comparability of Linguistic Graph Banks for Semantic Parsing. Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Silvie Cinková, Dan Flickinger, Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). European Language Resources Association (ELRA). the Tenth International Conference on Language Resources and Evaluation (LREC'16). European Language Resources Association (ELRA)Angelina Ivanova, and Zdeňka UrešováStephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Silvie Cinková, Dan Flickinger, Jan Hajič, Angelina Ivanova, and Zdeňka Urešová. 2016. Towards Comparability of Linguistic Graph Banks for Semantic Parsing. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16). European Language Resources Association (ELRA), 3991-3995.

Multi-Task Video Captioning with Video and Entailment Generation. Ramakanth Pasunuru, Mohit Bansal, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1Long Papers)Ramakanth Pasunuru and Mohit Bansal. 2017. Multi-Task Video Captioning with Video and Entailment Generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 1273-1283.

Continual and Multi-Task Architecture Search. Ramakanth Pasunuru, Mohit Bansal, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsRamakanth Pasunuru and Mohit Bansal. 2019. Continual and Multi-Task Architecture Search. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 1911-1922.

Deep Multitask Learning for Semantic Dependency Parsing. Hao Peng, Sam Thomson, Noah A Smith, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1Long Papers)Hao Peng, Sam Thomson, and Noah A. Smith. 2017. Deep Multitask Learning for Semantic Dependency Parsing. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2037-2048.

Multi-Task Networks with Universe, Group, and Task Feature Learning. Shiva Pentyala, Mengwen Liu, Markus Dreyer, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsShiva Pentyala, Mengwen Liu, and Markus Dreyer. 2019. Multi-Task Networks with Universe, Group, and Task Feature Learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 820-830.

Multi-Task Learning For Parsing The Alexa Meaning Representation Language. Vittorio Perera, Tagyoung Chung, Thomas Kollar, Emma Strubell, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceVittorio Perera, Tagyoung Chung, Thomas Kollar, and Emma Strubell. 2018. Multi-Task Learning For Parsing The Alexa Meaning Representation Language. In Proceedings of the AAAI Conference on Artificial Intelligence.

MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, Sebastian Ruder, arXiv:2005.00052Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingJonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross- Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. arXiv:2005.00052

Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data. Jonathan Pilault, Amine El, Christopher Pal, International Conference on Learning Representations. Jonathan Pilault, Amine El hattami, and Christopher Pal. 2021. Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data. In International Conference on Learning Representations.

Naranjo Question Answering Using End-to-End Multi-Task Learning Model. Fei Bhanu Pratap Singh Rawat, Hong Li, Yu, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningACMBhanu Pratap Singh Rawat, Fei Li, and Hong Yu. 2019. Naranjo Question Answering Using End-to-End Multi-Task Learning Model. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2547-2555.

Semi-Supervised Multitask Learning for Sequence Labeling. Marek Rei, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1Long Papers)Marek Rei. 2017. Semi-Supervised Multitask Learning for Sequence Labeling. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2121-2130.

Multi-Task Learning with Generative Adversarial Training for Multi-Passage Machine Reading Comprehension. Xiang Qiyu Ren, Sen Cheng, Su, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Qiyu Ren, Xiang Cheng, and Sen Su. 2020. Multi-Task Learning with Generative Adversarial Training for Multi-Passage Machine Reading Comprehension. Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (April 2020), 8705-8712.

Efficient Strategies for Hierarchical Text Classification: External Knowledge and Auxiliary Tasks. Gina Kervy Rivas Rojas, Arturo Bustamante, Marco Antonio Sobrevilla Oncevay, Cabezudo, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsKervy Rivas Rojas, Gina Bustamante, Arturo Oncevay, and Marco Antonio Sobrevilla Cabezudo. 2020. Efficient Strategies for Hierarchical Text Classification: External Knowledge and Auxiliary Tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2252-2257.

Latent Multi-Task Architecture Learning. Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, Anders Søgaard, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders Søgaard. 2019. Latent Multi-Task Architecture Learning. Proceedings of the AAAI Conference on Artificial Intelligence 33, 01 (July 2019), 4822-4829.

Dynamic Routing between Capsules. Sara Sabour, Nicholas Frosst, Geoffrey E Hinton, Advances in Neural Information Processing Systems. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017. Dynamic Routing between Capsules. In Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc.

A Hierarchical Multi-Task Approach for Learning Embeddings from Semantic Tasks. Victor Sanh, Thomas Wolf, Sebastian Ruder, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Victor Sanh, Thomas Wolf, and Sebastian Ruder. 2019. A Hierarchical Multi-Task Approach for Learning Embeddings from Semantic Tasks. Proceedings of the AAAI Conference on Artificial Intelligence 33 (July 2019), 6949-6956.

A Multi-Task Architecture on Relevance-Based Neural Query Translation. Hamed Sheikh Muhammad Sarwar, James Bonab, Allan, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsSheikh Muhammad Sarwar, Hamed Bonab, and James Allan. 2019. A Multi-Task Architecture on Relevance-Based Neural Query Translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 6339-6344.

Estimating the Influence of Auxiliary Tasks for Multi-Task Learning of Sequence Tagging Tasks. Fynn Schröder, Chris Biemann, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsFynn Schröder and Chris Biemann. 2020. Estimating the Influence of Auxiliary Tasks for Multi-Task Learning of Sequence Tagging Tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2971-2985.

Weakly Supervised Multi-Task Learning for Semantic Parsing. Bo Shao, Yeyun Gong, Junwei Bao, Jianshu Ji, Guihong Cao, Xiaola Lin, Nan Duan, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence OrganizationBo Shao, Yeyun Gong, Junwei Bao, Jianshu Ji, Guihong Cao, Xiaola Lin, and Nan Duan. 2019. Weakly Supervised Multi-Task Learning for Semantic Parsing. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, 3375-3381.

Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base. Tao Shen, Xiubo Geng, Tao Qin, Daya Guo, Duyu Tang, Nan Duan, Guodong Long, Daxin Jiang, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational LinguisticsTao Shen, Xiubo Geng, Tao Qin, Daya Guo, Duyu Tang, Nan Duan, Guodong Long, and Daxin Jiang. 2019. Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 2442-2451.

Text Categorization by Learning Predominant Sense of Words as Auxiliary Task. Kazuya Shimura, Jiyi Li, Fumiyo Fukumoto, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsKazuya Shimura, Jiyi Li, and Fumiyo Fukumoto. 2019. Text Categorization by Learning Predominant Sense of Words as Auxiliary Task. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 1109-1119.

A Multi-Task Approach to Learning Multilingual Representations. Karan Singla, Dogan Can, Shrikanth Narayanan, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2Short Papers)Karan Singla, Dogan Can, and Shrikanth Narayanan. 2018. A Multi-Task Approach to Learning Multilingual Representations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, 214-220.

Deep Multi-Task Learning with Low Level Tasks Supervised at Lower Layers. Anders Søgaard, Yoav Goldberg, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2Short Papers)Anders Søgaard and Yoav Goldberg. 2016. Deep Multi-Task Learning with Low Level Tasks Supervised at Lower Layers. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, 231-235.

Korean Morphological Analysis with Tied Sequence-to-Sequence Multi-Task Model. Hyun-Je Song, Seong-Bae Park, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational LinguisticsHyun-Je Song and Seong-Bae Park. 2019. Korean Morphological Analysis with Tied Sequence-to-Sequence Multi-Task Model. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 1436-1441.

ZPR2: Joint Zero Pronoun Recovery and Resolution Using Multi-Task Learning and BERT. Linfeng Song, Kun Xu, Yue Zhang, Jianshu Chen, Dong Yu, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsLinfeng Song, Kun Xu, Yue Zhang, Jianshu Chen, and Dong Yu. 2020. ZPR2: Joint Zero Pronoun Recovery and Resolution Using Multi-Task Learning and BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 5429-5434.

Hierarchical Multi-Task Learning for Organization Evaluation of Argumentative Student Essays. Wei Song, Ziyao Song, Lizhen Liu, Ruiji Fu, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Ninth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence OrganizationWei Song, Ziyao Song, Lizhen Liu, and Ruiji Fu. 2020. Hierarchical Multi-Task Learning for Organization Evaluation of Argumentative Student Essays. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, 3875-3881.

BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning. Asa Cooper Stickland, Iain Murray, PMLRProceedings of the 36th International Conference on Machine Learning (ICML). the 36th International Conference on Machine Learning (ICML)Asa Cooper Stickland and Iain Murray. 2019. BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning. In In Proceedings of the 36th International Conference on Machine Learning (ICML). PMLR, 5986-5995.

Learning General Purpose Distributed Sentence Representations via Large Scale Multi-Task Learning. Sandeep Subramanian, Adam Trischler, Yoshua Bengio, Christopher J Pal, International Conference on Learning Representations. Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J Pal. 2018. Learning General Purpose Distributed Sentence Representations via Large Scale Multi-Task Learning. In International Conference on Learning Representations.

CompGuessWhat?!: A Multi-Task Evaluation Framework for Grounded Language Learning. Alessandro Suglia, Ioannis Konstas, Andrea Vanzo, Emanuele Bastianelli, Desmond Elliott, Stella Frank, Oliver Lemon, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsAlessandro Suglia, Ioannis Konstas, Andrea Vanzo, Emanuele Bastianelli, Desmond Elliott, Stella Frank, and Oliver Lemon. 2020. CompGuessWhat?!: A Multi-Task Evaluation Framework for Grounded Language Learning. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 7625-7641.

Emotion Detection and Classification in a Multigenre Corpus with Joint Multi-Task Deep Learning. Shabnam Tafreshi, Mona Diab, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsAssociation for Computational LinguisticsShabnam Tafreshi and Mona Diab. 2018. Emotion Detection and Classification in a Multigenre Corpus with Joint Multi-Task Deep Learning. In Proceedings of the 27th International Conference on Computational Linguistics. Association for Computational Linguistics, 2905-2913.

HyperGrid Transformers: Towards A Single Model for Multiple Tasks. Yi Tay, Zhe Zhao, Dara Bahri, Donald Metzler, Da-Cheng Juan, International Conference on Learning Representations. Yi Tay, Zhe Zhao, Dara Bahri, Donald Metzler, and Da-Cheng Juan. 2020. HyperGrid Transformers: Towards A Single Model for Multiple Tasks. In International Conference on Learning Representations.

Hierarchical Inter-Attention Network for Document Classification with Multi-Task Learning. Bing Tian, Yong Zhang, Jin Wang, Chunxiao Xing, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence OrganizationBing Tian, Yong Zhang, Jin Wang, and Chunxiao Xing. 2019. Hierarchical Inter-Attention Network for Document Classification with Multi-Task Learning. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, 3569-3575.

One "Ruler" for All Languages: Multi-Lingual Dialogue Evaluation with Adversarial Multi-Task Learning. Xiaowei Tong, Zhenxin Fu, Mingyue Shang, Dongyan Zhao, Rui Yan, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Seventh International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence OrganizationXiaowei Tong, Zhenxin Fu, Mingyue Shang, Dongyan Zhao, and Rui Yan. 2018. One "Ruler" for All Languages: Multi-Lingual Dialogue Evaluation with Adversarial Multi-Task Learning. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, 4432-4438.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, arXiv:1706.03762arXiv:1706.03762Attention Is All You Need. csAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. arXiv:1706.03762 [cs] (Dec. 2017). arXiv:1706.03762 [cs]

Twitter Demographic Classification Using Deep Multi-Modal Multi-Task Learning. Prashanth Vijayaraghavan, Soroush Vosoughi, Deb Roy, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2Short Papers)Prashanth Vijayaraghavan, Soroush Vosoughi, and Deb Roy. 2017. Twitter Demographic Classification Using Deep Multi-Modal Multi-Task Learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, 478-483.

SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in Neural Information Processing Systems. Curran Associates, Inc32Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Advances in Neural Information Processing Systems, Vol. 32. Curran Associates, Inc.

GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, International Conference on Learning Representations. Association for Computational LinguisticsAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In International Conference on Learning Representations 2019. Association for Computational Linguistics, 353-355.

Sentiment Classification in Customer Service Dialogue with Topic-Aware Multi-Task Learning. Jiancheng Wang, Jingjing Wang, Changlong Sun, Shoushan Li, Xiaozhong Liu, Luo Si, Min Zhang, Guodong Zhou, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Jiancheng Wang, Jingjing Wang, Changlong Sun, Shoushan Li, Xiaozhong Liu, Luo Si, Min Zhang, and Guodong Zhou. 2020. Sentiment Classification in Customer Service Dialogue with Topic-Aware Multi-Task Learning. Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (April 2020), 9177-9184.

Multi-Task Self-Supervised Learning for Disfluency Detection. Shaolei Wang, Wangxiang Che, Qi Liu, Pengda Qin, Ting Liu, William Yang Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Shaolei Wang, Wangxiang Che, Qi Liu, Pengda Qin, Ting Liu, and William Yang Wang. 2020. Multi-Task Self-Supervised Learning for Disfluency Detection. Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (April 2020), 9193-9200.

Masking Orchestration: Multi-Task Pretraining for Multi-Role Dialogue Representation Learning. Tianyi Wang, Yating Zhang, Xiaozhong Liu, Changlong Sun, Qiong Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Tianyi Wang, Yating Zhang, Xiaozhong Liu, Changlong Sun, and Qiong Zhang. 2020. Masking Orchestration: Multi-Task Pretraining for Multi-Role Dialogue Representation Learning. Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (April 2020), 9217-9224.

Personalized Microblog Sentiment Classification via Adversarial Cross-Lingual Multi-Task Learning. Weichao Wang, Shi Feng, Wei Gao, Daling Wang, Yifei Zhang, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsWeichao Wang, Shi Feng, Wei Gao, Daling Wang, and Yifei Zhang. 2018. Personalized Microblog Sentiment Classification via Adversarial Cross-Lingual Multi-Task Learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 338-348.

Multi-Task Learning for Multilingual Neural Machine Translation. Yiren Wang, Chengxiang Zhai, Hany Hassan Awadalla, arXiv:2010.02523Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingYiren Wang, ChengXiang Zhai, and Hany Hassan Awadalla. 2020. Multi-Task Learning for Multilingual Neural Machine Translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. arXiv:2010.02523

Gradient Vaccine: Investigating and Improving Multi-Task Optimization in Massively Multilingual Models. Zirui Wang, Yulia Tsvetkov, Orhan Firat, Yuan Cao, International Conference on Learning Representations. Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao. 2020. Gradient Vaccine: Investigating and Improving Multi-Task Optimization in Massively Multilingual Models. In International Conference on Learning Representations.

Multi-Task Learning for Chemical Named Entity Recognition with Chemical Compound Paraphrasing. Taiki Watanabe, Akihiro Tamura, Takashi Ninomiya, Takuya Makino, Tomoya Iwakura, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational LinguisticsTaiki Watanabe, Akihiro Tamura, Takashi Ninomiya, Takuya Makino, and Tomoya Iwakura. 2019. Multi-Task Learning for Chemical Named Entity Recognition with Chemical Compound Paraphrasing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 6244-6249.

Collaborative Multi-Domain Sentiment Classification. Fangzhao Wu, Yongfeng Huang, 2015 IEEE International Conference on Data Mining. Fangzhao Wu and Yongfeng Huang. 2015. Collaborative Multi-Domain Sentiment Classification. In 2015 IEEE International Conference on Data Mining. 459-468.

Personalized Microblog Sentiment Classification via Multi-Task Learning. Fangzhao Wu, Yongfeng Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence7Fangzhao Wu and Yongfeng Huang. 2016. Personalized Microblog Sentiment Classification via Multi-Task Learning. Proceedings of the AAAI Conference on Artificial Intelligence (2016), 7.

Different Absorption from the Same Sharing: Sifted Multi-Task Learning for Fake News Detection. Lianwei Wu, Yuan Rao, Haolin Jin, Ambreen Nazir, Ling Sun, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational LinguisticsLianwei Wu, Yuan Rao, Haolin Jin, Ambreen Nazir, and Ling Sun. 2019. Different Absorption from the Same Sharing: Sifted Multi-Task Learning for Fake News Detection. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 4644-4653.

A Syntax-Aware Multi-Task Learning Framework for Chinese Semantic Role Labeling. Qingrong Xia, Zhenghua Li, Min Zhang, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational LinguisticsQingrong Xia, Zhenghua Li, and Min Zhang. 2019. A Syntax-Aware Multi-Task Learning Framework for Chinese Semantic Role Labeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 5382-5392.

Learning What to Share: Leaky Multi-Task Network for Text Classification. Liqiang Xiao, Honglun Zhang, Wenqing Chen, Yongkun Wang, Yaohui Jin, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsAssociation for Computational LinguisticsLiqiang Xiao, Honglun Zhang, Wenqing Chen, Yongkun Wang, and Yaohui Jin. 2018. Learning What to Share: Leaky Multi-Task Network for Text Classification. In Proceedings of the 27th International Conference on Computational Linguistics. Association for Computational Linguistics, 2055-2065.

MCapsNet: Capsule Network for Text with Multi-Task Learning. Liqiang Xiao, Honglun Zhang, Wenqing Chen, Yongkun Wang, Yaohui Jin, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsLiqiang Xiao, Honglun Zhang, Wenqing Chen, Yongkun Wang, and Yaohui Jin. 2018. MCapsNet: Capsule Network for Text with Multi-Task Learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 4565-4574.

Adaptive Multi-Task Transfer Learning for Chinese Word Segmentation in Medical Text. Junjie Xing, Kenny Zhu, Shaodian Zhang, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsAssociation for Computational LinguisticsJunjie Xing, Kenny Zhu, and Shaodian Zhang. 2018. Adaptive Multi-Task Transfer Learning for Chinese Word Segmentation in Medical Text. In Proceedings of the 27th International Conference on Computational Linguistics. Association for Computational Linguistics, 3619-3630.

A Unified Multi-Task Adversarial Learning Framework for Pharmacovigilance Mining. Shweta Yadav, Asif Ekbal, Sriparna Saha, Pushpak Bhattacharyya, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsShweta Yadav, Asif Ekbal, Sriparna Saha, and Pushpak Bhattacharyya. 2019. A Unified Multi-Task Adversarial Learning Framework for Pharmacovigilance Mining. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 5234-5245.

Knowledge-Enhanced Hierarchical Attention for Community Question Answering with Multi-Task and Adaptive Learning. Min Yang, Lei Chen, Xiaojun Chen, Qingyao Wu, Wei Zhou, Ying Shen, Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence OrganizationMin Yang, Lei Chen, Xiaojun Chen, Qingyao Wu, Wei Zhou, and Ying Shen. 2019. Knowledge-Enhanced Hierarchical Attention for Community Question Answering with Multi-Task and Adaptive Learning. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, 5349-5355.

A Unified Perspective on Multi-Domain and Multi-Task Learning. Yongxin Yang, Timothy M Hospedales, 9Yongxin Yang and Timothy M Hospedales. 2015. A Unified Perspective on Multi-Domain and Multi-Task Learning. (2015), 9.

Exploiting Entity BIO Tag Embeddings and Multi-Task Learning for Relation Extraction with Imbalanced Data. Wei Ye, Bo Li, Rui Xie, Zhonghao Sheng, Long Chen, Shikun Zhang, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsWei Ye, Bo Li, Rui Xie, Zhonghao Sheng, Long Chen, and Shikun Zhang. 2019. Exploiting Entity BIO Tag Embeddings and Multi-Task Learning for Relation Extraction with Imbalanced Data. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 1351-1360.

Gradient Surgery for Multi-Task Learning. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, Chelsea Finn, Advances in Neural Information Processing Systems. 33Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. 2020. Gradient Surgery for Multi-Task Learning. Advances in Neural Information Processing Systems 33 (2020), 5824-5836.

Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect Morphological Modeling. Nasser Zalmout, Nizar Habash, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsNasser Zalmout and Nizar Habash. 2019. Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect Morphological Modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 1775-1786.

Adaptive Knowledge Sharing in Multi-Task Learning: Improving Low-Resource Neural Machine Translation. Poorya Zaremoodi, Wray Buntine, Gholamreza Haffari, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2Short Papers)Poorya Zaremoodi, Wray Buntine, and Gholamreza Haffari. 2018. Adaptive Knowledge Sharing in Multi-Task Learning: Improving Low-Resource Neural Machine Translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, 656-661.

CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning. Daojian Zeng, Haoran Zhang, Qianying Liu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Daojian Zeng, Haoran Zhang, and Qianying Liu. 2020. CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning. Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (April 2020), 9507-9514.

Neural Simile Recognition with Cyclic Multitask Learning and Local Attention. Jiali Zeng, Linfeng Song, Jinsong Su, Jun Xie, Wei Song, Jiebo Luo, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Jiali Zeng, Linfeng Song, Jinsong Su, Jun Xie, Wei Song, and Jiebo Luo. 2020. Neural Simile Recognition with Cyclic Multitask Learning and Local Attention. Proceedings of the AAAI Conference on Artificial Intelligence 34, 05 (April 2020), 9515-9522.

Multi-Task Label Embedding for Text Classification. Honglun Zhang, Liqiang Xiao, Wenqing Chen, Yongkun Wang, Yaohui Jin, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsHonglun Zhang, Liqiang Xiao, Wenqing Chen, Yongkun Wang, and Yaohui Jin. 2018. Multi-Task Label Embedding for Text Classification. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 4545-4553.

A Generalized Recurrent Neural Architecture for Text Classification with Multi-Task Learning. Honglun Zhang, Liqiang Xiao, Yongkun Wang, Yaohui Jin, Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Sixth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence OrganizationHonglun Zhang, Liqiang Xiao, Yongkun Wang, and Yaohui Jin. 2017. A Generalized Recurrent Neural Architecture for Text Classification with Multi-Task Learning. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, 3385-3391.

Text Emotion Distribution Learning via Multi-Task Convolutional Neural Network. Yuxiang Zhang, Jiamei Fu, Dongyu She, Ying Zhang, Senzhang Wang, Jufeng Yang, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Seventh International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence OrganizationYuxiang Zhang, Jiamei Fu, Dongyu She, Ying Zhang, Senzhang Wang, and Jufeng Yang. 2018. Text Emotion Distribution Learning via Multi-Task Convolutional Neural Network. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, 4595-4601.

A Survey on Multi-Task Learning. Yu Zhang, Qiang Yang, IEEE Transactions on Knowledge and Data Engineering. Yu Zhang and Qiang Yang. 2021. A Survey on Multi-Task Learning. IEEE Transactions on Knowledge and Data Engineering (2021).

SpanMlt: A Span-Based Multi-Task Learning Framework for Pair-Wise Aspect and Opinion Terms Extraction. He Zhao, Longtao Huang, Rong Zhang, Quan Lu, Hui Xue, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsHe Zhao, Longtao Huang, Rong Zhang, Quan Lu, and Hui Xue. 2020. SpanMlt: A Span-Based Multi-Task Learning Framework for Pair-Wise Aspect and Opinion Terms Extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 3239-3248.

A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization. Sendong Zhao, Ting Liu, Sicheng Zhao, Fei Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Sendong Zhao, Ting Liu, Sicheng Zhao, and Fei Wang. 2019. A Neural Multi-Task Learning Framework to Jointly Model Medical Named Entity Recognition and Normalization. Proceedings of the AAAI Conference on Artificial Intelligence 33, 01 (July 2019), 817-824.

Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks. Renjie Zheng, Junkun Chen, Xipeng Qiu, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization. the Twenty-Seventh International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence OrganizationRenjie Zheng, Junkun Chen, and Xipeng Qiu. 2018. Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization, 4616-4622.

Multi-Task Learning with Language Modeling for Question Generation. Wenjie Zhou, Minghua Zhang, Yunfang Wu, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational LinguisticsWenjie Zhou, Minghua Zhang, and Yunfang Wu. 2019. Multi-Task Learning with Language Modeling for Question Generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 3394-3399.

Multi-Task Learning for Natural Language Generation in Task-Oriented Dialogue. Chenguang Zhu, Michael Zeng, Xuedong Huang, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational LinguisticsChenguang Zhu, Michael Zeng, and Xuedong Huang. 2019. Multi-Task Learning for Natural Language Generation in Task-Oriented Dialogue. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 1261-1266.

PinText: A Multitask Text Embedding System in Pinterest. Jinfeng Zhuang, Yu Liu, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningACMJinfeng Zhuang and Yu Liu. 2019. PinText: A Multitask Text Embedding System in Pinterest. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2653-2661.