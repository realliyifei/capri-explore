# A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches

CorpusID: 254408864 - [https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8](https://www.semanticscholar.org/paper/bad1ea60373fc77c1ff82c5ff99f1fd38c972ae8)

Fields: Computer Science, Linguistics

## (s3) Cloze-style:
### Question: What is the cloze-style task in machine reading comprehension?

#Reference=0

(p3.0) The correct answer is part of the question (usually a word or an entity) that is removed from question. The cloze style task needs to fill in the blank with the correct word or entity by learning the function , such that = ( , ).

(p3.1) To show the frequency of each task among multi-hop studies Figure 4 has been prepared. This figure contains some points:

(p3.2) • Most studies have focused on the Span-extraction and Multiple-choice tasks. One of the main reasons is that most available MRC datasets are also in these forms and automatically this fact encourages research to focus on these two tasks.
## (s6) MULTI-HOP MRC TECHNIQUES
### Question: What are the main categories and techniques in multi-hop MRC research?

#Reference=2

(p6.0) In this paper, 31 studies have been investigated, which propose a model for multi-hop MRC based on the presented problem in section 2. It is important to note that since there is a close relationship between MRC and Question Answering, most of the existing machine reading comprehension tasks are in the form of textual question answering [24], and also MRC is known as a basic task of textual question answering [19]. Thus, we consider cloze domain textual question answering as a typical MRC task in this paper .

(p6.1) Existing studies for multi-hop MRC can be mainly divided into three categories: decomposition, recurrent reasoning based on memory retrieval, and multi-step reasoning based on graph neural networks. For each category, after explaining the main technique, the multi-hop MRC models will be reviewed in detail; beside reviewing the detail architectures of each model, we also focus on the superiority and the motivation of them. Also, the disadvantages of each technique will be discussed at the end. In the next section (4) a comprehensive comparison of the techniques and models will be presented.

(p6.2) The techniques do not have a specific order, because all three techniques have been used by models from 2018 to 2022 ( Figure   40), but as much as possible, the studies within the techniques have been according to the order of published time.
## (s7) Decomposition technique:
### Question: What is the decomposition technique in multi-hop MRC?

#Reference=2

(p7.0) Complicated question is a basic challenge of multi-hop MRC, unlike the single-hop questions, they cannot be answered easily and require complicated reasoning. Since the human reasoning about complex questions is done by decomposition, answering subquestions, summarizing, and comparing [26], then this technique has focused on simplifying the problem by decomposition of a complex question into multiple simple sub-questions. It means it reduces multi-hop MRC to multiple single-hop MRC. This technique mostly uses the single-hop MRC models to find the answers to sub-questions and then combine the answers to obtain the final answer. In the following, the models which use this technique for multi-hop MRC will be reviewed in detail.

(p7.1) Self-assembling MNM: Jiang and Bansal [18] focused on identifying the sub-questions in the correct reasoning order and presented an interpretable and controller-based self-assembling neural modular network for the multi-hop reasoning process which includes two main parts, Modular Network with a Controller (top) and the Dynamically-assembled Modular Network (bottom) that can be seen in Figure 10. The main idea of the model to handle multi-hop questions is done with Controller that computes an attention distribution over all question words at every reasoning step, which finds the sub-question that should be answered at the current step. In summary, Controller reads the question and predicts a series of modules that could be executed in order to answer the given question. Each module deals with a single-hop sub-question, then they will be chained together according to the predicated order by controller to get the final answer. The mentioned modules are described as follows: All modules take the question representation , context representation ℎ, and sub-question vector as input.
## (s10) •
### Question: How do modern algorithms decompose multi-hop questions into simpler sub-questions?

#Reference=4

(p10.0) The NoOp module can be seen as a skip command. It is used to reduce computations when the controller decides no action is required. However, this system approaches question decomposition by having a decomposer model trained via human labels (Cao and Liu, 2021a). Figure 5: Architecture of Self-assembling MNM [18] ONUS: Perez et al. [27] proposed an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that map a complicate multi-hop question to some simpler single-hop sub-questions. Unlike other decomposition studies use a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions to decompose multi-hop question which require significant human effort, this model automatically learns to decompose different kinds of questions. The main idea has been shown in Figure 6. To decompose multi-hop question to simpler corpus , First some candidate sub-question from a simple corpus will be created by mining 10M possible sub-questions from Common Crawl with a classifier. It then trains a decomposition model on the mined data using Q and D with unsupervised sequence-to-sequence learning to map multi-hop questions to sub-question. With this idea, the model is able to train a large transformer model to generate decompositions and avoid heuristic/extractive decompositions.   RERC: Fu et al. [26] also handled multi-hop MRC with a focus on decomposing complicated questions into simpler ones. They proposed a three-stage Relation Extractor-Reader and Comparator (RERC) model. RERC is consist of 1) Relation Extractor to decompose the complex questions into simple sub-questions by automatically extracting the subject and key relations of the complex question, 2) Reader to find the answers to the sub-questions in turn by an advanced ALBERT model, and finally, 3) Comparator to perform the numerical comparison and summarizes all the answers to get the final answer ( Figure 9)

(p10.1) The main part of this model is Relation Extractor with two different structures: 1) classification-type (CRERC), where the evidence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; 2) span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations. Figure 9: Architecture of the RERC models [26] The Decomposition technique was one of the first ideas for multi-hop MRC and as you can see in recent years (2021) still has attracted attention. The main disadvantage of this technique is that, instead of focusing on multi-hop reasoning as an important key of multi-hop MRC, it focuses on reducing the problem to a single-hop MRC. Thus, they actually do not go far beyond single-hop models.
## (s11) Recurrent reasoning-based technique
### Question: What is the role of recurrent reasoning in multi-hop MRC tasks?

#Reference=1

(p11.0) The sequence models have been first used for single-hop MRC tasks, and most of them are based on Recurrent Neural Networks (RNNs), some studies focus on using them in multi-hop MRC. It can be called state-based reasoning models and they are closer to a standard attention-based RC model with an additional "state" representation that is iteratively updated. The changing state representation results in the model focusing on different parts of the passage during each iteration, allowing it to combine information from different parts of the passage [28]. Most models, presented in this section, have used advanced neural network concepts, such as attention mechanism and network memory for multi-hop reasoning. In the following the models which use this technique will be reviewed in detail including the architecture alongside the superiority and the motivation of them.
## (s13) Commonsense Algorithm consists of Commonsense Selection Representation to select useful relational knowledge paths and
### Question: What are the key features and goals of the Commonsense Algorithm, QFE, TAP, and PH-Model in multi-hop QA?

#Reference=5

(p13.0) Commonsense Model Incorporation to fill the gaps of reasoning between hops of inference using Necessary and Optional Information Cell (NOIC).  [29] QFE: Nishida et al. [30] proposed a model for explainable multi-hop QA named Query Focused Extractor (QFE) which is based on a summarization idea. They use the multi-task learning of the QA model for answer selection and QFE for evidence extraction.

(p13.1) QFE as the main part of this model adaptively determines the number of evidence sentences by considering the dependency among the evidence sentences and the coverage of the question. Unlike other approaches that extract each evidence sentence separately, QFE uses an RNN and attention mechanism to consider important information in the question and the relationships between sentences. This query-aware recurrent structure enables QFE to consider the dependency among the evidence sentences and cover the important information in the question sentence. In brief, the main goal of QFE is to summarize the context according to the question. Query-focused summarization is the task of summarizing the source document with regard to the given query. The multitask learning with QFE is general in the sense that it can be combined with any QA model. The overview of QFE is shown in Figure 11. is the current summarization vector, is the query vector considering the current summarization, is the extracted sentence, updates the RNN state. Figure 11: Overview of Query Focused Extractor at step t [30] TAP: Bhargav et al. [31] proposed a deep neural architecture, called TAP (Translucent Answer Prediction) cover of two main ideas: (1) Local Interaction: Each sentence should be understood in the context of its neighboring sentences and the question, (2) Global Interaction: A global (inter-passage) interaction among sentences must be identified and used for supporting facts. TAP is a hierarchical architecture that tries to capture the local and global interactions between the sentences and consists of two main parts: (Figure 12)

(p13.2) • Local and Global Interaction eXtractor (LoGIX) with three layers: local layer to obtain intra-passage dependencies, Global Layer to obtain inter-passage dependencies, and Supporting Facts Prediction Layer to calculate the probability that a sentence is a supporting fact.

(p13.3) • Answer Predictor (AP) to predict the final answer by reasoning over the supporting facts. It consists of four parts: Input Data Shaping to preprocess and concatenate the supporting facts, Context Encoding to encode the context using a pre-trained BERT model, Answer Type Predictor to classify the question into one of the three classes (yes, no and, span), and Answer Span Predictor to predict the final answer. Figure 12: Architecture of TAP [31] PH-Model: Cong et al. [32] focused on using the benefit of the hierarchical structure of the natural language text (documentpassage-sentence-word-character), while most existing studies ignore this information in the natural language context. Then they proposed a model for Chinese multi-hop MRC named (PH-Model), in which P stands for the Passage reranking framework, and H denotes the Hierarchical neural network. As you can see in Figure 13, PH-Model consists of multiple main parts: Bi_ONLSTM, that is an ordered neuron LSTM is used to obtain hierarchical information from a passage (Instead of traditional LSTM), Bidirectional Attention Flow is used to extract the hierarchical information form paragraphs to get the query-aware context and the context-aware query representation, Fused layer is used to merge all information and finally Pointer network to obtain the probability of the start and end positions of the answer 
## (s14) Path-based:
### Question: What are path-based models in multi-hop MRC and how do they work?

#Reference=1

(p14.0) In the following, we review models that focus on finding the right path in information to find the answer. As multi-hop MRC faces more information and complex questions, finding the right path has become more important and difficult, so many models in this technique are path-based. One of the important advantages of path-based models is that they are interpretable because they can provide the evidence chain to the final answer.

(p14.1) EPAr: Jiang et al. [33] proposed an interpretable model named Explore-Propose-Assemble reader (EPAr) to mimic the coarseto-fine-grained reasoning behavior of humans when facing multiple long documents. The main idea is to construct a reasoning tree according to the documents like a hierarchical memory network and use the path in this tree to extract the final answer. This model has three components as shown in Figure 14:

(p14.2) •

(p14.3) The T-hop Document Explorer (DE) module constructs the reasoning tree like a hierarchical memory network. In each step, it selects one document, updates the memory cell using the selected document, and iteratively selects the next related document.
## (s15) •
### Question: How does the PathNet model approach multi-hop MRC?

#Reference=3

(p15.0) The Answer Proposer (AP) module uses the constructed reasoning tree to predict an answer from every root-to-leaf path.

(p15.1) •

(p15.2) The Evidence Assembler (EA) module extracts a key sentence containing the proposed answer from every path and combines them to predict the final answer. Figure 14: Architecture of EPAr [33] PathNet: Kundu et al. [34] proposed a path-based reasoning approach for multi-hop MRC which first extracts all paths in the passages based on implicit relations between entities, and then composes the passage representations along each path to compute a passage-based representation. In other words, the passages representation is achieved by considering the paths.

(p15.3) They first find all possible path from passages. It starts with selecting a passage that contains a head entity from the question, and then finds all entities and noun phrases from the same sentence. Afterward, it selects the next passage that contains the potential intermediate entity identified above. Finally, it is checked whether the next passage contains any of the candidate answer choices or not. The resulting will be a set of entity sequences. After obtaining all potential paths, it is time to score each path using the PathNet model based on two perspectives: 1) Context-based Path Scoring, which is based on the interaction with the question encoding, and 2) Passage-based Path Scoring, which is based on the interaction between the passage-based path encoding vector and the candidate encoding. There is an example of the process in Figure 15 which In the Rank-1 path, the model composes the implicit located in relations between (Zoo lake, Johannesburg) and (Johannesburg, Gauteng). However, this method extracts many invalid paths, then causes wasting the computing resources [35].

(p15.4) Question: (zoo lake, located in the administrative territorial entity, ?) Answer: gauteng Rank-1 Path: (zoo lake, Johannesburg, gauteng) Passage1: ... Zoo Lake is a popular lake and public park in Johannesburg, South Africa.

(p15.5) It is part of the Hermann Eckstein Park and is ... Passage2: ... Johannesburg (also known as Jozi, Joburg and Egoli) is the largest city in South Africa and is one of the 50 largest urban areas in the world. It is the provincial capital of Gauteng, which is ...
## (s17) Graph-based technique
### Question: What is the graph-based technique's role in multihop MRC?

#Reference=2

(p17.0) Graph-based techniques because of their natural language relationship representation ability [39] has attracted attention in multihop MRC. It is natural to model natural language context into graph structure and the process of multi-hop reasoning as moving among nodes [14]. The main idea of the Graph-based technique is to construct a graph based on the context and question, and then the reasoning is performed by message passing over this structure using graph neural networks. The process of constructing the graph from large textual data and reasoning over it are challenging tasks. There are a lot of studies that focus on these challenges which are explained in this subsection in detail.

(p17.1) Constructing graphs from input data is one of the basic parts of this technique. Some studies construct an entity graph from the input data, which means the nodes of the graph nodes are the entities of the context and question. A lot of studies work on this kind of graph which will be reviewed in the following.
## (s19) MHQA-GRN:
### Question: What improvements do MHQA-GRN and DFGN models bring to multi-hop reading comprehension?

#Reference=3

(p19.0) Song et al. [40] focused on inferring global context as an important key in multi-hop reading comprehension, while previous studies approximate global evidence with local coreference information with DAG-styled GRU. They proposed a model for better connecting global evidence, with a more complex graph compared to DAGs. They construct an entity graph with three types of edges: the edge between the same entity within a passage, the edges between two mentions of different entities within a context window, and coreference-typed edges. The graph might also have cycles which makes it difficult to apply a DAG network to it. (A graph with three types of edges and a DAG graph are shown in Figure 21).

(p19.1) For inferring the global context, the related information of the constructed graph has been merged. In this study, two recent graph neural networks have been applied to this graph: graph convolutional network (GCN) and graph recurrent network (GRN) for evidence aggregation. Afterward, an attention mechanism is applied in order to match the hidden states at each graph encoding step with the question representation. Finally, a probability distribution is calculated from the matching results. The architecture of this model is shown in Figure 22. However, this model still only implicitly combines knowledge from all passages, and are therefore unable to provide explicit reasoning paths [28]. DFGN: Xiao et al. [41] proposed a model to improve the interaction between the information of documents and the entity graph.

(p19.2) They proposed a fusion process of Doc2Graph and Graph2Doc for multi-hop reasoning that leads to a less noisy entity graph and more accurate answers. The process of constructing dynamic entity graph iterates in multiple rounds to achieve multi-hop reasoning. In each round, DFGN generates and reasons on a dynamic graph, where irrelevant entities are masked out while only reasoning sources are preserved, via a mask prediction module. Then the fusion block not only aggregates information from documents to the entity graph (doc2graph) but also propagate the information of the entity graph back to document representations (graph2doc). Figure 23 illustrates the Fusion block in DFGN which consists of:
## (s27) Techniques frequency
### Question: What trends in technique popularity are observed in multi-hop MRC studies from 2018 to 2022?

#Reference=0

(p27.0) The frequency of each technique among reviewed studies is shown in Figure 44. As you can see, the number of studies of the Graph-based techniques is the most, and after that the Recurrent reasoning-based technique has achieved good attention. But the number of studies cannot be enough to have a proper investigation, and it is needed to show the growth trend of each technique in different years. In this regard, Figure 45 shows the growth trend of each technique from 2018 to 2022. The graph-based technique has achieved the most attention in 2019, 2021, and 2022 that proves that popularity trend of this technique in different years as well. The first graph-free study has been proposed in 2020 and immediately this question was raised that whether the graph was really necessary due to the expensive computational? After that, some other studies followed this question and it can be said that can affect the popularity trend of the graph-based technique in future. However, graph-based technique still can be considered the most popular technique in multi-hop MRC. 
## (s28) Models Performance:
### Question: How do models' performances in multi-hop MRC get evaluated and compared?

#Reference=2

(p28.0) In this section we will show the performance of the models. This investigation is helpful in several ways; it will determine the stat-of-the-result, and also shows which models and techniques has achieved the best result. On the other hand, it can show the overall performance and effectiveness of each technique in multi-hop MRC To evaluate the results of the models we need to use the evaluation metrics of the datasets. HotpotQA [11] and Wikihop [55], are two populare datasets among the reviewed studies as it has been clear in Figure 46 in which shows the percentage of use of two datasets among the reviewed models from 2018 to 2022. Then they provide a proper situation for evaluating the model's performance. 
## (s29) HotpotQA
### Question: How are models evaluated on HotpotQA using EM and F1 metrics?

#Reference=4

(p29.0) In this section, the performance of models on HotpotQA [11] will be investigated. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same. F1 measures the average overlap between the predicted answer and the ground-truth answer. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.

(p29.1) To have an accurate comparison, the year of publications, the technique, along with the results, for Distracter setting and Fullwiki setting have been presented in this table 1 and 2. In the distractor setting, the best result is for AMGN [39] which is a graph-based model. Although the results are very diverse due to a large number of evaluation metrics, in general, graph-based methods have achieved relatively better results, and these good results were a motivation for great attention to the graph-based technique. But S2G [14] which is a graph-free model in 2021 has achieved a comparable result to the best graph-based model ( Table 1). A few models have published the result for the Fullwiki setting due to the difficulty of this setting, and among them, HGN [48], which is a graph-based model, has achieved the best result (Table 2).   
## (s30) WikiHop
### Question: What is the WikiHop dataset and how is its performance evaluated?

#Reference=2

(p30.0) The papers that have used the WikiHop [55] dataset is investigated in this section. WikiHop consists of 51k questions, answers, and context where each context consists of several documents from Wikipedia .Each question in WikiHop is a tuple, which denotes two entities, and their relationship, then the answers in the WikiHop dataset are a single entity. Accuracy is a popular and fairly common metric to evaluate the performance of multiple-choice and Cloze-style MRC tasks. In the multiple-choice task, it is required to check whether the correct answer has been selected from the candidate answers. In contrast, in the Cloze-style task, it is required to check whether the correct words have been selected for the missing words

(p30.1) Since the answer type of this model is multiple-choice then accuracy is the evaluation metric on this dataset which is obtained for both the test and development set. For each paper, the year of publication, the technique along with the results are shown in Table 3. The best result is for ChainEX [38] which has used the recurrent reasoning technique. Besides that, the graph-based models could achieve the good result in this dataset too. 
