# Multimodal Learning with Transformers: A Survey

CorpusID: 249642175 - [https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7](https://www.semanticscholar.org/paper/622428f5122ad12a40229e1768ecb929fd747ee7)

Fields: Computer Science, Medicine

## (s3) Multimodal Learning (MML)
### Question: What is Multimodal Learning and its significance in modern applications?

#Reference=31

(p3.0) MML [1], [60], [61] has been an important research area in recent decades; an early multimodal application -audiovisual speech recognition was studied in 1980s [62]. MML is key to human societies. The world we humans live in is a multimodal environment, thus both our observations and behaviours are multimodal [63]. For instance, an AI navigation robot needs multimodal sensors to perceive the real-world environment [64], [65], [66], e.g., camera, LiDAR, radar, ultrasonic, GNSS, HD Map, odometer. Furthermore, human behaviours, emotions, events, actions, and humour are multimodal, thus various human-centred MML tasks are widely studied, including multimodal emotion recognition [67], multimodal event representation [68], understanding multimodal humor [69], face-body-voice based video person-clustering [70], etc.

(p3.1) Thanks to the development of the internet and a wide variety of intelligent devices in recent years, increasing amounts of multimodal data are being transmitted over the internet, thus an increasing number of multimodal application scenarios are emerging. In modern life, we can see various multimodal applications, including commercial services (e.g., e-commerce/commodity retrieval [71], visionand-language navigation (VLN) [72], [73], [74], [75], [76]), communication (e.g., lip reading [77], sign language translation [28], [29]), human-computer interaction [78], healthcare AI [79], [80], surveillance AI [81], etc.

(p3.2) Moreover, in the era of Deep Learning, deep neural networks greatly promote the development of MML, and Transformers [2] are a highly competitive architecture family, bringing new challenges and opportunities to MML. In particular, the recent success of large language models and their multimodal derivatives [82], [83], [84], [85], [86] further demonstrates the potential of Transformers in multimodal foundation models.
## (s4) Transformers: a Brief History and Milestones
### Question: What are the key developments and applications of Transformer models in AI?

#Reference=42

(p4.0) Transformers are emerging as promising learners. Vanilla Transformer [2] benefits from a self-attention mechanism, and is a breakthrough model for sequence-specific representation learning that was originally proposed for NLP, achieving the state-of-the-art on various NLP tasks. Following the great success of Vanilla Transformer, a lot of derivative models have been proposed, e.g., BERT [4], BART [87], GPT [88], Longformer [43], Transformer-XL [89], XLNet [90].

(p4.1) Transformers currently stand at the dominant position in NLP domains, and this motivates researchers try to apply Transformers to other modalities, such as visual domains. In early attempts for visual domain, the general pipeline is "CNN features + standard Transformer encoder", and researchers achieved BERT-style pretraining, via preprocessing raw images by resizing to a low resolution and reshaping into a 1D sequence [91].

(p4.2) Vision Transformer (ViT) [5] is a seminal work that contributes an end-to-end solution by applying the encoder of Transformer to images. Both ViT and its variants have been widely applied to various computer vision tasks, including low-level tasks [92], recognition [93], detection [94], segmentation [95], etc, and also work well for both supervised [93] and self-supervised [96], [97], [98] visual learning. Moreover, some recently-released works provide further theoretical understanding for ViT, e.g., its internal representation robustness [99], the continuous behaviour of its latent representation propagation [100], [101]. Motivated by the great success of Transformer, VideoBERT [7] is a breakthrough work that is the first work to extend Transformer to the multimodal tasks. VideoBERT demonstrates the great potential of Transformer in multimodal context. Following VideoBERT, a lot of Transformer based multimodal pretraining models (e.g., ViLBERT [102], LXMERT [103], VisualBERT [104], VL-BERT [105], UNITER [106], CBT [107], Unicoder-VL [108], B2T2 [109], VLP [110], 12-in-1 [111], Oscar [112], Pixel-BERT [113], ActBERT [114], ImageBERT [115], HERO [116], UniVL [117]) have become research topics of increasing interest in the field of machine learning.

(p4.3) In 2021, CLIP [9] was proposed. It is a new milestone that uses multimodal pretraining to convert classification as a retrieval task that enables the pretrained models to tackle zero-shot recognition. Thus, CLIP is a successful practice that makes full use of large-scale multimodal pretraining to enable zero-shot learning. Recently, the idea of CLIP is further studied, e.g., CLIP pretrained model based zero-shot semantic segmentation [118], ALIGN [119], CLIP-TD [120], ALBEF [121], and CoCa [122].
## (s5) Multimodal Big Data
### Question: What are the recent trends in multimodal big data development?

#Reference=37

(p5.0) In the past decade, with the rapid development of internet applications such as social media and online retail, massive multimodal datasets have been proposed, e.g., Conceptual Captions [123], COCO [124], VQA [125], Visual Genome [126], SBU Captions [127], Cooking312K [7], LAIT [115], e-SNLI-VE [128], ARCH [129], Adversarial VQA [130], OTT-QA [18], MULTIMODALQA (MMQA) [131], VALUE [132], Fashion IQ [133], LRS2-BBC [134], ActivityNet [135], VisDial [136].

(p5.1) Some emergent new trends among the recently released multimodal datasets are:

(p5.2) (1) Data scales are larger. Various recently released datasets are million-scale, e.g., Product1M [137], Conceptual 12M [138], RUC-CAS-WenLan [139] (30M), HowToVQA69M [140], HowTo100M [141], ALT200M [142], LAION-400M [143].

(p5.3) (2) More modalities. In addition to the general modalities of vision, text, and audio, further diverse modalities are emerging, e.g., Pano-AVQA [144] -the first large-scale spatial and audio-visual question answering dataset on 360 • videos, YouTube-360 (YT-360) [145] (360 • videos), AIST++ [146] (a new multimodal dataset of 3D dance motion and music), Artemis [147] (affective language for visual arts). In particular, MultiBench [148] provides a dataset including 10 modalities.

(p5.4) (3) More scenarios. In addition to common caption and QA datasets, more applications and scenarios have been studied, e.g., CIRR [149] (real-life images), Product1M [137], Bed and Breakfast (BnB) [150] (vision-and-language navigation), M3A [151] (financial dataset), X-World [152] (autonomous drive).

(p5.5) (4) Tasks are more difficult. Beyond the straightforward tasks, more abstract multimodal tasks are proposed, e.g., MultiMET [153] (a multimodal dataset for metaphor understanding), Hateful Memes [154] (hate speech in multimodal memes).

(p5.6) (5) Instructional videos have become increasingly popular, e.g., cooking video YouCookII [155]. Aligning a sequence of instructions to a video of someone carrying out a task is an example of a powerful pretraining pretext task [7], [156]. Pretext tasks are pre-designed problems to force the models to learn representation by solving them.

(p5.7) Similar to other deep neural network architectures, Transformers are also data hungry. Therefore, their highcapacity models and multimodal big data basis co-create the prosperity of the Transformer based multimodal machine learning. For instance, big data bring zero-shot learning capability to VLP Transformer models.
## (s7) Vanilla Transformer
### Question: What is the structure and key components of a Vanilla Transformer?

#Reference=4

(p7.0) Vanilla Transformer has an encoder-decoder structure and is the origin of the Transformer-based research field. It takes tokenized input (see Section 3.1.1). Both its encoder and decoder are stacked by the Transformer layers/blocks, as demonstrated in Figure 1. Each block has two sub-layers, i.e., a multi-head self-attention (MHSA) layer (see Section 3.1.2) and a position-wise fully-connected feed-forward network (FFN) (see Section 3.1.3). To help the back propagation of the gradient, both MHSA and FFN use Residual Connection [159] (given an input x, the residual connection of any mapping f (·) is defined as x ← f (x) + x), followed by normalization layer. Thus, assuming that the input tensor is Z, the output of MHSA and FFN sub-layers can be formulated as:

(p7.1) where sublayer(·) is the mapping implemented by the sublayer itself and N (·) denotes normalization, e.g., BN (·) [160], LN (·) [161]. Discussion There is an important unsolved problem that is post-normalization versus pre-normalization. The original Vanilla Transformer uses post-normalization for each MHSA and FFN sub-layer. However, if we consider this from the mathematical perspective, pre-normalization makes more sense [162]. This is similar to the basic principle of the theory of matrix, that normalization should be performed before projection, e.g., Gram-Schmidt process 2 . This problem should be studied further by both theoretical research and experimental validation.
## (s8) Input Tokenization
### Question: How do Transformers handle input tokenization and position embedding?

#Reference=2

(p8.0) Tokenization Vanilla Transformer was originally proposed for machine translation as a sequence-to-sequence model, thus it is straightforward to take the vocabulary sequences as input. As mentioned previously, the original selfattention can model an arbitrary input as a fully-connected graph, independently of modalities. Specifically, both Vanilla and variant Transformers take in the tokenized sequences, where each token can be regarded as a node of the graph. 1. In this survey, "multimodal Transformer" means "Transformer in multimodal learning context".

(p8.1) 2. https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt process Special/Customized Tokens In Transformers, various special/customized tokens can be semantically defined as place-holders in the token sequences, e.g., mask token [MASK] [4]. Some common special tokens are summarized in appendix. Special tokens can be used in both uni-modal and multimodal Transformers.

(p8.2) Position Embedding Position embeddings are added to the token embeddings to retain positional information [4]. Vanilla Transformer uses sine and cosine functions to produce position embedding. To date, various implementations of position embedding have been proposed. The concrete solutions are outside the focus of this survey.
## (s9) Discussion
### Question: What are the main advantages of input tokenization in processing multimodal inputs?

#Reference=3

(p9.0) The main advantages of input tokenization include the following:

(p9.1) (1) Tokenization is a more general approach from a geometrically topological perspective, achieved by minimizing constraints caused by different modalities. In general, every modality has intrinsic constraints on modelling. For instance, sentences have sequential structures that are well-suited by RNN, and photos are restricted in aligned grid matrices that CNN works well for. Tokenization helps Transformers inherently to process different modalities universally via irregular sparse structures. Thus even Vanilla Transformer can encode multimodal inputs flexibly by just concatenation, weighted summation, even without any multimodal tailor-made modifications.

(p9.2) (2) Tokenization is a more flexible approach to organize the input information via concatenation/stack, weighted summation, etc. Vanilla Transformer injects temporal information to the token embedding by summing position embedding. For instance, when use Transformer to model freehand sketch drawing [163], each input token can integrate various drawing stroke patterns, e.g., stroke coordinates, stroke ordering, pen state (start/end).

(p9.3) (3) Tokenization is compatible with the task-specific customized tokens, e.g., [MASK] token [4] for Masked Language Modelling, [CLASS] token [5] for classification.
## (s10) Discussion How to understand position embedding to
### Question: What is the role of position embedding in Transformers?

#Reference=4

(p10.0) Transformers is an open problem. It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer. For cloud point [164] and sketch drawing stroke [163], their token element is already a coordinate, meaning that position embedding is optional, not necessary. Furthermore, position embedding can be regarded as a kind of general additional information. In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, e.g., the pen state of sketch drawing stroke [163], cameras and viewpoints in surveillance [165]. There is a comprehensive survey [166] discussing the position information in Transformers. For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular), position embeddings help Transformers to learn or encode the underlying structures. Considered from the mathematical perspective of self-attention, i.e., scaled dot-product attention, attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing. Thus, in most cases, position embedding is necessary for Transformers.
## (s12) Self-Attention (SA)
### Question: How does self-attention work in Transformer models?

#Reference=7

(p12.0) After preprocessing, embedding Z will go through three projection matrices (W Q ∈ R d×dq , W K ∈ R d×d k , and W V ∈ R d×dv , d q = d k ) to generate three embeddings Q (Query), K (Key), and V (Value):

(p12.1) The output of self-attention is defined as

(p12.2) Given an input sequence, self-attention allows each element to attend to all the other elements, so that self-attention encodes the input as a fully-connected graph. Therefore, the encoder of Vanilla Transformer can be regarded as a fullyconnected GNN encoder, and the Transformer family has the non-local ability of global perception, similar to the Non-Local Network [167].

(p12.3) Masked Self-Attention (MSA) In practice, modification of self-attention is needed to help the decoder of Transformer to learn contextual dependence, to prevent positions from attending to subsequent positions, as

(p12.4) where M is a masking matrix. For instance, in GPT [88], an upper triangular mask to enable look-ahead attention where each token can only look at the past tokens. Masking can be used in both encoder [163], [168] and decoder of Transformer, and has flexible implementations, e.g., 0-1 hard mask [163], soft mask [168].

(p12.5) In both uni-modal and multimodal practices, specific masks are designed based on domain knowledge and prior knowledge. Essentially, MSA is used to inject additional knowledge to Transformer models, e.g., [24], [163], [169], [170].
## (s18) Tokenization and Embedding Processing
### Question: What are the key steps in preparing data for Transformers in multimodal inputs?

#Reference=14

(p18.0) Given an input from an arbitrary modality, users only need to perform two main steps, (1) tokenize the input, and (2) select an embedding space to represent the tokens, before inputting the data into Transformers. In practice, both the tokenizing input and selecting embedding for the token are vital for Transformers but highly flexible, with many alternatives. For instance, given an image, the solution of tokenizing and embedding is not unique. Users can choose or design tokenization at multiple granularity levels -coarse-grained vs. fine-grained. e.g., use ROIs (obtained by an object detector) and CNN features as tokens and token embeddings [102], use patches and linear projection as tokens and token embeddings [5], or use graph node (obtained by object detector and graph generator) and GNN features as tokens and token embeddings [181]. Given a tokenization plan, the subsequent embedding approaches can be diverse. For example, for video input, a common tokenization is to treat the non-overlapping windows (down-sampled) over the video as tokens, and their embeddings can then be extracted by various 3D CNNs, e.g., VideoBERT [7], CBT [107], and UniVL [117] use S3D [186], ActBERT uses ResNet-3D [187]. Table 1 summarizes some common practices of multimodal inputs for Transformers, including RGB, video, audio/speech/music, text, graph, etc.

(p18.1) Discussion When considered from the perspective of geometric topology, each of the modalities listed in Table 1 can be regarded as a graph. An RGB image is essentially a neat grid graph in the pixel space. Both video and audio are clip/segment based graphs over a complex space involving temporal and semantic patterns. Both 2D and 3D drawing sketches [78], [163] are a kind of sparse graph if we consider their key points along the drawing strokes. Similar to sketches, the human pose also is a kind of graph. 3D point cloud is a graph in which each coordinate is a node. Other abstract modalities also can be interpreted as graphs, e.g., source code [44], data flow of source code [44], table [18], SQL database schema [25], text question graph [24], and electronic health records (EHRs) [184].
## (s19) Token Embedding Fusion
### Question: What is token embedding fusion in Transformers and its applications?

#Reference=6

(p19.0) In practice, Transformers allow each token position to contain multiple embeddings. This is essentially a kind of early-fusion of embeddings, for both uni-modal and multimodal Transformer models.

(p19.1) (This will be discussed further in subsequent sections.) The most common fusion is the token-wise summing of the multiple embeddings, e.g., a specific token embedding ⊕ position embedding. Similar to the flexible tokenization, token embedding fusion is also flexible and widely applied to both uni-modal and multimodal Transformer applications. In [81], token-wise weighted summing is used to perform early-fusion of RGB and grey-scale images for multimodal surveillance AI. In particular, token embedding fusion has an important role in multimodal Transformer applications as various embeddings can be fused by tokenwise operators, e.g., in VisualBERT [104] and Unicoder-VL [108], segment embeddings are token-wise added to indicate which modality (vision or language) each token is from, VL-BERT [105] injects global visual context to linguistic domain by "linguistic token embedding ⊕ full image visual feature embedding", InterBERT [188] adds location information for ROI by "ROI embedding ⊕ location embedding", in Im-ageBERT [115], five kinds of embeddings are fused "image embedding ⊕ position embedding ⊕ linguistic embedding ⊕ segment embedding ⊕ sequence position embedding".
## (s20) Self-Attention Variants in Multimodal Context
### Question: How do self-attention variants facilitate multimodal interactions in Transformers?

#Reference=0

(p20.0) In multimodal Transformers, cross-modal interactions (e.g., fusion, alignment) are essentially processed by self-attention and its variants. Thus, in this section, we will review the main multimodal modelling practices of Transformers, from a perspective of self-attention designs, including (1) early summation (token-wise, weighted), (2) early concatenation, (3) hierarchical attention (multi-stream to one-stream), (4) hierarchical attention (one-stream to multi-stream), (5) TABLE 2 Self-attention variants for multi-modal interaction/fusion. α and β denote weightings. "Att.": Attention; "Concat."/"Con.": Concatenation; "Tfs":

(p20.1) Transformer layers. N (A) and N (B) denote the token sequence lengths of two modalities.
## (s23) (6) Cross-Attention to Concatenation
### Question: How does cross-attention to concatenation enhance multimodal interaction modeling?

#Reference=5

(p23.0) The two streams of cross-attention [102] can be further concatenated and processed by another Transformer to model the global context. This kind of hierarchically cross-modal interaction is also widely studied [137], [189], and alleviates the drawback of cross-attention.

(p23.1) Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks. Specifically, these interactions can be flexibly combined and nested. For instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model [191] T f 2 and T f 3 of Eq. 11 are implemented by cross-attention defined in Eq. 12. Moreover, they can be extended to multiple (≥ 3) modalities. TriBERT [183] is a tri-modal cross-attention (coattention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities. Cross-attention to concatenation is applied to three modalities (i.e., language, video, and audio) in [189].
## (s24) Network Architectures
### Question: How do multimodal Transformers' network architectures vary based on their internal attentions?

#Reference=9

(p24.0) Essentially, various multimodal Transformers work due to their internal multimodal attentions that are the aforementioned self-attention variants. Meanwhile, as illustrated in Figure 2, these attentions determine the external network structures of the multimodal Transformers where they are embedded.

(p24.1) In general, if we consider from the angle of network structures, (1) early summation and early concatenation work in single-stream, (2) cross-attention work in multistreams, (3) hierarchical attention and cross-attention to concatenation work in hybrid-streams. Thus, multimodal Transformers can be divided into single-stream (e.g., Uniter [106], Visualbert [104], Vl-bert [105] , Unified VLP [110]), multi-stream (e.g., ViLBERT [102], Lxmert [103], ActBERT [114]), hybrid-stream (e.g., InterBERT [188]), etc.

(p24.2) From the perspective of timing of interaction, these multimodal attentions fall into three categories, i.e., early interaction: early summation, early concatenation, and hierarchical attention (one-stream to multi-stream), late interaction: hierarchical attention (multi-stream to one-stream), or throughout interaction: cross-attention, cross-attention to concatenation.

(p24.3) As demonstrated in Figure 2 in [192], the multimodal Transformer models have another architecture taxonomy based on the computational size of the components.
## (s26) Transformers for Multimodal Pretraining
### Question: What are the key trends and benefits of Transformer-based multimodal pretraining?

#Reference=7

(p26.0) Inspired by the great success of Transformer based pretraining in NLP community, Transformers are also widely studied for multimodal pretraining as the various large-scale multimodal corpora is emerging. Recent work has demonstrated that if pretrained on large scale multimodal corpora Transformer based models [7], [102], [103], [104], [105], [106], [110] clearly outperform other competitors in a wide range of multimodal down-stream tasks, and moreover achieve the zero-shot generalization ability. These superiorities have led Transformer-based multimodal pretraining to become a hot topic, which has two main directions, i.e., general pretraining for agnostic down-stream tasks (Section 4.1.1), goal-oriented pretraining for specific down-stream tasks (Section 4.1.2).

(p26.1) We focus on these key points: (1) What trends are emerging? (2) Where/how do the cross-modal interactions take place during pretraining? (3) How to sort out and understand the pretraining pretext objectives? How can they drive Transformers to learn the cross-modal interactions?
## (s29) Pretext Tasks
### Question: What are pretext tasks in Transformer based multimodal pretraining?

#Reference=23

(p29.0) In Transformer based multimodal pretraining, the pretraining tasks/objectives are also termed pretext tasks/objectives. To date, various pretext tasks have been studied, e.g., masked language modelling (MLM) [137], masked image region prediction/classification (also termed masked object classification (MOC)) [137], [190], masked region regression (MRR) [115], visual-linguistic matching (VLM) (e.g., image-text matching (ITM) [188], image text matching (ITM), phrase-region alignment (PRA) [204], word-region alignment (WRA) [106], video-subtitle matching (VSM) [116]), masked frame modelling (MFM) [116], frame order modelling (FOM) [116], next sentence prediction (NSP) [4], [102], [190], masked sentence generation (MSG) [191], masked group modelling (MGM) [188], prefix language modelling (PrefixLM) [199], video conditioned masked language model [117], text conditioned masked frame model [117], visual translation language modelling (VTLM) [206], and image-conditioned masked language modelling (also termed image-attended masked language modelling) [207]. These down-stream task -agnostic pretext pretraining is optional, and the down-stream task objectives can be trained directly, which will be discussed in Section 4.1.2. Table 3 provides the common and representative pretext tasks for Transformer based multimodal pretraining. In practice, pretext tasks can be combined, and some representative cases are summarized in Table 3 of [57], Table 2 of [58].

(p29.1) The pretext tasks have multiple taxonomies:

(p29.2) (1) Supervision. The common multimodal pretraining Transformers use well-aligned, weakly-aligned, and even unaligned multimodal sample pairs/tuples, to work in supervised, weakly-supervised, and unsupervised manners, respectively. Meanwhile, if we consider the definitions of their pretext tasks/objectives from supervision, the pretexts can be sorted into unsupervised/self-supervised (e.g., masked language modelling (MLM) [7], [137]) and supervised (e.g., image-text matching (ITM) [188] [102], [103], [104], [106], [209]), etc. Nowadays, self-supervised attempts are the majority.

(p29.3) (2) Modality. Considering the mathematical formulations, some pretexts are defined on single modality, e.g., masked language modelling [7], masked acoustic modelling [200], masked region regression (MRR) [115], while other pretexts are defined on multiple modalities, e.g., imageconditioned masked language modelling (IMLM) [208], image-text matching (ITM) [188], video-subtitle matching (VSM) [116]. Thus, from this mathematical view, the pretext tasks can be divided into two categories, i.e., uni-modal and multimodal.

(p29.4) However, this classification is not really accurate. It should be highlighted that in multimodal pretraining Transformer models, even if the pretext objective formulations only include uni-modal elements, pretexts can still involve other modalities, essentially conditioned on the clues from other modalities, by (a) prepositive token level interactions and/or Transformer level interactions, (b) co-training with other pretexts that involve other modalities. For instance, VL-BERT [105] uses two dual pretext tasks, i.e., masked language modelling and masked RoI classification.

(p29.5) (3) Motivation. If consider their motivations, the pretext tasks include masking, describing, matching, ordering, etc.

(p29.6) Some recent surveys focus on VLP and compare the existing VLP Transformer models from the angles of domain (image-text or video-text), vision feature extraction, language feature extraction, architecture (single-or dualstream), decoder (w/, w/o), pretext tasks/objectives, pretraining datasets, and down-stream tasks, e.g., Table 3 of [57], Table 2 of [58]. Different from these views, in this survey, we would propose our comparisons from some new perspectives. Specifically: (1) The core of Transformer ecosystem is self-attention, thus we would compare the existing multimodal pretraining Transformer models from the angles of how and when the self-attention or its variants perform cross-modal interactions. (2) Considering from a geometrically topological perspective, self-attention helps Transformers intrinsically work in a modality agnostic pipeline that is compatible with various modalities by taking in the embedding of each token as a node of graph, thus we would highlight that the existing VLP can be applied to other modalities, beyond visual and linguistic domains.

(p29.7) (3) We suggest to treat the Transformer-based multimodal pretraining pipelines having three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision.
## (s30) Discussion
### Question: What are the challenges and potential improvements in multimodal pretraining Transformer methods?

#Reference=8

(p30.0) In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks. For instance, as discussed by [208] in VLP field, while the BERT-style cross-modal pretraining models produce excellent results on various down-stream visionlanguage tasks, they fail to be applied to generative tasks directly. As discussed in [208], both VideoBERT [7] and CBT [107] have to train a separate video-to-text decoder for video captioning. This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer. Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved. Again for instance, common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by [137].

(p30.1) Discussion As discussed in [208], the masked language and region modelling as pre-training task have a main advantage that the Transformer encoder learned from these supervisions can encode both vision and language patterns based on bidirectional context and it is naturally fit for the semantic understanding tasks, e.g., VQA, image-text retrieval.

(p30.2) Discussion How to boost the performance for multimodal pretraining Transformers is an open problem. Some practices demonstrate that multi-task training (by adding auxiliary loss) [111], [137] and adversarial training [210] improve multimodal pretraining Transformers to further boost the performance. Meanwhile, overly compound pretraining objectives potentially upgrade the challenge of balancing among different loss terms, thus complicate the training optimization [199]. Moreover, the difficulty of the pretexts is also worth discussing. In general, if aim to learn more explicit object concepts, more complex pretext losses will be used [204]. However, for pretexts, whether more complexity is better remains a question.
## (s31) Task-Specific Multimodal Pretraining
### Question: Why is task-specific multimodal pretraining often necessary in AI models?

#Reference=5

(p31.0) In practices of multimodal Transformers, the aforementioned down-stream task -agnostic pretraining is optional, not necessary, and down-stream task specific pretraining is also widely studied [150], [190], [208], [211]. The main reasons include: (1) Limited by the existing technique, it is extremely difficult to design a set of highly universal network architectures, pretext tasks, and corpora that work for all the various down-stream applications. (2) There are nonnegligible gaps among various down-stream applications, e.g., task logic, data form, making it difficult to transfer from pretraining to down-stream applications.

(p31.1) Therefore, a large number of down-stream tasks still need tailor-made pretraining to improve the performance. Guhur et al. [150] propose in-domain pretraining for visionand-language navigation, as the general VLP focuses on TABLE 3 Pretext task comparison of multi-modal pretraining Transformer models (for agnostic down-stream tasks). "C-M Loss": cross-modal loss; "Con.

(p31.2) Loss": loss conditioned on other modality/modalities.
## (s32) Types (Motivations) Tasks C-M Loss Con. Loss References
### Question: What are the different tasks and models in masked and multimodal language processing?

#Reference=28

(p32.0) Masking Masked Language Modelling (MLM) [7], [137] Image-Conditioned Masked Language Modelling (IMLM) [206], [207] [208] Text-Conditioned Masked Region Prediction [206] Masked Acoustic Modelling [180], [200] Masked Image Region Regression [115] Masked Image Region Prediction [190] Masked Frame Modelling (MFM) [116] Masked Sentence Generation (MSG) [191] Video Conditioned Masked Language Model [117] Text Conditioned Masked Frame Model [117] Describing Image-conditioned Denoising Autoencoding (IDA) [208] Text-conditioned Image Feature Generation (TIFG) [208] Prefix Language Modelling (PrefixLM) [199] Matching Image-Text Matching (ITM) [188] [102], [103], [104], [106], [209], Phrase-Region Alignment (PRA) [204] Word-Region Alignment (WRA) [106], [192] Video-Subtitle Matching (VSM) [116] Next Sentence Prediction (NSP) [4], [102], [190] Ordering Sentence Ordering Modelling (SOM) [201] Frame Ordering Modelling (FOM) [116] learning vision-language correlations, not designed for sequential decision making as required in embodied VLN. Murahari et al. [190] present a visual dialogue oriented approach to leverage pretraining on general vision-language datasets. XGPT [208] is tailor-made for image captioning, to overcome the limitation that BERT-based cross-modal pretrained models fail to be applied to generative tasks directly. ERNIE-ViLG [211] is designed for bidirectional image-text generation with Transformers. Special modalities have their own unique domain knowledge that can be used to design the specific pretrain pretexts. GraphCodeBERT [44] uses two structure-aware pretext tasks (i.e., predict where a variable is identified from, data flow edge prediction between variables) for programming source code. To learn from the spatial cues in 360 • video, Morgado et al. [145] propose to perform contrastive audio-visual spatial alignment of 360 • video and spatial audio. Med-BERT [184] is a contextualized embedding model pretrained on a structured electronic health record dataset of two million patients. Kaleido-BERT [212] is a VLP Transformer model tailor-made for the fashion domain.
## (s35) Fusion
### Question: How do MML Transformers fuse information across different modalities?

#Reference=7

(p35.0) In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion). Common early fusion based MML Transformer models [7], [104], [108] are also known as one-stream architecture, allowing the adoption of the merits of BERT due to minimal architectural modification. The main difference between these one-stream models is the usage of problem-specific modalities with variant masking techniques. With attention operation, a noticeable fusion scheme is introduced based on a notion of bottleneck tokens [175]. It applies for both early and middle fusion by simply choosing to-be-fused layers. We note that the simple prediction-based late fusion [247], [248] is less adopted in MML Transformers. This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power. For enhancing and interpreting the fusion of MML, probing the interaction and measuring the fusion between modalities [249] would be an interesting direction to explore.
## (s36) Alignment
### Question: What is the significance of cross-modal alignment in multimodal applications?

#Reference=33

(p36.0) Cross-modal alignment is the key to a number of real-world multimodal applications. Transformer based cross-modal alignment has been studied for various tasks, e.g., speaker localization in multi-speaker videos [250], speech translation [180], text-to-speech alignment [251], text-to-video retrieval [252], [253], [254], and visual grounding of natural language [255], [256], [257], [258], [259]. Recently, Transformer based alignment [9], [119], [260], [261], [262] has led to a surge of leveraging large quantities of web data (e.g., image-text pairs) for vision and language tasks.

(p36.1) A representative practice is to map two modalities into a common representation space with contrastive learning over paired samples. The models based on this idea are often enormous in size and expensive to optimize from millions or billions of training data. Consequently, successive works mostly exploit pretrained models for tackling various downstream tasks [120], [263], [264], [265], [266]. These alignment models have the ability of zero-shot transfer particularly for image classification via prompt engineering [267]. This novel perspective is mind-blowing, given that image classification is conventionally regarded as a unimodal learning problem and zero-shot classification remains an unsolved challenge despite extensive research [268]. This has been studied for more challenging and fine-grained tasks (e.g., object detection [269], visual question answering [103], [106], [112], [263], and instance retrieval [222], [263]) by imposing region (semantic parts such as objects) level alignment. Finegrained alignment will however incur more computational costs from explicit region detection and how to eliminate this whilst keeping the region-level learning capability becomes a challenge. Several ideas introduced recently include random sampling [113], learning concept dictionary [203], uniform masking [270], patch projection [192], joint learning of a region detector [271], and representation aligning before mask prediction [263].
## (s39) Robustness
### Question: What are the challenges and methods to improve multimodal Transformers' robustness?

#Reference=11

(p39.0) Multimodal Transformers pretrained on large-scale corpora achieve the state-of-the-art for various multimodal applications, while their robustness is still unclear and understudied. This at least involves two key challenges, i.e., how to theoretically analyse the robustness, how to improve the robustness.

(p39.1) Although that recent attempts [99], [182], [289], [290] study and evaluate how the Transformer components/sublayers contribute to the robustness, the main bottleneck is that the community lacks theoretical tools to analyse the Transformer family. Recently, the common practices to analyse robustness are mainly based on experiment evaluations [291], e.g., cross-dataset evaluations, perturbationbased evaluations. Thus, some multimodal datasets [130], [292] are proposed for evaluating the robustness.

(p39.2) Recent attempts mainly use two straightforward methods to improve the robustness for multimodal Transformer models: (1) augmentation and adversarial learning based strategies [293], [294], (2) fine-grained loss functions [295]. For instance: VILLA [210] is a generic adversarial training framework that can be applied to various multimodal Transformers. Akula et al. [292] empirically demonstrate that ViL-BERT fails to exploit linguistic structure, and they propose two methods to improve the robustness of ViLBERT, one based on contrastive learning and the other based on multitask learning.
## (s40) Universalness
### Question: What challenges do multimodal Transformer models face in achieving universalness?

#Reference=11

(p40.0) Due to the highly diversity of tasks and modalities of multimodal learning, universalness is an important problem for multimodal Transformer models. A large amount of recent attempts [117], [296], [297], [298] study how to use as unified as possible pipelines to handle various modalities and multimodal tasks. Ideally, the unified multimodal Transformers can be compatible with various data (e.g., aligned and unaligned, uni-modal and multimodal) and tasks (e.g., supervised and unsupervised, uni-modal and multimodal, discriminative and generative), and meanwhile have either few-shot or even zero-shot generalization ability. Thus, the current solutions for universalness goal for multimodal Transformers are preliminary probes.

(p40.1) The currently unifying-oriented attempts mainly include:

(p40.2) (1) Unifying the pipelines for both uni-modal and multimodal inputs/tasks. As discussed Section 5.3, in practical scenarios, multimodal Transformers need to handle unimodal data due to the issue of missing modalities. Distilling multimodal knowledge into small models that are adaptable to uni-modal data and tasks is a successful practice [275], [276].

(p40.3) (2) Unifying the pipelines for both multimodal understanding and generation. In general, for multimodal Transformer pipelines, understanding and discriminative tasks require Transformer encoders only, while generation/generative tasks require both Transformer encoders and decoders. Existing attempts use multi-task learning to combine the understanding and generation workflows, where two kinds of workflows are jointly trained by multitask loss functions. From the perspective of model structures, typical solutions include: (a) encoder + decoder, e.g., E2E-VLP [271]. (b) separate encoders + cross encoder + decoder, e.g., UniVL [117], CBT [107]. (c) single unified/combined encoder-decoder, e.g., VLP [110]. (d) twostream decoupled design [191].

(p40.4) (3) Unifying and converting the tasks themselves, e.g., CLIP [9] converts zero-shot recognition to retrieval, thus reduces the costs of modifying the model. However, the aforementioned practices suffer some obvious challenges and bottlenecks, at least including:

(p40.5) (1) Due to modality and task gaps, universal models should consider the trade-off between universalness and cost. Unifying the pipelines of different modalities and tasks generally cause larger or more complicated model configuration, whereas for a specific modality or task, some components are redundant.

(p40.6) (2) Multi-task loss functions increase the complexity of training. How to co-train multiple objectives properly and effectively is challenging, due to that different objectives generally should be optimized in different strategies.
## (s41) Interpretability
### Question: How do studies investigate multimodal Transformers' performance and interpretability?

#Reference=10

(p41.0) Why and how Transformers perform so well in multimodal learning has been investigated [106], [299], [300], [301], [302], [303], [304], [305], [306]. These attempts mainly use probing task and ablation study. Cao et al. [299] design a set of probing tasks on UNITER [106] and LXMERT [103], to evaluate what patterns are learned in pretraining. Hendricks et al. [301] probe the image-language Transformers by fine-grained image-sentence pairs, and find that verb understanding is harder than subject or object understanding. Chen et al. [106] examine the optimal combination of pretraining tasks via ablation study, to compare how different pretexts contribute to the Transformers. Despite these attempts, the interpretability of multimodal Transformers is still under-studied to date.
