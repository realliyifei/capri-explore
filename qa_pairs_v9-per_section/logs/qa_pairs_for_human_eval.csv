corpusid,title,domain,section,QA pair,subquestions
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",s1,"Question: How do BERT embeddings differ from conventional static embeddings?

1. Unlike the conventional static embeddings (Mikolov et al., 2013a;Pennington et al., 2014), BERT's representations are contextualized, i.e., every input token is represented by a vector dependent on the particular context of occurrence.
2. In the current studies of BERT's representation space, the term 'embedding' refers to the output vector of a given (typically final) Transformer layer.
3. Wiedemann et al. (2019) find that BERT's contextualized embeddings form distinct and clear clusters corresponding to word senses, which confirms that the basic distributional hypothesis holds for these representations.
4. However, Mickus et al. (2019) note that representations of the same word varies depending on position of the sentence in which it occurs, likely due to NSP objective.
5. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer and find that later BERT layers produce more contextspecific representations.
6. They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from lower to higher layers.
7. That is, two random words will on average have a much higher cosine similarity than expected if embeddings were directionally uniform (isotropic).",1. How do BERT embeddings differ from conventional static embeddings?
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",s2,"Question: What types of knowledge are encoded in BERT's weights?

1. A number of studies have looked at the types of knowledge encoded in BERT's weights.
2. The popular approaches include fill-in-the-gap probes of BERT's MLM, analysis of self-attention weights, and probing classifiers using different BERT representations as inputs.
3. showed that BERT representations are hierarchical rather than linear, i.e. there is something akin to syntactic tree structure in addition to the word order information.
4. Tenney et al. (2019b) and  also showed that BERT embeddings encode information about parts of speech, syntactic chunks and roles.
5. However, BERT's knowledge of syntax is partial, since probing classifiers could not recover the labels of distant parent nodes in the syntactic tree .",1. What types of knowledge are encoded in BERT's weights?
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",s3,"Question: How is syntactic knowledge represented and understood in BERT models?

1. As far as how syntactic information is represented, it seems that syntactic structure is not directly encoded in self-attention weights, but they can be transformed to reflect it.
2. Htut et al. (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root.
3. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from self-attention weights, but provide no quantitative evaluation.
4. However, Hewitt and Manning (2019) were able to learn transformation matrices that would successfully recover much of the Stanford Dependencies formalism for PennTreebank data (see Figure 2).
5. Jawahar et al. (2019) try to approximate BERT representations with Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that the dependency trees are the best match among 5 decomposition schemes (although the reported MSE differences are very small).
6. Regarding syntactic competence of BERT's MLM, Goldberg (2019) showed that BERT takes subject-predicate agreement into account when performing the cloze task.
7. This was the case even for sentences with distractor clauses between the subject and the verb, and meaningless sentences.
8. A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g. ""ever"") and the words that allow their use (e.g. ""whether"") than scope violations.
9. The above evidence of syntactic knowledge is belied by the fact that BERT does not ""understand"" negation and is insensitive to malformed input.
10. In particular, its predictions were not altered even with shuffled word order, truncated sentences, removed subjects and objects (Ettinger, 2019).
11. This is in line with the recent findings on adversarial attacks, with models disturbed by nonsensical inputs (Wallace et al., 2019a), and suggests that BERT's encoding of syntactic structure does not indicate that it actually relies on that knowledge.","1. How is syntactic knowledge represented in BERT models?
2. How is syntactic knowledge understood in BERT models?"
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",s4,"Question: How does BERT process semantic knowledge and struggle with numerical representations?

1. To date, more studies were devoted to BERT's knowledge of syntactic rather than semantic phenomena.
2. However, we do have evidence from an MLM probing study that BERT has some knowledge for semantic roles (Ettinger, 2019).
3. BERT is even able to prefer the incorrect fillers for semantic roles that are semantically related to the correct ones, to those that are unrelated (e.g. ""to tip a chef"" should be better than ""to tip a robin"", but worse than ""to tip a waiter"").
4. Tenney et al. (2019b) showed that BERT encodes information about entity types, relations, semantic roles, and proto-roles, since this information can be detected with probing classifiers.
5. BERT struggles with representations of numbers.
6. Addition and number decoding tasks showed that BERT does not form good representations for floating point numbers and fails to generalize away from the training data (Wallace et al., 2019b).
7. A part of the problem is BERT's wordpiece tokenization, since numbers of similar values can be divided up into substantially different word chunks.","1. How does BERT process semantic knowledge?
2. How does BERT struggle with numerical representations?"
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",s5,"Question: How does BERT adapt for knowledge induction and its limitations in reasoning?

1. MLM component of BERT is easy to adapt for knowledge induction by filling in the blanks (e.g. ""Cats like to chase [ ]"").
2. There is at least one probing study of world knowledge in BERT (Ettinger, 2019), but the bulk of evidence comes from  (Petroni et al., 2019) numerous practitioners using BERT to extract such knowledge.
3. Petroni et al. (2019) showed that, for some relation types, vanilla BERT is competitive with methods relying on knowledge bases (Figure 3).
4. Davison et al. (2019) suggest that it generalizes better to unseen data.
5. However, to retrieve BERT's knowledge we need good template sentences, and there is work on their automatic extraction and augmentation (Bouraoui et al., 2019; However, BERT cannot reason based on its world knowledge.
6. Forbes et al. (2019) show that BERT can ""guess"" the affordances and properties of many objects, but does not have the information about their interactions (e.g. it ""knows"" that people can walk into houses, and that houses are big, but it cannot infer that houses are bigger than people.)  and  also show that the performance drops with the number of necessary inference steps.
7. At the same time, Poerner et al. (2019) show that some of BERT's success in factoid knowledge retrieval comes from learning stereotypical character combinations, e.g. it would predict that a person with an Italian-sounding name is Italian, even when it is factually incorrect.","1. How does BERT adapt for knowledge induction?
2. What are BERT's limitations in reasoning?"
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",s10,"Question: What are the alternative training objectives proposed to improve BERT's performance?

1. The original BERT is a bidirectional Transformer pre-trained on two tasks: next sentence prediction (NSP) and masked language model (MLM).
2. Multiple studies have come up with alternative training objectives to improve on BERT.
3. Removing NSP does not hurt or slightly improves task performance (Liu et al., 2019b;Joshi et al., 2020;Clinchant et al., 2019), especially in cross-lingual setting .
4. Wang et al. (2019a) Mikolov et al. (2013b).
5. Permutation language modeling. Yang et al. (MLM)6 replace MLM with training on different permutations of word order in the input sequence, maximizing the probability of the original word order.
6. See also the n-gram word order reconstruction task (Wang et al., 2019a).
7. Span boundary objective aims to predict a masked span (rather than single words) using only the representations of the tokens at the span's boundary (Joshi et al., 2020);  Phrase masking and named entity masking  aim to improve representation of structured knowledge by masking entities rather than individual words;  Continual learning is sequential pre-training on a large number of tasks 4 , each with their own loss which are then combined to continually update the model (Sun et al., 2019b).
8. Conditional MLM by  replaces the segmentation embeddings with ""label embeddings"", which also include the label for a given sentence from an annotated task dataset (MLM)0.
9. Clinchant et al. (MLM)6 propose replacing the MASK token with (MLM)2 token, as this could help the model to learn certain representation for unknowns that could be exploited by a neural machine translation model.
10. Another obvious source of improvement is pretraining data.
11. Liu et al. (MLM)3 explore the benefits of increasing the corpus volume and longer training.
12. The data also does not have to be unstructured text: although BERT is actively used as a source of world knowledge (MLM)4, there are ongoing efforts to incorporate structured knowledge resources .
13. Another way to integrate external knowledge is use entity embeddings as input, as in E-BERT (MLM)5 and ERNIE .
14. Alternatively, SemBERT  integrates semantic role information with BERT representations.
15. Pre-training is the most expensive part of training BERT, and it would be informative to know how much benefit it provides.
16. Hao et al. (MLM)6 conclude that pre-trained weights help the fine-tuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (MLM)7.
17. However, on some tasks a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (MLM)8.",1. What are the alternative training objectives proposed to improve BERT's performance?
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",s11,"Question: What are the key findings on optimizing BERT model architecture?

1. To date, the most systematic study of BERT architecture was performed by .
2. They experimented with the number of layers, heads, and model parameters, varying one option and freezing the others.
3. They concluded that the number of heads was not as significant as the number of layers, which is consistent with the findings of Voita et al. (2019) and Michel et al. (2019), discussed in section 7, and also the observation by  that middle layers were the most transferable.
4. Larger hidden representation size was consistently better, but the gains varied by setting.
5. Liu et al. (2019c) show that large-batch training (8k examples) improves both the language model perplexity and downstream task performance.
6. They also publish their recommendations for other model parameters.
7. You et al. (2019) report that with a batch size of 32k BERT's training time can be significantly reduced with no degradation in performance.
8. observe that the embedding values of the trained [CLS] token are not centered around zero, their normalization stabilizes the training leading to a slight performance gain on text classification tasks.
9. Gong et al. (2019) note that, since self-attention patterns in higher layers resemble the ones in lower layers, the model training can be done in a recursive manner, where the shallower version is trained first and then the trained parameters are copied to deeper layers.
10. Such ""warm-start"" can lead to a 25% faster training speed while reaching similar accuracy to the original BERT on GLUE tasks.",1. What are the key findings on optimizing BERT model architecture?
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",s12,"Question: What are the key aspects and challenges of fine-tuning BERT?

1. Pre-training + fine-tuning workflow is a crucial part of BERT.
2. The former is supposed to provide taskindependent linguistic knowledge, and the finetuning process would presumably teach the model to rely on the representations that are more useful for the task at hand.
3. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks 5 : during fine-tuning, the most changes for 3 epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns.
4. It is understandable why fine-tuning would increase the attention to  Two-stage fine-tuning introduces an intermediate supervised training stage between pretraining and fine-tuning Garg et al., 2020).
5. Adversarial token perturbations improve robustness of the model (Zhu et al., 2019).
6. With larger and larger models even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated by inserting adapter modules.
7. They adapt BERT to 26 classification tasks, achieving competitive performance at a fraction of the computational cost.
8. Artetxe et al. (2019) also find adapters helpful in reusing monolingual BERT weights for cross-lingual transfer.
9. An alternative to fine-tuning is extracting features from frozen representations, but fine-tuning works better for BERT (Peters et al., 2019b). Initialization can have a dramatic effect on the training process (Petrov, 2010).
10. However, variation across initializations is not often reported, although the performance improvements claimed in many NLP modeling papers may be within the range of that variation (Crane, 2018).
11. Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks, where both weight initialization and training data order contribute to the variation.
12. They also propose an early-stopping technique to avoid full fine-tuning for the less-promising seeds.
13. 7 How big should BERT be?","1. What are the key aspects of fine-tuning BERT?
2. What are the challenges of fine-tuning BERT?"
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",s13,"Question: What are the implications of overparametrization in transformer-based models?

1. Transformer-based models keep increasing in size: e.g. T5 (Wu et al., 2016a) is over 30 times larger than the base BERT.
2. This raises concerns about computational complexity of self-attention , environmental issues (Strubell et al., 2019;Schwartz et al., 2019), as well as reproducibility and access to research resources in academia vs. industry.
3. Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have.
4. Voita et al. (Strubell et al., 2019;Schwartz et al., 2019)3 showed that all but a few Transformer heads could be pruned without significant losses in performance.
5. For BERT, Clark et al. (Strubell et al., 2019;Schwartz et al., 2019)3 observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in Distillation Compression Performance Speedup Model Evaluation DistilBERT (Sanh et al., 2019) ×2.5 90% ×1.6 BERT 6 All GLUE tasks BERT 6 -PKD (Sun et al., 2019a) ×1.6 97% ×1.9 BERT 6 No WNLI, CoLA and STS-B BERT 3 -PKD (Sun et al., 2019a) ×2  a layer is passed through the same MLP), which explains why Michel et al. (Strubell et al., 2019;Schwartz et al., 2019)3 were able to reduce most layers to a single head.
6. Depending on the task, some BERT heads/layers are not only useless, but also harmful to the downstream task performance.
7. Positive effects from disabling heads were reported for machine translation (Michel et al., 2019), and for GLUE tasks, both heads and layers could be disabled (Kovaleva et al., 2019).
8. Additionally, Tenney et al. (Strubell et al., 2019;Schwartz et al., 2019)0 examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (Strubell et al., 2019;Schwartz et al., 2019)1.
9. Many experiments comparing BERT-base and BERT-large saw the larger model perform better , but that is not always the case.
10. In particular, the opposite was observed for subjectverb agreement (Strubell et al., 2019;Schwartz et al., 2019)2 and sentence subject detection .
11. Given the complexity of language, and amounts of pre-training data, it is not clear why BERT ends up with redundant heads and layers.
12. Clark et al. (Strubell et al., 2019;Schwartz et al., 2019)3 suggest that one of the possible reasons is the use of attention dropouts, which causes some attention weights to be zeroed-out during training.",1. What are the implications of overparametrization in transformer-based models?
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",s14,"Question: What are the main approaches to compressing BERT with minimal accuracy loss?

1. Given the above evidence of overparametrization, it does not come as a surprise that BERT can be efficiently compressed with minimal accuracy loss.
2. Such efforts to date are summarized in Table 1.
3. Two main approaches include knowledge distillation and quantization.
4. The studies in the knowledge distillation framework (Hinton et al., 2015) use a smaller studentnetwork that is trained to mimic the behavior of a larger teacher-network (BERT-large or BERTbase).
5. This is achieved through experiments with loss functions (Sanh et al., 2019;Jiao et al., 2019), mimicking the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at different stages at the pre-training (Turc et al., 2019;Jiao et al., 2019; or at the fine-tuning stage (Jiao et al., 2019)).
6. The quantization approach aims to decrease BERT's memory footprint through lowering the precision of its weights (Shen et al., 2019;Zafrir et al., 2019).
7. Note that this strategy often requires compatible hardware.
8. Other techniques include decomposing BERT's embedding matrix into smaller matrices (Lan et al., 2019) and progressive model replacing (Xu et al., 2020).","1. What are the main approaches to compressing BERT?
2. How can BERT be compressed with minimal accuracy loss?"
211532403,A Primer in BERTology: What we know about how BERT works,"Linguistics, Computer Science",s16,"Question: What are the most promising directions for further BERTology research?

1. BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works.
2. In this section, we list what we believe to be the most promising directions for further research, together with the starting points that we already have.
3. Benchmarks that require verbal reasoning.
4. While BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its verbal reasoning abilities are not as impressive as it seems.
5. In particular, it was shown to rely on shallow heuristics in both natural language inference Zellers et al., 2019) and reading comprehension (Si et al., 2019;Rogers et al., 2020;Sugawara et al., 2020).
6. As with any optimization method, if there is a shortcut in the task, we have no reason to expect that BERT will not learn it.
7. To overcome this, the NLP community needs to incentivize dataset development on par with modeling work, which at present is often perceived as more prestigious.
8. Developing methods to ""teach"" reasoning.
9. While the community had success extracting knowledge from large pre-trained models, they often fail if any reasoning needs to be performed on top of the facts they possess (see subsection 4.3).
10. For instance,  propose a method to ""teach"" BERT quantification, conditionals, comparatives, and boolean coordination.
11. Learning what happens at inference time.
12. Most of the BERT analysis papers focused on different probes of the model, but we know much less about what knowledge actually gets used.
13. At the moment, we know that the knowledge represented in BERT does not necessarily get used in downstream tasks (Kovaleva et al., 2019).
14. As starting points for work in this direction, we also have other head ablation studies (Voita et al., 2019;Michel et al., 2019) and studies of how BERT behaves in reading comprehension task (van Aken et al., 2019; Arkhangelskaia and Dutta, 2019).",1. What are the most promising directions for further BERTology research?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s1,"Question: What are the definitions and objectives of neuron analysis in neural networks?

1. In this section, we define the terminologies used in the paper and the objective of neuron analysis more formally.
2. Neuron Neural networks, such as RNNs or Transformer models consist of various components such as gates/cells, blocks, layers, attention heads, etc. We use the term neuron (also called as features, experts, and units in the literature) to refer to the output of a single dimension from any neural network component.
3. For example, in the BERT base model, the output of a layer block has 768 neurons and the output of an attention head has 64 neurons.
4. Moreover, we refer to individual neurons that learn a single concept as focused neurons, and a set of neurons that in combination represent a concept as group neurons.
5. Concept A concept represents a coherent fragment of knowledge, such as ""a class containing certain objects as elements, where the objects have certain properties"" (Stock, 2010).
6. For example, a concept could be lexical: e.g. words ending with suffix ""ed"", morphological: e.g. gerund verbs, or semantic: e.g. names of cities.
7. We loosely define a concept C as a group of words that are coherent w.r.t to a linguistic property.
8. Table 1 shows an example sentence with different concept annotations.
9. Objective Figure 1 presents an overview of various objectives in neuron analysis.
10. Formally, given a model M and a set of neurons N (which may consist of all the neurons in the network or a specific subset from particular components like a layer or an attention head) and a concept C, neuron analysis aims to achieve one of the following objectives:  For a concept C, find a ranked list of |N | neurons with respect to the concept (dotted blue line)  Given a neuron n i ∈ N , find a set of concepts |C| the neuron represents (dashed purple line)  Given a set of neurons, find a subset of neurons that encode similar knowledge (solid green line) The former two aim to understand what concepts are encoded within the learned representation.
11. The last objective analyzes how knowledge is distributed across neurons.
12. Each neuron n i ∈ N is represented as a vector of activation values over some dataset D. Here, every element of the vector corresponds to a word.
13. For phrase or sentence-level concepts, an aggregation of neuron activations over words in the phrase/sentence is used.
14. Alternatively, [CLS] token representation is also used for transformer models that are transfer learned towards a downstream NLP task.","1. What are the definitions of neuron analysis in neural networks?
2. What are the objectives of neuron analysis in neural networks?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s2,"Question: What are the five broad categories of neuron analysis methods?

1. We have classified the work done on neuron analysis into 5 broader categories of methods, namely: i) visualizations, ii) corpus-based, iii) probingbased, iv) causation-based and v) miscellaneous methods, based on a set of attributes we describe below:  Scope: Does the method provide global or local interpretation?
2. Global methods accumulate statistics across a set of examples to discover the role of a neuron.
3. Local methods   provide interpretation of a neuron in a particular example and may not necessarily reflect its role over a large corpus.  Input and Output: What is the input (e.g. a set of neurons or concepts) to the method and what does it output?  Scalability: Can the method be scaled to a larger set of neurons?
4. HITL: Does the method require a human-inthe-loop for interpretation?  Supervision: Does the method depend on labeled data to provide interpretation?
5. Causation: Is the interpretation connected with the model's prediction?
6. Table 2 summarizes and compares each method in the light of these attributes.
7. We discuss them in detail below.
8. †",1. What are the five broad categories of neuron analysis methods?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s3,"Question: What are the benefits and limitations of visualizing neuron activations in deep NLP models?

1. A simple way to discover the role of a neuron is by visualizing its activations and manually identifying the underlying concept over a set of sentences (Karpathy et al., 2015;Fyshe et al., 2015;Li et al., 2016a).
2. Given that deep NLP models are † Table 3 in Appendix gives a more comprehensive list.
3. trained using billions of neurons, it is impossible to visualize all the neurons.
4. A number of clues have been used to shortlist the neurons for visualization, for example, selecting saturated neurons, high/low variance neurons, or ignoring dead neurons (Karpathy et al., 2015) when using ReLU activation function.
5. ‡ Limitation While visualization is a simple approach to find an explanation for a neuron, it has some major limitations: i)
6. it is qualitative and subjective, ii) it cannot be scaled to the entire network due to an extensive human-in-the-loop effort, iii) it is difficult to interpret polysemous neurons that acquire multiple roles in different contexts, iv) it is ineffective in identifying group neurons, and lastly and v) not all neurons are visually interpretable.
7. Visualization nevertheless remains a useful tool when applied in combination to other interpretation methods that are discussed below.","1. What are the benefits of visualizing neuron activations in deep NLP models?
2. What are the limitations of visualizing neuron activations in deep NLP models?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s4,"Question: How do corpus-based methods interpret and analyze neuron roles in neural networks?

1. Corpus-based methods discover the role of neurons by aggregating statistics over data activations.
2. They establish a connection between a neuron and a concept using co-occurrence between a neuron's activation values and existence of the concept in ‡ Saturated neurons have a gradient value of zero.
3. Dead neurons have an activation value of zero.
4. the underlying input instances (e.g. word, phrases or the entire sentence).
5. Corpus-based methods are global interpretation methods as they interpret the role of a neuron over a set of inputs.
6. They can be effectively used in combination with the visualization method to reduce the search space for finding the most relevant portions of data that activates a neuron, thus significantly reducing the humanin-the-loop effort.
7. Corpus-based methods can be broadly classified into two sets: i) the methods that take a neuron as an input and identify the concept the neuron has learned (Concept Search), ii) and others that take a concept as input and identify the neurons learning the concept (Neuron Search).
8. Concept Search This set of methods take a neuron as an input and search for a concept that the neuron has learned.
9. They sort the input instances based on the activation values of the given neuron.
10. The top activating instances represent a concept the neuron represents.
11. Kádár et al. (2017) discovered neurons that learn various linguistic con-cepts using this approach.
12. They extracted top-20, 5-gram contexts for each neuron based on the magnitude of activations and manually identified the underlying concepts.
13. This manual effort of identifying concepts is cumbersome and requires a human-in-the-loop.
14. Na et al. (2019) addressed this by using lexical concepts of various granularities.
15. Instead of 5-gram contexts, they extracted top-k activating sentences for each neuron.
16. They parsed the sentences to create concepts (words and phrases) using the nodes of the parse trees.
17. They then created synthetic sentences that highlight a concept e.g. a particular word occurring in all synthetic sentences.
18. The neurons that activates largely on these sentences are considered to have learned the concept.
19. This methodology is useful in analyzing neurons that are responsible for multi-word concepts such as phrases and idiomatic collocations.
20. However, the synthetic sentences are often ungrammatical and lead towards a risk of identifying neurons that exhibit arbitrary behavior (like repetition) instead of concept specific behavior.",1. How do corpus-based methods interpret neuron roles in neural networks?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s5,"Question: What are Neuron Search methods in corpus-based approaches for concept discovery?

1. The second class of corpusbased methods aim to discover neurons for a given concept.
2. The underlying idea is the same i.e. to establish a link between the concept and neurons based on co-occurrences stats, but in the opposite direction.
3. The activation values play a role in weighing these links to obtained a ranked list of neurons against the concept.
4. Mu and Andreas (2020) achieved this by creating a binary mask of a neuron based on a threshold on its activation values for every sentence in the corpus.
5. Similarly, they created a binary mask for every concept based on its presence or absence in a sentence.
6. They then computed the overlap between a given neuron mask vector and a concept mask vector using intersection-over-union (IoU), and use these to generate compositional explanations.
7. Differently from them, Suau et al. (2020) used the values of neuron activations as prediction scores and computed the average precision per neuron and per concept.
8. Finally, Antverg and Belinkov (2022) considered the mean activation values of a neuron with respect to instances that posses the concept of interest.
9. The two methods give an alternative view to neuron interpretation.
10. While Neuron Search methods aim to find the neuron that has learned a concept, Concept Search methods generate explanations for neurons by aligning them with a concept.
11. Limitation The corpus-based methods do not model the selection of group neurons that work together to learn a concept.
12. Concept Search methods consider every neuron independently.
13. Similarly, Neuron Search methods do not find the correlation of a group of neurons with respect to the given concept.",1. What are Neuron Search methods in corpus-based approaches for concept discovery?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s7,"Question: How do regularization techniques affect linear classifiers in concept learning?

1. The idea is to train a linear classifier towards the concept of interest, using the activation vectors generated by the model being analyzed.
2. The weights assigned to neurons (features to the classifier) serve as their importance score with respect to the concept.
3. The regularization of the classifier directly effects the weights and therefore the ranking of neurons.
4. Radford et al. (2019) used L1 regularization which forces the classifier to learn spiky weights, indicating the selection of very few specialized neurons learning a concept, while setting the majority of neurons' weights to zero.
5. Lakretz et al. (2019) on the other hand used L2 regularization to encourage grouping of features.
6. This translates to discovering group neurons that are jointly responsible for a concept.
7. Dalvi et al. (2019) used ElasticNet regularization which combines the benefits of L1 and L2, accounting for both highly correlated group neurons and specific focused neurons with respect to a concept.
8. Limitation A pitfall to probing classifiers is whether a probe faithfully reflects the concept learned within the representation or just memorizes the task (Hewitt and Liang, 2019;Zhang and Bowman, 2018).
9. Researchers have mitigated this pitfall for some analyses by using random initialization of neurons (Dalvi et al., 2019) and control tasks  to demonstrate that the knowledge is possessed within the neurons and not due to the probe's capacity for memorization.
10. Another discrepancy in the neuron probing framework, that especially affects the linear classifiers, is that variance patterns in neurons differ strikingly across the layers.
11. suggested to apply z-normalization as a pre-processing step to any neuron probing method to alleviate this issue.
12. Gaussian Classifier Hennigen et al. (2020) trained a generative classifier with the assumption that neurons exhibit a Gaussian distribution.
13. They fit a multivariate Gaussian over all neurons and extracted individual probes for single neurons.
14. A caveat to their approach is that activations do not always follow a Gaussian prior in practice -hence restricting their analysis to only the neurons that satisfy this criteria.
15. Moreover, the interpretation is limited to single neurons and identifying groups of neurons requires an expensive greedy search.
16. Limitation In addition to the shortcomings discussed above, a major limitation of probing-based methods is the requirement of supervised data for training the classifier, thus limiting the analysis only to pre-defined or annotated concepts.","1. How do regularization techniques affect linear classifiers?
2. How do regularization techniques affect concept learning in linear classifiers?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s8,"Question: What are causation-based methods in neural network analysis?

1. The methods we have discussed so far are limited to identifying neurons that have learned the encoded concepts.
2. They do not inherently reflect their importance towards the model's performance.
3. Causation-based methods identify neurons with respect to model's prediction.
4. Ablation The central idea behind ablation is to notice the affect of a neuron on model's performance by varying its value.
5. This is done either by clamping its value to zero or a fixed value and observe the change in network's performance.
6. Ablation has been effectively used to find i) salient neurons with respect to a model (unsupervised), ii) salient neurons with respect to a particular output class in the network (supervised).
7. The former identifies neurons that incur a large drop in model's performance when ablated (Li et al., 2016a).
8. The latter selects neurons that cause the model to flip its prediction with respect to a certain class (Lakretz et al., 2019).
9. Here, the output class serves as the concept against which we want to find the salient neurons.
10. Limitation Identifying group neurons require ablating all possible combinations of neurons which is an NP-hard problem (Binshtok et al., 2007).
11. Several researchers have tried to circumvent this by using leave-one-out estimates (Zintgraf et al., 2017), beam search (Feng et al., 2018), by learning end-to-end differentiable prediction model (De Cao et al., 2020) and by using correlation clustering to group similar neurons before ablation .
12. Nevertheless all these approaches are approximations and may incur search errors.
13. Knowledge Attribution Method Attributionbased methods highlight the importance of input features and neurons with respect to a prediction (Dhamdhere et al., 2018;Lundberg and Lee, 2017;Tran et al., 2018).
14. Dai et al. (supervised)2 used an attribution-based method to identify salient neurons with respect to a relational fact.
15. They hypothesized that factual knowledge is stored in the neurons of the feed-forward neural networks of the Transformer model and used integrated gradient (supervised)0 to identify top neu-rons that express a relational fact.
16. The work of Dai et al. (supervised)2 shows the applicability of attribution methods in discovering causal neurons with respect to a concept of interest and is a promising research direction.
17. Limitation The attribution-based methods highlight salient neurons with respect to a prediction.
18. What concepts these salient neurons have learned is unknown.
19. Dai et al. (supervised)2 worked around this by limiting their study to model classes where each class serves as a concept.
20. Attribution-based methods can be enriched by complementing them with other neuron analysis methods such as corpus search that associate salient neurons to a concept.",1. What are causation-based methods in neural network analysis?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s10,"Question: How are neuron analysis methods evaluated for correctness without standard benchmarks?

1. In this section, we survey the evaluation methods used to measure the correctness of the neuron analysis methods.
2. Due to the absence of interpretation benchmarks, it is difficult to precisely define ""correctness"".
3. Evaluation methods in interpretation mostly resonate with the underlying method to discovered salient neurons.
4. For example visualization methods often require qualitative evaluation via human in the loop, probing methods claim correctness of their rankings using classifier accuracy as a proxy.
5. Antverg and Belinkov (2022) highlighted this discrepancy and suggested to disentangle the analysis methodology from the evaluation framework, for example by using a principally different evaluation method compared to the underlying neuron analysis method.
6. In the following, we summarize various evaluation methods and their usage in the literature.","What advancements did Transformers bring to NLP since 2017, and how does BERT, a Transformer-based model, contribute to understanding and improving natural language processing tasks?
1. What advancements did Transformers bring to NLP since 2017?
2. How does BERT, a Transformer-based model, contribute to understanding and improving natural language processing tasks?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s11,"Question: How does ablation help evaluate neuron importance in models?

1. While ablation has been used to discover salient neurons for the model, it has also been used to evaluate the efficacy of the selected neurons.
2. More concretely, given a ranked list of neurons (e.g. the output of the probing method), we ablate neurons in the model in the order of their importance and measure the effect on the performance.
3. The idea is that removing the top neurons should result in a larger drop in performance compared to randomly selected neurons.
4. Dalvi et al. (2019);  used ablation in the probing classifier to demonstrate correctness of their neuron ranking method.
5. Similarly Bau et al. (2019) showed that ablating the most salient neurons, discovered using multi-model search, in NMT models lead to a much bigger drop in performance as opposed to removing randomly selected neurons.",1. How does ablation help evaluate neuron importance in models?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s12,"Question: How is the performance of salient neurons evaluated in concept classification?

1. Given salient neurons with respect to a concept, a simple method to evaluate their correctness is to train a classifier using them as features and predict the concept of interest.
2. The performance of the classifier relative to a classifier trained using random neurons and least important neurons is used as a metric to gauge the efficacy of the selected salient neurons.
3. However, it is important to ensure that the probe is truly representing the concepts encoded within the learned representations and not memorizing them during classifier training.
4. Hewitt and Liang (2019) introduced Controlled Tasks Selectivity as a measure to gauge this.
5. adapted controlled tasks for neuronprobing to show that their probes indeed reflect the underlying linguistic tasks.",1. How is the performance of salient neurons evaluated in concept classification?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s18,"Question: How do neurons capture lexical concepts in neuron analysis research?

1. Some of the research done on neuron analysis particularly the work using visualization and concept search methods identified neurons that capture lexical concepts.
2. Visualizations Karpathy et al. (2015) found neurons that learn position of a word in the input sentence: activating positively in the beginning, becoming neutral in the middle and negatively towards the end.
3. Li et al. (2016a) found intensification neurons that activate for words that intensify a sentiment.
4. For example ""I like this movie a lot"" or ""the movie is incredibly good"".
5. Similarly they discovered neurons that captured ""negation"".
6. Both intensification neurons and sentiment neurons are relevant for the sentiment classification task, for which the understudied model was trained.
7. Kádár et al. (2017) identified neurons that capture related groups of concepts in a multi-modal image captioning task.
8. For example, they discovered neurons that learn electronic items ""camera, laptop, cables"" and salad items ""broccoli, noodles, carrots etc"".
9. Similarly Na et al. (2019) found neurons that learn lexical concepts related to legislative terms, e.g. ""law, legal"" etc.
10. They also found neurons that learn phrasal concepts.
11. Poerner et al. (2018) showed that Concept Search can be enhanced via Corpus Generation.
12. They provided finer interpretation of the neurons by generating synthetic instances.
13. For example, they showed that a ""horse racing"" neuron identified via concept search method was in fact a general ""racing"" neuron by generating novel contexts against this neuron.",1. How do neurons capture lexical concepts in neuron analysis research?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s20,"Question: How do neurons in NMT models capture linguistic concepts?

1. A number of studies probed for neurons that capture core-linguistic concepts such as morphology, semantic tags, etc. Probing for linguistic structure is important to understand models' capacity to generalize (Marasović, 2018).
2. § For example, the holy grail in machine translation is that a proficient model needs to be aware of word morphology, grammatical structure, and semantics to do well (Vauquois, 1968;Jones et al., 2012).
3. Below we discuss major findings along this line of work: Neurons specialize in core linguistic concepts Dalvi et al. (Vauquois, 1968;Jones et al., 2012)2 in their analysis of LSTMbased NMT models found neurons that capture core linguistic concepts such as nouns, verb forms, numbers, articles, etc. They also showed that the number of neurons responsible for a concept varies based on the nature of the concept.
4. For example: closed class ¶ concepts such as Articles (morphological category), Months of Year (semantic category) are localized to fewer neurons, whereas open class concepts such as nouns (morphological category) or event (semantic category) are distributed among a large number of neurons.
5. Neurons exhibit monosemous and polysemous behavior.
6. Xin et al. (Vauquois, 1968;Jones et al., 2012)2 found neurons exhibiting a variety of roles where a few neurons were exclusive to a single concept while others were polysemous in nature and captured several § but is not the only reason to carry such an analysis.
7. ¶ closed class concepts are part of language where new words are not added as the language evolves, for example functional words such as can, be etc.
8. In contrast open class concepts are a pool where new words are constantly added as the language evolve, for example ""chillax"" a verb formed blending ""chill"" and ""relax"".
9. concepts. Suau et al. (2020) discovered neurons that capture different senses of a word.
10. Similarly, Bau et al. (Vauquois, 1968;Jones et al., 2012)2 found a switch neuron that activates positively for present-tense verbs and negatively for the past tense verbs.
11. Neurons capture syntactic concepts and complex semantic concepts.
12. Lakretz et al. (Vauquois, 1968;Jones et al., 2012)2 discovered neurons that capture subject-verb agreement within LSTM gates.
13. Karpathy et al. (Vauquois, 1968;Jones et al., 2012)1 also found neurons that activate within quotes and brackets capturing long-range dependency.
14. Na et al. (Vauquois, 1968;Jones et al., 2012)2 aligned neurons with syntactic parses to show that neurons learn syntactic phrases.
15. Seyffarth et al. (Vauquois, 1968;Jones et al., 2012)3 analyzed complex semantic properties underlying a given sentence.",1. How do neurons in NMT models capture linguistic concepts?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s21,"Question: How do researchers identify and evaluate the importance of salient neurons in neural networks?

1. In contrast to analyzing neurons with respect to a pre-defined concept, researchers also interpreted the concepts captured in the most salient neurons of the network.
2. For example, in the analysis of the encoder of LSTM-based models, Bau et al. (2019) used Pearson correlation to discover salient neurons in the network.
3. They found neurons that learn position of a word in the sentence among the most important neurons.
4. Other neurons found included parentheses, punctuation and conjunction neurons.
5. Moreover, Li et al. (2016b) found that the two most salient neurons in Glove were the frequency neurons that play an important role in all predictions.
6. The question of whether core-linguistic concepts are important for the end performance has been a less explored area.
7. Dalvi et al. (2019) compared neurons learning morphological concepts and semantic concepts with unsupervised ranking of neurons with respect to their effect on the end performance.
8. They found that the model is more sensitive to the top neurons obtained using unsupervised ranking compared to linguistic concepts.
9. They showed that the unsupervised ranking of neurons is dominated by position information and other closed class categories such as conjunction and punctuation which according to the ablation experiment are more critical concepts for the end performance than linguistic concepts.","1. How do researchers identify salient neurons in neural networks?
2. How do researchers evaluate the importance of salient neurons in neural networks?"
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s23,"Question: How do pre-trained language models represent linguistic hierarchy according to neuron distribution?

1. Human languages are hierarchical in structure where morphology and phonology sit at the bottom followed by lexemes, followed by syntactic structures.
2. Concepts such as semantics and pragmatics are placed on the top of the hierarchy.
3. analyzed linguistic hierarchy by studying the spread of neurons across layers in various pre-trained language models.
4. They extracted salient neurons with respect to different linguistic concepts (e.g. morphology and syntax) and found that neurons that capture word morphology were predominantly found in the lower and middle layers and those learning about syntax were found at the higher layers.
5. The observation was found to be true in both LSTMand the transformer-based architectures, and are inline with the findings of representation analysis (Liu et al., 2019;Tenney et al., 2019;Belinkov et al., 2020b).
6. Similarly Suau et al. (2020) analyzed sub-modules within GPT and RoBERTa transformer blocks and showed that lower layers within a transformer block accumulate more salient neurons than higher layers on the tasks of word sense disambiguation or homograph detection.
7. They also found that the neurons that learn homographs are distributed across the network as opposed to sense neurons that were more predominantly found at the lower layers.",1. How do pre-trained language models represent linguistic hierarchy according to neuron distribution?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s24,"Question: How do training choices like dropout affect information distribution in neural networks?

1. While it is exciting to see that networks somewhat preserve linguistic hierarchy, many authors found that information is not discretely preserved at any individual layer, but is distributed and is redundantly present in the network.
2. This is an artifact of various training choices such as dropout that encourages the model to distribute knowledge across the network.
3. For example, Li et al. (2016b) found specialized frequency neurons in a GloVe model trained without dropout, as opposed to the variant trained with dropout where the information was more redundantly available.
4. showed that a significant amount of redundancy existed within pre-trained models.
5. They showed that 85% of the neurons across the network are redundant and at least 92% of the neurons can be removed when optimizing towards a downstream task in feature-based transfer learning.",1. How do training choices like dropout affect information distribution in neural networks?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s25,"Question: How do neuron distributions vary across different neural network architectures?

1. The distribution of neurons across the network has led researchers to draw interesting crossarchitectural comparisons.
2. Wu et al. (2020) performed correlation clustering of neurons across architectures and found that different architectures may have similar representations, but their individual neurons behave differently.
3. Hennigen et al. (2020) compared neurons in contextualized (BERT) embedding with neurons in the static embedding (fastText) and found that fastText required two neurons to capture any morphosyntactic phenomenon as opposed to BERT which required up to 35 neurons to obtain the same performance.
4. showed that the linguistic knowledge in BERT (auto-encoder) is highly distributed across the network as opposed to XLNet (auto-regressive) where neurons from a few layers are mainly responsible for a concept (see Figure 2).
5. Similarly Suau et al. (2020) compared RoBERTa and GPT (auto-encoder vs. generative) models and found differences in the distribution of expert neurons.
6. extended the cross-architectural comparison towards fine-tuned models.
7. They showed that after finetuning on GLUE tasks, the neurons capturing linguistic knowledge are regressed to lower layers in RoBERTa and XLNet as opposed to BERT where it is still retained at the higher layers.",1. How do neuron distributions vary across different neural network architectures?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s26,"Question: What do neurons in Deep NLP models learn according to recent studies?

1. Below is a summary of the key findings that emerged from the work we covered in this survey.
2. Neurons learned within Deep NLP models capture non-trivial linguistic knowledge ranging from lexical phenomenon such as morphemes, words and multi-word expressions to highly complex global phenomenon such as semantic roles and syntactic dependencies.
3. Neuron analysis resonates with the findings of representation analysis (Belinkov et al., 2017a,b;Tenney et al., 2019;Liu et al., 2019) in demonstrating that the networks follow linguistic hierarchy.
4. Linguistic neurons are distributed across the network based on their complexity, with lower layers focused on the lexical concepts and middle and higher layers learning global phenomenon based on long-range contextual dependencies.
5. While the networks preserve linguistic hierarchy, many authors showed that information is not discretely preserved, but is rather distributed and redundantly present in the network.
6. It was also shown that a small optimal subset of neurons w.r.t any concept can be extracted from a network.
7. On another dimension, a few works showed that some concepts are localized to fewer neurons while others are distributed to a large group.
8. Finally, some interesting cross architectural analyses were drawn based on how the neurons are distributed within their layers.",1. What do neurons in Deep NLP models learn according to recent studies?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s28,"Question: How can identified neurons control a model's behavior regarding specific concepts?

1. Once we have identified neurons that capture a certain concept learned in a model, these can be utilized for controlling the model's behavior w.r.t to that concept.
2. Bau et al. (2019) identified Switch Neurons in NMT models that activate positively for the present-tense verbs and negatively for the past-tense verbs.
3. By manipulating the values of these neurons, they were able to successfully change output translations from present to past tense during inference.
4. The authors additionally found neurons that capture gender and number agreement concepts and manipulated them to control the system's output.
5. Another effort along this line was carried by Suau et al. (2020) Controlling model's behavior using neurons en-ables on-the-fly manipulation of output, for example it can be used to debias the output of the model against sensitive attributes like race and gender.",1. How can identified neurons control a model's behavior regarding specific concepts?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s29,"Question: How does model distillation improve efficiency in deep NLP models?

1. Deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments.
2. Identifying salient neurons and sub-networks can be useful for model distillation and efficiency.
3. devised an efficient featurebased transfer learning procedure, stemmed from their redundancy analysis.
4. By exploiting layer and neuron-specific redundancy in the transformer models, they were able to reduce the feature set size to less than 10% neurons for several tasks while maintaining more than 97% of the performance.
5. The procedure achieved a speedup of up to 6.2x in computation time for sequence labeling tasks as opposed to using all the features.",1. How does model distillation improve efficiency in deep NLP models?
237353268,Neuron-level Interpretation of Deep NLP Models: A Survey,Computer Science,s31,"Question: How do compositional explanations help understand model predictions?

1. Knowing the association of a neuron with a concept enables explanation of model's output.
2. Mu and Andreas (2020) identified neurons that learn certain concepts in vision and NLP models.
3. Using a composition of logical operators, they provided an explanation of model's prediction.
4. Figure 3 presents an explanation using a gender-sensitive neuron.
5. The neuron activates for contradiction when the premise contains the word man.
6. Such explanations provide a way to generate adversarial examples that change model's predictions.",1. How do compositional explanations help understand model predictions?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s1,"Question: What are the key observations in the evolution of LLMs according to the practical guide?

1. This section provides a brief introduction to state-of-the-art LLMs.
2. These models differ in their training strategies, model architectures, and use cases.
3. To provide a clearer understanding of the LLM landscape, we categorize them into two types: encoder-decoder or encoder-only language models and decoder-only language models.
4. In Figure 1, we show the detailed evolution process of language models.
5. From the evolutionary tree, we make the following interesting observations: a)
6. Decoder-only models have been gradually dominating the development of LLMs.
7. At the early stage of LLMs development, decoder-only models were not as popular as encoder-only and encoder-decoder models.
8. However, after 2021, with the introduction of game-changing LLMs -GPT-3, decoder-only models experienced a significant boom.
9. Meanwhile, after the initial explosive growth brought about by BERT, encoder-only models gradually began to fade away.
10. Fig. 1. The evolutionary tree of modern LLMs traces the development of language models in recent years and highlights some of the most well-known models.
11. Models on the same branch have closer relationships.
12. Transformer-based models are shown in non-grey colors: decoder-only models in the blue branch, encoder-only models in the pink branch, and encoder-decoder models in the green branch.
13. The vertical position of the models on the timeline represents their release dates.
14. Open-source models are represented by solid squares, while closed-source models are represented by hollow ones.
15. The stacked bar plot in the bottom right corner shows the number of models from various companies and institutions.
16. b) OpenAI consistently maintains its leadership position in LLM, both currently and potentially in the future.
17. Other companies and institutions are struggling to catch up with OpenAI in developing models comparable to GPT-3 and the current GPT-4.
18. This leadership position may be attributed to OpenAI's steadfast commitment to its technical path, even when it was not widely acknowledged initially.",1. What are the key observations in the evolution of LLMs according to the practical guide?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s3,"Question: What motivates the use of Masked Language Models in natural language processing?

1. As natural language data is readily available and unsupervised training paradigms have been proposed to better utilize extremely large datasets, this motivates the unsupervised learning of natural language.
2. One common approach is to predict masked words in a sentence while considering the surrounding context.
3. This training paradigm is known as the Masked Language Model.
4. This type of training allows the model to develop a deeper understanding of the relationships between words and the context in which they are used.
5. These models are trained on a large corpus of texts using techniques such as the Transformer architecture and have achieved state-of-the-art results in many NLP tasks, such as sentiment analysis and named entity recognition.
6. Notable examples of Masked Language Models include BERT [28], RoBERTa [65], and T5 [84].
7. MLMs have become an important tool in the field of natural language processing due to their success in a wide range of tasks.",1. What motivates the use of Masked Language Models in natural language processing?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s4,"Question: What makes autoregressive language models like GPT-3 effective for few-shot and zero-shot tasks?

1. Although language models are typically task-agnostic in architecture, these methods require fine-tuning on datasets of the specific downstream task.
2. Researchers found that scaling up language models significantly improves the few-shot, even zero-shot performance [16].
3. The most successful models for better few-shot and zero-show performance are Autoregressive Language Models, which are trained by generating the next word in a sequence given the preceding words.
4. These models have been widely used for downstream tasks such as text generation and question answering.
5. Examples of Autoregressive Language Models include GPT-3 [16], OPT [126], PaLM [22], and BLOOM [92].
6. The game changer, GPT-3, for the first time, demonstrated reasonable few-/zero-shot performance via prompting and in-context learning, thus showing the superiority of autoregressive language models.
7. There are also models such as CodeX [2] that are optimized for specific tasks such as code generation, BloombergGPT [117] for the financial domain.
8. The recent breakthrough is ChatGPT, which refines GPT-3 specifically for conversational tasks, resulting in more interactive, coherent, and context-aware conversational for various real-world applications.",1. What makes autoregressive language models like GPT-3 effective for few-shot and zero-shot tasks?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s7,"Question: What is the significance of pretraining data in large language model development?

1. Pre-training data plays a pivotal role in the development of large language models.
2. As the foundation of remarkable capabilities [5,47] of LLMs, the quality, quantitative, and diversity of pre-training data influence the performance of LLMs significantly [124].
3. The commonly used pretraining data consists of a myriad of text sources, including books, articles, and websites.
4. The data is carefully curated to ensure a comprehensive representation of human knowledge, linguistic nuances, and cultural perspectives.
5. The importance of pretraining data lies in its capacity to inform the language model with a rich understanding of word knowledge, grammar, syntax, and semantics, as well as the ability to recognize context and generate coherent responses.
6. The diversity of pretraining data also plays a crucial role in shaping the model's performance, and the selection of LLMs highly depends on the components of the pretraining data.
7. For example, PaLM [22] and BLOOM [92] excel in multilingual tasks and machine translation with an abundance of multilingual pretraining data.
8. Moreover, PaLM's performance in Question Answering tasks is enhanced by incorporating a considerable amount of social media conversations and Books corpus [22].
9. Likewise, code execution and code completion capabilities of GPT-3.5 (code-davinci-002) are amplified by the integration of code data in its pretraining dataset.
10. In brief, when selecting LLMs for downstream tasks, it is advisable to choose the model pre-trained on a similar field of data.",1. What is the significance of pretraining data in large language model development?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s8,"Question: How should models be deployed based on annotated data availability?

1. When deploying a model for downstream tasks, it is essential to consider three primary scenarios based on the availability of annotated data: zero, few, and abundant.
2. In this section, we provide a succinct overview of the appropriate models to employ for each scenario.
3. Zero annotated data: In scenarios where annotated data is unavailable, utilizing LLMs in a zero-shot setting proves to be the most suitable approach.
4. LLMs have been shown to outperform previous zero-shot methods [120].
5. Additionally, the absence of a parameter update process ensures that catastrophic forgetting [49] is avoided since the language model parameters remain unaltered.
6. Few annotated data: In this case, the few-shot examples are directly incorporated in the input prompt of LLMs, which is named as in-context learning, and these examples can effectively guide LLMs to generalize to the task.
7. As reported in [16], one-shot and few-shot performance make significant gains, even matching the performance of the SOTA fine-tuned open-domain models.
8. And LLMs' zero/few-shot ability can be improved further by scaling [16].
9. Alternatively, some few-shot learning methods are invented to enhance fine-tuned models, such as meta-learning [56] or transfer learning [88].
10. However, performance might be inferior compared to using LLMs due to fine-tuned models' smaller scale and overfitting.
11. Abundant annotated data: With a substantial amount of annotated data for a particular task available, both fine-tuned models and LLMs can be considered.
12. In most cases, fine-tuning the model can fit the data pretty well.
13. Although, LLMs can be used to meet some constraints such as privacy [99].In this scenario, the choice between using a fine-tuned model or a LLM is task-specific and also depends on many factors, including desired performance, computational resources, and deployment constraints.
14. In a brief summary: LLMs are more versatile w.r.t.
15. the data availability, while fine-tuned models can be considered with abundant annotated data.",1. How should models be deployed based on annotated data availability?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s9,"Question: How do LLMs handle distributional differences in test/user data effectively?

1. When deploying LLMs for downstream tasks, we often face challenges stemming from distributional differences between the test/user data and that of the training data.
2. These disparities may encompass domain shifts [132], out-of-distribution variations [31], or even adversarial examples [82].
3. Such challenges significantly hinder fine-tuned modes' effectiveness in real-world applications.
4. They fit into a specific distribution and have a poor ability to generalize to OOD data.
5. However, LLMs perform quite well facing such scenarios because they do not have an explicit fitting process.
6. Moreover, recent advancements have further enhanced the ability of language models in this regard.
7. The Reinforcement Learning from Human Feedback (RLHF) method has notably enhanced LLMs' generalization capabilities [77].
8. For example, InstructGPT demonstrates proficiency in following various instructions for a wide range of tasks and occasionally complying with instructions in different languages, even though such instructions are scarce.
9. Similarly, ChatGPT exhibits consistent advantages on most adversarial and out-of-distribution (OOD) classification and translation tasks [109].
10. Its superiority in understanding dialogue-related texts led to an impressive performance on the DDXPlus dataset [101], a medical diagnosis dataset designed for OOD evaluation.","What advancements did Transformers bring to NLP since 2017, and how does BERT, a Transformer-based model, contribute to understanding and improving natural language processing tasks?
1. What advancements did Transformers bring to NLP since 2017?
2. How does BERT, a Transformer-based model, contribute to understanding and improving natural language processing tasks?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s15,"Question: What are the strengths and applications of Large Language Models in generation tasks?

1. Due to their strong generation ability and creativity, LLMs show superiority at most generation tasks.
2. 4.2.1 Use case. Generation tasks require models to have a comprehensive understanding of the input contents or requirements and a certain level of creativity.
3. This is what LLMs excel at. For summarization tasks, although LLMs do not have an obvious advantage over fine-tuned models under traditional automatic evaluation metrics, such as ROUGE [60], human evaluation results indicate that humans tend to prefer the results generated by LLMs [38,127] compared to that of fine-tuned models.
4. For example, on CNN/DailyMail [71] and XSUM [72], fine-tuned models like Brio [66] and Pegasus [125] have much better performance than any LLMs w.r.t. ROUGE, but LLMs like OPT [126] perform far better in human evaluation considering all aspects including faithfulness, coherence, and relevance [127].
5. This demonstrates the superiority of LLMs in summarization tasks.
6. On the other hand, it implies that current summarization benchmarks don't contain summaries with high quality or the automatic metrics are not proper for the evaluation of summarization.
7. In machine translation (MT), LLMs can perform competent translation, although the average performance is slightly worse than some commercial translation tools [45] considering some automatic metrics like BLEU [38,127]0.
8. LLMs are particularly good at translating some low-resource language texts to English texts, such as in the Romanian-English translation of WMT'16 [38,127]1, zero-shot or few-shot LLMs can perform better than SOTA fine-tuned model [38,127]9.
9. This is mainly due to the fact that English resources compose the main part of the pre-training data.
10. BLOOM [38,127]3 is pre-trained on more multi-lingual data, leading to better translation quality in both rich-resource and low-resource translation.
11. Another interesting finding is that BLOOM achieves good translation quality among Romance languages, even for translation from Galician, which is not included in the pre-training data.
12. One reasonable explanation is that texts from some languages in the same language group can help the LLMs learn more from the similarity.
13. If more multi-lingual texts can be added to the pre-training data, the translation capability may be improved further.
14. Additionally, LLMs are highly skilled in open-ended generations.
15. One example is that the news articles generated by LLMs are almost indistinguishable from real news articles by humans [38,127]4.
16. LLMs are remarkably adept at code synthesis as well.
17. Either for text-code generation, such as HumanEval [38,127]5 and MBPP [38,127]6, or for code repairing, such as DeepFix [38,127]7, LLMs can perform pretty well.
18. GPT-4 can even pass 25% problems in Leetcode, which are not trivial for most human coders [38,127]8.
19. With training on more code data, the coding capability of LLMs can be improved further [38,127]9.
20. While performing well on such tasks, the codes generated by LLMs should be tested carefully to figure out any subtle bugs, which is one of the main challenges for applying LLMs in code synthesis.","1. What are the strengths of Large Language Models in generation tasks?
2. What are the applications of Large Language Models in generation tasks?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s19,"Question: How does scaling affect the abilities and performance of pretrained language models?

1. Scaling of LLMs (e.g. parameters, training computation, etc.) can greatly empower pretrained language models.
2. With the model scaling up, a model generally becomes more capable in a range of tasks.
3. Reflected in some metrics, the performance shows a power-law relationship with the model scale.
4. For example, the cross-entropy loss which is used to measure the performance for language modeling decreases linearly with the exponential increase in the model scale, which is also called 'scaling-law' [41,47].
5. For some crucial abilities, such as reasoning, scaling the model has gradually transformed these abilities from a very low state to a usable state, and even approaching human capabilities.
6. In this section, we provide an overview of the usage of LLMs in terms of the abilities and behaviors of LLMs along with scaling.","1. How does scaling affect the abilities of pretrained language models?
2. How does scaling affect the performance of pretrained language models?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s21,"Question: How do LLMs perform in arithmetic and commonsense reasoning tasks?

1. Reasoning, which involves making sense of information, drawing inferences, and making decisions, is one of the essential aspects of human intelligence.
2. It is challenging for NLP. Many existing reasoning tasks can be classified into commonsense reasoning and arithmetic reasoning.
3. Arithmetic reasoning/problem solving.
4. The arithmetic reasoning capability of LLMs benefits greatly from the scaling of model size.
5. For GPT-3, the ability of two-digit addition only becomes apparent when the number of parameters exceeds 13B [16].
6. Tasks to test arithmetic reasoning are trivial for humans and designed to challenge the capability of transferring natural language into mathematical symbols and multi-step inference.
7. On GSM8k [26], SVAMP [79] and AQuA [61], LLMs, as generalists, have competitive performance with most methods which have task-specific designs.
8. And GPT-4 overperforms any other methods [26]1, even some huge models particularly tuned for arithmetic problems [104].
9. Nevertheless, it should be noted that, without the intervention of external tools, LLMs may occasionally make mistakes in performing basic calculations, although chain-of-thought (CoT) prompting [115] can significantly improve LLMs' ability in calculations.
10. Commonsense reasoning. Commonsense reasoning not only requires LLMs to remember factual knowledge but also requires LLMs to do several inference steps about the facts.
11. Commonsense reasoning increases gradually with the growth of model size.
12. Compared to fine-tuned models, LLMs keep the superiority on most datasets, such as StrategyQA [36] and ARC-C [25].
13. Especially on ARC-C, which contains difficult questions in science exams from grade 3 to grade 9, GPT-4 has been close to the performance of 100% [26]0 [26]1.","1. How do LLMs perform in arithmetic tasks?
2. How do LLMs perform in commonsense reasoning tasks?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s22,"Question: What are emergent abilities in large language models and their examples?

1. Scaling of models also endows the model with some unprecedented, fantastic abilities that go beyond the power-law rule.
2. These abilities are called ""emergent ability"".
3. As defined in [113], emergent abilities of LLMs are abilities that are not present in smaller-scale models but are present in large-scale models.
4. This means such abilities cannot be predicted by extrapolating the performance improvements on smaller-scale models and the model suddenly gains good performance on some tasks once the scale exceeds a certain range.
5. The emergent ability is typically unpredictable and surprising, leading to tasks that emerge randomly or unexpectedly.
6. We examine concrete examples of the emergent abilities of LLMs and provide them as an important reference for deciding whether to leverage LLMs' emergent abilities.
7. Handling word manipulation is a typical emergent ability.
8. It refers to the ability to learn symbolic manipulations, such as the reversed words [16], in which the model is given a word spelled backwards, and must output the original word.
9. For example. GPT-3 [16] shows the emergent ability for word sorting, and word unscrambling tasks.
10. PaLM [22] exhibits the emergent ability on ASCII word recognition 4 and hyperbaton 5 task.
11. The logical abilities of language models tend to emerge as the model scales up, such as logical deduction, logical sequence, and logic grid puzzles.
12. Additionally, other tasks, such as advanced coding (e.g., auto debugging, code line description), and concept understanding (e.g., novel concepts, simple Turing concepts), are also use cases with the emergent abilities of large language models.","What are emergent abilities in large language models?
1. What are emergent abilities in large language models?
2. What are some examples of emergent abilities in large language models?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s23,"Question: What are the exceptions and phenomena affecting LLM performance on certain tasks?

1. Although in most cases, as discussed above, larger models bring better performance, there are still many exceptions that should be considered when choosing the appropriate model.
2. On certain tasks, with the size of LLMs increasing, the performance begins to decrease, such as Redefine-math: tests whether language models are able to work with common symbols when they are redefined to mean something else; Intothe-unknown: requires the model to choose which piece of information would help answer a question; Memo-trap: asks an LM to write a phrase in a way that starts like a famous quote but ends differently 6 .
3. This is also called Inverse Scaling Phenomenon.
4. Another interesting phenomenon observed in the scaling of LLMs is called the U-shaped Phenomenon [114].
5. As the name implies, This phenomenon refers to that as LLM size increases, their performance on certain tasks initially improves but then starts to decline before eventually improving again, such as on: Hindsight-neglect: it tests whether language models are able to assess whether a bet was worth taking based on its expected value; NegationQA: this task takes an existing multiple-choice dataset and negates a part of each question to see if language models are sensitive to negation; Quote-repetition: it asks models to repeat back sentences given in the prompt, with few-shot examples to help it recognize the task.
6. Hence the risk of diminishing performance should be noted and if the task is similar to those we just discussed, careful consideration should be given to whether or not to use huge LLMs.
7. Gaining a deeper understanding of emergent abilities, inverse scaling phenomenon and U-shape phenomenon in LLMs is essential for advancing research in this field.
8. In a certain sense, the U-shape phenomenon suggests that small-scale models and huge-scale models make predictions with different internal mechanisms.
9. From this perspective, the U-shape phenomenon can be seen as a transformation of the inverse-scaling phenomenon due to some emergent abilities from sufficiently large models [114].
10. GPT-4 [76] exhibits a reversal of the inverse scaling phenomenon in some cases, such as on a task called Hindsight Neglect.
11. The explanation for these behaviors of LLMs during scaling is still an open problem.
12. Several hypotheses have been proposed.
13. For emergent abilities, one explanation is that there may be multiple key steps for a task and the LLM cannot handle this task until it's large enough to handle every step, and another explanation is focused on the granularity of evaluation metrics [113].
14. For inverse-scaling phenomenon and u-shape phenomenon, the explanations mainly focus on the model's over-reliance on information from its prior rather than the input prompts, valid but misleading few-shot examples, and distracting easier tasks within a hard task [114].","1. What are the exceptions affecting LLM performance on certain tasks?
2. What are the phenomena affecting LLM performance on certain tasks?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s26,"Question: Why do LLMs perform poorly on regression tasks compared to discrete label tasks?

1. LLMs generally struggle with some tasks due to differences in objectives and training data.
2. Although LLMs have achieved remarkable success in various natural language processing tasks, their performance in regression tasks has been less impressive.
3. For example, ChatGPT's performance on the GLUE STS-B dataset, which is a regression task evaluating sentence similarity, is inferior to a fine-tuned RoBERTa performance [130].
4. The Regression tasks typically involve predicting a continuous value rather than a discrete label, posing unique challenges for LLMs.
5. One primary reason for their subpar performance is the inherent difference between the language modeling objective and the regression task objective.
6. LLMs are designed to predict the next word in a sequence or generate coherent text, with their pre-training focused on capturing linguistic patterns and relationships.
7. Consequently, their internal representations may not be well-suited for modeling continuous numerical outputs.
8. Besides, LLMs have predominantly been trained on text data, focusing on capturing the intricacies of natural language processing.
9. As a result, their performance on multimodal data, which involves handling multiple data types such as text, images, audio, video, actions, and robotics, remains largely unexplored.
10. And fine-tuned multimodal models, like BEiT [110] and PaLI [19], still dominate many tasks such as visual question answering (VQA) and image captioning.
11. Nonetheless, the recently introduced GPT-4 [76] has taken the step in multimodal fusion, but there is still a lack of detailed evaluation of its capabilities.","1. Why do LLMs perform poorly on regression tasks?
2. Why do LLMs perform better on discrete label tasks?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s27,"Question: What are the key applications and strengths of Large Language Models (LLMs)?

1. LLMs are particularly suitable for certain tasks.
2. LLMs are very good at mimicking humans, acting as a chatbot, and performing various kinds of tasks.
3. The LLMspowered ChatGPT 7 is surprising for its consistency, reliability, informativeness, and robustness during multiple utterances with humans.
4. The human-feedback procedure plays an important role in acquiring such abilities LLMs can both act as a good annotator and data generator for data augmentation, such as in [27,29,99,121,122].
5. Some LLMs have been found as good as human annotators [37] in some tasks.
6. And the collected texts from GPT-3.5 (text-davinci-003) have been used as human-like instruction-following demonstrations to train other language models [100].
7. LLMs can also be used for quality assessment on some NLG tasks, such as summarization and translation.
8. On summarization tasks, GPT-4 as an evaluator achieves a higher correlation with humans than other methods with a large margin [64].
9. Some other evaluators based on LLMs [34,50,64,108] also show good human alignment in more NLG tasks, especially compared with traditional automatic metrics.
10. But the LLM evaluator may have a bias towards the LLM-generated texts [64]. Also, as we discussed above, some abilities of LLMs bring bonuses in addition to performance improvement, such as interpretability.
11. The CoT reasoning ability of LLMs can show how an LLM reaches the prediction, which is a good interpretation on the instance level, while it also improves the performance.","1. What are the key applications of Large Language Models (LLMs)?
2. What are the strengths of Large Language Models (LLMs)?"
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s28,"Question: What challenges do models face when applied to real-world tasks?

1. In the last part of this section, we would like to discuss the usage of LLMs and fine-tuned models in real-world ""tasks"".
2. We use the term ""tasks"" loosely, as real-world scenarios often lack well-formatted definitions like those found in academia.
3. Many requests to models even cannot be treated as NLP tasks.
4. Models face challenges in the real world from three perspectives:  Noisy/Unstructured input.
5. Real-world input comes from real-world non-experts.
6. They have little knowledge about how to interact with the model or even cannot use texts fluently.
7. As a result, real-world input data can be messy, containing typos, colloquialisms, and mixed languages, unlike those well-formed data used for pre-training or fine-tuning.
8. Tasks not formalized by academia.
9. In real-world scenarios, tasks are often ill-defined by academia and much more diverse than those in academic settings.
10. Users frequently present queries or requests that do not fall neatly into predefined categories, and sometimes multiple tasks are in a single query.
11. Following users' instructions. A user's request may contain multiple implicit intents (e.g. specific requirement to output format), or their desired predictions may be unclear without follow-up questions.
12. Models need to understand user intents and provide outputs that align with those intents.
13. Essentially, these challenges in the real world come from that users' requests deviate significantly from the distribution of any NLP datasets designed for specific tasks.
14. Public NLP datasets are not reflective of how the models are used [77].",1. What challenges do models face when applied to real-world tasks?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s29,"Question: Why are LLMs considered better for real-world scenarios than fine-tuned models?

1. LLMs are better suited to handle real-world scenarios compared to fine-tuned models.
2. However, evaluating the effectiveness of models in the real world is still an open problem.
3. Handling such real-world scenarios requires coping with ambiguity, understanding context, and handling noisy input.
4. Compared to fine-tuned models, LLMs are better equipped for this because they have been trained on diverse data sets that encompass various writing styles, languages, and domains.
5. Additionally, LLMs demonstrate a strong ability to generate open-domain responses, making them well-suited for these scenarios.
6. Fine-tuned models, on the other hand, are often tailored to specific, well-defined tasks and may struggle to adapt to new or unexpected user requests.
7. They heavily rely on clear objectives and well-formed training data that specify the types of instructions the models should learn to follow.
8. Fine-tuned models may struggle with noisy input due to their narrower focus on specific distributions and structured data.
9. An additional system is often required as an assistant for fine-tuned models to process unstructured context, determine possible intents, and refine model responses accordingly.
10. Additionally, some mechanics such as instruction tuning [91,112] and human alignment tuning [77] further boost the capabilities of LLMs to better comprehend and follow user instructions.
11. These methods improve the model's ability to generate helpful, harmless, and honest responses while maintaining coherence and consistency [77,91,112].
12. While both methods can make LLMs better generalize to unseen tasks and instructions, it has been noticed that while human labelers prefer models tuned for human alignment [77] to models tuned with instructions from public NLP tasks, such as FLAN [112] and T0 [91].
13. The reason may be similar to reasons for fine-tuned models' inferiority: public NLP tasks/datasets are designed for easy and automatic evaluation, and they can only cover a small part of real-world usage.
14. One of the main issues when it comes to real-world scenarios is how to evaluate whether the model is good or not.
15. Without any formalized tasks or metrics, the evaluation of model effectiveness can only rely on feedback from human labelers.
16. Considering the complexity and cost of human evaluation, there's no massive and systematic comparison between fine-tuned models and LLMs yet.
17. Nevertheless, the huge success and popularity of LLMs such as chatGPT, have confirmed the superiority of LLMs to some extent.",1. Why are LLMs considered better for real-world scenarios than fine-tuned models?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s31,"Question: What are the considerations for choosing between LLMs and fine-tuned models?

1. (1) Light, local, fine-tuned models should be considered rather than LLMs, especially for those who are sensitive to the cost or have strict latency requirements.
2. Parameter-Efficient tuning can be a viable option for model deployment and delivery. (2)
3. The zero-shot approach of LLMs prohibits the learning of shortcuts from task-specific datasets, which is prevalent in fine-tuned models.
4. Nevertheless, LLMs still demonstrate a degree of shortcut learning issues. (3)
5. Safety concerns associated with LLMs should be given utmost importance as the potentially harmful or biased outputs, and hallucinations from LLMs can result in severe consequences.
6. Some methods such as human feedback have shown promise in mitigating these problems.",1. What are the considerations for choosing between LLMs and fine-tuned models?
258331833,Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond,"Linguistics, Computer Science",s34,"Question: What are the safety challenges associated with large language models (LLMs)?

1. LLMs have demonstrated their extremely strong capabilities in many areas such as reasoning, knowledge retention, and coding.
2. As they become more powerful and human-like, their potential to influence people's opinions and actions in significant ways grows.
3. As a result, some new safety challenges to our society should be considered and have caught lots of attention in recent works [75,76]. Hallucinations.
4. The potential for LLMs to ""hallucinate,"" or generate nonsensical or untruthful content, can have significant negative impacts on the quality and reliability of information in various applications.
5. As LLMs become increasingly convincing and believable, users may develop an overreliance on them and trust them to provide accurate information in areas with which they are somewhat familiar.
6. This can be particularly dangerous if the model produces content that is entirely false or misleading, leading to incorrect decisions or actions taken based on that information.
7. Such outcomes can have serious consequences in many domains, such as healthcare, finance, or public policy, where the accuracy and reliability of information are critical.
8. To mitigate these issues, reinforcement learning from human feedback (RLHF) is widely used [75,77] and LLMs themselves have been integrated into the loop [75].
9. Harmful content. Due to the high coherence, quality, and plausibility of texts generated by LLMs, harmful contents from LLMs can cause significant harm, including hate speech, discrimination, incitement to violence, false narratives, and even social engineering attack.
10. The implementation of safeguards to detect and correct those contents can be mitigation [97].
11. These LLMs can also have dual-use potential by providing required illicit information, leading to risks such as the proliferation of weapons [75] and even terrorism attack planning.
12. It is crucial to ensure using these LLMs responsibly, with safeguards in place to prevent harm.
13. Also, in existing work, feedback from humans plays an important role in getting rid of harmful outputs.
14. Privacy. LLMs can face serious security issues.
15. An example is the issue of user privacy.
16. It is reported that Samsung employees were using ChatGPT to process their work when they inadvertently leaked top-secret data, including the source code proper of the new program, internal meeting minutes related to the hardware, etc.
17. The Italian data protection agency declared that OpenAI, the developer of ChatGPT, illicitly gathered personal user data, leading Italy to become the first government to prohibit ChatGPT over privacy concerns [1].",1. What are the safety challenges associated with large language models (LLMs)?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s3,"Question: What is the cloze-style task in machine reading comprehension?

1. The correct answer is part of the question (usually a word or an entity) that is removed from question.
2. The cloze style task needs to fill in the blank with the correct word or entity by learning the function , such that = ( , ).
3. To show the frequency of each task among multi-hop studies Figure 4 has been prepared.
4. This figure contains some points:  Most studies have focused on the Span-extraction and Multiple-choice tasks.
5. One of the main reasons is that most available MRC datasets are also in these forms and automatically this fact encourages research to focus on these two tasks.",1. What is the cloze-style task in machine reading comprehension?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s6,"Question: What are the main categories and techniques in multi-hop MRC research?

1. In this paper, 31 studies have been investigated, which propose a model for multi-hop MRC based on the presented problem in section 2.
2. It is important to note that since there is a close relationship between MRC and Question Answering, most of the existing machine reading comprehension tasks are in the form of textual question answering [24], and also MRC is known as a basic task of textual question answering [19].
3. Thus, we consider cloze domain textual question answering as a typical MRC task in this paper .
4. Existing studies for multi-hop MRC can be mainly divided into three categories: decomposition, recurrent reasoning based on memory retrieval, and multi-step reasoning based on graph neural networks.
5. For each category, after explaining the main technique, the multi-hop MRC models will be reviewed in detail; beside reviewing the detail architectures of each model, we also focus on the superiority and the motivation of them.
6. Also, the disadvantages of each technique will be discussed at the end.
7. In the next section (4) a comprehensive comparison of the techniques and models will be presented.
8. The techniques do not have a specific order, because all three techniques have been used by models from 2018 to 2022 ( Figure   40), but as much as possible, the studies within the techniques have been according to the order of published time.","1. What are the main categories in multi-hop MRC research?
2. What are the techniques used in multi-hop MRC research?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s7,"Question: What is the decomposition technique in multi-hop MRC?

1. Complicated question is a basic challenge of multi-hop MRC, unlike the single-hop questions, they cannot be answered easily and require complicated reasoning.
2. Since the human reasoning about complex questions is done by decomposition, answering subquestions, summarizing, and comparing [26], then this technique has focused on simplifying the problem by decomposition of a complex question into multiple simple sub-questions.
3. It means it reduces multi-hop MRC to multiple single-hop MRC.
4. This technique mostly uses the single-hop MRC models to find the answers to sub-questions and then combine the answers to obtain the final answer.
5. In the following, the models which use this technique for multi-hop MRC will be reviewed in detail.
6. Self-assembling MNM: Jiang and Bansal [18] focused on identifying the sub-questions in the correct reasoning order and presented an interpretable and controller-based self-assembling neural modular network for the multi-hop reasoning process which includes two main parts, Modular Network with a Controller (top) and the Dynamically-assembled Modular Network (bottom) that can be seen in Figure 10.
7. The main idea of the model to handle multi-hop questions is done with Controller that computes an attention distribution over all question words at every reasoning step, which finds the sub-question that should be answered at the current step.
8. In summary, Controller reads the question and predicts a series of modules that could be executed in order to answer the given question.
9. Each module deals with a single-hop sub-question, then they will be chained together according to the predicated order by controller to get the final answer.
10. The mentioned modules are described as follows: All modules take the question representation , context representation ℎ, and sub-question vector as input.",1. What is the decomposition technique in multi-hop MRC?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s10,"Question: How do modern algorithms decompose multi-hop questions into simpler sub-questions?

1. The NoOp module can be seen as a skip command.
2. It is used to reduce computations when the controller decides no action is required.
3. However, this system approaches question decomposition by having a decomposer model trained via human labels (Cao and Liu, 2021a).
4. Figure 5: Architecture of Self-assembling MNM [18] ONUS: Perez et al. [27] proposed an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that map a complicate multi-hop question to some simpler single-hop sub-questions.
5. Unlike other decomposition studies use a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions to decompose multi-hop question which require significant human effort, this model automatically learns to decompose different kinds of questions.
6. The main idea has been shown in Figure 6.
7. To decompose multi-hop question to simpler corpus , First some candidate sub-question from a simple corpus will be created by mining 10M possible sub-questions from Common Crawl with a classifier.
8. It then trains a decomposition model on the mined data using Q and D with unsupervised sequence-to-sequence learning to map multi-hop questions to sub-question.
9. With this idea, the model is able to train a large transformer model to generate decompositions and avoid heuristic/extractive decompositions.
10. RERC: Fu et al. [26] also handled multi-hop MRC with a focus on decomposing complicated questions into simpler ones.
11. They proposed a three-stage Relation Extractor-Reader and Comparator (RERC) model.
12. RERC is consist of 1) Relation Extractor to decompose the complex questions into simple sub-questions by automatically extracting the subject and key relations of the complex question, 2) Reader to find the answers to the sub-questions in turn by an advanced ALBERT model, and finally, 3) Comparator to perform the numerical comparison and summarizes all the answers to get the final answer ( Figure 9) The main part of this model is Relation Extractor with two different structures: 1) classification-type (CRERC), where the evidence relation information in the dataset is used as prior knowledge, and the question text is mapped to question relations through the classifier; 2) span-type (SRERC), where the type of question relations is unrestricted, and the Relation Extractor can automatically extract multiple corresponding spans from the question text as question relations.
13. Figure 9: Architecture of the RERC models [26]
14. The Decomposition technique was one of the first ideas for multi-hop MRC and as you can see in recent years [18]0 still has attracted attention.
15. The main disadvantage of this technique is that, instead of focusing on multi-hop reasoning as an important key of multi-hop MRC, it focuses on reducing the problem to a single-hop MRC.
16. Thus, they actually do not go far beyond single-hop models.","1. How do modern algorithms decompose multi-hop questions?
2. How do they break down multi-hop questions into simpler sub-questions?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s11,"Question: What is the role of recurrent reasoning in multi-hop MRC tasks?

1. The sequence models have been first used for single-hop MRC tasks, and most of them are based on Recurrent Neural Networks (RNNs), some studies focus on using them in multi-hop MRC.
2. It can be called state-based reasoning models and they are closer to a standard attention-based RC model with an additional ""state"" representation that is iteratively updated.
3. The changing state representation results in the model focusing on different parts of the passage during each iteration, allowing it to combine information from different parts of the passage [28].
4. Most models, presented in this section, have used advanced neural network concepts, such as attention mechanism and network memory for multi-hop reasoning.
5. In the following the models which use this technique will be reviewed in detail including the architecture alongside the superiority and the motivation of them.",1. What is the role of recurrent reasoning in multi-hop MRC tasks?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s13,"Question: What are the key features and goals of the Commonsense Algorithm, QFE, TAP, and PH-Model in multi-hop QA?

1. Commonsense Model Incorporation to fill the gaps of reasoning between hops of inference using Necessary and Optional Information Cell (NOIC).
2. [29] QFE: Nishida et al. [30] proposed a model for explainable multi-hop QA named Query Focused Extractor (QFE) which is based on a summarization idea.
3. They use the multi-task learning of the QA model for answer selection and QFE for evidence extraction. QFE as the main part of this model adaptively determines the number of evidence sentences by considering the dependency among the evidence sentences and the coverage of the question.
4. Unlike other approaches that extract each evidence sentence separately, QFE uses an RNN and attention mechanism to consider important information in the question and the relationships between sentences.
5. This query-aware recurrent structure enables QFE to consider the dependency among the evidence sentences and cover the important information in the question sentence.
6. In brief, the main goal of QFE is to summarize the context according to the question.
7. Query-focused summarization is the task of summarizing the source document with regard to the given query.
8. The multitask learning with QFE is general in the sense that it can be combined with any QA model.
9. The overview of QFE is shown in Figure 11.
10. is the current summarization vector, is the query vector considering the current summarization, is the extracted sentence, updates the RNN state.
11. Figure 11: Overview of Query Focused Extractor at step t
12. [30] TAP: Bhargav et al. [29]4 proposed a deep neural architecture, called TAP (Translucent Answer Prediction) cover of two main ideas: (1) Local Interaction: Each sentence should be understood in the context of its neighboring sentences and the question, (2)
13. Global Interaction: A global (inter-passage) interaction among sentences must be identified and used for supporting facts.
14. TAP is a hierarchical architecture that tries to capture the local and global interactions between the sentences and consists of two main parts: [29]0  Local and Global Interaction eXtractor [29]1 with three layers: local layer to obtain intra-passage dependencies, Global Layer to obtain inter-passage dependencies, and Supporting Facts Prediction Layer to calculate the probability that a sentence is a supporting fact.
15. Answer Predictor [29]2 to predict the final answer by reasoning over the supporting facts.
16. It consists of four parts: Input Data Shaping to preprocess and concatenate the supporting facts, Context Encoding to encode the context using a pre-trained BERT model, Answer Type Predictor to classify the question into one of the three classes [29]3, and Answer Span Predictor to predict the final answer.
17. Figure 12: Architecture of TAP [29]4 PH-Model: Cong et al. [29]5 focused on using the benefit of the hierarchical structure of the natural language text [29]6, while most existing studies ignore this information in the natural language context.
18. Then they proposed a model for Chinese multi-hop MRC named [29]7, in which P stands for the Passage reranking framework, and H denotes the Hierarchical neural network.
19. As you can see in Figure 13, PH-Model consists of multiple main parts: Bi_ONLSTM, that is an ordered neuron LSTM is used to obtain hierarchical information from a passage [29]8, Bidirectional Attention Flow is used to extract the hierarchical information form paragraphs to get the query-aware context and the context-aware query representation, Fused layer is used to merge all information and finally Pointer network to obtain the probability of the start and end positions of the answer","1. What are the key features of the Commonsense Algorithm, QFE, TAP, and PH-Model in multi-hop QA?
2. What are the goals of the Commonsense Algorithm, QFE, TAP, and PH-Model in multi-hop QA?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s14,"Question: What are path-based models in multi-hop MRC and how do they work?

1. In the following, we review models that focus on finding the right path in information to find the answer.
2. As multi-hop MRC faces more information and complex questions, finding the right path has become more important and difficult, so many models in this technique are path-based.
3. One of the important advantages of path-based models is that they are interpretable because they can provide the evidence chain to the final answer.
4. EPAr: Jiang et al. [33] proposed an interpretable model named Explore-Propose-Assemble reader (EPAr) to mimic the coarseto-fine-grained reasoning behavior of humans when facing multiple long documents.
5. The main idea is to construct a reasoning tree according to the documents like a hierarchical memory network and use the path in this tree to extract the final answer.
6. This model has three components as shown in Figure 14:  The T-hop Document Explorer (DE) module constructs the reasoning tree like a hierarchical memory network.
7. In each step, it selects one document, updates the memory cell using the selected document, and iteratively selects the next related document.","1. What are path-based models in multi-hop MRC?
2. How do path-based models work in multi-hop MRC?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s15,"Question: How does the PathNet model approach multi-hop MRC?

1. The Answer Proposer (AP) module uses the constructed reasoning tree to predict an answer from every root-to-leaf path.
2. The Evidence Assembler (EA) module extracts a key sentence containing the proposed answer from every path and combines them to predict the final answer.
3. Figure 14: Architecture of EPAr [33] PathNet: Kundu et al. [34] proposed a path-based reasoning approach for multi-hop MRC which first extracts all paths in the passages based on implicit relations between entities, and then composes the passage representations along each path to compute a passage-based representation.
4. In other words, the passages representation is achieved by considering the paths.
5. They first find all possible path from passages.
6. It starts with selecting a passage that contains a head entity from the question, and then finds all entities and noun phrases from the same sentence.
7. Afterward, it selects the next passage that contains the potential intermediate entity identified above.
8. Finally, it is checked whether the next passage contains any of the candidate answer choices or not.
9. The resulting will be a set of entity sequences.
10. After obtaining all potential paths, it is time to score each path using the PathNet model based on two perspectives: 1) Context-based Path Scoring, which is based on the interaction with the question encoding, and 2) Passage-based Path Scoring, which is based on the interaction between the passage-based path encoding vector and the candidate encoding.
11. There is an example of the process in Figure 15 which In the Rank-1 path, the model composes the implicit located in relations between (Zoo lake, Johannesburg) and (Johannesburg, Gauteng).
12. However, this method extracts many invalid paths, then causes wasting the computing resources [35].
13. Question: (zoo lake, located in the administrative territorial entity, ?) Answer: gauteng Rank-1 Path: (zoo lake, Johannesburg, gauteng) Passage1: ...
14. Zoo Lake is a popular lake and public park in Johannesburg, South Africa.
15. It is part of the Hermann Eckstein Park and is ... Passage2: ...
16. Johannesburg (also known as Jozi, Joburg and Egoli) is the largest city in South Africa and is one of the 50 largest urban areas in the world.
17. It is the provincial capital of Gauteng, which is ...",1. How does the PathNet model approach multi-hop MRC?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s17,"Question: What is the graph-based technique's role in multihop MRC?

1. Graph-based techniques because of their natural language relationship representation ability [39] has attracted attention in multihop MRC.
2. It is natural to model natural language context into graph structure and the process of multi-hop reasoning as moving among nodes [14].
3. The main idea of the Graph-based technique is to construct a graph based on the context and question, and then the reasoning is performed by message passing over this structure using graph neural networks.
4. The process of constructing the graph from large textual data and reasoning over it are challenging tasks.
5. There are a lot of studies that focus on these challenges which are explained in this subsection in detail.
6. Constructing graphs from input data is one of the basic parts of this technique.
7. Some studies construct an entity graph from the input data, which means the nodes of the graph nodes are the entities of the context and question.
8. A lot of studies work on this kind of graph which will be reviewed in the following.",1. What is the graph-based technique's role in multihop MRC?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s19,"Question: What improvements do MHQA-GRN and DFGN models bring to multi-hop reading comprehension?

1. Song et al. [40] focused on inferring global context as an important key in multi-hop reading comprehension, while previous studies approximate global evidence with local coreference information with DAG-styled GRU.
2. They proposed a model for better connecting global evidence, with a more complex graph compared to DAGs.
3. They construct an entity graph with three types of edges: the edge between the same entity within a passage, the edges between two mentions of different entities within a context window, and coreference-typed edges.
4. The graph might also have cycles which makes it difficult to apply a DAG network to it.
5. (A graph with three types of edges and a DAG graph are shown in Figure 21).
6. For inferring the global context, the related information of the constructed graph has been merged.
7. In this study, two recent graph neural networks have been applied to this graph: graph convolutional network (GCN) and graph recurrent network (GRN) for evidence aggregation.
8. Afterward, an attention mechanism is applied in order to match the hidden states at each graph encoding step with the question representation.
9. Finally, a probability distribution is calculated from the matching results.
10. The architecture of this model is shown in Figure 22.
11. However, this model still only implicitly combines knowledge from all passages, and are therefore unable to provide explicit reasoning paths [28].
12. DFGN: Xiao et al. [41] proposed a model to improve the interaction between the information of documents and the entity graph.
13. They proposed a fusion process of Doc2Graph and Graph2Doc for multi-hop reasoning that leads to a less noisy entity graph and more accurate answers.
14. The process of constructing dynamic entity graph iterates in multiple rounds to achieve multi-hop reasoning.
15. In each round, DFGN generates and reasons on a dynamic graph, where irrelevant entities are masked out while only reasoning sources are preserved, via a mask prediction module.
16. Then the fusion block not only aggregates information from documents to the entity graph (doc2graph) but also propagate the information of the entity graph back to document representations (graph2doc).
17. Figure 23 illustrates the Fusion block in DFGN which consists of:",1. What improvements do MHQA-GRN and DFGN models bring to multi-hop reading comprehension?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s27,"Question: What trends in technique popularity are observed in multi-hop MRC studies from 2018 to 2022?

1. The frequency of each technique among reviewed studies is shown in Figure 44.
2. As you can see, the number of studies of the Graph-based techniques is the most, and after that the Recurrent reasoning-based technique has achieved good attention.
3. But the number of studies cannot be enough to have a proper investigation, and it is needed to show the growth trend of each technique in different years.
4. In this regard, Figure 45 shows the growth trend of each technique from 2018 to 2022.
5. The graph-based technique has achieved the most attention in 2019, 2021, and 2022 that proves that popularity trend of this technique in different years as well.
6. The first graph-free study has been proposed in 2020 and immediately this question was raised that whether the graph was really necessary due to the expensive computational?
7. After that, some other studies followed this question and it can be said that can affect the popularity trend of the graph-based technique in future.
8. However, graph-based technique still can be considered the most popular technique in multi-hop MRC.",1. What trends in technique popularity are observed in multi-hop MRC studies from 2018 to 2022?
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s28,"Question: How do models' performances in multi-hop MRC get evaluated and compared?

1. In this section we will show the performance of the models.
2. This investigation is helpful in several ways; it will determine the stat-of-the-result, and also shows which models and techniques has achieved the best result.
3. On the other hand, it can show the overall performance and effectiveness of each technique in multi-hop MRC To evaluate the results of the models we need to use the evaluation metrics of the datasets.
4. HotpotQA [11] and Wikihop [55], are two populare datasets among the reviewed studies as it has been clear in Figure 46 in which shows the percentage of use of two datasets among the reviewed models from 2018 to 2022.
5. Then they provide a proper situation for evaluating the model's performance.","1. How do models' performances in multi-hop MRC get evaluated?
2. How are models' performances in multi-hop MRC compared?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s29,"Question: How are models evaluated on HotpotQA using EM and F1 metrics?

1. In this section, the performance of models on HotpotQA [11] will be investigated.
2. Exact-match (EM) and F1 metrics have been used in this study to evaluate the model performance.
3. EM is used to show whether the predicated answer and the ground-truth answer are exactly the same.
4. F1 measures the average overlap between the predicted answer and the ground-truth answer.
5. In addition, there are three sets of metrics: 1) Answer EM/F1 to evaluate the answer span prediction, 2) Support EM/F1 to evaluate the supporting facts prediction, and 3) Joint EM/F1 to combine the two previous ones.
6. To have an accurate comparison, the year of publications, the technique, along with the results, for Distracter setting and Fullwiki setting have been presented in this table 1 and 2.
7. In the distractor setting, the best result is for AMGN [39] which is a graph-based model.
8. Although the results are very diverse due to a large number of evaluation metrics, in general, graph-based methods have achieved relatively better results, and these good results were a motivation for great attention to the graph-based technique.
9. But S2G [14] which is a graph-free model in 2021 has achieved a comparable result to the best graph-based model ( Table 1).
10. A few models have published the result for the Fullwiki setting due to the difficulty of this setting, and among them, HGN [48], which is a graph-based model, has achieved the best result (Table 2).","1. How are models evaluated on HotpotQA?
2. What are the EM and F1 metrics used in the evaluation process?"
254408864,A Comprehensive Survey on Multi-hop Machine Reading Comprehension Approaches,"Linguistics, Computer Science",s30,"Question: What is the WikiHop dataset and how is its performance evaluated?

1. The papers that have used the WikiHop [55] dataset is investigated in this section.
2. WikiHop consists of 51k questions, answers, and context where each context consists of several documents from Wikipedia .Each question in WikiHop is a tuple, which denotes two entities, and their relationship, then the answers in the WikiHop dataset are a single entity.
3. Accuracy is a popular and fairly common metric to evaluate the performance of multiple-choice and Cloze-style MRC tasks.
4. In the multiple-choice task, it is required to check whether the correct answer has been selected from the candidate answers.
5. In contrast, in the Cloze-style task, it is required to check whether the correct words have been selected for the missing words Since the answer type of this model is multiple-choice then accuracy is the evaluation metric on this dataset which is obtained for both the test and development set.
6. For each paper, the year of publication, the technique along with the results are shown in Table 3.
7. The best result is for ChainEX [38] which has used the recurrent reasoning technique.
8. Besides that, the graph-based models could achieve the good result in this dataset too.","1. What is the WikiHop dataset?
2. How is the performance of the WikiHop dataset evaluated?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",s2,"Question: How does multi-task learning utilize supervision at different feature levels for NLP tasks?

1. Feature Levels. Models using the parallel architecture handle multiple tasks in parallel.
2. These tasks may concern features at different abstraction levels.
3. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level.
4. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c.
5. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers.
6. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features.
7. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified.
8. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.
9. In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful.
10. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154].
11. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task.
12. [28,57]0 adds attention-level supervision to improve consistency of the two primary language generation tasks.
13. [28,57]1 minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.",1. How does multi-task learning utilize supervision at different feature levels for NLP tasks?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",s3,"Question: What is hierarchical architecture and how does it function in task processing?

1. The hierarchical architecture considers hierarchical relationships among multiple tasks.
2. The features and output of one task can be used by another task as an extra input or additional control signals.
3. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures.
4. Fig. 2 illustrates different hierarchical architectures.
5. 2.2.1 Hierarchical Feature Fusion.
6. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features.
7. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism.
8. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task.
9. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum.
10. [124] fuses topic features of different roles into the main model via a gating mechanism.
11. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.","1. What is hierarchical architecture?
2. How does hierarchical architecture function in task processing?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",s5,"Question: What is hierarchical interactive multi-task learning and how does it work?

1. Interactive Architecture. Different from most machine learning models that give predictions in a single pass, hierarchical interactive MTL explicitly models the interactions between tasks via a multi-turn prediction mechanism which allows a model to refine its predictions over multiple steps with the help of the previous outputs from other tasks in a way similar to recurrent neural networks.
2. [44] maintains a shared latent representation which is updated by iterations.
3. Multi-step attention network [51] refines its prediction by attending to input representations in previous steps.
4. In cyclic MTL [146], the output of one task is used as an extra input to its successive lower-level task and the output of the last task is fed to the first one, forming a loop.
5. Most hierarchical interactive MTL models as introduced above report that performance converges quickly at = 2 steps, showing the benefit and efficiency of doing multi-step prediction.","1. What is hierarchical interactive multi-task learning?
2. How does hierarchical interactive multi-task learning work?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",s7,"Question: How do Generative Adversarial Networks enhance computer vision and NLP tasks?

1. Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision.
2. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones.
3. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images.
4. A similar idea can be used in MTL for NLP tasks.
5. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model.
6. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.
7. An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized.
8. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents.
9. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach.
10. First, a discriminator that rates the quality of candidate answers is trained on labeled samples.
11. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.","1. How do Generative Adversarial Networks enhance computer vision tasks?
2. How do Generative Adversarial Networks enhance NLP tasks?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",s10,"Question: How do data sampling techniques address imbalanced data in multi-task learning models?

1. Machine learning models often suffer from imbalanced data distributions.
2. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved.
3. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets.
4. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.
5. A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ∝ |D |.
6. (2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting.
7. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e., As a generalization of proportional sampling in Eq. (2), for task can take the following form as where 1 is called the sampling temperature [128].
8. Similar to task loss weights, researchers have proposed various techniques to adjust .
9. When < 1, the divergence of sampling probabilities is reduced.
10. can be viewed as a hyperparameter to be set by users or be changed dynamically during training.
11. For example, the annealed sampling method [113] adjusts as training proceeds.
12. Given a total number of epochs, at epoch is set to In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference.
13. [128] defines as where 0 and denote initial and maximum values of , respectively.
14. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.","1. How do data sampling techniques address imbalanced data?
2. How do data sampling techniques work in multi-task learning models?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",s13,"Question: What is Auxiliary MTL and how is it applied in NLP?

1. Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning.
2. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks.
3. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.
4. Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data.
5. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging.
6. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction.
7. [50] trains an extractive summarization model together with an auxiliary document-level classification task.
8. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture.
9. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model.
10. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features.
11. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task.
12. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation.
13. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1.
14. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved.
15. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively.
16. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks.
17. and rebuttal pairs of scientific papers.
18. [3]0 makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task.
19. To handle the primary disfluency detection task, [3]1 pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.","1. What is Auxiliary MTL?
2. How is Auxiliary MTL applied in NLP?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",s16,"Question: How does multi-lingual MTL benefit NLP models and facilitate cross-lingual knowledge transfer?

1. Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections.
2. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models.
3. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English.
4. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively.
5. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.
6. Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages.
7. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese.
8. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process.
9. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts.
10. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English.
11. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.
12. Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations.
13. [108] learns multi-lingual representations from two tasks.
14. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples.
15. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification.
16. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training.
17. In [78]0, a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels.
18. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way.
19. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.","1. How does multi-lingual MTL benefit NLP models?
2. How does multi-lingual MTL facilitate cross-lingual knowledge transfer?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",s17,"Question: How does Multimodal MTL enhance NLP tasks with cross-modal features?

1. As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community.
2. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks.
3. MTL is a natural choice for implicitly injecting multimodal features into a single model.
4. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language.
5. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.
6. [89] builds a video captioning model with two auxiliary tasks.
7. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability.
8. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism.
9. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.
10. Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding.
11. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance.
12. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.
13. In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment.
14. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels.
15. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning.
16. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.","1. How does Multimodal MTL enhance NLP tasks?
2. How does it utilize cross-modal features?"
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",s18,"Question: What factors influence the performance of multi-task learning in NLP?

1. A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training.
2. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks.
3. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy.
4. This finding also holds for rumor verification [53].
5. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other.
6. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels.
7. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets.
8. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.
9. As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness.
10. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks.
11. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted.
12. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar.
13. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks.
14. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.",1. What factors influence the performance of multi-task learning in NLP?
237571793,Multi-Task Learning in Natural Language Processing: An Overview,"Linguistics, Computer Science",s21,"Question: How are multi-label datasets created and utilized in various research tasks?

1. Instances in multi-label datasets have one label space for each task, i.e. ∀ ≠ , X = X = X , which makes it possible to optimize all task-specific components at the same time.
2. In this case, Multi-label datasets can be created by giving extra manual annotations to existing data.
3. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input.
4. [121] labels Twitter posts with 4 demographic labels.
5. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.
6. The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset.
7. Extra labels can be obtained using pre-defined rules [62,97].
8. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels.
9. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation.
10. [116] uses hashtags to represent genres of tweet posts.
11. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database.
12. Unicoder [49] uses translated text from the source language to fine-tune on the target language.
13. [121]0 creates disfluent sentences by randomly repeating or inserting -grams.
14. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models.
15. [121]1 obtains dominant word sense labels from WordNet [121]2.
16. [121]3 applies entity linking for QA data over databases through an entity linker.
17. [121]4 assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method.
18. [121]5 uses the output of a meta-network as labels for unsupervised training data.
19. As a special case of multi-label dataset, mask orchestration [121]6 provides different parts of an instance to different tasks by applying different masks.
20. That is, labels for one task may become the input for another task and vice versa.","What advancements did Transformers bring to NLP since 2017, and how does BERT, a Transformer-based model, contribute to understanding and improving natural language processing tasks?
1. What advancements did Transformers bring to NLP since 2017?
2. How does BERT, a Transformer-based model, contribute to understanding and improving natural language processing tasks?"
231603122,Persuasive Natural Language Generation -A Literature Review,"Linguistics, Computer Science, Business",s4,"Question: What are the results of applying vom Brocke et al.'s framework to persuasive NLG research?

1. The following paragraph shows the results of previous research (literature analysis and synthesis) and therefore is the next step (4) of the vom
2. Brocke et al. (2009) literature review framework.
3. Here, we focus on the four identified categories of persuasive natural language generation that underlie the business framework introduced in step (2) conceptualization.
4. We ordered the identified categories alphabetically, hence, we do not imply a differentiation in degrees of persuasiveness.
5. Additionally, we provide relevant tools and datasets required for implementation of a persuasive NLG.",1. What are the results of applying vom Brocke et al.'s framework to persuasive NLG research?
231603122,Persuasive Natural Language Generation -A Literature Review,"Linguistics, Computer Science, Business",s7,"Question: What is the role of analogy and causal cohesion in framing issues?

1. Analogy Reframes issues through the usage of analogy or metaphor.
2. In the bible Moses saved all animals.
3. Why don't you save those people?
4. Walton et al. 2008, Olguin et al. 2017
5. Causal Cohesion Related to causal relationships of actions and events that help to form relations between sentence clauses.
6. The ratio of causal verbs (e.g., break) to particles (e.g., because, due to).","1. What is the role of analogy in framing issues?
2. What is the role of causal cohesion in framing issues?"
231603122,Persuasive Natural Language Generation -A Literature Review,"Linguistics, Computer Science, Business",s8,"Question: How do connectives and consistency contribute to cohesion and persuasion in text?

1. Connectives Create explicit between clauses and sentences, and thus create cohesion between ideas.
2. E.g., 'moreover' or 'on the other hand'.
3. Longo 1994, Graesser et al. 2011
4. Consistency When references to previous commitments are made in order to persuade.
5. As I did this, you'll do that.","1. How do connectives contribute to cohesion and persuasion in text?
2. How does consistency contribute to cohesion and persuasion in text?"
231603122,Persuasive Natural Language Generation -A Literature Review,"Linguistics, Computer Science, Business",s9,"Question: What are strategies for establishing trade-off ranges in negotiations?

1. Establishing Ranges Referencing to similar deals to establish the best possible trade-off range.
2. In the other deal, they agreed to pay only 5k but got a small car.
3. Does that work for you? Williams, 1983, Hyder et al. 2000 Favors/Debts
4. When persuader implies that persuadee is indebted to him or her, e.g., coming from previous solicited or unsolicited favors.
5. or decrease of trustworthiness (column one, sorted alphabetically).
6. The following columns provide a synopsis (column two), a corresponding example (column three) and the source in which the determinant was identified (column four).",1. What are strategies for establishing trade-off ranges in negotiations?
231603122,Persuasive Natural Language Generation -A Literature Review,"Linguistics, Computer Science, Business",s10,"Question: What tools and datasets are used in NLP for analyzing persuasion techniques?

1. In the analyzed academic studies, we found that the authors use different datasets and tools to computationally process data for technical analyses of persuasion in NLP or NLG (e.g., in Guerini et al 2008a/b, Li et al. 2020, Iyer/Sycara 2019. Logically, the implementation of a persuasive NLG AI also depends on a variety of relevant tools and datasets which we identified and consolidated in Table 6. This table classifies our findings in types which are either tool or datasets (column one).
2. We identified six tools and seventeen persuasion or message datasets.
3. A software tool that is used in the context of persuasion and NLP, and datasets were chosen if they were used in the context of persuasion, textual exchange/debate and NLP.
4. We further added a synopsis (column three) explaining every tool and Authority Appealing or making reference to higher authority or experts to persuade.
5. We called your mom Mariam and she says please put the gun down and come outside.
6. Cialdini & Goldstein 2002, Catellani et al. 2020
7. Seeking Comprehension Instead of prioritizing own arguments, it is wise to focus on understanding the persuadee.
8. What do you mean by that ? Fisher Uri 1981, Kouzehgar et al. 2015 Construal Learning involves the generalization and abstraction from one's repeated experiences which is a high-construal mental process.
9. A short-term investor as opposed to long-term investor may rely more on a financial artificial intelligence.
10. Kim & Duhachek 2020, Abdallah et al. 2009
11. Emotionality The elicitation of positive or negative emotions to impose more weight on words.
12. Inclusion of words or expressions such as ""amazing"" or ""excellent"".
13. Rocklage et al. 2018 Empathy This is a very good positive book that will make you very happy.
14. Guerini et al. 2008b, Zarouali et al. 2020 dataset, providing a link (column four, if applicable) and the respective citation of the tool or dataset (column five).
15. The tools and datasets are sorted alphabetically.","1. What tools are used in NLP for analyzing persuasion techniques?
2. What datasets are used in NLP for analyzing persuasion techniques?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s3,"Question: What is Multimodal Learning and its significance in modern applications?

1. MML [1], [60], [61] has been an important research area in recent decades; an early multimodal application -audiovisual speech recognition was studied in 1980s [62].
2. MML is key to human societies. The world we humans live in is a multimodal environment, thus both our observations and behaviours are multimodal [63].
3. For instance, an AI navigation robot needs multimodal sensors to perceive the real-world environment [64], [65], [66], e.g., camera, LiDAR, radar, ultrasonic, GNSS, HD Map, odometer.
4. Furthermore, human behaviours, emotions, events, actions, and humour are multimodal, thus various human-centred MML tasks are widely studied, including multimodal emotion recognition [67], multimodal event representation [68], understanding multimodal humor [60]0, face-body-voice based video person-clustering [60]1, etc. Thanks to the development of the internet and a wide variety of intelligent devices in recent years, increasing amounts of multimodal data are being transmitted over the internet, thus an increasing number of multimodal application scenarios are emerging.
5. In modern life, we can see various multimodal applications, including commercial services [60]2 [60]3, [60]4, [60]5, [60]6, [60]7), communication [60]8, human-computer interaction [60]9, healthcare AI [61]0, [61]1, surveillance AI [61]2, etc. Moreover, in the era of Deep Learning, deep neural networks greatly promote the development of MML, and Transformers [61]3 are a highly competitive architecture family, bringing new challenges and opportunities to MML.
6. In particular, the recent success of large language models and their multimodal derivatives [61]4, [61]5, [61]6, [61]7, [61]8 further demonstrates the potential of Transformers in multimodal foundation models.","1. What is Multimodal Learning?
2. What is the significance of Multimodal Learning in modern applications?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s4,"Question: What are the key developments and applications of Transformer models in AI?

1. Transformers are emerging as promising learners.
2. Vanilla Transformer [2] benefits from a self-attention mechanism, and is a breakthrough model for sequence-specific representation learning that was originally proposed for NLP, achieving the state-of-the-art on various NLP tasks.
3. Following the great success of Vanilla Transformer, a lot of derivative models have been proposed, e.g., BERT [4], BART [87], GPT [88], Longformer [43], Transformer-XL [89], XLNet [90].
4. Transformers currently stand at the dominant position in NLP domains, and this motivates researchers try to apply Transformers to other modalities, such as visual domains.
5. In early attempts for visual domain, the general pipeline is ""CNN features + standard Transformer encoder"", and researchers achieved BERT-style pretraining, via preprocessing raw images by resizing to a low resolution and reshaping into a 1D sequence [91]. Vision Transformer (ViT) [5] is a seminal work that contributes an end-to-end solution by applying the encoder of Transformer to images.
6. Both ViT and its variants have been widely applied to various computer vision tasks, including low-level tasks [4]0, recognition [4]4, detection [4]2, segmentation [4]3, etc, and also work well for both supervised [4]4 and self-supervised [4]5, [4]6, [4]7 visual learning.
7. Moreover, some recently-released works provide further theoretical understanding for ViT, e.g., its internal representation robustness [4]8, the continuous behaviour of its latent representation propagation [4]9, [87]0.
8. Motivated by the great success of Transformer, VideoBERT [87]1 is a breakthrough work that is the first work to extend Transformer to the multimodal tasks.
9. VideoBERT demonstrates the great potential of Transformer in multimodal context.
10. Following VideoBERT, a lot of Transformer based multimodal pretraining models [87]2 have become research topics of increasing interest in the field of machine learning.
11. In 2021, CLIP [87]3 was proposed.
12. It is a new milestone that uses multimodal pretraining to convert classification as a retrieval task that enables the pretrained models to tackle zero-shot recognition.
13. Thus, CLIP is a successful practice that makes full use of large-scale multimodal pretraining to enable zero-shot learning.
14. Recently, the idea of CLIP is further studied, e.g., CLIP pretrained model based zero-shot semantic segmentation [87]4, ALIGN [87]5, CLIP-TD [87]6, ALBEF [87]7, and CoCa [87]8.","1. What are the key developments of Transformer models in AI?
2. What are the applications of Transformer models in AI?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s5,"Question: What are the recent trends in multimodal big data development?

1. In the past decade, with the rapid development of internet applications such as social media and online retail, massive multimodal datasets have been proposed, e.g., Conceptual Captions [123], COCO [124], VQA [125], Visual Genome [126], SBU Captions [127], Cooking312K [7], LAIT [115], e-SNLI-VE [128], ARCH [129], Adversarial VQA [130], OTT-QA [124]0, MULTIMODALQA [124]1 [124]2, VALUE [124]3, Fashion IQ [124]4, LRS2-BBC [124]5, ActivityNet [124]6, VisDial [124]7.
2. Some emergent new trends among the recently released multimodal datasets are:
3. [124]8 Data scales are larger. Various recently released datasets are million-scale, e.g., Product1M [127]0, Conceptual 12M [125]0, RUC-CAS-WenLan [125]1 [125]2, HowToVQA69M [125]3, HowTo100M [125]4, ALT200M [125]5, LAION-400M [125]6.
4. [125]7 More modalities. In addition to the general modalities of vision, text, and audio, further diverse modalities are emerging, e.g., Pano-AVQA [125]8 -the first large-scale spatial and audio-visual question answering dataset on 360  videos, YouTube-360 [125]9 [126]0 [126]1, AISTetc.0 [126]2 [126]3, Artemis [126]4 [126]5.
5. In particular, MultiBench [126]6 provides a dataset including 10 modalities.
6. [126]7 More scenarios. In addition to common caption and QA datasets, more applications and scenarios have been studied, e.g., CIRR [126]8 [126]9, Product1M [127]0, Bed and Breakfast [127]1 [127]2 [127]3, M3A [127]4 [127]5, X-World [127]6 [127]7.
7. [127]8 Tasks are more difficult.
8. Beyond the straightforward tasks, more abstract multimodal tasks are proposed, e.g., MultiMET [127]9 (a multimodal dataset for metaphor understanding), Hateful Memes [154] (hate speech in multimodal memes).
9. (5) Instructional videos have become increasingly popular, e.g., cooking video YouCookII [155].
10. Aligning a sequence of instructions to a video of someone carrying out a task is an example of a powerful pretraining pretext task [7], [156].
11. Pretext tasks are pre-designed problems to force the models to learn representation by solving them.
12. Similar to other deep neural network architectures, Transformers are also data hungry.
13. Therefore, their highcapacity models and multimodal big data basis co-create the prosperity of the Transformer based multimodal machine learning.
14. For instance, big data bring zero-shot learning capability to VLP Transformer models.",1. What are the recent trends in multimodal big data development?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s7,"Question: What is the structure and key components of a Vanilla Transformer?

1. Vanilla Transformer has an encoder-decoder structure and is the origin of the Transformer-based research field.
2. It takes tokenized input (see Section 3.1.1).
3. Both its encoder and decoder are stacked by the Transformer layers/blocks, as demonstrated in Figure 1.
4. Each block has two sub-layers, i.e., a multi-head self-attention (MHSA) layer (see Section 3.1.2) and a position-wise fully-connected feed-forward network (FFN) (see Section 3.1.3).
5. To help the back propagation of the gradient, both MHSA and FFN use Residual Connection [159] (given an input x, the residual connection of any mapping f (·) is defined as x ← f (x) + x), followed by normalization layer.
6. Thus, assuming that the input tensor is Z, the output of MHSA and FFN sub-layers can be formulated as: where sublayer(MHSA)2 is the mapping implemented by the sublayer itself and N (MHSA)2 denotes normalization, e.g., BN (MHSA)2 (MHSA)1, LN (MHSA)2 (MHSA)3.
7. Discussion There is an important unsolved problem that is post-normalization versus pre-normalization.
8. The original Vanilla Transformer uses post-normalization for each MHSA and FFN sub-layer.
9. However, if we consider this from the mathematical perspective, pre-normalization makes more sense (MHSA)4.
10. This is similar to the basic principle of the theory of matrix, that normalization should be performed before projection, e.g., Gram-Schmidt process 2 .
11. This problem should be studied further by both theoretical research and experimental validation.","1. What is the structure of a Vanilla Transformer?
2. What are the key components of a Vanilla Transformer?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s8,"Question: How do Transformers handle input tokenization and position embedding?

1. Tokenization Vanilla Transformer was originally proposed for machine translation as a sequence-to-sequence model, thus it is straightforward to take the vocabulary sequences as input.
2. As mentioned previously, the original selfattention can model an arbitrary input as a fully-connected graph, independently of modalities.
3. Specifically, both Vanilla and variant Transformers take in the tokenized sequences, where each token can be regarded as a node of the graph.
4. 1. In this survey, ""multimodal Transformer"" means ""Transformer in multimodal learning context"".
5. 2. https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt process Special/Customized Tokens In Transformers, various special/customized tokens can be semantically defined as place-holders in the token sequences, e.g., mask token [MASK] [4].
6. Some common special tokens are summarized in appendix.
7. Special tokens can be used in both uni-modal and multimodal Transformers.
8. Position Embedding Position embeddings are added to the token embeddings to retain positional information [4].
9. Vanilla Transformer uses sine and cosine functions to produce position embedding.
10. To date, various implementations of position embedding have been proposed.
11. The concrete solutions are outside the focus of this survey.","1. How do Transformers handle input tokenization?
2. How do Transformers handle position embedding?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s9,"Question: What are the main advantages of input tokenization in processing multimodal inputs?

1. The main advantages of input tokenization include the following: (1) Tokenization is a more general approach from a geometrically topological perspective, achieved by minimizing constraints caused by different modalities.
2. In general, every modality has intrinsic constraints on modelling.
3. For instance, sentences have sequential structures that are well-suited by RNN, and photos are restricted in aligned grid matrices that CNN works well for.
4. Tokenization helps Transformers inherently to process different modalities universally via irregular sparse structures.
5. Thus even Vanilla Transformer can encode multimodal inputs flexibly by just concatenation, weighted summation, even without any multimodal tailor-made modifications. (2)
6. Tokenization is a more flexible approach to organize the input information via concatenation/stack, weighted summation, etc. Vanilla Transformer injects temporal information to the token embedding by summing position embedding.
7. For instance, when use Transformer to model freehand sketch drawing [163], each input token can integrate various drawing stroke patterns, e.g., stroke coordinates, stroke ordering, pen state (start/end).
8. (3) Tokenization is compatible with the task-specific customized tokens, e.g., [MASK] token [4] for Masked Language Modelling, [CLASS] token [5] for classification.",1. What are the main advantages of input tokenization in processing multimodal inputs?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s10,"Question: What is the role of position embedding in Transformers?

1. Transformers is an open problem.
2. It can be understood as a kind of implicit coordinate basis of feature space, to provide temporal or spatial information to the Transformer.
3. For cloud point [164] and sketch drawing stroke [163], their token element is already a coordinate, meaning that position embedding is optional, not necessary.
4. Furthermore, position embedding can be regarded as a kind of general additional information.
5. In other words, from a mathematical point of view, any additional information can be added, such as detail of the manner of position embedding, e.g., the pen state of sketch drawing stroke [163], cameras and viewpoints in surveillance [165].
6. There is a comprehensive survey [166] discussing the position information in Transformers.
7. For both sentence structures (sequential) and general graph structures (sparse, arbitrary, and irregular), position embeddings help Transformers to learn or encode the underlying structures.
8. Considered from the mathematical perspective of self-attention, i.e., scaled dot-product attention, attentions are invariant to the positions of words (in text) or nodes (in graphs), if position embedding information is missing.
9. Thus, in most cases, position embedding is necessary for Transformers.",1. What is the role of position embedding in Transformers?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s12,"Question: How does self-attention work in Transformer models?

1. After preprocessing, embedding Z will go through three projection matrices (W Q ∈ R d×dq , W K ∈ R d×d k , and W V ∈ R d×dv , d q = d k ) to generate three embeddings Q (Query), K (Key), and V (Value):
2. The output of self-attention is defined as Given an input sequence, self-attention allows each element to attend to all the other elements, so that self-attention encodes the input as a fully-connected graph.
3. Therefore, the encoder of Vanilla Transformer can be regarded as a fullyconnected GNN encoder, and the Transformer family has the non-local ability of global perception, similar to the Non-Local Network [167].
4. Masked Self-Attention (MSA) In practice, modification of self-attention is needed to help the decoder of Transformer to learn contextual dependence, to prevent positions from attending to subsequent positions, as where M is a masking matrix.
5. For instance, in GPT [88], an upper triangular mask to enable look-ahead attention where each token can only look at the past tokens.
6. Masking can be used in both encoder (Query)2, (Query)0 and decoder of Transformer, and has flexible implementations, e.g., 0-1 hard mask (Query)2, soft mask (Query)0.
7. In both uni-modal and multimodal practices, specific masks are designed based on domain knowledge and prior knowledge.
8. Essentially, MSA is used to inject additional knowledge to Transformer models, e.g., (Query)1, (Query)2, (Query)3, (Query)4.",1. How does self-attention work in Transformer models?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s18,"Question: What are the key steps in preparing data for Transformers in multimodal inputs?

1. Given an input from an arbitrary modality, users only need to perform two main steps, (1) tokenize the input, and (2) select an embedding space to represent the tokens, before inputting the data into Transformers.
2. In practice, both the tokenizing input and selecting embedding for the token are vital for Transformers but highly flexible, with many alternatives.
3. For instance, given an image, the solution of tokenizing and embedding is not unique.
4. Users can choose or design tokenization at multiple granularity levels -coarse-grained vs. fine-grained.
5. e.g., use ROIs (obtained by an object detector) and CNN features as tokens and token embeddings [102], use patches and linear projection as tokens and token embeddings [5], or use graph node (obtained by object detector and graph generator) and GNN features as tokens and token embeddings [181].
6. Given a tokenization plan, the subsequent embedding approaches can be diverse.
7. For example, for video input, a common tokenization is to treat the non-overlapping windows (down-sampled) over the video as tokens, and their embeddings can then be extracted by various 3D CNNs, e.g., VideoBERT [7], CBT [107], and UniVL (2)0 use S3D (2)1, ActBERT uses ResNet-3D (2)2.
8. Table 1 summarizes some common practices of multimodal inputs for Transformers, including RGB, video, audio/speech/music, text, graph, etc. Discussion When considered from the perspective of geometric topology, each of the modalities listed in Table 1 can be regarded as a graph.
9. An RGB image is essentially a neat grid graph in the pixel space.
10. Both video and audio are clip/segment based graphs over a complex space involving temporal and semantic patterns.
11. Both 2D and 3D drawing sketches (2)3, (2)4 are a kind of sparse graph if we consider their key points along the drawing strokes.
12. Similar to sketches, the human pose also is a kind of graph.
13. 3D point cloud is a graph in which each coordinate is a node.
14. Other abstract modalities also can be interpreted as graphs, e.g., source code (2)6, data flow of source code (2)6, table (2)7, SQL database schema (2)8, text question graph (2)9, and electronic health records (obtained by an object detector)0 (obtained by an object detector)1.",1. What are the key steps in preparing data for Transformers in multimodal inputs?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s19,"Question: What is token embedding fusion in Transformers and its applications?

1. In practice, Transformers allow each token position to contain multiple embeddings.
2. This is essentially a kind of early-fusion of embeddings, for both uni-modal and multimodal Transformer models.
3. (This will be discussed further in subsequent sections.)
4. The most common fusion is the token-wise summing of the multiple embeddings, e.g., a specific token embedding ⊕ position embedding.
5. Similar to the flexible tokenization, token embedding fusion is also flexible and widely applied to both uni-modal and multimodal Transformer applications.
6. In [81], token-wise weighted summing is used to perform early-fusion of RGB and grey-scale images for multimodal surveillance AI.
7. In particular, token embedding fusion has an important role in multimodal Transformer applications as various embeddings can be fused by tokenwise operators, e.g., in VisualBERT [104] and Unicoder-VL [108], segment embeddings are token-wise added to indicate which modality (vision or language) each token is from, VL-BERT [105] injects global visual context to linguistic domain by ""linguistic token embedding ⊕ full image visual feature embedding"", InterBERT [188] adds location information for ROI by ""ROI embedding ⊕ location embedding"", in Im-ageBERT [115], five kinds of embeddings are fused ""image embedding ⊕ position embedding ⊕ linguistic embedding ⊕ segment embedding ⊕ sequence position embedding"".","1. What is token embedding fusion in Transformers?
2. What are the applications of token embedding fusion in Transformers?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s20,"Question: How do self-attention variants facilitate multimodal interactions in Transformers?

1. In multimodal Transformers, cross-modal interactions (e.g., fusion, alignment) are essentially processed by self-attention and its variants.
2. Thus, in this section, we will review the main multimodal modelling practices of Transformers, from a perspective of self-attention designs, including (1) early summation (token-wise, weighted), (2) early concatenation, (3) hierarchical attention (multi-stream to one-stream), (4) hierarchical attention (one-stream to multi-stream), (5) TABLE 2 Self-attention variants for multi-modal interaction/fusion.
3. α and β denote weightings. ""Att."": Attention; ""Concat.""/""Con.
4. "": Concatenation; ""Tfs"": Transformer layers.
5. N (A) and N (1)0 denote the token sequence lengths of two modalities.",1. How do self-attention variants facilitate multimodal interactions in Transformers?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s23,"Question: How does cross-attention to concatenation enhance multimodal interaction modeling?

1. The two streams of cross-attention [102] can be further concatenated and processed by another Transformer to model the global context.
2. This kind of hierarchically cross-modal interaction is also widely studied [137], [189], and alleviates the drawback of cross-attention.
3. Discussion All these aforementioned self-attention variants for multimodal interactions are modality-generic, and can be applied in flexible strategies and for multi-granular tasks.
4. Specifically, these interactions can be flexibly combined and nested.
5. For instance, multiple cross-attention streams are used in hierarchical attention (one-stream to multi-stream) that in a two-stream decoupled model
6. [191] T f 2 and T f 3 of Eq. 11 are implemented by cross-attention defined in Eq. 12.
7. Moreover, they can be extended to multiple (≥ 3) modalities.
8. TriBERT [183] is a tri-modal cross-attention (coattention) for vision, pose, and audio, where given a Query embedding, its Key and Value embeddings are the concatenation from the other modalities.
9. Cross-attention to concatenation is applied to three modalities (i.e., language, video, and audio) in [189].","What advancements did Transformers bring to NLP since 2017, and how does BERT, a Transformer-based model, contribute to understanding and improving natural language processing tasks?
1. What advancements did Transformers bring to NLP since 2017?
2. How does BERT, a Transformer-based model, contribute to understanding and improving natural language processing tasks?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s24,"Question: How do multimodal Transformers' network architectures vary based on their internal attentions?

1. Essentially, various multimodal Transformers work due to their internal multimodal attentions that are the aforementioned self-attention variants.
2. Meanwhile, as illustrated in Figure 2, these attentions determine the external network structures of the multimodal Transformers where they are embedded.
3. In general, if we consider from the angle of network structures, (1) early summation and early concatenation work in single-stream, (2) cross-attention work in multistreams, (3) hierarchical attention and cross-attention to concatenation work in hybrid-streams.
4. Thus, multimodal Transformers can be divided into single-stream (e.g., Uniter [106], Visualbert [104], Vl-bert [105] , Unified VLP [110]), multi-stream (e.g., ViLBERT [102], Lxmert [103], ActBERT [114]), hybrid-stream (e.g., InterBERT [188]), etc. From the perspective of timing of interaction, these multimodal attentions fall into three categories, i.e., early interaction: early summation, early concatenation, and hierarchical attention (one-stream to multi-stream), late interaction: hierarchical attention (multi-stream to one-stream), or throughout interaction: cross-attention, cross-attention to concatenation.
5. As demonstrated in Figure 2 in [192], the multimodal Transformer models have another architecture taxonomy based on the computational size of the components.","What advancements did Transformers bring to NLP since 2017, and how does BERT, a Transformer-based model, contribute to understanding and improving natural language processing tasks?
1. What advancements did Transformers bring to NLP since 2017?
2. How does BERT, a Transformer-based model, contribute to understanding and improving natural language processing tasks?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s26,"Question: What are the key trends and benefits of Transformer-based multimodal pretraining?

1. Inspired by the great success of Transformer based pretraining in NLP community, Transformers are also widely studied for multimodal pretraining as the various large-scale multimodal corpora is emerging.
2. Recent work has demonstrated that if pretrained on large scale multimodal corpora Transformer based models [7], [102], [103], [104], [105], [106], [110] clearly outperform other competitors in a wide range of multimodal down-stream tasks, and moreover achieve the zero-shot generalization ability.
3. These superiorities have led Transformer-based multimodal pretraining to become a hot topic, which has two main directions, i.e., general pretraining for agnostic down-stream tasks (Section 4.1.1), goal-oriented pretraining for specific down-stream tasks (Section 4.1.2).
4. We focus on these key points: (1)
5. What trends are emerging? [102]0 Where/how do the cross-modal interactions take place during pretraining?
6. [102]1 How to sort out and understand the pretraining pretext objectives?
7. How can they drive Transformers to learn the cross-modal interactions?","1. What are the key trends of Transformer-based multimodal pretraining?
2. What are the benefits of Transformer-based multimodal pretraining?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s29,"Question: What are pretext tasks in Transformer based multimodal pretraining?

1. In Transformer based multimodal pretraining, the pretraining tasks/objectives are also termed pretext tasks/objectives.
2. To date, various pretext tasks have been studied, e.g., masked language modelling (MLM) [137], masked image region prediction/classification (also termed masked object classification (MOC)) [137], (also termed masked object classification (MOC)4, masked region regression (MRR) [115], visual-linguistic matching (VLM) (e.g., image-text matching (ITM) [188], image text matching (ITM), phrase-region alignment (PRA) [204], word-region alignment (WRA) [106], video-subtitle matching (VSM) [116]), masked frame modelling (MFM) [116], frame order modelling (FOM) [116], next sentence prediction (also termed masked object classification (MOC)1 (also termed masked object classification (MOC)2, [102], (also termed masked object classification (MOC)4, masked sentence generation (also termed masked object classification (MOC)5 (also termed masked object classification (MOC)6, masked group modelling (also termed masked object classification (MOC)7 [188], prefix language modelling (also termed masked object classification (MOC)9 [199], video conditioned masked language model [117], text conditioned masked frame model [117], visual translation language modelling (VTLM) [206], and image-conditioned masked language modelling (also termed image-attended masked language modelling) [207].
3. These down-stream task -agnostic pretext pretraining is optional, and the down-stream task objectives can be trained directly, which will be discussed in Section 4.1.2.
4. Table 3 provides the common and representative pretext tasks for Transformer based multimodal pretraining.
5. In practice, pretext tasks can be combined, and some representative cases are summarized in Table 3 of [57], Table 2 of [58].
6. The pretext tasks have multiple taxonomies: (VLM)0 Supervision.
7. The common multimodal pretraining Transformers use well-aligned, weakly-aligned, and even unaligned multimodal sample pairs/tuples, to work in supervised, weakly-supervised, and unsupervised manners, respectively.
8. Meanwhile, if we consider the definitions of their pretext tasks/objectives from supervision, the pretexts can be sorted into unsupervised/self-supervised (e.g., masked language modelling (MLM) [7], [137]) and supervised (e.g., image-text matching (ITM) [188] [102], [103], [104], [106], [209]), etc. Nowadays, self-supervised attempts are the majority.
9. (VLM)1 Modality. Considering the mathematical formulations, some pretexts are defined on single modality, e.g., masked language modelling [7], masked acoustic modelling [200], masked region regression (MRR) [115], while other pretexts are defined on multiple modalities, e.g., imageconditioned masked language modelling (IMLM) [208], image-text matching (ITM) [188], video-subtitle matching (VSM) [116].
10. Thus, from this mathematical view, the pretext tasks can be divided into two categories, i.e., uni-modal and multimodal.
11. However, this classification is not really accurate.
12. It should be highlighted that in multimodal pretraining Transformer models, even if the pretext objective formulations only include uni-modal elements, pretexts can still involve other modalities, essentially conditioned on the clues from other modalities, by (a) prepositive token level interactions and/or Transformer level interactions, (b) co-training with other pretexts that involve other modalities.
13. For instance, VL-BERT [105] uses two dual pretext tasks, i.e., masked language modelling and masked RoI classification.
14. (VLM)2 Motivation. If consider their motivations, the pretext tasks include masking, describing, matching, ordering, etc. Some recent surveys focus on VLP and compare the existing VLP Transformer models from the angles of domain (image-text or video-text), vision feature extraction, language feature extraction, architecture (single-or dualstream), decoder (w/, w/o), pretext tasks/objectives, pretraining datasets, and down-stream tasks, e.g., Table 3 of [57], Table 2 of [58].
15. Different from these views, in this survey, we would propose our comparisons from some new perspectives.
16. Specifically: (VLM)0 The core of Transformer ecosystem is self-attention, thus we would compare the existing multimodal pretraining Transformer models from the angles of how and when the self-attention or its variants perform cross-modal interactions.
17. (VLM)1 Considering from a geometrically topological perspective, self-attention helps Transformers intrinsically work in a modality agnostic pipeline that is compatible with various modalities by taking in the embedding of each token as a node of graph, thus we would highlight that the existing VLP can be applied to other modalities, beyond visual and linguistic domains.
18. (VLM)2 We suggest to treat the Transformer-based multimodal pretraining pipelines having three key components, from bottom to top, i.e., tokenization, Transformer representation, objective supervision.",1. What are pretext tasks in Transformer based multimodal pretraining?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s30,"Question: What are the challenges and potential improvements in multimodal pretraining Transformer methods?

1. In spite of the recent advances, multimodal pretraining Transformer methods still have some obvious bottlenecks.
2. For instance, as discussed by [208] in VLP field, while the BERT-style cross-modal pretraining models produce excellent results on various down-stream visionlanguage tasks, they fail to be applied to generative tasks directly.
3. As discussed in [208], both VideoBERT [7] and CBT [107] have to train a separate video-to-text decoder for video captioning.
4. This is a significant gap between the pretraining models designed for discriminative and generative tasks, as the main reason is discriminative task oriented pretraining models do not involve the decoders of Transformer.
5. Therefore, how to design more unified pipelines that can work for both discriminative and generative down-stream tasks is also an open problem to be solved.
6. Again for instance, common multimodal pretraining models often underperform for fine-grained/instance-level tasks as discussed by [137]. Discussion As discussed in [208], the masked language and region modelling as pre-training task have a main advantage that the Transformer encoder learned from these supervisions can encode both vision and language patterns based on bidirectional context and it is naturally fit for the semantic understanding tasks, e.g., VQA, image-text retrieval.
7. Discussion How to boost the performance for multimodal pretraining Transformers is an open problem.
8. Some practices demonstrate that multi-task training (by adding auxiliary loss) [111], [137] and adversarial training [210] improve multimodal pretraining Transformers to further boost the performance.
9. Meanwhile, overly compound pretraining objectives potentially upgrade the challenge of balancing among different loss terms, thus complicate the training optimization [199].
10. Moreover, the difficulty of the pretexts is also worth discussing.
11. In general, if aim to learn more explicit object concepts, more complex pretext losses will be used [204].
12. However, for pretexts, whether more complexity is better remains a question.","1. What are the challenges in multimodal pretraining Transformer methods?
2. What are the potential improvements in multimodal pretraining Transformer methods?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s31,"Question: Why is task-specific multimodal pretraining often necessary in AI models?

1. In practices of multimodal Transformers, the aforementioned down-stream task -agnostic pretraining is optional, not necessary, and down-stream task specific pretraining is also widely studied [150], [190], [208], [211].
2. The main reasons include: (1) Limited by the existing technique, it is extremely difficult to design a set of highly universal network architectures, pretext tasks, and corpora that work for all the various down-stream applications.
3. (2) There are nonnegligible gaps among various down-stream applications, e.g., task logic, data form, making it difficult to transfer from pretraining to down-stream applications.
4. Therefore, a large number of down-stream tasks still need tailor-made pretraining to improve the performance.
5. Guhur et al. [150] propose in-domain pretraining for visionand-language navigation, as the general VLP focuses on TABLE 3 Pretext task comparison of multi-modal pretraining Transformer models (for agnostic down-stream tasks).
6. ""C-M Loss"": cross-modal loss; ""
7. Con. Loss"": loss conditioned on other modality/modalities.",1. Why is task-specific multimodal pretraining often necessary in AI models?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s32,"Question: What are the different tasks and models in masked and multimodal language processing?

1. Masking Masked Language Modelling (MLM)
2. [7], [137] Image-Conditioned Masked Language Modelling (IMLM) [206], [207] [208] Text-Conditioned Masked Region Prediction [206]
3. Masked Acoustic Modelling [180], [200] Masked Image Region Regression [7]0 Masked Image Region Prediction [190] Masked Frame
4. Modelling [7]2 [116] Masked Sentence Generation [7]4 [7]5 Video Conditioned Masked Language Model [7]7 Text Conditioned Masked Frame Model [7]7 Describing Image-conditioned Denoising Autoencoding [7]8 [208] Text-conditioned Image Feature Generation [137]0 [208] Prefix Language Modelling [137]2 [137]3 Matching Image-Text Matching [137]4 [137]5 [102], [137]7, [137]8, (IMLM)4, (IMLM)0, Phrase-Region Alignment (IMLM)1 (IMLM)2 Word-Region Alignment (IMLM)3 (IMLM)4, (IMLM)5 Video-Subtitle Matching (IMLM)6 [116] Next Sentence Prediction (IMLM)8 (IMLM)9, [102], [190] Ordering Sentence Ordering Modelling (SOM) [201] Frame Ordering Modelling (FOM) [116] learning vision-language correlations, not designed for sequential decision making as required in embodied VLN.
5. Murahari et al. [190] present a visual dialogue oriented approach to leverage pretraining on general vision-language datasets.
6. XGPT [208] is tailor-made for image captioning, to overcome the limitation that BERT-based cross-modal pretrained models fail to be applied to generative tasks directly.
7. ERNIE-ViLG [211] is designed for bidirectional image-text generation with Transformers.
8. Special modalities have their own unique domain knowledge that can be used to design the specific pretrain pretexts.
9. GraphCodeBERT [44] uses two structure-aware pretext tasks [207]0 for programming source code.
10. To learn from the spatial cues in 360  video, Morgado et al. [207]1 propose to perform contrastive audio-visual spatial alignment of 360  video and spatial audio.
11. Med-BERT [207]2 is a contextualized embedding model pretrained on a structured electronic health record dataset of two million patients.
12. Kaleido-BERT [207]3 is a VLP Transformer model tailor-made for the fashion domain.","1. What are the different tasks in masked and multimodal language processing?
2. What are the different models in masked and multimodal language processing?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s35,"Question: How do MML Transformers fuse information across different modalities?

1. In general, MML Transformers fuse information across multiple modalities primarily at three levels: input (i.e., early fusion), intermediate representation (i.e., middle fusion), and prediction (i.e., late fusion).
2. Common early fusion based MML Transformer models [7], [104], [108] are also known as one-stream architecture, allowing the adoption of the merits of BERT due to minimal architectural modification.
3. The main difference between these one-stream models is the usage of problem-specific modalities with variant masking techniques.
4. With attention operation, a noticeable fusion scheme is introduced based on a notion of bottleneck tokens [175].
5. It applies for both early and middle fusion by simply choosing to-be-fused layers.
6. We note that the simple prediction-based late fusion [247], [248] is less adopted in MML Transformers.
7. This makes sense considering the motivations of learning stronger multimodal contextual representations and great advance of computing power.
8. For enhancing and interpreting the fusion of MML, probing the interaction and measuring the fusion between modalities [249] would be an interesting direction to explore.",1. How do MML Transformers fuse information across different modalities?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s36,"Question: What is the significance of cross-modal alignment in multimodal applications?

1. Cross-modal alignment is the key to a number of real-world multimodal applications.
2. Transformer based cross-modal alignment has been studied for various tasks, e.g., speaker localization in multi-speaker videos [250], speech translation [180], text-to-speech alignment [251], text-to-video retrieval [252], [253], [254], and visual grounding of natural language [255], [256], [257], [258], [180]0.
3. Recently, Transformer based alignment [180]1, [180]2, [180]3, [180]4, [180]5 has led to a surge of leveraging large quantities of web data [180]6 for vision and language tasks.
4. A representative practice is to map two modalities into a common representation space with contrastive learning over paired samples.
5. The models based on this idea are often enormous in size and expensive to optimize from millions or billions of training data.
6. Consequently, successive works mostly exploit pretrained models for tackling various downstream tasks [180]7, [252]1, [180]9, [251]0, [251]1.
7. These alignment models have the ability of zero-shot transfer particularly for image classification via prompt engineering [251]2.
8. This novel perspective is mind-blowing, given that image classification is conventionally regarded as a unimodal learning problem and zero-shot classification remains an unsolved challenge despite extensive research [251]3.
9. This has been studied for more challenging and fine-grained tasks (e.g., object detection
10. [269], visual question answering [103], [106], [112], [252]1, and instance retrieval [222], [252]1) by imposing region [251]5 level alignment.
11. Finegrained alignment will however incur more computational costs from explicit region detection and how to eliminate this whilst keeping the region-level learning capability becomes a challenge.
12. Several ideas introduced recently include random sampling [251]6, learning concept dictionary [251]7, uniform masking [251]8, patch projection [251]9, joint learning of a region detector [252]0, and representation aligning before mask prediction [252]1.",1. What is the significance of cross-modal alignment in multimodal applications?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s39,"Question: What are the challenges and methods to improve multimodal Transformers' robustness?

1. Multimodal Transformers pretrained on large-scale corpora achieve the state-of-the-art for various multimodal applications, while their robustness is still unclear and understudied.
2. This at least involves two key challenges, i.e., how to theoretically analyse the robustness, how to improve the robustness.
3. Although that recent attempts [99], [182], [289], [290] study and evaluate how the Transformer components/sublayers contribute to the robustness, the main bottleneck is that the community lacks theoretical tools to analyse the Transformer family.
4. Recently, the common practices to analyse robustness are mainly based on experiment evaluations [291], e.g., cross-dataset evaluations, perturbationbased evaluations.
5. Thus, some multimodal datasets [130], [182]3 are proposed for evaluating the robustness.
6. Recent attempts mainly use two straightforward methods to improve the robustness for multimodal Transformer models: (1) augmentation and adversarial learning based strategies [293], [294], [182]0 fine-grained loss functions [182]1.
7. For instance: VILLA [182]2 is a generic adversarial training framework that can be applied to various multimodal Transformers.
8. Akula et al. [182]3 empirically demonstrate that ViL-BERT fails to exploit linguistic structure, and they propose two methods to improve the robustness of ViLBERT, one based on contrastive learning and the other based on multitask learning.","1. What are the challenges to improve multimodal Transformers' robustness?
2. What methods can be used to improve multimodal Transformers' robustness?"
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s40,"Question: What challenges do multimodal Transformer models face in achieving universalness?

1. Due to the highly diversity of tasks and modalities of multimodal learning, universalness is an important problem for multimodal Transformer models.
2. A large amount of recent attempts [117], [296], [297], [298] study how to use as unified as possible pipelines to handle various modalities and multimodal tasks.
3. Ideally, the unified multimodal Transformers can be compatible with various data (e.g., aligned and unaligned, uni-modal and multimodal) and tasks (e.g., supervised and unsupervised, uni-modal and multimodal, discriminative and generative), and meanwhile have either few-shot or even zero-shot generalization ability.
4. Thus, the current solutions for universalness goal for multimodal Transformers are preliminary probes.
5. The currently unifying-oriented attempts mainly include: [297]1 Unifying the pipelines for both uni-modal and multimodal inputs/tasks.
6. As discussed Section 5.3, in practical scenarios, multimodal Transformers need to handle unimodal data due to the issue of missing modalities.
7. Distilling multimodal knowledge into small models that are adaptable to uni-modal data and tasks is a successful practice [275], [276].
8. [297]2 Unifying the pipelines for both multimodal understanding and generation.
9. In general, for multimodal Transformer pipelines, understanding and discriminative tasks require Transformer encoders only, while generation/generative tasks require both Transformer encoders and decoders.
10. Existing attempts use multi-task learning to combine the understanding and generation workflows, where two kinds of workflows are jointly trained by multitask loss functions.
11. From the perspective of model structures, typical solutions include: [296]0 encoder + decoder, e.g., E2E-VLP [296]1.
12. [296]2 separate encoders + cross encoder + decoder, e.g., UniVL [117], CBT [296]4.
13. [296]5 single unified/combined encoder-decoder, e.g., VLP [296]6.
14. [296]7 twostream decoupled design [296]8.
15. [296]9 Unifying and converting the tasks themselves, e.g., CLIP [297]0 converts zero-shot recognition to retrieval, thus reduces the costs of modifying the model.
16. However, the aforementioned practices suffer some obvious challenges and bottlenecks, at least including: [297]1 Due to modality and task gaps, universal models should consider the trade-off between universalness and cost.
17. Unifying the pipelines of different modalities and tasks generally cause larger or more complicated model configuration, whereas for a specific modality or task, some components are redundant.
18. [297]2 Multi-task loss functions increase the complexity of training.
19. How to co-train multiple objectives properly and effectively is challenging, due to that different objectives generally should be optimized in different strategies.",1. What challenges do multimodal Transformer models face in achieving universalness?
249642175,Multimodal Learning with Transformers: A Survey,"Medicine, Computer Science",s41,"Question: How do studies investigate multimodal Transformers' performance and interpretability?

1. Why and how Transformers perform so well in multimodal learning has been investigated [106], [299], [300], [301], [302], [303], [304], [305], [306].
2. These attempts mainly use probing task and ablation study.
3. Cao et al. [299] design a set of probing tasks on UNITER [106] and LXMERT [103], to evaluate what patterns are learned in pretraining.
4. Hendricks et al. [301] probe the image-language Transformers by fine-grained image-sentence pairs, and find that verb understanding is harder than subject or object understanding.
5. Chen et al. [106] examine the optimal combination of pretraining tasks via ablation study, to compare how different pretexts contribute to the Transformers.
6. Despite these attempts, the interpretability of multimodal Transformers is still under-studied to date.","1. How do studies investigate multimodal Transformers' performance?
2. How do studies interpret the interpretability of multimodal Transformers?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s1,"Question: What does the survey paper on transformer-based BPLMs cover?

1. The highlights of this survey paper are  First survey paper to present the recent trends in transformer-based BPLMs.
2. We present a brief overview of various foundational concepts like embedding layer, transformer encoder layer and self-supervised learning (Section 2).
3. We explain various core concepts related to transformer-based BPLMs like pretraining methods, pretraining tasks, fine-tuning methods, and embeddings.
4. We discuss each concept in detail, classify and compare various methods in each (Section 3).
5. We present a taxonomy of transformer-based BPLMs and present a brief overview of all the models (Section 4).
6. We explain how transformer-based BPLMs are applied in various biomedical NLP tasks (Section 5).
7. We present solutions to some of the challenges like low-cost domain adaptation, small biomedical datasets, ontology knowledge injection, robustness to noise, quality in-domain word representations, quality sequence representation and pretraining using less in-domain corpora (Section 7).
8. We discuss possible future directions which will drive the researchers to further enhance transformer-based BPLMs (Section 8).",1. What does the survey paper on transformer-based BPLMs cover?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s2,"Question: What are the core components of transformer-based PLMs like BERT and RoBERTa?

1. In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4).
2. The embedding layer takes input tokens and returns a vector for each.
3. The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens.
4. The final input vector for each token is obtained by summing all the vectors of each embedding type.
5. The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism.
6. By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors.
7. Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type.
8. In some models, there are more than three also.
9. For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings.
10. A detailed description of various embedding types is presented in Section 3.4.
11. The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation.
12. The first sublayer can be char, sub-word, or code embedding based.
13. For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings.
14. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes.
15. The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5).",1. What are the core components of transformer-based PLMs like BERT and RoBERTa?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s6,"Question: How does Self-Attention encode global contextual information in input tokens?

1. SA is a much better alternative compared to convolution and recurrent layers to encode global contextual information.
2. For a sequence of input tokens, SA updates each input token vector by encoding global contextual information i.e., it expresses each token vector as a weighted sum of all the token vectors where the weights are given by attention scores.
3. The final input representation matrix X is transformed into Query (Q ∈ R n × q ), Key (K ∈ R n × k ) and Value (V ∈ R n × v ) matrices using three weight matrices W Q ∈ R e × q , W K ∈ R e × k
4. and Here h represents the number of self-attention heads.
5. The output of SA layer is computed as 1) Compute similarity matrix ( S ∈ R n × n ) as Q.K T .
6. 2) To obtain stable gradients, scale the similarity matrix values using √ q and then use softmax to convert similarity scores to probability values to get matrix P ∈ R n × n .
7. Formally, P = Sof tmax((Q.K T )/ √ q) 3) Compute the final weighted values matrix Z ∈ R n × v as P.V",1. How does Self-Attention encode global contextual information in input tokens?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s7,"Question: How does Multi-Head Self Attention improve word encoding in transformers?

1. With only one self-attention layer, the meaning of a word may largely depend on the same word itself.
2. To avoid this, SA is applied multiple times in parallel each with different weight matrices.
3. Thus, MHSA allows the transformer to attend to multiple positions while encoding a word.
4. Let Z 1 , Z 2 , Z 3 ,..,Z h represent the weighted values matrices of h self-attention heads.
5. Then the final weighted value matrix is obtained by concatenating all these individual weight matrices and then projecting it.",1. How does Multi-Head Self Attention improve word encoding in transformers?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s8,"Question: What constitutes a Position-wise Feed Forward Network in models like BERT?

1. Two linear layers with a non-linear activation constitutes the PFN.
2. PFN is applied to every input token vector.
3. Models like BERT uses Gelu [33] activation function.
4. Here the parameters of PFNs applied on each of the token vectors are the same.
5. Formally,",1. What constitutes a Position-wise Feed Forward Network in models like BERT?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s9,"Question: What do Add and Norm represent in transformer encoder layers?

1. Add represents residual connection while Norm represents layer normalization.
2. Add and Norm is applied on both MHSA and PFN of transformer encoder to stay away from vanishing and exploding gradients.
3. In general, a transformed-based PLM consists of a sequence of transformer encoder layers after the embedding layer.
4. Each transformer encoder layer updates the input token vectors by encoding global contextual information.
5. By updating the input token vector using a sequence of transformer encoders help the model to encode more language information.
6. Formally, Here LN represents Layer Normalization,Ê m−1 represents the output after applying Add and Norm over the output of MHSA and E m represents the output after applying Add and Norm over the output of PFN in m th encoder layer.
7. Overall, E m represents the output of m th encoder layer with E m−1 as input.
8. Here the input for the first transformer encoder layer is, E 0 = X.",1. What do Add and Norm represent in transformer encoder layers?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s10,"Question: What is Self-Supervised Learning and its significance in AI?

1. Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade.
2. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions.
3. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances.
4. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data.
5. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples.
6. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34]. Robotics is the first AI field to use self-supervised learning methods [34].
7. Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38].
8. SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods.
9. SSL is similar to unsupervised learning as it does not depend on human-labeled instances.
10. It is also similar to supervised learning as it learns using supervision.
11. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data.
12. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks.
13. The pseudo labels are generated depending on the definitions of pre-training tasks.
14. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34].
15. In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling).
16. In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization).
17. In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection).
18. For more details about different SSL methods, please refer to the survey paper written by Liu et al. [34].","1. What is Self-Supervised Learning?
2. What is the significance of Self-Supervised Learning in AI?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s13,"Question: What is mixed-domain pretraining and its types in NLP?

1. Mixed domain pretraining involves training the model using both general and in-domain text.
2. Depending on whether the pretraining is done simultaneously or not, mixed domain pretraining can be classified into a) Continual pretraining -initially the model is pre-trained over general domain text and then adapted to the biomedical domain [16] and b)
3. Simultaneous pretraining -the model is pre-trained over the combined corpora having both general and in-domain text where the indomain text is up sampled to ensure balanced pretraining [21].
4. Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs.
5. It is also referred to as further pretraining.
6. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7).
7. For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16].
8. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].
9. Simultaneous Pretraining (SPT) : Continual pretraining achieved good results by adapting general models to the biomedical domain [16], [18], [19], [21]0, [21]1.
10. However, it requires large volumes of in-domain text.
11. Otherwise, CPT may result in suboptimal performance.
12. Simultaneous pretraining comes to the rescue when only a small amount of in-domain text is available.
13. Here, the pretraining corpora consist of both in-domain and general domain text where the in-domain text is up sampled to ensure a balanced pretraining [21]2.
14. For example, BERT [21]3 [21]4 is developed by simultaneous pretraining over a small amount of Japanese clinical text and a large amount of Japanese Wikipedia text.
15. This model outperformed UTH-BERT in clinical text classification.
16. UTH-BERT [21]5 is trained from scratch over Japanese clinical text.","1. What is mixed-domain pretraining?
2. What are the types of mixed-domain pretraining in NLP?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s14,"Question: What is the main drawback of continual pretraining and how does Domain-Specific Pretraining address it?

1. The main drawback in continual pretraining is the general domain vocabulary.
2. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2].
3. As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning.
4. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords.
5. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10).
6. For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles [20].
7. PubMed achieved state-of-the-art results in the BLURB benchmark.
8. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11).
9. In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained.
10. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences.","1. What is the main drawback of continual pretraining?
2. How does Domain-Specific Pretraining address the main drawback of continual pretraining?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s16,"Question: What are pretraining tasks in language model development?

1. During pretraining, the language models learn language representations based on the supervision provided by one or more pretraining tasks.
2. A pretraining task is a pseudo-supervised task whose labels are generated automatically.
3. A pretraining task can be main or auxiliary.
4. The main pretraining tasks allow the model to learn language representations while auxiliary pretraining tasks allow the model to gain knowledge from human-curated sources like Ontology [34], [45]- [47].
5. The classification of pretraining tasks is given in Figure 12 and a brief summary of various pretraining tasks is presented in Table 1.",1. What are pretraining tasks in language model development?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s17,"Question: What are the main pretraining tasks for learning language representations?

1. The main pretraining tasks allow the model to learn language representations.
2. Some of the commonly used main pretraining tasks are masked language modelling (MLM) [2], replaced token detection (RTD)
3. [50], sentence boundary objective (SBO) [49], next sentence prediction (NSP) [2] and sentence order prediction (SOP)
4. [15]. Masked Language Modeling (MLM).
5. It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2].
6. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens.
7. However, the meaning of a word depends on both the left and right contexts.
8. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model.
9. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI.
10. Less challenging as it involves topic prediction which is a relatively easy task.",1. What are the main pretraining tasks for learning language representations?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s19,"Question: How do auxiliary pretraining tasks enhance in-domain models with human-curated knowledge sources?

1. Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them.
2. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45].
3. This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models.
4. Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.
5. Similarly, Liu et al. [34] used multi-similarity loss-based pretraining task to inject UMLS synonym knowledge into PubMedBERT.","1. How do auxiliary pretraining tasks enhance in-domain models?
2. How do human-curated knowledge sources contribute to enhancing in-domain models?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s20,"Question: What are the key components and categories of fine-tuning methods in machine learning?

1. Pretraining allows the model to learn general or indomain knowledge which is useful across the tasks.
2. However, for a model to perform well in a particular task, it must have task-specific knowledge along with general or in-domain knowledge.
3. The model gains taskspecific knowledge by fine-tuning on the task-specific datasets.
4. Task-specific layers are included on the top of transformer-based BPLMs.
5. For example, to perform text classification, we need a) a contextual encoder to learn contextual token representations from the given input token vectors and b) a classifier to project the final sequence vector and then generate the probability vector.
6. Here classifier is the task-specific layer which is usually a softmax layer in text classification.
7. Fine-tuning methods fall into two categories.","1. What are the key components of fine-tuning methods in machine learning?
2. What are the categories of fine-tuning methods in machine learning?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s21,"Question: What are the four ways to perform Intermediate Fine-Tuning (IFT) on datasets?

1. IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets.
2. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains.
3. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58].
4. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].
5. Same Task Same Domain -Here, the source and target datasets are from the same task and domain.
6. But the source dataset is a more generic one while the target dataset is more specific [62], [63].
7. For example, Gao et al. [63] fine-tuned BlueBERT on a large general biomedical NER corpus like MedMentions [64] or Semantic Medline before fine-tuning on the small target NER corpus.
8. Different Task Same Domain -Here, the source and target datasets are from different tasks but the same domain.
9. Fine-tuning on source dataset which is from the same domain allows the model to gain more domainspecific knowledge which improves the performance of the model on the same domain target task [58]1.
10. McCreery et al. [58]1 fine-tuned the model on the medical questionanswer pairs dataset to enhance its performance on the medical question similarity dataset.
11. Different Task Different Domain -Here, the source and target datasets are from different tasks and different domains.
12. For example, Jeong et al. [58]2 fine-tuned BioBERT on a general MultiNLI dataset to improve the performance of the model in biomedical QA.
13. Here the model learns sentence level reasoning skills which are useful in biomedical QA.",1. What are the four ways to perform Intermediate Fine-Tuning (IFT) on datasets?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s22,"Question: What are the benefits and limitations of multi-task fine-tuning in machine learning models?

1. Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]-[69].
2. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer.
3. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks.
4. At the same time, due to the increase in training set size, the model is less prone to over-fitting.
5. Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69].
6. Moreover, having a single model for multitasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs [70].
7. Multi-task fine-tuning may not provide the best results all the time [70].
8. In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks [71].
9. For example, Mahanjan et al. [71] applied multi-task finetuning iteratively to choose the best subset of related datasets.
10. Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS [72] dataset.
11. After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model [73].","1. What are the benefits of multi-task fine-tuning in machine learning models?
2. What are the limitations of multi-task fine-tuning in machine learning models?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s25,"Question: What are auxiliary embeddings and their types in enhancing model learning?

1. Main embeddings represent the given input sequence in low dimensional space.
2. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better.
3. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings.
4. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence.
5. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings.
6. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].
7. Segment Embeddings -Segment embeddings help to Age Embeddings -In models like BEHRT [27] and BERT-EHR [32], age embeddings are used in addition to other embeddings.
8. Age embeddings provide the age of the patient and help the model to leverage temporal information.
9. Age embedding is the same for all the codes in a single patient visit.
10. Gender Embeddings -In models like BEHRT [27] and BERT-EHR [32], gender embeddings are used in addition to other embeddings.
11. Gender embeddings provide the gender information of the patient to the model.
12. Gender embedding is the same for all the codes in all the patient visits.
13. Semantic Group Embeddings -Semantic group embeddings are used in UmlsBERT [46] to explicitly inform the model to learn similar representations for words from the same semantic group i.e., semantic group embedding is same for all the words which fall into the same semantic group.
14. Besides, it also helps to provide better representations for rare words.
15. Figure 14 shows transformer-based BPLMs taxonomy.","1. What are auxiliary embeddings in enhancing model learning?
2. What are the types of auxiliary embeddings in enhancing model learning?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s28,"Question: How are Electronic Health Records utilized in medical research and patient care?

1. In the last decade, most hospitals have been using Electronic Health Records (EHRs) to record patient as well as treatment details right from admission to discharge [82].
2. EHRs contain a vast amount of medical data which can be used to provide better patient care by knowledge discovery and the development of better algorithms.
3. As EHR contains sensitive information related to patients, medical data must be de-identified before sharing.
4. EHRs include both structured and unstructured data [83], [84].
5. Structured data includes laboratory test results, various medical codes, etc. Unstructured data include clinical notes like medication instructions, progress notes, discharge summaries, etc. Clinical notes include the most valuable patient information which is difficult and expensive to extract manually.
6. So, there is a need for automatic information extraction methods to utilize the abundant medical data from EHRs in research as well as applications [84]- [86].
7. Following the success of transformer-based PLMs in the general domain, researchers in biomedical NLP also developed EHR-based T-BPLMs by pretraining over clinical notes or medical codes or both.
8. MIMIC [87], [88] [19] further pre-trained XLNetbase on MIMIC-III Clinical Notes.
9. The authors released two models namely a) pretrained model on nursing notes and b) pretrained model on discharge summaries.
10. Yang et al. [82]2 further pre-trained general models like BERT, ELECTRA, RoBERTa, XLNet, and ALBERT on MIMIC-III and released in-domain PLMs.
11. It is the first work to release in-domain models based on all the popular transformer-based PLMs.
12. Unlike the above pretrained models which are pretrained on clinical text, recent works [82]3, [82]4, [82]2 released models which are pre-trained on disease codes or multi-modal EHR data.
13. BEHRT [82]3 is trained from scratch using 1.6 million patient EHR data with MLM as pretraining task.
14. The authors used code, position, age, and segment embeddings.
15. Med-BERT [82]4 is trained from scratch using 28,490,650 patient EHR data with MLM and LOS [82]5 as pretraining tasks.
16. The authors used code, serialization and visit embeddings.
17. BERT-EHR [82]6 is trained from scratch using multi-modal data from 43967 patient records with MLM as a pretraining task.
18. Table 2 contains summary of various EHR based BPLMs.","1. How are Electronic Health Records utilized in medical research?
2. How are Electronic Health Records utilized in patient care?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s30,"Question: How has social media transformed health-related information sharing and analysis?

1. In the last decade, social media has become the first choice for internet users to express their thoughts.
2. Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers [107], [108].
3. Recent works have shown that health-related social media data is useful in many applications to provide better health-related services [109], [110].
4. The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets [92].
5. This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos.
6. Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts.
7. CT-BERT [92] is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30% improvement compared to BERT-large, on five different classification datasets.
8. BioRedditBERT [94] is initialized from BioBERT and further pretrained on healthrelated Reddit posts.
9. The authors showed that BioRed-ditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in normalizing health-related entity mentions.
10. RuDR-BERT [108]0 is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews).
11. The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins.
12. EnRuDR-BERT [108]0 and EnDR-BERT [108]0 are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively.
13. Table 4 contains summary of social media text-based BPLMs.","1. How has social media transformed health-related information sharing?
2. How has social media transformed health-related information analysis?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s32,"Question: What is the purpose of using hybrid corpora in transformer-based PLM pretraining?

1. It is difficult to obtain a large amount of in-domain text in some cases.
2. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.
3. The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC).
4. However, to pretrain a transformer-based PLM from scratch, we require large volumes of text.
5. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97].
6. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.",1. What is the purpose of using hybrid corpora in transformer-based PLM pretraining?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s35,"Question: How do T-BPLMs like BioBERT improve in biomedical NLP tasks through ontology enrichment?

1. T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks.
2. These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text.
3. Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] .
4. Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets.
5. Here, pretraining involves three loss functions namely MLM, NSP, and triple classification.
6. Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model.
7. Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP.
8. The novel multi-label loss function allows the model to connect all the words under the same CUI.
9. CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss.
10. Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge.
11. SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss.","1. How do T-BPLMs like BioBERT improve in biomedical NLP tasks?
2. How does ontology enrichment contribute to the improvement of T-BPLMs in biomedical NLP tasks?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s36,"Question: What are Green Models and how do they adapt to in-domain vocabulary cost-effectively?

1. CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus.
2. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords.
3. This kind of representation increases the overall length of the input as well as hinders the model learning.
4. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text.
5. However, both these approaches involve learning the model parameters from scratch which is highly expensive.
6. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].
7. Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123].
8. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach.
9. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors.
10. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors.
11. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge.
12. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction.
13. This approach is completely inexpensive as it requires only CPU.
14. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module.
15. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text.
16. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive.
17. Table 9 contains summary of Green T-BPLMs.","1. What are Green Models?
2. How do Green Models adapt to in-domain vocabulary cost-effectively?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s37,"Question: How can debiasing improve fairness in Transformer-Based Pretrained Language Models (T-PLMs)?

1. T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others.
2. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126].
3. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group.
4. Zhang et al. [127] further pretrained SciBERT [105] on MIMIC-III clinical notes and showed that the performance of the model is different for different groups.
5. The authors applied adversarial pretraining debiasing to reduce the gender bias in the model.
6. The authors released both the models publicly to encourage further research in debiasing T-BPLMs.",1. How can debiasing improve fairness in Transformer-Based Pretrained Language Models (T-PLMs)?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s38,"Question: What are the advancements in multi-modal models for the biomedical domain?

1. T-PLMs have achieved success in many of the NLP tasks including in specific domains like Biomedical.
2. Recent research works have focused on developing pretrained models that can handle multi-modal data i.e., video + text [128], [129], image + text [129]1- [134] etc.
3. In Biomedical domain, models like BERTHop [134] and Medical-VLBERT [129]0 have been proposed recently to handle image + text data.
4. BERTHop [134] is a multi-modal T-BPLM developed for Chest X-ray disease diagnosis.
5. Like ViLBERT [131] and LXMERT [132], BERTHop uses separate encoder to encoder image and text inputs.
6. BERTHop uses PixelHopetc.0 [135] to encode image data and BlueBERT as text encoder.
7. Medical-VLBERT is developed for automatic report generation from COVID-19 scans.
8. Unlike BERTHop, Medical-VLBERT [129]0 uses shared encoder based on VL-BERT [129]1 to encode image and text data.",1. What are the advancements in multi-modal models for the biomedical domain?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s40,"Question: What is Natural Language Inference and its significance in NLP?

1. Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics.
2. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence.
3. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain.
4. NLI is framed as a three-way sentence pair classification problem.
5. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair.
6. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes.
7. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%.
8. Cengiz et al. [136]0 applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%.
9. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [136]1 and MultiNLI [136]2 and then finetuned them on MedNLI.","1. What is Natural Language Inference?
2. What is the significance of Natural Language Inference in NLP?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s41,"Question: What is entity extraction and its significance in various fields?

1. Entity Extraction is the first step in unlocking valuable information in unstructured text data.
2. Entity Extraction is useful in many tasks like entity linking, relation extraction, knowledge graph construction, etc. Extracting clinical entities like drug and adverse drug reactions is useful in pharmacovigilance and biomedical entities like proteins, chemicals and drugs is useful to discover knowledge in scientific literature.
3. Some of the popular entity extraction datasets are I2B2 2010 [141], VAERS [142], CADEC [143], N2C2 2018 [144] , BC4CHEMD [145], B5CDR-Chem [146], JNLPBA [147] and NCBI-Disease [148].
4. Most of the existing work approaches entity extraction as sequence labeling or machine reading comprehension.
5. In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer [143]8, [149], [142]1, BiLSTM+Softmax [142]1, BiLSTM+CRF [143]8, [142]8- [142]4 or CRF [143]0, [143]8, [142]8 is applied.
6. Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance [142]8, [142]9.
7. This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM.
8. Some of the works experimented with general BERT for extracting clinical and biomedical entities.
9. For example, Portelli et al. [143]0 showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text.
10. Boudjellal et al. [143]1 developed ABioNER by further pretraining AraBERT [143]2 on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT [143]3 and AraBERT on Arabic biomedical entity extraction.
11. This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model.
12. As in-domain datasets are comparatively small, some of the recent works [143]8, [143]7, [143]6 initially fine-tuned the models on similar datasets before fine-tuning on small target datasets.
13. This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets.
14. For example, Gao et al. [143]7 proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning.
15. Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels.
16. Recently Sun et al. [143]8 formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ [143]9 on six datasets.","1. What is entity extraction?
2. What is the significance of entity extraction in various fields?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s42,"Question: What is Semantic Textual Similarity and its applications in various tasks?

1. Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences.
2. Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity.
3. Both NLI and STS require sentence-level semantics.
4. STS is useful in tasks like concept relatedness [139], medical concept normalization [137], [138] , duplicate text detection [138]1, question answering [137]7, [156] and text summarization [157].
5. Moreover, Reimers et al. [136] showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences.
6. Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value.
7. BIOSSES [158] and Clinical STS [72] are the commonly used datasets to train and evaluate indomain STS models.
8. Recent works exploited general models for clinical STS [137]5, [137]6, [137]7, [138]5.
9. For example, Yang et al. [137]5 achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model.
10. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [137]5, [137]6, [137]7, [138]2.
11. Xiong et al. [137]9 enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [138]0 based entity representations.
12. Mutinda et al. [138]1 achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets.
13. Mahajan et al. [138]2 proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset.
14. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning.
15. The main drawback in the above existing works is giving '[138]4' vector as sentence pair representation to the sigmoid layer.
16. This is because the '[138]4' vector contains only partial information.
17. Unlike existing works, Wang et al. [138]5 applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.","1. What is Semantic Textual Similarity?
2. What are the applications of Semantic Textual Similarity in various tasks?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s43,"Question: What is relation extraction and its significance in information processing?

1. Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text.
2. Entity extraction followed by relation extraction helps to convert unstructured text into structured data.
3. Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering.
4. Some of relation extraction datasets are I2B2 2012 [162], AIMED [163], ChemProt [164], DDI [165], I2B2 2010 [141] and EU-ADR [166].
5. Wei et al. [167] achieved the best results on two datasets using MIMIC-BERT [40] +Softmax.
6. Thillaisundaram and Togia [168] applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks [169].
7. Liu et al. [163]0 proposed SciBERT+Softmax for relation extraction in biomedical text.
8. They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets.
9. The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of '[163]3' vector.
10. Su et al. [163]2 added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets.
11. The authors generated the final representation by concatenating '[163]3' vector and weighted sum vector of final hidden state vectors.
12. When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results.","1. What is relation extraction?
2. What is the significance of relation extraction in information processing?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s44,"Question: What is text classification and how is it implemented using transformer-based models?

1. Text classification involves assigning one of the predefined labels to variable-length texts like phrases, sentences, paragraphs, or documents.
2. Text classification involves an encoder which is usually a transformerbased PLM and a task-specific softmax classifier.
3. '[CLS]' vector or weighted sum of final hidden state vectors is treated as an aggregate representation of given text.
4. The fully connected dense layer in the classifier projects text representation vector into n-dimensional vector space where 'n' represents the number of predefined labels.
5. Finally, the softmax function is applied to get the probabilities of all the labels.
6. Garadi et al. [172] formulated prescription medication (PM) identification from tweets as four-way text classification.
7. They achieved good results using models like BERT, RoBERTa, XLNet, ALBERT, and DistillBERT.
8. They trained different ML-based meta classifiers with predictions from pre-trained models as inputs and further improved the results.
9. Shen et al. [173] applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT.
10. They generated labels for the training instances using a rule-based NLP algorithm.
11. Chen et al. [174] further pretrained the general BERT model on 1.5 drugrelated tweets and showed that further pretraining improves the performance of the model on ADR tweets classification.
12. Recent works [175], [176] showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification.
13. They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability.","1. What is text classification?
2. How is text classification implemented using transformer-based models?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s45,"Question: What are the challenges and advancements in biomedical question answering systems?

1. Question Answering (QA) aims to extract answers for the given queries.
2. QA helps to quickly find information in clinical notes or biomedical literature and thus saves a lot of time.
3. The main challenge in developing automated QA systems for the clinical or biomedical domain is the small size of datasets.
4. Developing large QA datasets in the clinical or biomedical domain is quite expensive and requires a lot of time also.
5. Some of the popular in-domain QA datasets are emrQA [177], CliCR [178], PubMedQA [179] COVID-QA [180], MASH-QA [181] and Health-QA [182].
6. Chakraborty et al. [177]4 showed BioMedBERT obtained by further pretraining BERT-Large on BREATHE 1.0 corpus outperformed BioBERT on biomedical question answering.
7. The main reason for this is the diversity of text in BREATHE 1.0 corpus.
8. Pergola et al. [52] introduced Biomedical Entity Masking (BEM) which allows the model to learn entity-centric knowledge during further pretraining.
9. They showed that BEM improved the performance of both general and indomain models on two in-domain QA datasets.
10. Recent works used intermediate fine-tuning on general QA [177]5, [177]4 or NLI [177]2 datasets or multi-tasking [177]6 to improve the performance of in-domain QA models.
11. For example, Soni et al. [177]4 achieved the best results on a) CliCR by intermediate fine-tuning on SQuaD using BioBERT and b) emrQA by intermediate fine-tuning on SQuaD and CliCR using Clinical BERT.
12. Yoon et al. [177]5 showed that intermediate fine-tuning on general domain Squad datasets improves the performance on biomedical question answering datasets.
13. Akdemir et al. [177]6 proposed a novel multi-task model based on BioBERT for biomedical question answering.
14. They used biomedical NER as an auxiliary task and showed that transfer learning from the bioNER task improves performance on question answering tasks.","1. What are the challenges in biomedical question answering systems?
2. What advancements have been made in biomedical question answering systems?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s46,"Question: What are the methods and challenges in biomedical text summarization?

1. In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature.
2. Researchers and domain experts need to go through a number of biomedical documents.
3. As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186].
4. Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187].
5. There are no standard datasets for biomedical text summarization.
6. Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].
7. Moradi et al. [188] proposed a novel approach to summarize biomedical scientific articles.
8. They embedded sentences, generated clusters, and then extracted the most informative sentences from each of the clusters.
9. They showed that BERT-large outperformed other models including the in-domain BERT models like BioBERT.
10. In the case of small models, BioBERT outperformed others.
11. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text.
12. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences.
13. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text.
14. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence.
15. The sentences with the highest score are considered as the summary.
16. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.","1. What are the methods in biomedical text summarization?
2. What are the challenges in biomedical text summarization?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s47,"Question: What are the benchmarks for evaluating pretrained models in NLP and biomedical research?

1. Benchmarks are useful to evaluate the progress in pretrained models.
2. GLUE is the first benchmark proposed to evaluate pretrained models.
3. Following GLUE, a number of benchmarks are proposed in general NLP.
4. Inspired by the benchmarks in general NLP, Biomedical research community proposed benchmarks like BLUE, BLURB and CBLUE.
5. We summarize the performance of various T-BPLMs in Table 10.","1. What are the benchmarks for evaluating pretrained models in NLP?
2. What are the benchmarks for evaluating pretrained models in biomedical research?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s49,"Question: What are the challenges and alternatives in low-cost domain adaptation for T-BPLMs?

1. The two popular approaches for developing T-BPLMs are MDPT and DSPT.
2. These approaches involve pretraining on large volumes of in-domain text using highend GPUs or TPUs for days.
3. These two approaches are quite successful in developing BPLMs.
4. However, these approaches are quite expensive requiring high computing resources with long pretraining durations [122].
5. For example, BioBERT -it took around ten days to adapt general BERT to the biomedical domain using eight GPUs [16].
6. Moreover, DSPT is more expensive compared to continual pretraining as it involves learning model weights from scratch [122], [123].
7. So, there is a need for lost cost domain adaptation methods to adapt general Approach Description Pros Cons Intermediate Fine-Tuning Model is fine-tuned on source dataset before fine-tuning on target dataset.
8. Allows the model to gain domain or task-specific knowledge.
9. Requirement of labeled datasets.","1. What are the challenges in low-cost domain adaptation for T-BPLMs?
2. What are the alternatives in low-cost domain adaptation for T-BPLMs?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s52,"Question: What is semi-supervised learning and how does it apply to BERT models in biomedicine?

1. Fine-tunes the model on training instances along with pseudo labeled instances Allows the model to leverage task-related unlabelled instances.
2. Fine-tuning must be done iteratively to reduce the noisy labeled instances.
3. BERT models to the biomedical domain.
4. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44].
5. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances.
6. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191].
7. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123].
8. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].","1. What is semi-supervised learning?
2. How does semi-supervised learning apply to BERT models in biomedicine?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s54,"Question: How can performance on small datasets be improved for machine learning models?

1. Pretraining on large volumes of in-domain text allows the model language representations while fine-tuning allows the model to learn task-specific knowledge.
2. During fine-tuning the model must learn sufficient task-specific knowledge to achieve good results on the task where the input distribution, as well as label space, is different from pretraining [193], [194].
3. With small target datasets, the models are not able to learn enough task-specific which limits the performance.
4. To over the small size of target datasets, the possible solutions are Intermediate Fine-Tuning -Intermediate fine-tuning on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets [55]- [58], [62], [194]6, [65], [66] and Multi-Task Fine-Tuning -Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [194]0.
5. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer.
6. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks.
7. Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [194]0- [194]1, [194]2. Data Augmentation -Data augmentation helps us to create new training instances from existing instances.
8. These newly creating training instances are close to original training data and helpful in low resource scenarios.
9. Back translation and EDA [194]3 are the top popular techniques for data augmentation.
10. For example, Wang et al. [194]4 used back translation to augment the training instances to train the clinical text similarity model.
11. The domain-specific ontologies like UMLS can also be used to augment the training instances [194]5. Semi-Supervised Learning -Semi-supervised learning augments the training set with pseudo-labeled instances.
12. The model which is fine-tuned on the original training set is used to label the task-related unlabelled instances.
13. The model is again fine-tuned on the augmented training set and this process is repeated until the model converges [194]6, [194]7 .
14. Table 11 contains a brief summary of these approaches.",1. How can performance on small datasets be improved for machine learning models?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s55,"Question: How do transformed based PLMs improve robustness to noise in sensitive domains?

1. Transformed based PLMs have achieved the best results in many of the tasks.
2. However, the performance of these models on noisy test instances is limited [197]- [200].
3. This is because the model is mostly trained on less noisy instances.
4. As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances.
5. Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning.
6. The robustness of models is crucial particularly insensitive domains like biomedical [199], [200].
7. Two possible solutions are a) CharBERT [28] -replaced the WordPiece based embedding layer with CharCNN based embedding layer.
8. Here word representation is generated from character embeddings using CharCNN.
9. b) Adversarial Training [200] -Here, the training set is augmented with the noisy instances.
10. Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances.","1. How do transformed based PLMs improve robustness to noise?
2. How do they improve robustness in sensitive domains?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s56,"Question: How can T-PLMs adapt to in-domain vocabulary for meaningful representation?

1. Continual pretraining allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of in-domain text.
2. Though the models are adapted to in-domain, they still contain general vocabulary.
3. As the vocabulary is learned over general text, it mostly includes subwords and words which are specific to the general domain.
4. As a result, many of the in-domain words are not represented in a meaningful way.
5. The two possible options to represent in-domain words in a meaningful way are a) in-domain vocabulary through DSPT [20] b) extending the general vocabulary with indomain vocabulary [122], [123].",1. How can T-PLMs adapt to in-domain vocabulary for meaningful representation?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s57,"Question: What is simultaneous pretraining in low resource language model training?

1. CPT or DSPT involves pretraining the language model on large volumes of in-domain text.
2. During pretraining, the model learns language representations that are useful across many tasks.
3. The size of the pretraining corpus influences how well the model learns the language representations.
4. It is not possible to get a large volume of in-domain text all the time.
5. In such scenarios with less in-domain corpus, the model may not learn well when trained using any of the above two methods.
6. The possible solution for this is simultaneous pretraining.
7. In simultaneous pretraining [21], the model is trained on combined corpora having both general and in-domain text.
8. As the in-domain text is comparatively less, upsampling can be used to have a balanced pretraining.",1. What is simultaneous pretraining in low resource language model training?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s58,"Question: What methods improve final sequence representation in text classification tasks?

1. For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation.
2. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information.
3. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space.
4. Finally, a softmax is applied to convert it into a vector of probabilities.
5. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.",What methods improve final sequence representation in text classification tasks?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s60,"Question: How can bias in deep learning models affect decision-making in healthcare?

1. With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation [112], disease prediction [31], [32] etc. However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others.
2. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126].
3. Real-world datasets have a bias in many forms.
4. It can be based on various attributes like gender, age, ethnicity, and marital status.
5. These attributes are considered as protected or sensitive [201].
6. For example, in the MIMIC-III dataset [88] a) heart disease is more common in males compared to females-an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups -an example of ethnicity bias.
7. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group.
8. There are few works that identified and addressed bias in transformer-based biomedical language models.
9. Zhang et al. [127] using a simple word competition task showed that SciBERT [105] exhibits ethnicity bias.
10. Moreover, the authors showed SciBERT model when further-pretrained on clinical notes exhibits performance differences for different protected attributes.
11. They further showed that adversarial pretraining debiasing has little impact in reducing bias.
12. Minot et al. [202] proposed an approach based on data augmentation to identify and reduce gender bias in patient notes.
13. This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions.","1. How can bias in deep learning models affect decision-making?
2. How specifically does this impact decision-making in healthcare?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s61,"Question: What are the privacy concerns related to clinical records and biomedical language models?

1. Every patient visit is recorded in the clinical records.
2. Apart from patient visits, clinical records contain the past and the present medical history of the patient.
3. Such sensitive data should not be disclosed as it may harm the patients physically or mentally [203].
4. Usually, the clinical records are shared for research purposes only after de-identifying the sensitive information.
5. However, it is possible to recover sensitive patient information from the de-identified medical records.
6. Recent works showed that there is data leakage from pre-trained models in the general domain i.e., it is possible to recover personal information present in the pretraining corpora [204], [205].
7. Due to data leakage, the models pre-trained on proprietary corpora, cannot be released publicly.
8. Recently, Nakamura et al. [203] proposed KART framework which can conduct various attacks to assess the leakage of sensitive information from pre-trained biomedical language models.
9. We strongly believe there is a need for more work in this area to assess as well as address the data leakage in biomedical language models.",1. What are the privacy concerns related to clinical records and biomedical language models?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s62,"Question: What are the challenges and solutions in domain adaptation for biomedical language models?

1. In the beginning, the standard approach to develop BPLMs is to start with general PLMs and then further pretrain them on large volumes of biomedical text [16].
2. The main drawback of this approach is the lack of indomain vocabulary.
3. Without domain-specific vocabulary, many of the in-domain are split into a number of subwords which hinders model learning during pretraining or fine-tuning.
4. Moreover, continual pretraining is quite expensive as it involves pretraining on large volumes of unlabeled text.
5. To overcome these drawbacks, there are low-cost domain adaptation approaches that extend the general domain vocabulary with in-domain vocabulary [122], [123].
6. The extra in-domain vocabulary is generated using Word2vec and then aligned [122] or generated directly using WordPiece [123] over biomedical text.
7. The main drawback in these low-cost domain adaptation approaches is an increase in the size of the model with the addition of in-domain vocabulary.
8. Further research on this topic can result in more novel methods for lowcost domain adaptation.","1. What are the challenges in domain adaptation for biomedical language models?
2. What are the solutions in domain adaptation for biomedical language models?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s63,"Question: What are the limitations of MLM in biomedical language models and proposed improvements?

1. Most of the biomedical language models (except ELECTRA-based models) are pre-trained using MLM.
2. In MLM, only 15% of tokens are randomly masked and the model learns by predicting that 15% of masked tokens only.
3. Here the main drawbacks are a) as tokens are randomly chosen for masking, the model may not learn much by predicting random tokens b) as only 15% of tokens are predicted, the training signal per example is less.
4. So, the model has to see more examples to learn enough language information which results in the requirement of large pretraining corpora and more computational resources.
5. There is a need for novel pretraining tasks like Replaced Token Detection (RTD) which can provide more training signal per example.
6. Moreover, when the model is pretrained using multiple pretraining tasks, the model receives more training signals per example and hence can learn enough language information using less pretraining corpora and computational resources [206].","1. What are the limitations of MLM in biomedical language models?
2. What proposed improvements have been suggested for MLM in biomedical language models?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s64,"Question: What is the purpose of benchmarks in evaluating NLP model performance?

1. In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks.
2. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks.
3. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models.
4. In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching).
5. In biomedical domain there are three benchmarks namely BLUE [208]0, BLURB [20] and ChineseBLUE [48].
6. BLUE introduced by Peng et al. [208]0 contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets.
7. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets.
8. The semantics of EHR and medical social media texts are different from biomedical scientific literature.
9. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets.",1. What is the purpose of benchmarks in evaluating NLP model performance?
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s65,"Question: What are intrinsic probes and their role in evaluating PLMs' pretraining knowledge?

1. During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14].
2. Intrinsic probes through light on the knowledge learned by PLMs during pretraining.
3. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models.
4. For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models.
5. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining.
6. This is an area which requires much attention from Biomedical NLP community.","1. What are intrinsic probes?
2. What is their role in evaluating PLMs' pretraining knowledge?"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Medicine, Linguistics, Computer Science",s66,"Question: What are the benefits of efficient models like ConvBERT and DeBERTa in NLP pretraining?

1. Pretraining provides BPLMs with necessary background knowledge which can be transferred to downstream tasks.
2. However, pretraining is computationally very expensive and also requires large volumes of pretraining data.
3. So, there is need of novel model architecture which reduces the pretraining time as well as the amount of pretraining corpus.
4. In general NLP, recently efficient models like ConvBERT [215] and DeBERTa [216] are proposed which reduces the pretraining time and amount of pretraining corpus required respectively.
5. DeBERTa with two novel improvements (Disentangled attention mechanism and enhanced masked decoder) achieves better compared to RoBERTa. DeBERTa is pretrained on just 78GB of data while RoBERTa is pretrained on 160GB of data.
6. ConvBERT with mixed attention block outperforms ELECTRA while using just 1/4 th of its pretraining cost.
7. Biomedical NLP research community must focus on developing pretrained models based on these novel model architectures.",1. What are the benefits of efficient models like ConvBERT and DeBERTa in NLP pretraining?
