# AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models

CorpusID: 233481730 - [https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c](https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c)

Fields: Computer Science, Linguistics, Medicine

## (s1) Literature Search and Selection
### Question: What does the survey paper on transformer-based BPLMs cover?

#Reference=0

(p1.0) The highlights of this survey paper are • First survey paper to present the recent trends in transformer-based BPLMs. • We present a brief overview of various foundational concepts like embedding layer, transformer encoder layer and self-supervised learning (Section 2). • We explain various core concepts related to transformer-based BPLMs like pretraining methods, pretraining tasks, fine-tuning methods, and embeddings. We discuss each concept in detail, classify and compare various methods in each (Section 3). • We present a taxonomy of transformer-based BPLMs and present a brief overview of all the models (Section 4). • We explain how transformer-based BPLMs are applied in various biomedical NLP tasks (Section 5). • We present solutions to some of the challenges like low-cost domain adaptation, small biomedical datasets, ontology knowledge injection, robustness to noise, quality in-domain word representations, quality sequence representation and pretraining using less in-domain corpora (Section 7). • We discuss possible future directions which will drive the researchers to further enhance transformer-based BPLMs (Section 8). 
## (s2) FOUNDATIONS
### Question: What are the core components of transformer-based PLMs like BERT and RoBERTa?

#Reference=6

(p2.0) In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4). The embedding layer takes input tokens and returns a vector for each. The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens. The final input vector for each token is obtained by summing all the vectors of each embedding type. The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism. By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors. Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type. In some models, there are more than three also. For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings. A detailed description of various embedding types is presented in Section 3.4. The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation. The first sublayer can be char, sub-word, or code embedding based. For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes. The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5).
## (s6) Self-Attention (SA)
### Question: How does Self-Attention encode global contextual information in input tokens?

#Reference=0

(p6.0) SA is a much better alternative compared to convolution and recurrent layers to encode global contextual information. For a sequence of input tokens, SA updates each input token vector by encoding global contextual information i.e., it expresses each token vector as a weighted sum of all the token vectors where the weights are given by attention scores. The final input representation matrix X is transformed into Query (Q ∈ R n × q ), Key (K ∈ R n × k ) and Value (V ∈ R n × v ) matrices using three weight matrices W Q ∈ R e × q , W K ∈ R e × k and

(p6.1) Here h represents the number of self-attention heads. The output of SA layer is computed as 1) Compute similarity matrix ( S ∈ R n × n ) as Q.K T .

(p6.2) 2) To obtain stable gradients, scale the similarity matrix values using √ q and then use softmax to convert similarity scores to probability values to get matrix P ∈ R n × n . Formally, P = Sof tmax((Q.K T )/ √ q)

(p6.3) 3) Compute the final weighted values matrix Z ∈ R n × v as P.V
## (s7) Multi-Head Self Attention (MHSA)
### Question: How does Multi-Head Self Attention improve word encoding in transformers?

#Reference=0

(p7.0) With only one self-attention layer, the meaning of a word may largely depend on the same word itself. To avoid this, SA is applied multiple times in parallel each with different weight matrices. Thus, MHSA allows the transformer to attend to multiple positions while encoding a word. Let Z 1 , Z 2 , Z 3 ,..,Z h represent the weighted values matrices of h self-attention heads. Then the final weighted value matrix is obtained by concatenating all these individual weight matrices and then projecting it.
## (s8) Position-wise Feed Forward Network (PFN)
### Question: What constitutes a Position-wise Feed Forward Network in models like BERT?

#Reference=1

(p8.0) Two linear layers with a non-linear activation constitutes the PFN. PFN is applied to every input token vector. Models like BERT uses Gelu [33] activation function.

(p8.1) Here the parameters of PFNs applied on each of the token vectors are the same. Formally,
## (s9) Add and Norm
### Question: What do Add and Norm represent in transformer encoder layers?

#Reference=0

(p9.0) Add represents residual connection while Norm represents layer normalization. Add and Norm is applied on both MHSA and PFN of transformer encoder to stay away from vanishing and exploding gradients. In general, a transformed-based PLM consists of a sequence of transformer encoder layers after the embedding layer. Each transformer encoder layer updates the input token vectors by encoding global contextual information. By updating the input token vector using a sequence of transformer encoders help the model to encode more language information. Formally,

(p9.1) Here LN represents Layer Normalization,Ê m−1 represents the output after applying Add and Norm over the output of MHSA and E m represents the output after applying Add and Norm over the output of PFN in m th encoder layer. Overall, E m represents the output of m th encoder layer with E m−1 as input. Here the input for the first transformer encoder layer is, E 0 = X. 
## (s10) Self-Supervised Learning
### Question: What is Self-Supervised Learning and its significance in AI?

#Reference=8

(p10.0) Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].

(p10.1) Robotics is the first AI field to use self-supervised learning methods [34]. Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38]. SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods. SSL is similar to unsupervised learning as it does not depend on human-labeled instances. It is also similar to supervised learning as it learns using supervision. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks. The pseudo labels are generated depending on the definitions of pre-training tasks. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34]. In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling). In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization). In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection). For more details about different SSL methods, please refer to the survey paper written by Liu et al. [34].
## (s13) Mixed-Domain Pretraining (MDPT)
### Question: What is mixed-domain pretraining and its types in NLP?

#Reference=8

(p13.0) Mixed domain pretraining involves training the model using both general and in-domain text. Depending on whether the pretraining is done simultaneously or not, mixed domain pretraining can be classified into a) Continual pretraining -initially the model is pre-trained over general domain text and then adapted to the biomedical domain [16] and b) Simultaneous pretraining -the model is pre-trained over the combined corpora having both general and in-domain text where the indomain text is up sampled to ensure balanced pretraining [21].

(p13.1) Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs. It is also referred to as further pretraining. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7). For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16]. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].

(p13.2) Simultaneous Pretraining (SPT) : Continual pretraining achieved good results by adapting general models to the biomedical domain [16], [18], [19], [39], [40]. However, it requires large volumes of in-domain text. Otherwise, CPT may result in suboptimal performance. Simultaneous pretraining comes to the rescue when only a small amount of in-domain text is available. Here, the pretraining corpora consist of both in-domain and general domain text where the in-domain text is up sampled to ensure a balanced pretraining (refer Figure  8). For example, BERT (jpCR+jpW) [41] is developed by simultaneous pretraining over a small amount of Japanese clinical text and a large amount of Japanese Wikipedia text. This model outperformed UTH-BERT in clinical text classification. UTH-BERT [42] is trained from scratch over Japanese clinical text. 
## (s14) Domain-Specific Pretraining (DSPT)
### Question: What is the main drawback of continual pretraining and how does Domain-Specific Pretraining address it?

#Reference=4

(p14.0) The main drawback in continual pretraining is the general domain vocabulary. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2]. As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10). For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles [20]. PubMed achieved state-of-the-art results in the BLURB benchmark. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11). In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences.
## (s16) Pretraining Tasks
### Question: What are pretraining tasks in language model development?

#Reference=3

(p16.0) During pretraining, the language models learn language representations based on the supervision provided by one or more pretraining tasks. A pretraining task is a pseudo-supervised task whose labels are generated automatically. A pretraining task can be main or auxiliary. The main pretraining tasks allow the model to learn language representations while auxiliary pretraining tasks allow the model to gain knowledge from human-curated sources like Ontology [34], [45]- [47]. The classification of pretraining tasks is given in Figure 12 and a brief summary of various pretraining tasks is presented in Table 1.
## (s17) Main Pretraining Tasks
### Question: What are the main pretraining tasks for learning language representations?

#Reference=4

(p17.0) The main pretraining tasks allow the model to learn language representations. Some of the commonly used main pretraining tasks are masked language modelling (MLM) [2], replaced token detection (RTD) [50], sentence boundary objective (SBO) [49], next sentence prediction (NSP) [2] and sentence order prediction (SOP) [15].

(p17.1) Masked Language Modeling (MLM). It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2]. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens. However, the meaning of a word depends on both the left and right contexts. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI. Less challenging as it involves topic prediction which is a relatively easy task.
## (s19) Auxiliary Pretraining Tasks
### Question: How do auxiliary pretraining tasks enhance in-domain models with human-curated knowledge sources?

#Reference=4

(p19.0) Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45]. This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models. Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.

(p19.1) Similarly, Liu et al. [34] used multi-similarity loss-based pretraining task to inject UMLS synonym knowledge into PubMedBERT.
## (s20) Fine-Tuning Methods
### Question: What are the key components and categories of fine-tuning methods in machine learning?

#Reference=0

(p20.0) Pretraining allows the model to learn general or indomain knowledge which is useful across the tasks.

(p20.1) However, for a model to perform well in a particular task, it must have task-specific knowledge along with general or in-domain knowledge. The model gains taskspecific knowledge by fine-tuning on the task-specific datasets. Task-specific layers are included on the top of transformer-based BPLMs. For example, to perform text classification, we need a) a contextual encoder to learn contextual token representations from the given input token vectors and b) a classifier to project the final sequence vector and then generate the probability vector. Here classifier is the task-specific layer which is usually a softmax layer in text classification. Fine-tuning methods fall into two categories.
## (s21) Intermediate Fine-Tuning (IFT)
### Question: What are the four ways to perform Intermediate Fine-Tuning (IFT) on datasets?

#Reference=10

(p21.0) IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58]. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].

(p21.1) Same Task Same Domain -Here, the source and target datasets are from the same task and domain. But the source dataset is a more generic one while the target dataset is more specific [62], [63]. For example, Gao et al. [63] fine-tuned BlueBERT on a large general biomedical NER corpus like MedMentions [64] or Semantic Medline before fine-tuning on the small target NER corpus.

(p21.2) Different Task Same Domain -Here, the source and target datasets are from different tasks but the same domain. Fine-tuning on source dataset which is from the same domain allows the model to gain more domainspecific knowledge which improves the performance of the model on the same domain target task [65]. McCreery et al. [65] fine-tuned the model on the medical questionanswer pairs dataset to enhance its performance on the medical question similarity dataset.

(p21.3) Different Task Different Domain -Here, the source and target datasets are from different tasks and different domains. For example, Jeong et al. [66] fine-tuned BioBERT on a general MultiNLI dataset to improve the performance of the model in biomedical QA. Here the model learns sentence level reasoning skills which are useful in biomedical QA.
## (s22) Multi-Task Fine-Tuning
### Question: What are the benefits and limitations of multi-task fine-tuning in machine learning models?

#Reference=6

(p22.0) Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. At the same time, due to the increase in training set size, the model is less prone to over-fitting. Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]. Moreover, having a single model for multitasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs [70]. Multi-task fine-tuning may not provide the best results all the time [70]. In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks [71]. For example, Mahanjan et al. [71] applied multi-task finetuning iteratively to choose the best subset of related datasets. Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS [72] dataset. After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model [73].
## (s25) Auxiliary Embeddings
### Question: What are auxiliary embeddings and their types in enhancing model learning?

#Reference=4

(p25.0) Main embeddings represent the given input sequence in low dimensional space. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].

(p25.1) Segment Embeddings -Segment embeddings help to Age Embeddings -In models like BEHRT [27] and BERT-EHR [32], age embeddings are used in addition to other embeddings. Age embeddings provide the age of the patient and help the model to leverage temporal information. Age embedding is the same for all the codes in a single patient visit.

(p25.2) Gender Embeddings -In models like BEHRT [27] and BERT-EHR [32], gender embeddings are used in addition to other embeddings. Gender embeddings provide the gender information of the patient to the model. Gender embedding is the same for all the codes in all the patient visits.

(p25.3) Semantic Group Embeddings -Semantic group embeddings are used in UmlsBERT [46] to explicitly inform the model to learn similar representations for words from the same semantic group i.e., semantic group embedding is same for all the words which fall into the same semantic group. Besides, it also helps to provide better representations for rare words. Figure 14 shows transformer-based BPLMs taxonomy.
## (s28) Electronic Health Records
### Question: How are Electronic Health Records utilized in medical research and patient care?

#Reference=11

(p28.0) In the last decade, most hospitals have been using Electronic Health Records (EHRs) to record patient as well as treatment details right from admission to discharge [82]. EHRs contain a vast amount of medical data which can be used to provide better patient care by knowledge discovery and the development of better algorithms. As EHR contains sensitive information related to patients, medical data must be de-identified before sharing. EHRs include both structured and unstructured data [83], [84]. Structured data includes laboratory test results, various medical codes, etc. Unstructured data include clinical notes like medication instructions, progress notes, discharge summaries, etc. Clinical notes include the most valuable patient information which is difficult and expensive to extract manually. So, there is a need for automatic information extraction methods to utilize the abundant medical data from EHRs in research as well as applications [84]- [86]. Following the success of transformer-based PLMs in the general domain, researchers in biomedical NLP also developed EHR-based T-BPLMs by pretraining over clinical notes or medical codes or both. MIMIC [87], [88] [19] further pre-trained XLNetbase on MIMIC-III Clinical Notes. The authors released two models namely a) pretrained model on nursing notes and b) pretrained model on discharge summaries. Yang et al. [39] further pre-trained general models like BERT, ELECTRA, RoBERTa, XLNet, and ALBERT on MIMIC-III and released in-domain PLMs. It is the first work to release in-domain models based on all the popular transformer-based PLMs. Unlike the above pretrained models which are pretrained on clinical text, recent works [27], [31], [39] released models which are pre-trained on disease codes or multi-modal EHR data. BEHRT [27] is trained from scratch using 1.6 million patient EHR data with MLM as pretraining task. The authors used code, position, age, and segment embeddings. Med-BERT [31] is trained from scratch using 28,490,650 patient EHR data with MLM and LOS (Length of Stay) as pretraining tasks. The authors used code, serialization and visit embeddings. BERT-EHR [32] is trained from scratch using multi-modal data from 43967 patient records with MLM as a pretraining task. Table 2 contains summary of various EHR based BPLMs.
## (s30) Social Media
### Question: How has social media transformed health-related information sharing and analysis?

#Reference=7

(p30.0) In the last decade, social media has become the first choice for internet users to express their thoughts. Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers [107], [108]. Recent works have shown that health-related social media data is useful in many applications to provide better health-related services [109], [110]. The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets [92]. This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos. Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts. CT-BERT [92] is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30% improvement compared to BERT-large, on five different classification datasets. BioRedditBERT [94] is initialized from BioBERT and further pretrained on healthrelated Reddit posts. The authors showed that BioRed-ditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in normalizing health-related entity mentions. RuDR-BERT [95] is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews). The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins. EnRuDR-BERT [95] and EnDR-BERT [95] are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively. Table 4 contains summary of social media text-based BPLMs.
## (s32) Hybrid Corpora
### Question: What is the purpose of using hybrid corpora in transformer-based PLM pretraining?

#Reference=9

(p32.0) It is difficult to obtain a large amount of in-domain text in some cases. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.  The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC). However, to pretrain a transformer-based PLM from scratch, we require large volumes of text. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97]. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.
## (s35) Ontology Enriched
### Question: How do T-BPLMs like BioBERT improve in biomedical NLP tasks through ontology enrichment?

#Reference=5

(p35.0) T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks. These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text. Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] . Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets. Here, pretraining involves three loss functions namely MLM, NSP, and triple classification. Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model. Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP. The novel multi-label loss function allows the model to connect all the words under the same CUI. CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss. Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge. SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss.  
## (s36) Green Models
### Question: What are Green Models and how do they adapt to in-domain vocabulary cost-effectively?

#Reference=2

(p36.0) CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords. This kind of representation increases the overall length of the input as well as hinders the model learning. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text. However, both these approaches involve learning the model parameters from scratch which is highly expensive. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].

(p36.1) Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123]. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction. This approach is completely inexpensive as it requires only CPU. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive. Table 9 contains summary of Green T-BPLMs.
## (s37) Debiased Models
### Question: How can debiasing improve fairness in Transformer-Based Pretrained Language Models (T-PLMs)?

#Reference=4

(p37.0) T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. Zhang et al. [127] further pretrained SciBERT [105] on MIMIC-III clinical notes and showed that the performance of the model is different for different groups. The authors applied adversarial pretraining debiasing to reduce the gender bias in the model. The authors released both the models publicly to encourage further research in debiasing T-BPLMs.
## (s38) Multi-Modal Models
### Question: What are the advancements in multi-modal models for the biomedical domain?

#Reference=8

(p38.0) T-PLMs have achieved success in many of the NLP tasks including in specific domains like Biomedical. Recent research works have focused on developing pretrained models that can handle multi-modal data i.e., video + text [128], [129], image + text [130]- [134] etc. In Biomedical domain, models like BERTHop [134] and Medical-VLBERT [133] have been proposed recently to handle image + text data. BERTHop [134] is a multi-modal T-BPLM developed for Chest X-ray disease diagnosis. Like ViLBERT [131] and LXMERT [132], BERTHop uses separate encoder to encoder image and text inputs. BERTHop uses PixelHop++ [135] to encode image data and BlueBERT as text encoder. Medical-VLBERT is developed for automatic report generation from COVID-19 scans. Unlike BERTHop, Medical-VLBERT [133] uses shared encoder based on VL-BERT [130] to encode image and text data.  
## (s40) Natural Language Inference
### Question: What is Natural Language Inference and its significance in NLP?

#Reference=12

(p40.0) Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain. NLI is framed as a three-way sentence pair classification problem. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%. Cengiz et al. [55] applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [59] and MultiNLI [60] and then finetuned them on MedNLI.
## (s41) Entity Extraction
### Question: What is entity extraction and its significance in various fields?

#Reference=20

(p41.0) Entity Extraction is the first step in unlocking valuable information in unstructured text data. Entity Extraction is useful in many tasks like entity linking, relation extraction, knowledge graph construction, etc. Extracting clinical entities like drug and adverse drug reactions is useful in pharmacovigilance and biomedical entities like proteins, chemicals and drugs is useful to discover knowledge in scientific literature. Some of the popular entity extraction datasets are I2B2 2010 [141], VAERS [142], CADEC [143], N2C2 2018 [144] , BC4CHEMD [145], B5CDR-Chem [146], JNLPBA [147] and NCBI-Disease [148].

(p41.1) Most of the existing work approaches entity extraction as sequence labeling or machine reading comprehension. In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer [62], [149], [150], BiLSTM+Softmax [150], BiLSTM+CRF [62], [151]- [153] or CRF [53], [62], [151] is applied. Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance [151], [152]. This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM. Some of the works experimented with general BERT for extracting clinical and biomedical entities. For example, Portelli et al. [53] showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text. Boudjellal et al. [104] developed ABioNER by further pretraining AraBERT [41] on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT [2] and AraBERT on Arabic biomedical entity extraction. This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model. As in-domain datasets are comparatively small, some of the recent works [62], [63], [154] initially fine-tuned the models on similar datasets before fine-tuning on small target datasets. This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets. For example, Gao et al. [63] proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning. Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels. Recently Sun et al. [62] formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ (Softmax / CRF / BiLSTM-CRF) on six datasets.
## (s42) Semantic Textual Similarity
### Question: What is Semantic Textual Similarity and its applications in various tasks?

#Reference=16

(p42.0) Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences. Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity. Both NLI and STS require sentence-level semantics. STS is useful in tasks like concept relatedness [139], medical concept normalization [137], [138] , duplicate text detection [155], question answering [65], [156] and text summarization [157]. Moreover, Reimers et al. [136] showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences. Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value. BIOSSES [158] and Clinical STS [72] are the commonly used datasets to train and evaluate indomain STS models.

(p42.1) Recent works exploited general models for clinical STS [56], [57], [65], [159]. For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71]. Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations. Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets. Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning. The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer. This is because the '[CLS]' vector contains only partial information. Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.
## (s43) Relation Extraction
### Question: What is relation extraction and its significance in information processing?

#Reference=12

(p43.0) Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text. Entity extraction followed by relation extraction helps to convert unstructured text into structured data. Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering. Some of relation extraction datasets are I2B2 2012 [162], AIMED [163], ChemProt [164], DDI [165], I2B2 2010 [141] and EU-ADR [166]. Wei et al. [167] achieved the best results on two datasets using MIMIC-BERT [40] +Softmax. Thillaisundaram and Togia [168] applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks [169]. Liu et al. [170] proposed SciBERT+Softmax for relation extraction in biomedical text. They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets. The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of '[CLS]' vector. Su et al. [171] added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets. The authors generated the final representation by concatenating '[CLS]' vector and weighted sum vector of final hidden state vectors. When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results.
## (s44) Text Classification
### Question: What is text classification and how is it implemented using transformer-based models?

#Reference=5

(p44.0) Text classification involves assigning one of the predefined labels to variable-length texts like phrases, sentences, paragraphs, or documents. Text classification involves an encoder which is usually a transformerbased PLM and a task-specific softmax classifier. '[CLS]' vector or weighted sum of final hidden state vectors is treated as an aggregate representation of given text. The fully connected dense layer in the classifier projects text representation vector into n-dimensional vector space where 'n' represents the number of predefined labels. Finally, the softmax function is applied to get the probabilities of all the labels. Garadi et al. [172] formulated prescription medication (PM) identification from tweets as four-way text classification. They achieved good results using models like BERT, RoBERTa, XLNet, ALBERT, and DistillBERT. They trained different ML-based meta classifiers with predictions from pre-trained models as inputs and further improved the results.

(p44.1) Shen et al. [173] applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT. They generated labels for the training instances using a rule-based NLP algorithm. Chen et al. [174] further pretrained the general BERT model on 1.5 drugrelated tweets and showed that further pretraining improves the performance of the model on ADR tweets classification. Recent works [175], [176] showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification. They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability.
## (s45) Question Answering
### Question: What are the challenges and advancements in biomedical question answering systems?

#Reference=11

(p45.0) Question Answering (QA) aims to extract answers for the given queries. QA helps to quickly find information in clinical notes or biomedical literature and thus saves a lot of time. The main challenge in developing automated QA systems for the clinical or biomedical domain is the small size of datasets. Developing large QA datasets in the clinical or biomedical domain is quite expensive and requires a lot of time also. Some of the popular in-domain QA datasets are emrQA [177], CliCR [178], PubMedQA [179] COVID-QA [180], MASH-QA [181] and Health-QA [182]. Chakraborty et al. [183] showed BioMedBERT obtained by further pretraining BERT-Large on BREATHE 1.0 corpus outperformed BioBERT on biomedical question answering. The main reason for this is the diversity of text in BREATHE 1.0 corpus. Pergola et al. [52] introduced Biomedical Entity Masking (BEM) which allows the model to learn entity-centric knowledge during further pretraining. They showed that BEM improved the performance of both general and indomain models on two in-domain QA datasets. Recent works used intermediate fine-tuning on general QA [58], [183] or NLI [66] datasets or multi-tasking [184] to improve the performance of in-domain QA models. For example, Soni et al. [183] achieved the best results on a) CliCR by intermediate fine-tuning on SQuaD using BioBERT and b) emrQA by intermediate fine-tuning on SQuaD and CliCR using Clinical BERT. Yoon et al. [58] showed that intermediate fine-tuning on general domain Squad datasets improves the performance on biomedical question answering datasets. Akdemir et al. [184] proposed a novel multi-task model based on BioBERT for biomedical question answering. They used biomedical NER as an auxiliary task and showed that transfer learning from the bioNER task improves performance on question answering tasks.
## (s46) Text Summarization
### Question: What are the methods and challenges in biomedical text summarization?

#Reference=7

(p46.0) In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature. Researchers and domain experts need to go through a number of biomedical documents. As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186]. Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187]. There are no standard datasets for biomedical text summarization. Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].

(p46.1) Moradi et al. [188] proposed a novel approach to summarize biomedical scientific articles. They embedded sentences, generated clusters, and then extracted the most informative sentences from each of the clusters. They showed that BERT-large outperformed other models including the in-domain BERT models like BioBERT.

(p46.2) In the case of small models, BioBERT outperformed others. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence. The sentences with the highest score are considered as the summary. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.
## (s47) EVALUATION
### Question: What are the benchmarks for evaluating pretrained models in NLP and biomedical research?

#Reference=0

(p47.0) Benchmarks are useful to evaluate the progress in pretrained models. GLUE is the first benchmark proposed to evaluate pretrained models. Following GLUE, a number of benchmarks are proposed in general NLP. Inspired by the benchmarks in general NLP, Biomedical research community proposed benchmarks like BLUE, BLURB and CBLUE. We summarize the performance of various T-BPLMs in Table 10.
## (s49) Low Cost Domain Adaptation
### Question: What are the challenges and alternatives in low-cost domain adaptation for T-BPLMs?

#Reference=3

(p49.0) The two popular approaches for developing T-BPLMs are MDPT and DSPT. These approaches involve pretraining on large volumes of in-domain text using highend GPUs or TPUs for days. These two approaches are quite successful in developing BPLMs. However, these approaches are quite expensive requiring high computing resources with long pretraining durations [122]. For example, BioBERT -it took around ten days to adapt general BERT to the biomedical domain using eight GPUs [16]. Moreover, DSPT is more expensive compared to continual pretraining as it involves learning model weights from scratch [122], [123]. So, there is a need for lost cost domain adaptation methods to adapt general Approach Description Pros Cons

(p49.1) Intermediate Fine-Tuning Model is fine-tuned on source dataset before fine-tuning on target dataset.

(p49.2) Allows the model to gain domain or task-specific knowledge.

(p49.3) Requirement of labeled datasets.
## (s52) Semi-Supervised Learning
### Question: What is semi-supervised learning and how does it apply to BERT models in biomedicine?

#Reference=4

(p52.0) Fine-tunes the model on training instances along with pseudo labeled instances

(p52.1) Allows the model to leverage task-related unlabelled instances.

(p52.2) Fine-tuning must be done iteratively to reduce the noisy labeled instances. BERT models to the biomedical domain. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44]. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191]. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123]. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].
## (s54) Small Datasets
### Question: How can performance on small datasets be improved for machine learning models?

#Reference=16

(p54.0) Pretraining on large volumes of in-domain text allows the model language representations while fine-tuning allows the model to learn task-specific knowledge. During fine-tuning the model must learn sufficient task-specific knowledge to achieve good results on the task where the input distribution, as well as label space, is different from pretraining [193], [194]. With small target datasets, the models are not able to learn enough task-specific which limits the performance. To over the small size of target datasets, the possible solutions are Intermediate Fine-Tuning -Intermediate fine-tuning on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets [55]- [58], [62], [63], [65], [66] and

(p54.1) Multi-Task Fine-Tuning -Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]- [71], [73].

(p54.2) Data Augmentation -Data augmentation helps us to create new training instances from existing instances. These newly creating training instances are close to original training data and helpful in low resource scenarios. Back translation and EDA [195] are the top popular techniques for data augmentation. For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model. The domain-specific ontologies like UMLS can also be used to augment the training instances [153].

(p54.3) Semi-Supervised Learning -Semi-supervised learning augments the training set with pseudo-labeled instances. The model which is fine-tuned on the original training set is used to label the task-related unlabelled instances. The model is again fine-tuned on the augmented training set and this process is repeated until the model converges [63], [196] . Table 11 contains a brief summary of these approaches.
## (s55) Robustness to Noise
### Question: How do transformed based PLMs improve robustness to noise in sensitive domains?

#Reference=4

(p55.0) Transformed based PLMs have achieved the best results in many of the tasks. However, the performance of these models on noisy test instances is limited [197]- [200]. This is because the model is mostly trained on less noisy instances. As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances. Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning. The robustness of models is crucial particularly insensitive domains like biomedical [199], [200]. Two possible solutions are a) CharBERT [28] -replaced the WordPiece based embedding layer with CharCNN based embedding layer. Here word representation is generated from character embeddings using CharCNN. b) Adversarial Training [200] -Here, the training set is augmented with the noisy instances. Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances.
## (s56) Quality In-Domain Word Representations
### Question: How can T-PLMs adapt to in-domain vocabulary for meaningful representation?

#Reference=3

(p56.0) Continual pretraining allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of in-domain text. Though the models are adapted to in-domain, they still contain general vocabulary. As the vocabulary is learned over general text, it mostly includes subwords and words which are specific to the general domain. As a result, many of the in-domain words are not represented in a meaningful way. The two possible options to represent in-domain words in a meaningful way are a) in-domain vocabulary through DSPT [20] b) extending the general vocabulary with indomain vocabulary [122], [123].
## (s57) Low Resource (In-Domain Corpus) Pretraining
### Question: What is simultaneous pretraining in low resource language model training?

#Reference=1

(p57.0) CPT or DSPT involves pretraining the language model on large volumes of in-domain text. During pretraining, the model learns language representations that are useful across many tasks. The size of the pretraining corpus influences how well the model learns the language representations. It is not possible to get a large volume of in-domain text all the time. In such scenarios with less in-domain corpus, the model may not learn well when trained using any of the above two methods. The possible solution for this is simultaneous pretraining. In simultaneous pretraining [21], the model is trained on combined corpora having both general and in-domain text. As the in-domain text is comparatively less, upsampling can be used to have a balanced pretraining.
## (s58) Quality Sequence Representation
### Question: What methods improve final sequence representation in text classification tasks?

#Reference=7

(p58.0) For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space. Finally, a softmax is applied to convert it into a vector of probabilities. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.
## (s60) Mitigating Bias
### Question: How can bias in deep learning models affect decision-making in healthcare?

#Reference=10

(p60.0) With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation [112], disease prediction [31], [32] etc. However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. Real-world datasets have a bias in many forms. It can be based on various attributes like gender, age, ethnicity, and marital status. These attributes are considered as protected or sensitive [201]. For example, in the MIMIC-III dataset [88] a) heart disease is more common in males compared to females-an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups -an example of ethnicity bias. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. There are few works that identified and addressed bias in transformer-based biomedical language models. Zhang et al. [127] using a simple word competition task showed that SciBERT [105] exhibits ethnicity bias. Moreover, the authors showed SciBERT model when further-pretrained on clinical notes exhibits performance differences for different protected attributes. They further showed that adversarial pretraining debiasing has little impact in reducing bias. Minot et al. [202] proposed an approach based on data augmentation to identify and reduce gender bias in patient notes. This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions.
## (s61) Privacy Issues
### Question: What are the privacy concerns related to clinical records and biomedical language models?

#Reference=3

(p61.0) Every patient visit is recorded in the clinical records. Apart from patient visits, clinical records contain the past and the present medical history of the patient. Such sensitive data should not be disclosed as it may harm the patients physically or mentally [203]. Usually, the clinical records are shared for research purposes only after de-identifying the sensitive information. However, it is possible to recover sensitive patient information from the de-identified medical records. Recent works showed that there is data leakage from pre-trained models in the general domain i.e., it is possible to recover personal information present in the pretraining corpora [204], [205]. Due to data leakage, the models pre-trained on proprietary corpora, cannot be released publicly. Recently, Nakamura et al. [203] proposed KART framework which can conduct various attacks to assess the leakage of sensitive information from pre-trained biomedical language models. We strongly believe there is a need for more work in this area to assess as well as address the data leakage in biomedical language models.
## (s62) Domain Adaptation
### Question: What are the challenges and solutions in domain adaptation for biomedical language models?

#Reference=3

(p62.0) In the beginning, the standard approach to develop BPLMs is to start with general PLMs and then further pretrain them on large volumes of biomedical text [16]. The main drawback of this approach is the lack of indomain vocabulary. Without domain-specific vocabulary, many of the in-domain are split into a number of subwords which hinders model learning during pretraining or fine-tuning. Moreover, continual pretraining is quite expensive as it involves pretraining on large volumes of unlabeled text. To overcome these drawbacks, there are low-cost domain adaptation approaches that extend the general domain vocabulary with in-domain vocabulary [122], [123]. The extra in-domain vocabulary is generated using Word2vec and then aligned [122] or generated directly using WordPiece [123] over biomedical text. The main drawback in these low-cost domain adaptation approaches is an increase in the size of the model with the addition of in-domain vocabulary. Further research on this topic can result in more novel methods for lowcost domain adaptation.
## (s63) Novel Pretraining Tasks
### Question: What are the limitations of MLM in biomedical language models and proposed improvements?

#Reference=1

(p63.0) Most of the biomedical language models (except ELECTRA-based models) are pre-trained using MLM. In MLM, only 15% of tokens are randomly masked and the model learns by predicting that 15% of masked tokens only. Here the main drawbacks are a) as tokens are randomly chosen for masking, the model may not learn much by predicting random tokens b) as only 15% of tokens are predicted, the training signal per example is less. So, the model has to see more examples to learn enough language information which results in the requirement of large pretraining corpora and more computational resources. There is a need for novel pretraining tasks like Replaced Token Detection (RTD) which can provide more training signal per example. Moreover, when the model is pretrained using multiple pretraining tasks, the model receives more training signals per example and hence can learn enough language information using less pretraining corpora and computational resources [206].
## (s64) Benchmarks
### Question: What is the purpose of benchmarks in evaluating NLP model performance?

#Reference=7

(p64.0) In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models. In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching). In biomedical domain there are three benchmarks namely BLUE [18], BLURB [20] and ChineseBLUE [48]. BLUE introduced by Peng et al. [18] contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets. The semantics of EHR and medical social media texts are different from biomedical scientific literature. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets.
## (s65) Intrinsic Probes
### Question: What are intrinsic probes and their role in evaluating PLMs' pretraining knowledge?

#Reference=5

(p65.0) During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14]. Intrinsic probes through light on the knowledge learned by PLMs during pretraining. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models. For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining. This is an area which requires much attention from Biomedical NLP community.
## (s66) Efficient Models
### Question: What are the benefits of efficient models like ConvBERT and DeBERTa in NLP pretraining?

#Reference=2

(p66.0) Pretraining provides BPLMs with necessary background knowledge which can be transferred to downstream tasks. However, pretraining is computationally very expensive and also requires large volumes of pretraining data. So, there is need of novel model architecture which reduces the pretraining time as well as the amount of pretraining corpus. In general NLP, recently efficient models like ConvBERT [215] and DeBERTa [216] are proposed which reduces the pretraining time and amount of pretraining corpus required respectively. DeBERTa with two novel improvements (Disentangled attention mechanism and enhanced masked decoder) achieves better compared to RoBERTa. DeBERTa is pretrained on just 78GB of data while RoBERTa is pretrained on 160GB of data. ConvBERT with mixed attention block outperforms ELECTRA while using just 1/4 th of its pretraining cost. Biomedical NLP research community must focus on developing pretrained models based on these novel model architectures.
