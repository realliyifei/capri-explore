# Multi-Task Learning in Natural Language Processing: An Overview

CorpusID: 237571793 - [https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a](https://www.semanticscholar.org/paper/760f807406272b5ede591f19241824f2d17c319a)

Fields: Computer Science, Linguistics

## (s2) Supervision at Different
### Question: How does multi-task learning utilize supervision at different feature levels for NLP tasks?

#Reference=16

(p2.0) Feature Levels. Models using the parallel architecture handle multiple tasks in parallel. These tasks may concern features at different abstraction levels. For NLP tasks, such levels can be character-level, token-level, sentence-level, paragraph-level, and document-level. It is natural to give supervision signals at different depths of an MTL model for tasks at different levels [20,80,102,109] as illustrated in Fig. 1c. For example, in [28,57], token-level tasks receive supervisions at lower-layers while sentence-level tasks receive supervision at higher layers. [96] supervises a higher-level QA task on both sentence and document-level features in addition to a sentence similarity prediction task that only relies on sentence-level features. In addition, [33,93] add skip connections so that signals from higher-level tasks are amplified. [11] learns the task of semantic goal navigation at a lower level and learns the task of embodied question answering at a higher level.

(p2.1) In some settings where MTL is used to improve the performance of a primary task, the introduction of auxiliary tasks at different levels could be helpful. Several works integrate a language modeling task on lower-level encoders for better performance on simile detection [97], sequence labeling [68], question generation [154], and task-oriented dialogue generation [154]. [62] adds sentence-level sentiment classification and attention-level supervision to assist the primary stance detection task. [85] adds attention-level supervision to improve consistency of the two primary language generation tasks. [16] minimizes an auxiliary cosine softmax loss based on the audio encoder to learn more accurate speech-to-semantic mappings.
## (s3) Hierarchical Architectures
### Question: What is hierarchical architecture and how does it function in task processing?

#Reference=4

(p3.0) The hierarchical architecture considers hierarchical relationships among multiple tasks. The features and output of one task can be used by another task as an extra input or additional control signals. The design of hierarchical architectures depends on the tasks at hand and is usually more complicated than parallel architectures. Fig. 2 illustrates different hierarchical architectures.

(p3.1) 2.2.1 Hierarchical Feature Fusion. Different from parallel feature fusion that combines features of different tasks at the same depth, hierarchical feature fusion can explicitly combine features at different depths and allow different processing for different features. To solve the Twitter demographic classification problem, [121] encodes the name, following network, profile description, and profile picture features of each user by different neural models and combines the outputs using an attention mechanism. [68] takes the hidden states for tokens in simile extraction as an extra feature in the sentence-level simile classification task. For knowledge base question answering, [24] combines lower level word and knowledge features with more abstract semantic and knowledge semantic features by a weighted sum. [124] fuses topic features of different roles into the main model via a gating mechanism. In [12], text and video features are combined through inter-modal attention mechanisms of different granularity to improve performance of sarcasm detection.
## (s5) Hierarchical
### Question: What is hierarchical interactive multi-task learning and how does it work?

#Reference=3

(p5.0) Interactive Architecture. Different from most machine learning models that give predictions in a single pass, hierarchical interactive MTL explicitly models the interactions between tasks via a multi-turn prediction mechanism which allows a model to refine its predictions over multiple steps with the help of the previous outputs from other tasks in a way similar to recurrent neural networks. [44] maintains a shared latent representation which is updated by iterations. Multi-step attention network [51] refines its prediction by attending to input representations in previous steps. In cyclic MTL [146], the output of one task is used as an extra input to its successive lower-level task and the output of the last task is fed to the first one, forming a loop. Most hierarchical interactive MTL models as introduced above report that performance converges quickly at = 2 steps, showing the benefit and efficiency of doing multi-step prediction.
## (s7) Generative Adversarial Architectures
### Question: How do Generative Adversarial Networks enhance computer vision and NLP tasks?

#Reference=7

(p7.0) Recently Generative Adversarial Networks (GANs) have achieved great success in generative tasks for computer vision. The basic idea of GANs is to train a discriminator that distinguishes generated images from ground truth ones. By optimizing the discriminator, we can obtain a generator that can produce more vivid images and a discriminator that is good at spotting synthesized images. A similar idea can be used in MTL for NLP tasks. By introducing a discriminator that predicts which task a given training instance comes from, the shared feature extractor is forced to produce more generalized task-invariant features [70,78,119,127,138] and therefore improve the performance and robustness of the entire model. In the training process of such models, the adversarial objective is usually formulated as where and denote model parameters for the feature extractor and discriminator, respectively, and denotes the one-hot task label.

(p7.1) An additional benefit of generative adversarial architectures is that unlabeled data can be fully utilized. [124] adds an auxiliary generative model that reconstructs documents from document representations learned by the primary model and improves the quality of document representations by training the generative model on unlabeled documents. To improve the performance of an extractive machine reading comprehension model, [98] uses a self-supervised approach. First, a discriminator that rates the quality of candidate answers is trained on labeled samples. Then, during unsupervised adversarial training, the answer extractor tries to obtain a high score from the discriminator.
## (s10) Data Sampling
### Question: How do data sampling techniques address imbalanced data in multi-task learning models?

#Reference=4

(p10.0) Machine learning models often suffer from imbalanced data distributions. MTL further complicates this issue in that training datasets of multiple tasks with potentially different sizes and data distributions are involved. To handle data imbalance, various data sampling techniques have been proposed to properly construct training datasets. In practice, given tasks and their datasets {D 1 , . . . , D }, a sampling weight is assigned to task to control the probability of sampling a data batch from D in each training step.

(p10.1) A straightforward sampling strategy is proportional sampling [102], where the probability of sampling from a task is proportional to the size of its dataset as ‚àù |D |.

(p10.2) (2) While adopted by many MTL models, proportional sampling (Eq. (2)) favors tasks with larger datasets, thus increasing the risk of overfitting. To alleviate this problem, task-oriented sampling [148] randomly samples the same amount of instances from all tasks, i.e.,

(p10.3) As a generalization of proportional sampling in Eq. (2), for task can take the following form as

(p10.4) where 1 is called the sampling temperature [128]. Similar to task loss weights, researchers have proposed various techniques to adjust . When < 1, the divergence of sampling probabilities is reduced. can be viewed as a hyperparameter to be set by users or be changed dynamically during training. For example, the annealed sampling method [113] adjusts as training proceeds. Given a total number of epochs, at epoch is set to

(p10.5) In this way, the model is trained more evenly for different tasks towards the end of the training process to reduce inter-task interference. [128] defines as

(p10.6) where 0 and denote initial and maximum values of , respectively. The noise level of the self-supervised denoising autoencoding task is scheduled similarly, increasing difficulty after a set warm-up period.
## (s13) Auxiliary MTL
### Question: What is Auxiliary MTL and how is it applied in NLP?

#Reference=12

(p13.0) Auxiliary MTL aims to improve the performance of certain primary tasks by introducing auxiliary tasks and is widely used in the NLP field for different types of primary task, including sequence tagging, classification, text generation, and representation learning. Table 1 generally show types of auxiliary tasks used along with different types of primary tasks. As shown in Table 1, auxiliary tasks are usually closely related to primary tasks.

(p13.1) Targeting sequence tagging tasks, [97] adds a language modeling objective into a sequence labeling model to counter the sparsity of named entities and make full use of training data. [3] adds five auxiliary tasks for scientific keyphrase boundary classification, including syntactic chunking, frame target annotation, hyperlink prediction, multi-word expression identification, and semantic super-sense tagging. [61] uses opinion word extraction and sentence-level sentiment identification to assist aspect term extraction. [50] trains an extractive summarization model together with an auxiliary document-level classification task. [137] transfers knowledge from a large opendomain corpus to the data-scarce medical domain for Chinese word segmentation by developing a parallel MTL architecture. HanPaNE [130] improves NER for chemical compounds by jointly training a chemical compound paraphrase model. [134] enhances Chinese semantic role labeling by adding a dependency parsing model and uses the output of dependency parsing as additional features. [84] improves the evidence extraction capability of an explainable multi-hop QA model by viewing evidence extraction as an auxiliary summarization task. [1] improves the character-level diacritic restoration with word-level syntactic diacritization, POS tagging, and word segmentation. In [15], the performance of argument mining is improved by the argument pairing task on review Table 1. A summary of auxiliary MTL studies according to types of primary and auxiliary tasks involved. 'W', 'S', and 'D' in the three rightmost columns represent word-level, sentence-level, and document-level tasks for auxiliary tasks, respectively. 'LM' denotes language modeling tasks and 'Gen' denotes text generation tasks. and rebuttal pairs of scientific papers. [58] makes use of the similarity between word sense disambiguation and metaphor detection to improve the performance of the latter task. To handle the primary disfluency detection task, [125] pre-trains two self-supervised tasks using constructed pseudo training data before fine-tuning on the primary task.
## (s16) Multi-lingual MTL
### Question: How does multi-lingual MTL benefit NLP models and facilitate cross-lingual knowledge transfer?

#Reference=8

(p16.0) Multi-lingual machine learning has always been a hot topic in the NLP field with a representative example as NMT systems mentioned in previous sections. Since a single data source may be limited and biased, leveraging data from multiple languages through MTL can benefit multi-lingual machine learning models. In [79], a partially shared multi-task model is built for language intent learning through dialogue act classification, extended named entity classification, and question type classification in Japanese and English. This model is further enhanced in [78] by adversarial training so that language-specific and task-specific components learn task-invariant and language-invariant features, respectively. [127] jointly learns sentiment classification models for microblog posts by the same user in Chinese (i.e., Weibo) and English (i.e., Twitter), where sentiment-related features between the two languages are mapped into a unified feature space via generative adversarial training.

(p16.1) Another aim of multi-lingual MTL is to facilitate efficient knowledge transfer between languages. [119] proposes a multi-lingual dialogue evaluation metric in English and Chinese. Through generative adversarial training, the shared encoder could produce high quality language-invariant features for the subsequent estimation process. The multi-lingual metric achieves a high correlation with human annotations and performs even better than corresponding monolingual counterparts. Given an English corpus with formality tags and unlabeled English-French parallel data, [86] develops a formality-sensitive translation system from English to French by jointly optimizing the formality transfer in English. The proposed system learns formality related features from English and performs competitively against existing models trained on parallel data with formality labels.

(p16.2) Cross-lingual knowledge transfer can also be achieved by learning high-quality cross-lingual language representations. [108] learns multi-lingual representations from two tasks. One task is the multi-lingual skip-gram task where a model predicts masked words according to both monolingual and cross-lingual contexts and another task is the cross-lingual sentence similarity estimation task using parallel training data and randomly selected negative samples. Unicoder [49] learns multi-lingual representations by sharing a single vocabulary and encoder between different languages and is pre-trained on five tasks, including masked language model (MLM), translation language model, cross-lingual MLM, cross-lingual word recovery, and cross-lingual paraphrase classification. Experiments show that removing any one of these tasks leads to a drop in performance, thus confirming the effectiveness of multi-task pre-training. In [65], a multi-lingual multi-task model is proposed to facilitate low-resource knowledge transfer by sharing model parameters at different levels. Besides training on the POS tagging and NER tasks, a domain adversarial method is used to align monolingual word embedding matrices in an unsupervised way. Experiments show that such cross-lingual embeddings could substantially boost performance under the low-resource setting.
## (s17) Multimodal MTL
### Question: How does Multimodal MTL enhance NLP tasks with cross-modal features?

#Reference=5

(p17.0) As one step further from multi-lingual machine learning, multimodal learning has attracted an increasing interest in the NLP research community. Researchers have incorporated features from other modalities, including auditory, visual, and cognitive features, to perform text-related cross-modal tasks. MTL is a natural choice for implicitly injecting multimodal features into a single model. [16] builds an end-to-end speech translation model, where the audio recordings in the source language is transformed into corresponding text in the target language. Besides the primary translation task and auxiliary speech recognition task, [16] uses pre-trained word embeddings to force the recognition model to capture the correct semantics from the source audio and help improve the quality of translation.

(p17.1) [89] builds a video captioning model with two auxiliary tasks. The auxiliary unsupervised video prediction task uses the encoder of the primary model to improve the ability of understanding visual features and the decoder is shared with the auxiliary text entailment generation task to enhance the inference capability. [12] handles the sarcasm detection using both video and text features through a novel cross-modal attention mechanism. Specifically, after a video is sliced into segments of utterances, the inter-segment inter-modal attention learns relationships between segments of the same utterance across different modalities while the intra-segment inter-modal attention learns cross-modal features for a single segment.

(p17.2) Through MTL, researchers could introduce linguistic signals into interactive agents that traditionally rely on visual inputs by the knowledge grounding. [80] incorporates a human cognitive process, the gaze behavior while reading, into a sentiment classification model by adding a gaze prediction task and obtains improved performance. [11] builds a semantic goal navigation system where agents could respond to natural language navigation commands.

(p17.3) In this system, a one-to-one mapping between visual feature maps and text tokens is established through a dualattention mechanism and the visual question answering and object detection tasks are added to enforce such an alignment. The resulting model could easily adapt to new instructions by adding new words together with the corresponding feature map extractors as extra channels. To comprehensively evaluate models with knowledge grounding, [115] proposes a multi-task benchmark for grounded language learning. In addition to the intended cross-modal task, such as video captioning and visual question answering, an object attribute prediction task and a zero-shot evaluation test are added to evaluate the quality and generalization ability of knowledge grounding.
## (s18) Task Relatedness in MTL
### Question: What factors influence the performance of multi-task learning in NLP?

#Reference=9

(p18.0) A key issue that affects the performance of MTL is how to properly choose a set of tasks for joint training. Generally, tasks that are similar and complementary to each other are suitable for multi-task learning, and there are some works that studies this issue for NLP tasks. For semantic sequence labeling tasks, [77] reports that MTL works best when the label distribution of auxiliary tasks has low kurtosis and high entropy. This finding also holds for rumor verification [53]. Similarly, [71] reports that tasks with major differences, such as implicit and explicit discourse classification, may not benefit much from each other. To quantitatively estimate the likelihood of two tasks benefiting from joint training, [104] proposes a dataset similarity metric which considers both tokens and their labels. The proposed metric is based on the normalized mutual information of the confusion matrix between label clusters of two datasets. Such similarity metrics could help identify helpful tasks and improve the performance of MTL models that are empirically hard to achieve through manual selection.

(p18.1) As MTL assumes certain relatedness and complementarity between the chosen tasks, the performance gain brought by MTL can in turn reveal the strength of such relatedness. [10] studies the pairwise impact of joint training among 11 tasks under 3 different MTL schemes and shows that MTL on a set of properly selected tasks outperforms MTL on all tasks. The harmful tasks either are totally unrelated to other tasks or possess a small dataset that can be easily overfitted. For dependency parsing problems, [54,91] claim that MTL works best for formalisms that are more similar. [22] models the interplay of the metaphor and emotion via MTL and reports that metaphorical features are beneficial to sentiment analysis tasks. Unicoder [49] presents results of jointly fine-tuning on different sets of languages as well as pairwise cross-language transfer among 15 languages, and finds that knowledge transfer between English, Spanish, and French is easier than other sets of languages.
## (s21) Multi-label Datasets.
### Question: How are multi-label datasets created and utilized in various research tasks?

#Reference=18

(p21.0) Instances in multi-label datasets have one label space for each task, i.e. ‚àÄ ‚â† , X = X = X , which makes it possible to optimize all task-specific components at the same time. In this case,

(p21.1) Multi-label datasets can be created by giving extra manual annotations to existing data. For example, [54,91] annotate dependency parse trees of three different formalisms for each text input. [121] labels Twitter posts with 4 demographic labels. [29] annotates two distinct sets of relations over the same set of underlying chemical compounds.

(p21.2) The extra annotations can be created automatically as well, resulting in a self-supervised multi-label dataset. Extra labels can be obtained using pre-defined rules [62,97]. In [56], to synthesize unlabeled dataset for the auxiliary unsupervised implicit discourse classification task, explicit discourse connectives (e.g., because, but, etc.) are removed from a large corpus and such connectives are used as implicit relation labels. [86] combines an English corpus with formality labels and an unlabeled English-French parallel corpus by random selection and concatenation to facilitate the joint training of formality style transfer and formality-sensitive translation. [116] uses hashtags to represent genres of tweet posts. [130] generates sentence pairs by replacing chemical named entities with their paraphrases in the PubChemDic database. Unicoder [49] uses translated text from the source language to fine-tune on the target language. [125] creates disfluent sentences by randomly repeating or inserting -grams. Besides annotating in the aforementioned ways, some researchers create self-supervised labels with the help of external tools or previously trained models. [107] obtains dominant word sense labels from WordNet [31]. [24] applies entity linking for QA data over databases through an entity linker. [33] assigns NER and segmentation labels for three tasks using an unsupervised dynamic programming method. [64] uses the output of a meta-network as labels for unsupervised training data. As a special case of multi-label dataset, mask orchestration [126] provides different parts of an instance to different tasks by applying different masks. That is, labels for one task may become the input for another task and vice versa.
