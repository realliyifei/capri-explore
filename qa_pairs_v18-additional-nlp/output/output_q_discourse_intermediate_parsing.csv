corpusid,title,url,section_title,num_reference,section,section_sentence_prefixed,qud_analysis
257386000,Assessment of the Healthcare Administration of Senior Citizens from Survey Data using Sentiment Analysis,https://www.semanticscholar.org/paper/687f93be9a6c5f9deb053ca8a2fa1875afeb7a5d,C. VADER Sentiment Analysis,6,"VADER is also known as Valence Aware Lexicon and Sentiment Reasoner. The VADER vocabulary was created using conventional sentiment lexicons. Also, this work offers machine learning techniques for sentiment analysis as well as sentiment intensity and orientation lexicons. In order to better understand how the public feels about different entities, this sentiment analysis methods try to identify the feelings of written reviews. Emotions are linked to many characteristics of a product or service as part of the analysis of consumer feedback data. Moreover, VADER sentiment outperformed seven sentiment analysis lexicons, either better or equally well. [18][19] [20].

According to a study by [21], VADER maintains and even improves on the advantages of conventional sentiment lexicons like LIWC or Linguistic Inquiry and Word Count: it is larger, yet just as easily examined, understood, and swiftly deployed without requiring substantial learning/training), and it is easily extended. VADER differs from LIWC in that it is both more perceptive of sentiment expressions in social media and more tolerant of generalization to other domains. This can be downloaded and used without charge from the website.

Other research confirms the ease of use of VADER's rulebased sentiment analysis. A compilation of lexical features and their corresponding emotion metrics make up this document. Several guidelines are developed based on the language's grammatical and syntactical usage, and these rules are utilized to assess the text's mood. VADER employs a rule-based method and assigns values to each word in the text in order to consider both the sentiment category and the intensity or strength of the text in addition to the sentiment category. It also performs far faster than machine learning algorithms [22] [23].

VADER excels across a range of domain types. Compared to machine learning techniques, VADER has a number of advantages. It is firstly quick and computationally effective. The second advantage is that the terminology and regulations of the VADER are clear and not hidden. Because of this, VADER is easy to understand, build upon, and alter. By setting the threshold at 0.05, VADER is a preferable option if processing the sentiment quickly and if it was the only thing that had been planned. VADER also adheres to grammatical and syntactical rules for expressing and highlighting sentiment intensity. VADER outperforms Text blob and NLTK sentiment analysis technologies in terms of performance. [24].

According to empirical findings, the technique utilized is the best technique for ranking many choices. Additionally, users of the healthcare sector's decision-making processes and healthcare providers' goals for quality improvement can both benefit from ranking information.","sent1: VADER is also known as Valence Aware Lexicon and Sentiment Reasoner.
sent2: The VADER vocabulary was created using conventional sentiment lexicons.
sent3: Also, this work offers machine learning techniques for sentiment analysis as well as sentiment intensity and orientation lexicons.
sent4: In order to better understand how the public feels about different entities, this sentiment analysis methods try to identify the feelings of written reviews.
sent5: Emotions are linked to many characteristics of a product or service as part of the analysis of consumer feedback data.
sent6: Moreover, VADER sentiment outperformed seven sentiment analysis lexicons, either better or equally well.
sent7: [18][19] [20]. According to a study by [21], VADER maintains and even improves on the advantages of conventional sentiment lexicons like LIWC or Linguistic Inquiry and Word Count: it is larger, yet just as easily examined, understood, and swiftly deployed without requiring substantial learning/training), and it is easily extended.
sent8: VADER differs from LIWC in that it is both more perceptive of sentiment expressions in social media and more tolerant of generalization to other domains.
sent9: This can be downloaded and used without charge from the website.
sent10: Other research confirms the ease of use of VADER's rulebased sentiment analysis.
sent11: A compilation of lexical features and their corresponding emotion metrics make up this document.
sent12: Several guidelines are developed based on the language's grammatical and syntactical usage, and these rules are utilized to assess the text's mood.
sent13: VADER employs a rule-based method and assigns values to each word in the text in order to consider both the sentiment category and the intensity or strength of the text in addition to the sentiment category.
sent14: It also performs far faster than machine learning algorithms
sent15: [22] [23].VADER excels across a range of domain types.
sent16: Compared to machine learning techniques, VADER has a number of advantages.
sent17: It is firstly quick and computationally effective.
sent18: The second advantage is that the terminology and regulations of the VADER are clear and not hidden.
sent19: Because of this, VADER is easy to understand, build upon, and alter.
sent20: By setting the threshold at 0.05, VADER is a preferable option if processing the sentiment quickly and if it was the only thing that had been planned.
sent21: VADER also adheres to grammatical and syntactical rules for expressing and highlighting sentiment intensity.
sent22: VADER outperforms Text blob and NLTK sentiment analysis technologies in terms of performance.[24].
sent23: According to empirical findings, the technique utilized is the best technique for ranking many choices.
sent24: Additionally, users of the healthcare sector's decision-making processes and healthcare providers' goals for quality improvement can both benefit from ranking information.","1. What is VADER also known as? sent1
2. How was the VADER vocabulary created? sent2
3. What does this work offer in terms of sentiment analysis? sent3
4. What is the purpose of the sentiment analysis methods mentioned in the text? sent4
5. How are emotions linked to consumer feedback data? sent5
6. How does VADER perform compared to other sentiment analysis lexicons? sent6
7. What are the advantages of VADER over conventional sentiment lexicons like LIWC? sent7
8. How does VADER differ from LIWC? sent8
9. Is VADER available for free download and use? sent9
10. What do other research findings confirm about VADER? sent10
11. What makes up the document mentioned in the text? sent11
12. How are guidelines developed and utilized in VADER? sent12
13. What method does VADER employ and what does it consider in text analysis? sent13
14. How does VADER's performance compare to machine learning algorithms? sent14
15. What are the advantages of VADER over machine learning techniques? sent16
    15.1. What is the first advantage of VADER? sent17
    15.2. What is the second advantage of VADER? sent18
        15.2.1. Why is VADER easy to understand, build upon, and alter? sent19
16. Under what condition is VADER a preferable option for processing sentiment? sent20
17. What rules does VADER adhere to for expressing sentiment intensity? sent21
18. How does VADER's performance compare to Text blob and NLTK sentiment analysis technologies? sent22
19. What do empirical findings suggest about the technique used? sent23
20. How can users in the healthcare sector benefit from ranking information? sent24"
259376518,Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of Biomedical Research Articles,https://www.semanticscholar.org/paper/3bef44ef3aacddaae56d6efc9f6767e64bc65b31,Pos. Team,5,"Relevance  Arizona Sky This team first truncate input documents, before using them to train two separate BART base models.

IKM_Lab (Wu et al., 2023) This team experimented with the use of a LED model trained on both datasets, as well as the adoption of different formats for including additional article information, such as keywords and section headings, in the input.

NCUEE-NLP (Chen et al., 2023) This team also made use of different models for each submission, including Primera (Xiao et al., 2022), a PEGASUS model (Zhang et al., 2020a) pretrained on PubMed, and a BART-large Longformer model.

himil The team experimented with both BERT (Devlin et al., 2019) and Longformer-based models, trained individually on each dataset.","sent1: Relevance  Arizona Sky This team first truncate input documents, before using them to train two separate BART base models.
sent2: IKM_Lab (Wu et al., 2023) This team experimented with the use of a LED model trained on both datasets, as well as the adoption of different formats for including additional article information, such as keywords and section headings, in the input.
sent3: NCUEE-NLP (Chen et al., 2023) This team also made use of different models for each submission, including Primera (Xiao et al., 2022), a PEGASUS model (Zhang et al., 2020a) pretrained on PubMed, and a BART-large Longformer model.
sent4: himil The team experimented with both BERT (Devlin et al., 2019) and Longformer-based models, trained individually on each dataset.","1. What approach did the Relevance Arizona Sky team use for training their models? sent1
2. How did the IKM_Lab team experiment with their model and input formats? sent2
3. What models did the NCUEE-NLP team use for their submissions? sent3
4. What models did the himil team experiment with, and how were they trained? sent4"
264833081,Emotion Detection for Misinformation: A Review,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,Interpretability,5,"Understanding how and why misinformation detection models have arrived at their decision about whether or not a post or news article represents true or fake information can be important to make their reasoning processes more transparent and make it easier to understand why errors occur.However, despite the high levels of performance achieved by many DL approaches, their black-box nature means that no such reasoning information is available, and that their decisions are hard to justify.Although it remains a challenge to develop models that are both sufficiently accurate and whose results are interpretable, several studies have proposed possible solutions for explainable misinformation detection.These include the use of topic-based features for classification [221], Explainable Artificial Intelligence (XAI) techniques [222] and Commonsense Knowledge Graphs [223].Recent research has also begun to focus on the development of interpretable LLMs [224], such as MentalLLaMA [225], which is an interpretable mental health analysis model based on LLaMA-2.Accordingly, it is hoped that researchers working in misinformation detection will begin to place greater emphasis on exploring the increasing range of options that could be used to improve the interpretability of their models.","sent1: Understanding how and why misinformation detection models have arrived at their decision about whether or not a post or news article represents true or fake information can be important to make their reasoning processes more transparent and make it easier to understand why errors occur.
sent2: However, despite the high levels of performance achieved by many DL approaches, their black-box nature means that no such reasoning information is available, and that their decisions are hard to justify.
sent3: Although it remains a challenge to develop models that are both sufficiently accurate and whose results are interpretable, several studies have proposed possible solutions for explainable misinformation detection.
sent4: These include the use of topic-based features for classification [221], Explainable Artificial Intelligence (XAI) techniques [222] and Commonsense Knowledge Graphs [223].Recent research has also begun to focus on the development of interpretable LLMs [224], such as MentalLLaMA [225], which is an interpretable mental health analysis model based on LLaMA-2.Accordingly, it is hoped that researchers working in misinformation detection will begin to place greater emphasis on exploring the increasing range of options that could be used to improve the interpretability of their models.","1. Why is it important to understand the reasoning processes of misinformation detection models? sent1
2. What challenge is associated with the black-box nature of many DL approaches in misinformation detection? sent2
3. What remains a challenge in developing misinformation detection models? sent3
4. What are some proposed solutions for explainable misinformation detection? sent4
    4.1. What recent research focus is mentioned regarding interpretable LLMs? sent4"
227061,Detection of User Demographics on Social Media: A Review of Methods and Recommendations for Best Practices,https://www.semanticscholar.org/paper/43b55cc08eba13fb309e77a1033da0ad60bd7a5e,Social Media and Public Health Research,21,"Digital traces generated by social media users offer a wealth of data for public health research. For example, studies using these data have focused on attitudes toward tobacco use [2]- [6] and vaccines [7]- [10], the onset of postpartum depression [11], sleep disorders [12], suicide risk [13], patientperceived quality of care in hospitals [14], the dynamics of infectious diseases [15]- [21], neighborhood level trends in health, diet and weight loss [22], [23], mapping of the presence of food deserts [24], monitoring of foodborne illness [25]- [28], the geographic distribution of fitness activity (specifically, cycling) [29], population representation [30], [31] etc.

There are several benefits to using social media data for public health research. First, social media data provide real-time updates on users' thoughts, feelings and experiences, allowing researchers to track users' attitudes and behaviors as they emerge. Both the population and individual-level scale of these data create the opportunity to study behaviors that are difficult to assess through traditional means of data collection [32]. Second, because social media posts are unsolicited, users may report opinions and behaviors with greater fidelity than they would in the context of interviews or surveys. Welldocumented forms of response bias such as, social desirability bias, present in the context of surveys and interviews [33], may be weaker within social media spaces. Factors that influence the emergence of response bias (e.g., a desire to withhold poor behaviors from someone conducting a study) may be absent or lessened within social media spaces. Third, compared to surveys and other traditional means of data collection, these data are low cost and the process of data collection can be automated.

However, there are some limitations regarding the use of these data. For example, similar to traditional data streams, social media data are not always representative of the population of interest. While researchers have proposed methods for minimizing this limitation -such as probabilistically adjusting the data to match the population under study [34] -identifying and quantifying Big Data bias is challenging. This is partly due to a lack of demographic indicators -such as age, race and gender. The ability to accurately and reliably detect users' demographic traits would expand the use of social media as a research tool for social and behavioral sciences and public health. Additionally, linking demographic information to social media data would allow researchers to study disparities and trends in health-related attitudes and behaviors. It should be noted that there are privacy and ethical concerns associated with the use of social media data and the application of methods to infer user demographics, which we discuss in a later section.","sent1: Digital traces generated by social media users offer a wealth of data for public health research.
sent2: For example, studies using these data have focused on attitudes toward tobacco use [2]- [6] and vaccines [7]- [10], the onset of postpartum depression [11], sleep disorders [12], suicide risk [13], patientperceived quality of care in hospitals [14], the dynamics of infectious diseases [15]- [21], neighborhood level trends in health, diet and weight loss [22], [23], mapping of the presence of food deserts [24], monitoring of foodborne illness [25]- [28], the geographic distribution of fitness activity (specifically, cycling) [29], population representation [30], [31] etc.There are several benefits to using social media data for public health research.
sent3: First, social media data provide real-time updates on users' thoughts, feelings and experiences, allowing researchers to track users' attitudes and behaviors as they emerge.
sent4: Both the population and individual-level scale of these data create the opportunity to study behaviors that are difficult to assess through traditional means of data collection [32].
sent5: Second, because social media posts are unsolicited, users may report opinions and behaviors with greater fidelity than they would in the context of interviews or surveys.
sent6: Welldocumented forms of response bias such as, social desirability bias, present in the context of surveys and interviews [33], may be weaker within social media spaces.
sent7: Factors that influence the emergence of response bias (e.g., a desire to withhold poor behaviors from someone conducting a study) may be absent or lessened within social media spaces.
sent8: Third, compared to surveys and other traditional means of data collection, these data are low cost and the process of data collection can be automated.
sent9: However, there are some limitations regarding the use of these data.
sent10: For example, similar to traditional data streams, social media data are not always representative of the population of interest.
sent11: While researchers have proposed methods for minimizing this limitation -such as probabilistically adjusting the data to match the population under study [34] -identifying and quantifying Big Data bias is challenging.
sent12: This is partly due to a lack of demographic indicators -such as age, race and gender.
sent13: The ability to accurately and reliably detect users' demographic traits would expand the use of social media as a research tool for social and behavioral sciences and public health.
sent14: Additionally, linking demographic information to social media data would allow researchers to study disparities and trends in health-related attitudes and behaviors.
sent15: It should be noted that there are privacy and ethical concerns associated with the use of social media data and the application of methods to infer user demographics, which we discuss in a later section.","1. What do digital traces generated by social media users offer for public health research? sent1
2. What are some examples of studies using social media data for public health research? sent2
3. What are the benefits of using social media data for public health research? sent3
    3.1. How do social media data provide real-time updates for researchers? sent3
    3.2. What opportunities do the scale of social media data create for studying behaviors? sent4
    3.3. Why might social media posts provide more accurate opinions and behaviors than interviews or surveys? sent5
        3.3.1. How does social media reduce response bias compared to traditional methods? sent6
        3.3.2. What factors influencing response bias might be absent or lessened in social media spaces? sent7
    3.4. How do social media data compare to traditional data collection methods in terms of cost and automation? sent8
4. What are some limitations of using social media data for public health research? sent9
    4.1. How are social media data similar to traditional data streams in terms of representativeness? sent10
    4.2. What methods have researchers proposed to minimize the limitation of representativeness in social media data? sent11
    4.3. Why is identifying and quantifying Big Data bias challenging? sent12
5. How would the ability to detect users' demographic traits impact the use of social media in research? sent13
6. What additional benefits would linking demographic information to social media data provide? sent14
7. What concerns are associated with the use of social media data and inferring user demographics? sent15"
244786380,A Review of Factors Affecting the Effectiveness of Phishing,https://www.semanticscholar.org/paper/b83c7598f874602facb4d8b13cc494960dbceb50,Reverse Shell,4,"A reverse shell is a connection shell that is virtual and open to the attacker""s machine that initiates from the victims"" device. With an open connection, an attacker is capable of running scripts and executing various evil commands on the victim""s machine. This mechanism is normally used by many common RATs and backdoors. The hacker gains the privileges of the user who was logged in when the session was initiated and may work to elevate privileges to gain further access. Most reverse shells use the Transmission Control Protocol (TCP) for the establishment, but the Internet Control Message Protocol (ICMP) has also been observed in use. Any port may be used to create the communication link. When hiding under allowed ports, the firewall and other intrusion detection systems face difficulty in identifying an incident because the ports are approved to communicate. When the shell is run under port 443 Secure Sockets Layer (SSL), content inspection is very hard as the traffic is encrypted. (Lu, 2019) Virus A computer virus is a dangerous piece of code that self-replicates by infecting other programs within its reach by injecting malicious code in them. Many viruses are dependent on some kind of executable, of which they are hosted by, before they may be able to start running their code. They may become very severe, to the point where they may completely ruin all of your hardware and software. A virus cannot propagate without human intervention. This is what makes it different from a worm. A virus needs some kind of host program to keep it in action. Viruses may be spread very rapidly and unintentionally as victims may tend to share poisoned files or send attachments in the email that have been infected. (Kumar & Dey, 2019) Worm A computer worm is a malicious program that replicates on its own without the need of interacting with any other file, and spreads across machines in a network. It transmits copies of its code throughout the network without attaching to any programs like in the case of a virus. They normally consume a large amount of the network bandwidth and cause a lack of service availability to users on the network. The first case of a worm propagating over the internet was the Morris worm released by Robert Tappan Morris on November 2, 1988. (Jajoo, 2017) Rootkit A rootkit is a program written primarily for evading detection while maintaining privileged access to the system. Host Intrusion Detection Systems (HIDS), Host Intrusion Prevention Systems (HIPS), Network Intrusion Detection Systems (NIDS), Network Intrusion Prevention Systems (NIPS), firewalls, Security Information and Event Management (SIEM) solutions all find it very hard to detect rootkits as they can modify their underlying operating system kernel code that they utilized for detection and thus usurping the control of security. (Nadim et al., 2021) Despite the various technical attacks which a hacker may deploy on a target, psychological attacks are the most dangerous. Victims"" emotions are the most vulnerable asset in a phishing operation and it is the attacker's main priority. To efficiently be able to reduce the effects of such an attack, it is essential to ask ourselves, what emotion are they trying to break? Our study focuses on applying machine learning to train a model to be able to classify the emotion present from a group of words. The model is trained using a large dataset of social sentences and phrases with differing emotions. We match positive emotions to positive sentiments, and vice versa. Likewise, neutral emotions are matched to neutral sentiments. The model is trained against the resulting dataset of emotions and related text.

Section 2 is a literature review section where we shall look at various contributions to detect, improve awareness, or counteract phishing. Section 3 is the methodology section. Insights of the attacking methods hackers use to achieve their phishing attempts are discussed here. The implementation methodology of our emotion detection scheme to capture the emotion attackers exploit in their phishing email is explained. Section 4 is the results and discussions section. We talk of the findings of what factors motivate hackers to perform phishing and make victims more susceptible to phishing attacks. We also look into how our emotion detection scheme performs. Section 5 is the conclusion section where we conclude our exploratory research review and propose future studies to enhance phishing detection schemes.","sent1: A reverse shell is a connection shell that is virtual and open to the attacker""s machine that initiates from the victims"" device.
sent2: With an open connection, an attacker is capable of running scripts and executing various evil commands on the victim""s machine.
sent3: This mechanism is normally used by many common RATs and backdoors.
sent4: The hacker gains the privileges of the user who was logged in when the session was initiated and may work to elevate privileges to gain further access.
sent5: Most reverse shells use the Transmission Control Protocol (TCP) for the establishment, but the Internet Control Message Protocol (ICMP) has also been observed in use.
sent6: Any port may be used to create the communication link.
sent7: When hiding under allowed ports, the firewall and other intrusion detection systems face difficulty in identifying an incident because the ports are approved to communicate.
sent8: When the shell is run under port 443 Secure Sockets Layer (SSL), content inspection is very hard as the traffic is encrypted.
sent9: (Lu, 2019) Virus A computer virus is a dangerous piece of code that self-replicates by infecting other programs within its reach by injecting malicious code in them.
sent10: Many viruses are dependent on some kind of executable, of which they are hosted by, before they may be able to start running their code.
sent11: They may become very severe, to the point where they may completely ruin all of your hardware and software.
sent12: A virus cannot propagate without human intervention.
sent13: This is what makes it different from a worm.
sent14: A virus needs some kind of host program to keep it in action.
sent15: Viruses may be spread very rapidly and unintentionally as victims may tend to share poisoned files or send attachments in the email that have been infected.
sent16: (Kumar & Dey, 2019) Worm A computer worm is a malicious program that replicates on its own without the need of interacting with any other file, and spreads across machines in a network.
sent17: It transmits copies of its code throughout the network without attaching to any programs like in the case of a virus.
sent18: They normally consume a large amount of the network bandwidth and cause a lack of service availability to users on the network.
sent19: The first case of a worm propagating over the internet was the Morris worm released by Robert Tappan Morris on November 2, 1988.
sent20: (Jajoo, 2017) Rootkit A rootkit is a program written primarily for evading detection while maintaining privileged access to the system.
sent21: Host Intrusion Detection Systems (HIDS), Host Intrusion Prevention Systems (HIPS), Network Intrusion Detection Systems (NIDS), Network Intrusion Prevention Systems (NIPS), firewalls, Security Information and Event Management (SIEM) solutions all find it very hard to detect rootkits as they can modify their underlying operating system kernel code that they utilized for detection and thus usurping the control of security.
sent22: (Nadim et al., 2021) Despite the various technical attacks which a hacker may deploy on a target, psychological attacks are the most dangerous.
sent23: Victims"" emotions are the most vulnerable asset in a phishing operation and it is the attacker's main priority.
sent24: To efficiently be able to reduce the effects of such an attack, it is essential to ask ourselves, what emotion are they trying to break?
sent25: Our study focuses on applying machine learning to train a model to be able to classify the emotion present from a group of words.
sent26: The model is trained using a large dataset of social sentences and phrases with differing emotions.
sent27: We match positive emotions to positive sentiments, and vice versa.
sent28: Likewise, neutral emotions are matched to neutral sentiments.
sent29: The model is trained against the resulting dataset of emotions and related text.
sent30: Section 2 is a literature review section where we shall look at various contributions to detect, improve awareness, or counteract phishing.
sent31: Section 3 is the methodology section.
sent32: Insights of the attacking methods hackers use to achieve their phishing attempts are discussed here.
sent33: The implementation methodology of our emotion detection scheme to capture the emotion attackers exploit in their phishing email is explained.
sent34: Section 4 is the results and discussions section.
sent35: We talk of the findings of what factors motivate hackers to perform phishing and make victims more susceptible to phishing attacks.
sent36: We also look into how our emotion detection scheme performs.
sent37: Section 5 is the conclusion section where we conclude our exploratory research review and propose future studies to enhance phishing detection schemes.","1. What is a reverse shell and how does it function? sent1
    1.1. What can an attacker do with an open connection in a reverse shell? sent2
    1.2. Which tools commonly use the reverse shell mechanism? sent3
    1.3. What privileges does a hacker gain through a reverse shell, and how can they escalate them? sent4
    1.4. What protocols are used by most reverse shells for establishment? sent5
    1.5. How can any port be used in a reverse shell, and what challenges does this pose for detection? sent6, sent7
    1.6. Why is content inspection difficult when a reverse shell runs under port 443 SSL? sent8
2. What is a computer virus and how does it propagate? sent9
    2.1. What do many viruses depend on to start running their code? sent10
    2.2. How severe can a virus become, and what makes it different from a worm? sent11, sent12, sent13
    2.3. What is required for a virus to remain active, and how can it spread rapidly? sent14, sent15
3. What is a computer worm and how does it differ from a virus? sent16
    3.1. How does a worm transmit its code across a network? sent17
    3.2. What impact do worms have on network bandwidth and service availability? sent18
    3.3. What was the first case of a worm propagating over the internet? sent19
4. What is a rootkit and what is its primary purpose? sent20
    4.1. Why is it difficult for security systems to detect rootkits? sent21
5. What are the most dangerous types of attacks according to the text, and why? sent22
6. What is the main priority of an attacker in a phishing operation? sent23
7. How can the effects of a phishing attack be reduced according to the study? sent24
8. What does the study focus on in terms of machine learning and emotion detection? sent25
    8.1. How is the model trained for emotion detection? sent26
    8.2. How are emotions matched to sentiments in the study? sent27, sent28
    8.3. Against what dataset is the model trained? sent29
9. What is covered in Section 2 of the paper? sent30
10. What is the focus of Section 3 in the paper? sent31
    10.1. What insights are discussed in Section 3? sent32
    10.2. What methodology is explained in Section 3? sent33
11. What is discussed in Section 4 of the paper? sent34
    11.1. What findings are discussed regarding phishing in Section 4? sent35
    11.2. What is evaluated in Section 4 regarding the emotion detection scheme? sent36
12. What is the purpose of Section 5 in the paper? sent37"
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,Textual Instructions,4,"LLM show ability to follow explicit instructions even in zero-shot scenarios (Ouyang et al., 2022;Sanh et al., 2022).Inspired by this, some work finds explicitly prompting LLM with an active tex-tual instruction like ""Let's think step by step"" can guide a progressive reasoning (Kojima et al., 2022;Zhou et al., 2022d).Without any demonstrations, this simple zero-shot strategy shows impressive result comparing to non-CoT methods, implying these textual instructions can similarly elicit the reasoning ability of LLM.Some work also finds combining these textual instruction with few-shot CoT can achieve a further performance increment (Kojima et al., 2022).","sent1: LLM show ability to follow explicit instructions even in zero-shot scenarios (Ouyang et al., 2022;Sanh et al., 2022).Inspired by this, some work finds explicitly prompting LLM with an active tex-tual instruction like ""Let's think step by step"" can guide a progressive reasoning (Kojima et al., 2022;Zhou et al., 2022d).Without any demonstrations, this simple zero-shot strategy shows impressive result comparing to non-CoT methods, implying these textual instructions can similarly elicit the reasoning ability of LLM.Some work also finds combining these textual instruction with few-shot CoT can achieve a further performance increment (Kojima et al., 2022).","1. What ability do LLMs show in zero-shot scenarios? sent1
    1.1. How can explicitly prompting LLMs with textual instructions guide reasoning? sent1
    1.2. What is the result of using a simple zero-shot strategy with textual instructions compared to non-CoT methods? sent1
    1.3. What further performance increment can be achieved by combining textual instructions with few-shot CoT? sent1"
53114081,Overview of CAIL2018: Legal Judgment Prediction Competition,https://www.semanticscholar.org/paper/6556dcbb5433a01fc75711ccd4ac44522f7d7952,General Architecture,9,"Pre-processing. For most contestants, they conduct the following pre-processing steps to transform the raw documents into the format which is suitable for their models.

• Word Segmentation. As all the documents are written in Chinese, it is important for the contestants to conduct a high-quality word segmentation. For word segmentation, the contestants usually choose jieba 2 , ICTCLAS 3 , THULAC 4 or other Chinese word segmentation tools.

• Word Embedding. After word segmentation, we need to transform the discrete word symbols into continuous word embeddings.

Generally, the contestants employ word2vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fast-Text (Joulin et al., 2017) to pre-train word embeddings on these criminal cases.

Text Classification Models. After preprocessing, we need to classify these processed fact de-scriptions into corresponding categories. For most contestants, they employ existing neural network based text classification models to extract efficient text features. The most commonly used text classification models are listed as follows:

• Text-CNN (Kim, 2014b): CNN with multiple filter widths.

• LSTM (Hochreiter and Schmidhuber, 1997)) or bidirectional LSTM.

• GRU, Gated Recurrent Unit (Cho et al., 2014).

• HAN, Hierarchical Attention Networks (Yang et al., 2016).

• RCNN, Recurrent Convolutional Neural Networks (Lai et al., 2015).

• DPCNN, Deep Pyramid Convolutional Neural Networks (Johnson and Zhang, 2017).

According to the technical reports of contestants, it has been proven that these neural models can achieve good performance in high-frequency categories.","sent1: Pre-processing. For most contestants, they conduct the following pre-processing steps to transform the raw documents into the format which is suitable for their models.
sent2: • Word Segmentation. As all the documents are written in Chinese, it is important for the contestants to conduct a high-quality word segmentation.
sent3: For word segmentation, the contestants usually choose jieba 2 , ICTCLAS 3 , THULAC 4 or other Chinese word segmentation tools.
sent4: • Word Embedding. After word segmentation, we need to transform the discrete word symbols into continuous word embeddings.
sent5: Generally, the contestants employ word2vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fast-Text (Joulin et al., 2017) to pre-train word embeddings on these criminal cases.
sent6: Text Classification Models. After preprocessing, we need to classify these processed fact de-scriptions into corresponding categories.
sent7: For most contestants, they employ existing neural network based text classification models to extract efficient text features.
sent8: The most commonly used text classification models are listed as follows:• Text-CNN (Kim, 2014b): CNN with multiple filter widths.
sent9: • LSTM (Hochreiter and Schmidhuber, 1997)) or bidirectional LSTM.
sent10: • GRU, Gated Recurrent Unit (Cho et al., 2014).
sent11: • HAN, Hierarchical Attention Networks (Yang et al., 2016).
sent12: • RCNN, Recurrent Convolutional Neural Networks (Lai et al., 2015).• DPCNN, Deep Pyramid Convolutional Neural Networks (Johnson and Zhang, 2017).
sent13: According to the technical reports of contestants, it has been proven that these neural models can achieve good performance in high-frequency categories.","1. What pre-processing steps do most contestants conduct to transform raw documents for their models? sent1
    1.1. Why is word segmentation important for the contestants? sent2
        1.1.1. What tools do contestants usually choose for word segmentation? sent3
    1.2. What is the next step after word segmentation in the pre-processing phase? sent4
        1.2.1. Which methods are generally employed by contestants to pre-train word embeddings? sent5
2. What is the purpose of text classification models in the competition? sent6
    2.1. What type of models do most contestants employ for text classification? sent7
    2.2. What are the most commonly used text classification models? sent8, sent9, sent10, sent11, sent12
3. What has been proven about the performance of these neural models according to the technical reports of contestants? sent13"
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,Topic 2: Clinical Care Practices During the COVID-19 Pandemic,5,"A total of 17.71% (5118/28,904) of all included publications were mainly about clinical care practices for non-COVID-19 patients during the COVID-19 pandemic (eg, [29][30][31][32][33]). The following authors published the highest number of publications related to this topic: Karthik Rajasekaran (n=14), Francesco Esperto (n=12), Raju Vaishya (n=9), Namrata Sharma (n=8), and Santosh G Honavar (n=8). The top 5 journals publishing articles related to this topic were Otolaryngology-Head and Neck Surgery (n=115), the Journal of the European Academy of Dermatology and Venereology (n=45), Cureus Journal of Medical Science (n=41), Anaesthesia (n=40), and World Neurosurgery (n=35). In this cluster, the first article was published on January 3, 2020. There was a considerable rise in the number of weekly publications from week 12 until it reached a peak in week 23 (n=479); this was followed by a sharp decrease (Multimedia Appendix 1). The mean number of weekly publications in this cluster was 175.2 (SD 159.6).","sent1: A total of 17.71% (5118/28,904) of all included publications were mainly about clinical care practices for non-COVID-19 patients during the COVID-19 pandemic (eg, [29][30][31][32][33]).
sent2: The following authors published the highest number of publications related to this topic: Karthik Rajasekaran (n=14), Francesco Esperto (n=12), Raju Vaishya (n=9), Namrata Sharma (n=8), and Santosh G Honavar (n=8).
sent3: The top 5 journals publishing articles related to this topic were Otolaryngology-Head and Neck Surgery (n=115), the Journal of the European Academy of Dermatology and Venereology (n=45), Cureus Journal of Medical Science (n=41), Anaesthesia (n=40), and World Neurosurgery (n=35).
sent4: In this cluster, the first article was published on January 3, 2020.
sent5: There was a considerable rise in the number of weekly publications from week 12 until it reached a peak in week 23 (n=479); this was followed by a sharp decrease (Multimedia Appendix 1).
sent6: The mean number of weekly publications in this cluster was 175.2 (SD 159.6).","1. What percentage of publications were about clinical care practices for non-COVID-19 patients during the COVID-19 pandemic? sent1
2. Who are the authors with the highest number of publications related to clinical care practices during the COVID-19 pandemic? sent2
3. What are the top 5 journals publishing articles related to clinical care practices during the COVID-19 pandemic? sent3
4. When was the first article in this cluster published? sent4
5. How did the number of weekly publications change over time in this cluster? sent5
6. What was the mean number of weekly publications in this cluster? sent6"
264833081,Emotion Detection for Misinformation: A Review,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,Large Language Models,5,"The popularity of ChatGPT and GPT-4 [9] has resulted in the powerful capabilities of LLMs becoming widely known [226].As mentioned above, there is potential for LLMs to be employed in misinformation detection in multiple ways, including sentiment and emotion detection, multimodal analysis, and to enhance the interpretability of detection models.Some studies have additionally begun to explore the use of LLMs for rumor and fake news prediction.For example, Hu et al. [25] designed a framework for fake news detection in which a small language model (i.e., BERT) is complemented by an LLM, which provides multi-perspective guiding principles to improve prediction accuracy.Meanwhile, Pavlyshenko et al. [26] designed prompts to fine-tune LLaMA for rumor and fake news detection.Cheung et al [27] used external knowledge to bridge the gap between knowledge encoded in the LLM and the most up-to-date information available on the Internet, in order to enhance fake news detection performance.The promising results achieved by these approaches, combined with the indisputable power and advanced capabilities of LLMs, motivate further exploration of how they can be best exploited to further improve the accuracy of rumor and fake news detection.","sent1: The popularity of ChatGPT and GPT-4 [9] has resulted in the powerful capabilities of LLMs becoming widely known [226].As mentioned above, there is potential for LLMs to be employed in misinformation detection in multiple ways, including sentiment and emotion detection, multimodal analysis, and to enhance the interpretability of detection models.
sent2: Some studies have additionally begun to explore the use of LLMs for rumor and fake news prediction.
sent3: For example, Hu et al. [25] designed a framework for fake news detection in which a small language model (i.e., BERT) is complemented by an LLM, which provides multi-perspective guiding principles to improve prediction accuracy.
sent4: Meanwhile, Pavlyshenko et al. [26] designed prompts to fine-tune LLaMA for rumor and fake news detection.
sent5: Cheung et al [27] used external knowledge to bridge the gap between knowledge encoded in the LLM and the most up-to-date information available on the Internet, in order to enhance fake news detection performance.
sent6: The promising results achieved by these approaches, combined with the indisputable power and advanced capabilities of LLMs, motivate further exploration of how they can be best exploited to further improve the accuracy of rumor and fake news detection.","1. What has the popularity of ChatGPT and GPT-4 resulted in regarding LLMs? sent1
    1.1. How can LLMs be employed in misinformation detection? sent1
2. What additional uses of LLMs are being explored in studies? sent2
    2.1. How did Hu et al. design a framework for fake news detection using LLMs? sent3
    2.2. How did Pavlyshenko et al. utilize LLaMA for rumor and fake news detection? sent4
    2.3. How did Cheung et al. enhance fake news detection performance using external knowledge? sent5
3. What motivates further exploration of LLMs in rumor and fake news detection? sent6"
56657817,Analysis Methods in Neural Language Processing: A Survey,https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,Linguistic Phenomena,5,"Different kinds of linguistic information have been analyzed, ranging from basic properties like sentence length, word position, word presence, or simple word order, to morphological, syntactic, and semantic information. Phonetic/phonemic information, speaker information, and style and accent information have been studied in neural network models for speech, or in joint audio-visual models. See Table SM1 for references.

While it is difficult to synthesize a holistic picture from this diverse body of work, it appears that neural networks are able to learn a substantial amount of information on various linguistic phenomena. These models are especially successful at capturing frequent properties, while some rare properties are more difficult to learn. Linzen et al. (2016), for instance, found that long short-term memory (LSTM) language models are able to capture subject-verb agreement in many common cases, while direct supervision is required for solving harder cases.

Another theme that emerges in several studies is the hierarchical nature of the learned representations. We have already mentioned such findings regarding NMT (Shi et al., 2016b) and a visually grounded speech model (Alishahi et al., 2017). Hierarchical representations of syntax were also reported to emerge in other RNN models (Blevins et al., 2018).

Finally, a couple of papers discovered that models trained with latent trees perform better on natural language inference (NLI) (Williams et al., 2018;Maillard and Clark, 2018) than ones trained with linguistically annotated trees. Moreover, the trees in these models do not resemble syntactic trees corresponding to known linguistic theories, which casts doubts on the importance of syntax-learning in the underlying neural network. 6","sent1: Different kinds of linguistic information have been analyzed, ranging from basic properties like sentence length, word position, word presence, or simple word order, to morphological, syntactic, and semantic information.
sent2: Phonetic/phonemic information, speaker information, and style and accent information have been studied in neural network models for speech, or in joint audio-visual models.
sent3: See Table SM1 for references. While it is difficult to synthesize a holistic picture from this diverse body of work, it appears that neural networks are able to learn a substantial amount of information on various linguistic phenomena.
sent4: These models are especially successful at capturing frequent properties, while some rare properties are more difficult to learn.
sent5: Linzen et al. (2016), for instance, found that long short-term memory (LSTM) language models are able to capture subject-verb agreement in many common cases, while direct supervision is required for solving harder cases.
sent6: Another theme that emerges in several studies is the hierarchical nature of the learned representations.
sent7: We have already mentioned such findings regarding NMT (Shi et al., 2016b) and a visually grounded speech model (Alishahi et al., 2017).
sent8: Hierarchical representations of syntax were also reported to emerge in other RNN models (Blevins et al., 2018).
sent9: Finally, a couple of papers discovered that models trained with latent trees perform better on natural language inference (NLI) (Williams et al., 2018;Maillard and Clark, 2018) than ones trained with linguistically annotated trees.
sent10: Moreover, the trees in these models do not resemble syntactic trees corresponding to known linguistic theories, which casts doubts on the importance of syntax-learning in the underlying neural network.
sent11: 6","1. What kinds of linguistic information have been analyzed in neural network models? sent1
2. What additional types of information have been studied in neural network models for speech or joint audio-visual models? sent2
3. What is the challenge in synthesizing a holistic picture from the diverse body of work on linguistic phenomena in neural networks? sent3
4. How successful are neural networks at capturing linguistic phenomena, and what are they particularly good at capturing? sent4
5. What did Linzen et al. (2016) find about LSTM language models and subject-verb agreement? sent5
6. What is another theme that emerges in studies of neural network models regarding learned representations? sent6
    6.1. What findings have been mentioned regarding NMT and visually grounded speech models? sent7
    6.2. What has been reported about hierarchical representations of syntax in other RNN models? sent8
7. What did some papers discover about models trained with latent trees in comparison to those trained with linguistically annotated trees? sent9
8. What doubts are cast by the findings regarding the importance of syntax-learning in neural networks? sent10"
252724722,A Review of the Research on the Evaluation Metrics for Automatic Grammatical Error Correction System,https://www.semanticscholar.org/paper/4241a0cb51c3780c57cae8b4f4d28341ccba2176,GLEU.,5,"A variant of the machine translation system measure BLEU (bilateral evaluation understudy) [19], called GLEU (generalized language evaluation understanding), was put forth by IBM researchers in 2001. GLEU is largely used to assess the output of machine translation models, which is typically between 0.0 and 1.0. If the two sentences match perfectly, BLEU � 1.0. Conversely, if the two sentences mismatch perfectly, BLEU � 0.0 [20]. e core of BLEU translation evaluation metric is to detect the number of cooccurrence words between the hypothetical sentences and the reference sentences. e specific implementation method is to calculate the n-grams of the hypothetical sentence and the reference sentence and then count the number of matches to get the score. e more grams the system translation matches with the manual reference translation, the higher the BLEU score. Examples are as follows [21]: Reference: this is a small test. Candidate: this is a test. Table 5 shows that the BLEU score under 1-gram is 0.8 since the hypothetical sentence shares 4 words with the reference sentence. e number of the words in the hypothetical sentence divided by the words in the reference sentence is the final score.

Under 2-gram, as shown in Table 6: Every two words in the sentence are divided into a 2gram group. e calculation logic is the same as that of 1-gram. Under the conditions of the same reference sentence and hypothetical sentence, BLEU score is 0.5.

Napoles et al. [22] contend that because there is still a crucial distinction between translation tasks and error correction tasks, it is inaccurate to consider machine translation as merely a monolingual translation. Direct application of BLEU to GEC tasks could result in less-thanideal output scores. Because of this, researchers have created a simple BLEU metric variant called GLEU that is suited to the requirements of the error correction task. e accuracy of the GEC system is calculated through the comparison of reference sentence and source sentence, giving more weight to the gram with correct correction, rewarding the correct correction result and the correct source text without correction, and punishing the gram with incorrect correction. e calculation formula is as follows (10):

In the formulas, p n is the accuracy after n-gram calculation and BP is the penalty factor. In formula (11), H is the length of the hypothetical sentence and R is the length of the reference sentence. e function of penalty factor is to avoid the bias of system scoring. In the scoring process, the matching degree of n-gram may become better with the ( is ⟶ these), (is ⟶ are),(for ⟶ to) 0.67 0.67 0.67     shortening of sentence length. erefore, in order to control this situation, the length of the sentence will be taken into account in the calculation. When the length of the hypothetical sentence is greater than the source sentence, the penalty factor is 1 and no penalty will be imposed. When the length of the hypothetical sentence is greater than the source sentence, the punishment will be carried out. N is the value of n in n-gram of GLEU formula, and its upper limit is 4. N (H, R) is the overlapping n-grams in the hypothetical sentence and the source sentence, and N(H, S, R) is the overlapping n-grams in the hypothetical sentence, the source sentence, and the reference sentence, respectively. w n is the weighted average adopted by the system, and the value is 1/ N.","sent1: A variant of the machine translation system measure BLEU (bilateral evaluation understudy) [19], called GLEU (generalized language evaluation understanding), was put forth by IBM researchers in 2001.
sent2: GLEU is largely used to assess the output of machine translation models, which is typically between 0.0 and 1.0.
sent3: If the two sentences match perfectly, BLEU � 1.0.
sent4: Conversely, if the two sentences mismatch perfectly, BLEU � 0.0 [20].
sent5: e core of BLEU translation evaluation metric is to detect the number of cooccurrence words between the hypothetical sentences and the reference sentences.
sent6: e specific implementation method is to calculate the n-grams of the hypothetical sentence and the reference sentence and then count the number of matches to get the score.
sent7: e more grams the system translation matches with the manual reference translation, the higher the BLEU score.
sent8: Examples are as follows [21]: Reference: this is a small test.
sent9: Candidate: this is a test. Table 5 shows that the BLEU score under 1-gram is 0.8 since the hypothetical sentence shares 4 words with the reference sentence.
sent10: e number of the words in the hypothetical sentence divided by the words in the reference sentence is the final score.
sent11: Under 2-gram, as shown in Table 6: Every two words in the sentence are divided into a 2gram group.
sent12: e calculation logic is the same as that of 1-gram.
sent13: Under the conditions of the same reference sentence and hypothetical sentence, BLEU score is 0.5.Napoles et al. [22] contend that because there is still a crucial distinction between translation tasks and error correction tasks, it is inaccurate to consider machine translation as merely a monolingual translation.
sent14: Direct application of BLEU to GEC tasks could result in less-thanideal output scores.
sent15: Because of this, researchers have created a simple BLEU metric variant called GLEU that is suited to the requirements of the error correction task.
sent16: e accuracy of the GEC system is calculated through the comparison of reference sentence and source sentence, giving more weight to the gram with correct correction, rewarding the correct correction result and the correct source text without correction, and punishing the gram with incorrect correction.
sent17: e calculation formula is as follows (10):In the formulas, p n is the accuracy after n-gram calculation and BP is the penalty factor.
sent18: In formula (11), H is the length of the hypothetical sentence and R is the length of the reference sentence.
sent19: e function of penalty factor is to avoid the bias of system scoring.
sent20: In the scoring process, the matching degree of n-gram may become better with the ( is ⟶ these), (is ⟶ are),(for ⟶ to) 0.67 0.67 0.67     shortening of sentence length.
sent21: erefore, in order to control this situation, the length of the sentence will be taken into account in the calculation.
sent22: When the length of the hypothetical sentence is greater than the source sentence, the penalty factor is 1 and no penalty will be imposed.
sent23: When the length of the hypothetical sentence is greater than the source sentence, the punishment will be carried out.
sent24: N is the value of n in n-gram of GLEU formula, and its upper limit is 4.
sent25: N (H, R) is the overlapping n-grams in the hypothetical sentence and the source sentence, and N(H, S, R) is the overlapping n-grams in the hypothetical sentence, the source sentence, and the reference sentence, respectively.
sent26: w n is the weighted average adopted by the system, and the value is 1/ N.","1. What is GLEU and who proposed it? sent1
2. What is the primary use of GLEU? sent2
3. How is a perfect match between two sentences represented in BLEU? sent3
4. How is a perfect mismatch between two sentences represented in BLEU? sent4
5. What is the core concept of the BLEU translation evaluation metric? sent5
6. How is the BLEU score calculated? sent6
7. What does a higher BLEU score indicate? sent7
8. Can you provide an example of a BLEU score calculation under 1-gram? sent8, sent9
9. How is the final BLEU score determined? sent10
10. How is the BLEU score calculated under 2-gram? sent11, sent12
11. What is the argument against considering machine translation as merely a monolingual translation? sent13
12. Why might the direct application of BLEU to GEC tasks be problematic? sent14
13. What is GLEU and why was it created? sent15
14. How is the accuracy of the GEC system calculated in GLEU? sent16
15. What is the purpose of the penalty factor in the GLEU calculation? sent17, sent19
16. How does sentence length affect the GLEU scoring process? sent18, sent20, sent21
17. What happens when the length of the hypothetical sentence is greater than the source sentence in GLEU? sent22, sent23
18. What is the upper limit of n in the GLEU formula? sent24
19. What do N(H, R) and N(H, S, R) represent in the GLEU formula? sent25
20. What is the value of w_n in the GLEU formula? sent26"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Green Models,4,"CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords. This kind of representation increases the overall length of the input as well as hinders the model learning. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text. However, both these approaches involve learning the model parameters from scratch which is highly expensive. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].

Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123]. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction. This approach is completely inexpensive as it requires only CPU. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive. Table 9 contains summary of Green T-BPLMs.","sent1: CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus.
sent2: As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords.
sent3: This kind of representation increases the overall length of the input as well as hinders the model learning.
sent4: DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text.
sent5: However, both these approaches involve learning the model parameters from scratch which is highly expensive.
sent6: These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].
sent7: Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123].
sent8: These models are referred to as Green Models as they are developed in a low cost environment-friendly approach.
sent9: GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors.
sent10: The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors.
sent11: With the addition of domain-specific word vectors, the model acquires domain-specific knowledge.
sent12: The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction.
sent13: This approach is completely inexpensive as it requires only CPU.
sent14: exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module.
sent15: The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text.
sent16: During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive.
sent17: Table 9 contains summary of Green T-BPLMs.","1. What is CPT and how does it adapt general T-PLMs to in-domain? sent1
2. Why can't general T-PLMs represent in-domain words meaningfully? sent2
3. What are the consequences of the representation of in-domain words in general T-PLMs? sent3
4. What do DSPT and SPT allow for in-domain vocabulary? sent4
5. What is a major drawback of DSPT and SPT approaches? sent5
6. Why are DSPT and SPT approaches considered environmentally unfriendly? sent6
7. What is the recent trend in the biomedical research community regarding T-PLMs? sent7
8. What are Green Models and how are they developed? sent8
    8.1. How does GreenBioBERT extend general BERT to the biomedical domain? sent9
        8.1.1. How is the in-domain vocabulary generated and aligned in GreenBioBERT? sent10
        8.1.2. What knowledge does GreenBioBERT acquire with domain-specific word vectors? sent11
        8.1.3. How does GreenBioBERT perform compared to BioBERT in entity extraction? sent12
        8.1.4. Why is the GreenBioBERT approach considered inexpensive? sent13
    8.2. How does exBERT extend general BERT to the biomedical domain? sent14
        8.2.1. How are the in-domain WordPiece vectors and extension module parameters learned in exBERT? sent15
        8.2.2. Why is the exBERT approach considered inexpensive? sent16
9. Where can a summary of Green T-BPLMs be found? sent17"
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,Shades of Truth,6,"The notion of shades of truth is important in mis-and disinformation detection. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false. We believe that such shades could be applied to stance and used in a larger pipeline. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).

Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.","sent1: The notion of shades of truth is important in mis-and disinformation detection.
sent2: For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false.
sent3: We believe that such shades could be applied to stance and used in a larger pipeline.
sent4: In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).
sent5: Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance.
sent6: As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance.
sent7: Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.","1. Why is the notion of shades of truth important in mis-and disinformation detection? sent1
    1.1. How do fact-checking labels go beyond binary true/false labels? sent2
    1.2. How could shades of truth be applied to stance detection? sent3
2. What is common in the related task of Sentiment Analysis regarding labels? sent4
3. How has the definition of stance detection and label inventories evolved? sent5
    3.1. What are the common labels used in stance detection, and why are they insufficient? sent6
    3.2. How can adding label granularity improve the transfer between datasets? sent7"
256900869,A Survey on Event-based News Narrative Extraction,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,Others.,14,"Here we present works that use a sentence-level resolution but differ from the majority of the other works that follow the traditional timeline summarization approach. In particular, we consider works on extracting disaster storylines from news and works that present variations on the traditional TLS task.

Disaster Storylines Zhou et al. 's works [133,134] present a framework to construct spatio-temporal storylines for disaster management from news data based on how the disaster location moves over time (e.g., a typhoon moving through different areas). This approach generates timelines for two levels of representation: a global level that follows the progress of the disaster through each location and a local level that focuses on a specific location. To extract the storyline, a series of snippets (i.e., event sentences) are extracted from the news articles using named entity recognition methods and grouped together based on a similarity graph. Then, a set of representative sentences is selected by finding the minimum dominating set [102] using a greedy algorithm. Next, an integer linear programming approach is used to select the optimal sequence for the main route of the disaster by maximizing the coherence of the story chain, subject to a series of structural, chronological, and length constraints. In this method, coherence is defined based on consecutive content similarity rather than word influence. However, the key difference is that this formulation includes a smoothness constraint, which is specifically designed to track the moving location of disasters through time. Smoothness is based on simulating the natural trajectory of a disaster. In particular, the constraints set a maximum distance for consecutive events (i.e., avoiding jumps to locations too far away) and seek to avoid acute angles that could be formed by two consecutive connections (i.e., avoiding sharp turns in the trajectory of the disaster). Once the main storyline has been constructed, the next step is to analyze the local level storylines. For each main storyline event, a set of similar articles are selected and used to construct a multi-view graph that represents the event relationships based on content similarity.

Then, a Steiner tree algorithm is used on the multi-view graph to generate a local storyline for that location.

Yuan et al. [131] propose dTexSL, a disaster storyline extraction approach that extends Zhou et al. 's works [133,134].

Unlike the previous approach, the news articles are first divided into different subsets based on location and are represented using neural embeddings. Locations are found by measuring the distance of the locations described in each article-using named entity recognition to find location references-and merging locations that are close enough based on a user-defined threshold. Then, an integer linear programming approach is used to select the key locations Manuscript submitted to ACM (i.e., document clusters). Instead of choosing events to maximize coherence like before, the goal is to maximize the number of documents covered on the map. The model has similar constraints as the original approach: chronological order, length, and smoothness. Once the main storyline has been constructed, a word embedding method is used to construct a multi-view graph that represents the event relationships based on content similarity. Using this graph, a set of representative articles are selected based on two criteria: uniqueness-computed using information gain-and relevance-computed using a measure of node importance. Then, a dynamic Steiner tree algorithm is used on the multiview graph to generate a local storyline for that specific location. Finally, a traditional multi-document summarization method [39] is applied to generate a high-level event description for that specific location.

Task Variations Duan et al. [28] introduce another variation on the timeline summarization task called comparative timeline summarization. In this task, the goal is to provide timelines consisting of major contrasting events from two data sets. Their approach is based on three core characteristics: coverage, distinctness, and diversity. Coverage is based on the idea that the timelines should cover most of the important information or topics from each data set. Distinctness is based on the idea that the events in a timeline should be distinct from the events on the other timeline at each time point, to allow for a proper contrast between them. Diversity is based on the idea that each timeline should cover a diverse set of events from its data set. To model these attributes, the authors propose a dynamic Markov model that is built around sentence similarity at a document level for each time step. In particular, sentences are selected from news articles to describe events based on local and global importance measures through the use of an affinity-preserving mutually reinforced Markov random walk model based on the PageRank algorithm. The output is a timeline that contains contrasting events from both data sets.

Yu et al. [130] propose a variation on the basic timeline summarization task, called Multi-TimeLine Summarization (MTLS). In this task, events are represented as sets of sentences and computationally represented by the neural embedding model sentence-BERT [90]. Given a set of time-stamped news articles, MLTS seeks to automatically extract timelines for important and different stories found in the data set. The authors propose a framework to solve this task called 2SAPS (Two-Stage Affinity Propagation Summarization). There are two key components in their framework: an event generation module and a timeline generation module. The event generation module seeks to extract important events from the document collection. To do so, it uses an affinity propagation approach to cluster similar sentences [34] and to identify the event of the article and any other previously referenced event. Furthermore, there is a temporal similarity term that uses an exponential decay function to penalize similarities of events that are temporally far away.

Once the events are identified, a subset of these events is selected based on a weighted average of a salience metric-based on event frequency-and a consistency metric-based on the intra-event similarity. Next, the timeline generation module has three internal steps: event link, time selection, and timeline summarization itself. Event linking is based on the weighted average between a co-reference score (based on entities or terms shared between events) and semantic similarity (e.g., cosine similarity). Based on these average scores, the system builds an event graph and uses affinity propagation on it to determine the initial clusters (i.e., timeline sets). Next, there is a timeline selection based on the weighted average of timeline salience-the average event salience of the timeline-and timeline coherence-the average semantic similarity scores between chronologically adjacent events. The timeline summarizing step selects an exemplar sentence for each event in the timelines, as the most typical and representative member of each event. Finally, there is an add-on timeline tagging step which assigns a label to each timeline, based on the most frequent words of the events.

Summarize Dates First (SDF) [55] is a timeline summarization pipeline that follows a different paradigm for timeline summarization based on generating a summary for each individual date first, and then selecting the most relevant dates using these summaries. This is different from the traditional approach where the relevant dates are selected first. Furthermore, this approach aggregates dates by leveraging higher-level temporal references (i.e., references to previous events in the article). SDF consists of three steps: temporal tagging, per-date summary extraction, and summary-drive date selection. In the temporal tagging stage, the raw text is annotated to identify date-level references (e.g., 31 December 2021) and high-level references (e.g., last December). The per-date summary extraction step uses any traditional sentence-based summarization algorithm from the multi-document summarization literature (e.g., TextRank [73]). Summary-driven date selection is the last step and uses a selection strategy, called Graph-Based Date Selection, which uses graph ranking algorithms (e.g., PageRank, HITS). In particular, a directed date graph model is built using the temporal references of the data set, where the edge weight connecting two dates is influenced by the count of date-level references and the similarity between the date summary and the high-level references to the earlier date.","sent1: Here we present works that use a sentence-level resolution but differ from the majority of the other works that follow the traditional timeline summarization approach.
sent2: In particular, we consider works on extracting disaster storylines from news and works that present variations on the traditional TLS task.
sent3: Disaster Storylines Zhou et al. 's works [133,134] present a framework to construct spatio-temporal storylines for disaster management from news data based on how the disaster location moves over time (e.g., a typhoon moving through different areas).
sent4: This approach generates timelines for two levels of representation: a global level that follows the progress of the disaster through each location and a local level that focuses on a specific location.
sent5: To extract the storyline, a series of snippets (i.e., event sentences) are extracted from the news articles using named entity recognition methods and grouped together based on a similarity graph.
sent6: Then, a set of representative sentences is selected by finding the minimum dominating set [102] using a greedy algorithm.
sent7: Next, an integer linear programming approach is used to select the optimal sequence for the main route of the disaster by maximizing the coherence of the story chain, subject to a series of structural, chronological, and length constraints.
sent8: In this method, coherence is defined based on consecutive content similarity rather than word influence.
sent9: However, the key difference is that this formulation includes a smoothness constraint, which is specifically designed to track the moving location of disasters through time.
sent10: Smoothness is based on simulating the natural trajectory of a disaster.
sent11: In particular, the constraints set a maximum distance for consecutive events (i.e., avoiding jumps to locations too far away) and seek to avoid acute angles that could be formed by two consecutive connections (i.e., avoiding sharp turns in the trajectory of the disaster).
sent12: Once the main storyline has been constructed, the next step is to analyze the local level storylines.
sent13: For each main storyline event, a set of similar articles are selected and used to construct a multi-view graph that represents the event relationships based on content similarity.
sent14: Then, a Steiner tree algorithm is used on the multi-view graph to generate a local storyline for that location.
sent15: Yuan et al. [131] propose dTexSL, a disaster storyline extraction approach that extends Zhou et al. 's works [133,134].
sent16: Unlike the previous approach, the news articles are first divided into different subsets based on location and are represented using neural embeddings.
sent17: Locations are found by measuring the distance of the locations described in each article-using named entity recognition to find location references-and merging locations that are close enough based on a user-defined threshold.
sent18: Then, an integer linear programming approach is used to select the key locations Manuscript submitted to ACM (i.e., document clusters).
sent19: Instead of choosing events to maximize coherence like before, the goal is to maximize the number of documents covered on the map.
sent20: The model has similar constraints as the original approach: chronological order, length, and smoothness.
sent21: Once the main storyline has been constructed, a word embedding method is used to construct a multi-view graph that represents the event relationships based on content similarity.
sent22: Using this graph, a set of representative articles are selected based on two criteria: uniqueness-computed using information gain-and relevance-computed using a measure of node importance.
sent23: Then, a dynamic Steiner tree algorithm is used on the multiview graph to generate a local storyline for that specific location.
sent24: Finally, a traditional multi-document summarization method [39] is applied to generate a high-level event description for that specific location.
sent25: Task Variations Duan et al. [28] introduce another variation on the timeline summarization task called comparative timeline summarization.
sent26: In this task, the goal is to provide timelines consisting of major contrasting events from two data sets.
sent27: Their approach is based on three core characteristics: coverage, distinctness, and diversity.
sent28: Coverage is based on the idea that the timelines should cover most of the important information or topics from each data set.
sent29: Distinctness is based on the idea that the events in a timeline should be distinct from the events on the other timeline at each time point, to allow for a proper contrast between them.
sent30: Diversity is based on the idea that each timeline should cover a diverse set of events from its data set.
sent31: To model these attributes, the authors propose a dynamic Markov model that is built around sentence similarity at a document level for each time step.
sent32: In particular, sentences are selected from news articles to describe events based on local and global importance measures through the use of an affinity-preserving mutually reinforced Markov random walk model based on the PageRank algorithm.
sent33: The output is a timeline that contains contrasting events from both data sets.
sent34: Yu et al. [130] propose a variation on the basic timeline summarization task, called Multi-TimeLine Summarization (MTLS).
sent35: In this task, events are represented as sets of sentences and computationally represented by the neural embedding model sentence-BERT [90].
sent36: Given a set of time-stamped news articles, MLTS seeks to automatically extract timelines for important and different stories found in the data set.
sent37: The authors propose a framework to solve this task called 2SAPS (Two-Stage Affinity Propagation Summarization).
sent38: There are two key components in their framework: an event generation module and a timeline generation module.
sent39: The event generation module seeks to extract important events from the document collection.
sent40: To do so, it uses an affinity propagation approach to cluster similar sentences [34] and to identify the event of the article and any other previously referenced event.
sent41: Furthermore, there is a temporal similarity term that uses an exponential decay function to penalize similarities of events that are temporally far away.
sent42: Once the events are identified, a subset of these events is selected based on a weighted average of a salience metric-based on event frequency-and a consistency metric-based on the intra-event similarity.
sent43: Next, the timeline generation module has three internal steps: event link, time selection, and timeline summarization itself.
sent44: Event linking is based on the weighted average between a co-reference score (based on entities or terms shared between events) and semantic similarity (e.g., cosine similarity).
sent45: Based on these average scores, the system builds an event graph and uses affinity propagation on it to determine the initial clusters (i.e., timeline sets).
sent46: Next, there is a timeline selection based on the weighted average of timeline salience-the average event salience of the timeline-and timeline coherence-the average semantic similarity scores between chronologically adjacent events.
sent47: The timeline summarizing step selects an exemplar sentence for each event in the timelines, as the most typical and representative member of each event.
sent48: Finally, there is an add-on timeline tagging step which assigns a label to each timeline, based on the most frequent words of the events.
sent49: Summarize Dates First (SDF) [55] is a timeline summarization pipeline that follows a different paradigm for timeline summarization based on generating a summary for each individual date first, and then selecting the most relevant dates using these summaries.
sent50: This is different from the traditional approach where the relevant dates are selected first.
sent51: Furthermore, this approach aggregates dates by leveraging higher-level temporal references (i.e., references to previous events in the article).
sent52: SDF consists of three steps: temporal tagging, per-date summary extraction, and summary-drive date selection.
sent53: In the temporal tagging stage, the raw text is annotated to identify date-level references (e.g., 31 December 2021) and high-level references (e.g., last December).
sent54: The per-date summary extraction step uses any traditional sentence-based summarization algorithm from the multi-document summarization literature (e.g., TextRank [73]).
sent55: Summary-driven date selection is the last step and uses a selection strategy, called Graph-Based Date Selection, which uses graph ranking algorithms (e.g., PageRank, HITS).
sent56: In particular, a directed date graph model is built using the temporal references of the data set, where the edge weight connecting two dates is influenced by the count of date-level references and the similarity between the date summary and the high-level references to the earlier date.","1. What is the focus of the works presented in this section? sent1
    1.1. What specific types of storylines are considered in these works? sent2
        1.1.1. How do Zhou et al.'s works construct disaster storylines? sent3
            1.1.1.1. What levels of representation are used in Zhou et al.'s approach? sent4
            1.1.1.2. How are snippets extracted and grouped in Zhou et al.'s framework? sent5
            1.1.1.3. How is the main route of the disaster selected in Zhou et al.'s method? sent6, sent7
            1.1.1.4. What is the key difference in the formulation of coherence in Zhou et al.'s method? sent8, sent9
            1.1.1.5. How is smoothness defined in Zhou et al.'s approach? sent10, sent11
            1.1.1.6. What is the next step after constructing the main storyline in Zhou et al.'s method? sent12
            1.1.1.7. How is a local storyline generated in Zhou et al.'s framework? sent13, sent14
        1.1.2. How does Yuan et al.'s dTexSL approach extend Zhou et al.'s works? sent15
            1.1.2.1. How are news articles processed differently in Yuan et al.'s approach? sent16
            1.1.2.2. How are locations identified and merged in Yuan et al.'s method? sent17
            1.1.2.3. What is the goal of the integer linear programming approach in Yuan et al.'s method? sent18, sent19
            1.1.2.4. What constraints are similar to the original approach in Yuan et al.'s method? sent20
            1.1.2.5. How is a local storyline generated in Yuan et al.'s framework? sent21, sent22, sent23
            1.1.2.6. What is the final step in Yuan et al.'s method? sent24
    1.2. What is the task variation introduced by Duan et al.? sent25
        1.2.1. What is the goal of the comparative timeline summarization task? sent26
        1.2.2. What are the core characteristics of Duan et al.'s approach? sent27
            1.2.2.1. How is coverage defined in Duan et al.'s approach? sent28
            1.2.2.2. How is distinctness defined in Duan et al.'s approach? sent29
            1.2.2.3. How is diversity defined in Duan et al.'s approach? sent30
            1.2.2.4. How are these attributes modeled in Duan et al.'s approach? sent31
            1.2.2.5. What is the output of Duan et al.'s approach? sent33
    1.3. What is the variation introduced by Yu et al. called? sent34
        1.3.1. How are events represented in the Multi-TimeLine Summarization task? sent35
        1.3.2. What is the goal of the MLTS task? sent36
        1.3.3. What framework is proposed to solve the MLTS task? sent37
            1.3.3.1. What are the key components of the 2SAPS framework? sent38
            1.3.3.2. How does the event generation module work? sent39, sent40
            1.3.3.3. What is the role of the temporal similarity term? sent41
            1.3.3.4. How are events selected in the event generation module? sent42
            1.3.3.5. What are the internal steps of the timeline generation module? sent43
            1.3.3.6. How is event linking performed? sent44, sent45
            1.3.3.7. How is timeline selection performed? sent46
            1.3.3.8. How is the timeline summarizing step conducted? sent47
            1.3.3.9. What is the purpose of the timeline tagging step? sent48
    1.4. What is the Summarize Dates First (SDF) pipeline? sent49
        1.4.1. How does SDF differ from the traditional timeline summarization approach? sent50
        1.4.2. How does SDF aggregate dates? sent51
        1.4.3. What are the steps involved in the SDF pipeline? sent52
            1.4.3.1. What happens in the temporal tagging stage? sent53
            1.4.3.2. How is the per-date summary extraction step conducted? sent54
            1.4.3.3. What is the summary-driven date selection step? sent55
            1.4.3.4. How is the directed date graph model built in SDF? sent56"
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,Explainable Model Structure,8,"The explainable model structure is to analyze and understand the internal structure of the model through explainable technology and to understand the working principle and working mechanism of the model.Structural analysis involves comprehending the operating mechanism and fundamental principles of the model structure.Only by fully understanding the working mechanism and working principle of the model structure can researchers and developers determine what problems exist in the model and when it is difficult to continue improving its performance.Only then, on the premise of understanding the characteristics of the model structure, can they point out the next optimization direction of the model.This enables them to improve the performance of the model in a better and faster way.Most of these explainable models use deep learning methods such as knowledge graphs and attention mechanisms.

Chien et al. [139] proposed the Explainable AI (XAI) framework XFlag, used LSTM [140] to carry out the fake news detection model and used the Layered Relevance Propagation (LRP) [141] algorithm to explain the model.Wu et al. [142] used knowledge graphs to enhance the embedded representation learning framework to detect fake news while providing interpretations of relationships.In this study, an external dataset was used to extract a knowledge graph, and a graph neural network was utilized to pre-train structured features for entities and relationships.The pre-trained features and semantic features were then combined to integrate explainable structured knowledge for recognizing fake news.Chen et al. [143] designed an explainable modular structure for automatically detecting rumors on social media.They utilized a two-level attention mechanism to capture the relative importance both between features and between feature classes.Furthermore, they highlighted the most significant features in the news to explain the algorithmic results.In addition, based on multidisciplinary explainable fake news detection, Qiao et al. [144] used multidisciplinary language synthesis methods to train features that are understandable to humans and then used these features to train a deep learning classifier with a bidirectional recurrent neural network (BRNN) structure [145], so that the classifier can obtain more explainable detection results in news data.

Silva et al. [146] proposed a novel fake news early detection technology called Propa-gation2Vec, as shown in Figure 7.The technology assigns different levels of importance to nodes and cascades in the propagation network and reconstructs knowledge of the complete propagation network based on their partial propagation network during the early detection phase.The study further presents a comprehensive explanation of the underlying logic of Propagation2Vec according to the attention weights assigned to different nodes and cascades.This enhances the applicability of the method and stimulates future research in the domain of fake news detection utilizing propagation networks.We summarize the explainable model structure methods in Table 7, including the main techniques they use, datasets and accuracy performance.We summarize the explainable model structure methods in Table 7, including the main techniques they use, datasets and accuracy performance.","sent1: The explainable model structure is to analyze and understand the internal structure of the model through explainable technology and to understand the working principle and working mechanism of the model.
sent2: Structural analysis involves comprehending the operating mechanism and fundamental principles of the model structure.
sent3: Only by fully understanding the working mechanism and working principle of the model structure can researchers and developers determine what problems exist in the model and when it is difficult to continue improving its performance.
sent4: Only then, on the premise of understanding the characteristics of the model structure, can they point out the next optimization direction of the model.
sent5: This enables them to improve the performance of the model in a better and faster way.
sent6: Most of these explainable models use deep learning methods such as knowledge graphs and attention mechanisms.
sent7: Chien et al. [139] proposed the Explainable AI (XAI) framework XFlag, used LSTM [140] to carry out the fake news detection model and used the Layered Relevance Propagation (LRP) [141] algorithm to explain the model.
sent8: Wu et al. [142] used knowledge graphs to enhance the embedded representation learning framework to detect fake news while providing interpretations of relationships.
sent9: In this study, an external dataset was used to extract a knowledge graph, and a graph neural network was utilized to pre-train structured features for entities and relationships.
sent10: The pre-trained features and semantic features were then combined to integrate explainable structured knowledge for recognizing fake news.
sent11: Chen et al. [143] designed an explainable modular structure for automatically detecting rumors on social media.
sent12: They utilized a two-level attention mechanism to capture the relative importance both between features and between feature classes.
sent13: Furthermore, they highlighted the most significant features in the news to explain the algorithmic results.
sent14: In addition, based on multidisciplinary explainable fake news detection, Qiao et al. [144] used multidisciplinary language synthesis methods to train features that are understandable to humans and then used these features to train a deep learning classifier with a bidirectional recurrent neural network (BRNN) structure [145], so that the classifier can obtain more explainable detection results in news data.
sent15: Silva et al. [146] proposed a novel fake news early detection technology called Propa-gation2Vec, as shown in Figure 7.The technology assigns different levels of importance to nodes and cascades in the propagation network and reconstructs knowledge of the complete propagation network based on their partial propagation network during the early detection phase.
sent16: The study further presents a comprehensive explanation of the underlying logic of Propagation2Vec according to the attention weights assigned to different nodes and cascades.
sent17: This enhances the applicability of the method and stimulates future research in the domain of fake news detection utilizing propagation networks.
sent18: We summarize the explainable model structure methods in Table 7, including the main techniques they use, datasets and accuracy performance.
sent19: We summarize the explainable model structure methods in Table 7, including the main techniques they use, datasets and accuracy performance.","1. What is the purpose of the explainable model structure in analyzing models? sent1
    1.1. What does structural analysis involve in the context of explainable model structures? sent2
    1.2. Why is it important to fully understand the working mechanism and principles of a model structure? sent3
        1.2.1. What can researchers and developers do once they understand the characteristics of the model structure? sent4
        1.2.2. How does understanding the model structure help improve its performance? sent5
2. What methods do most explainable models use? sent6
3. What is the Explainable AI (XAI) framework XFlag, and how is it used in fake news detection? sent7
4. How do Wu et al. enhance the embedded representation learning framework for fake news detection? sent8
    4.1. What approach is used in the study to integrate explainable structured knowledge for recognizing fake news? sent9, sent10
5. What did Chen et al. design for detecting rumors on social media, and how does it work? sent11
    5.1. How does the two-level attention mechanism function in Chen et al.'s model? sent12
    5.2. What is highlighted in the news to explain the algorithmic results? sent13
6. How does Qiao et al.'s approach to explainable fake news detection work? sent14
7. What is the novel fake news early detection technology proposed by Silva et al., and how does it function? sent15
    7.1. How does Propagation2Vec enhance the applicability of fake news detection methods? sent16, sent17
8. Where can a summary of the explainable model structure methods be found? sent18, sent19"
181562553,A Systematic Literature Review on Image Captioning,https://www.semanticscholar.org/paper/92ccf5a39c63cb5e1639be518e6db2e357acd58e,Human-Like Feeling,4,"In the last year, two keywords have come into the vocabulary of almost every article written under the image captioning topic-novel and semantics. These keywords are important for solving the biggest challenge in this exercise i.e. generating a caption in a way that it would be inseparable from human written ones. Semantics implementation [49] is supposed to design a clean way of injecting sentiment into the current image captioning system. Novel objects must be included for the expansion of scenarios. There have been several insights on why this is still an open issue. First of all, usually models are built on very specific datasets, which do not cover all possible scenarios and are not applicable in describing diverse environment. The same with vocabulary as it has a limited number of words and their combinations. Second, models are usually thought to perform on one specific task, while humans are able to work on many tasks simultaneously. Ref. [35] has already tried to overcome this problem and has provided a solution although it was not further continued. Another great approach for dealing with unseen data, as it is currently impossible to feed all existing data into the machine, was proposed in ref. [56,96]. Lifelong learning is based on a questioning approach i.e. making a discussion directly with the user or inside the model. This approach relies on a natural way of human communication; from early childhood children mostly learn by asking questions. The model is intended to learn also like a child-by asking specific questions and learning from the answers. This method falls under the question answering topic-a literature research in depth might be done on this topic as here we have presented only what appeared during this study on image captioning. This can be targeted as a separate problem, but it also makes a great impact in image captioning.","sent1: In the last year, two keywords have come into the vocabulary of almost every article written under the image captioning topic-novel and semantics.
sent2: These keywords are important for solving the biggest challenge in this exercise i.e. generating a caption in a way that it would be inseparable from human written ones.
sent3: Semantics implementation [49] is supposed to design a clean way of injecting sentiment into the current image captioning system.
sent4: Novel objects must be included for the expansion of scenarios.
sent5: There have been several insights on why this is still an open issue.
sent6: First of all, usually models are built on very specific datasets, which do not cover all possible scenarios and are not applicable in describing diverse environment.
sent7: The same with vocabulary as it has a limited number of words and their combinations.
sent8: Second, models are usually thought to perform on one specific task, while humans are able to work on many tasks simultaneously.
sent9: Ref. [35] has already tried to overcome this problem and has provided a solution although it was not further continued.
sent10: Another great approach for dealing with unseen data, as it is currently impossible to feed all existing data into the machine, was proposed in ref.
sent11: [56,96]. Lifelong learning is based on a questioning approach i.e. making a discussion directly with the user or inside the model.
sent12: This approach relies on a natural way of human communication; from early childhood children mostly learn by asking questions.
sent13: The model is intended to learn also like a child-by asking specific questions and learning from the answers.
sent14: This method falls under the question answering topic-a literature research in depth might be done on this topic as here we have presented only what appeared during this study on image captioning.
sent15: This can be targeted as a separate problem, but it also makes a great impact in image captioning.","1. What are the two keywords that have become prominent in the image captioning topic recently? sent1
    1.1. Why are these keywords important for image captioning? sent2
2. What is the purpose of semantics implementation in image captioning? sent3
3. Why must novel objects be included in image captioning? sent4
4. What are some insights into why generating human-like captions is still an open issue? sent5
    4.1. What is the first reason for the challenge in generating human-like captions? sent6
    4.2. What is the limitation related to vocabulary in image captioning models? sent7
    4.3. What is the second reason for the challenge in generating human-like captions? sent8
5. Has there been any attempt to overcome the problem of generating human-like captions? sent9
6. What is another approach for dealing with unseen data in image captioning? sent10
    6.1. What is the basis of the lifelong learning approach? sent11
    6.2. How does the lifelong learning approach relate to human communication? sent12
    6.3. How is the model intended to learn in the lifelong learning approach? sent13
    6.4. Under which topic does this method fall, and what might be done for further research? sent14
7. How does the questioning approach impact image captioning? sent15"
21693765,Graph-based Ontology Summarization: A Survey,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,Coverage-based Measures,12,"Top-ranked nodes in a graph representation of an ontology may not form the best ontology summary. For many applications, a good summary is expected to have a good coverage of the contents of an ontology, to form a comprehensive and unbiased overview. Accordingly, the quality of a subset of nodes forming a summary is to be assessed as a whole.

Coverage (Co) Peroni et al. [12] propose the coverage criterion which aims to show how well the selected set of classes are spread over the whole class hierarchy. For each node v, let N + (v) be the set of nodes covered by v, including v and its neighbors, i.e., its subclasses and superclasses in the class hierarchy. The coverage of a set of selected nodes V is defined as the proportion of nodes in the graph that are covered by V :

where n is the number of nodes in the graph. Further, Peroni et al. [12] consider an interesting measure called balance which is directly related to coverage. It measures how balanced the selected nodes are, i.e., the degree to which each selected node contributes to the overall coverage of the set, which is characterized by standard deviation.

Diversity-based Re-ranking (Di) In [23,22], the coverage of a summary is improved by a re-ranking step after centrality-based ranking. In these approaches, nodes are iteratively selected to form a summary. In each iteration, the next node to be selected may not be the top-ranked one among the remaining nodes, which will be re-ranked such that a node similar to those selected in previous iterations will be penalized. Specifically, let score(v) be the centrality score of node v, and let sim(v, u) be the similarity between nodes v and u. Given a set of nodes V s which are already selected into the summary and a set of candidate nodes V c , the next node to be selected from V c is

Zhang et al. [23,22] use this algorithm to rank RDF sentences, where two RDF sentences are similar if they share terms. The resulting ontology summary is diversified with regard to the terms it contains.

Comments on Coverage-based Measures Coverage-based methods complement centrality-based measures, but their current implementations are suboptimal. Coverage in Eq. (12) considers the neighborhood of each node, not taking the global graph structure into account. Diversity-based re-ranking in Eq. (13) has a greedy nature, and may not find the optimum summary in terms of centrality and diversity.

Top-ranked nodes in a graph representation of an ontology may not form the best ontology summary. For many applications, a good summary is expected to have a good coverage of the contents of an ontology, to form a comprehensive and unbiased overview. Accordingly, the quality of a subset of nodes forming a summary is to be assessed as a whole.

Coverage (Co) Peroni et al. [12] propose the coverage criterion which aims to show how well the selected set of classes are spread over the whole class hierarchy. For each node v, let N + (v) be the set of nodes covered by v, including v and its neighbors, i.e., its subclasses and superclasses in the class hierarchy. The coverage of a set of selected nodes V is defined as the proportion of nodes in the graph that are covered by V :

where n is the number of nodes in the graph. Further, Peroni et al. [12] consider an interesting measure called balance which is directly related to coverage. It measures how balanced the selected nodes are, i.e., the degree to which each selected node contributes to the overall coverage of the set, which is characterized by standard deviation.

Diversity-based Re-ranking (Di) In [23,22], the coverage of a summary is improved by a re-ranking step after centrality-based ranking. In these approaches, nodes are iteratively selected to form a summary. In each iteration, the next node to be selected may not be the top-ranked one among the remaining nodes, which will be re-ranked such that a node similar to those selected in previous iterations will be penalized. Specifically, let score(v) be the centrality score of node v, and let sim(v, u) be the similarity between nodes v and u. Given a set of nodes V s which are already selected into the summary and a set of candidate nodes V c , the next node to be selected from V c is

Zhang et al. [23,22] use this algorithm to rank RDF sentences, where two RDF sentences are similar if they share terms. The resulting ontology summary is diversified with regard to the terms it contains.

Comments on Coverage-based Measures Coverage-based methods complement centrality-based measures, but their current implementations are suboptimal. Coverage in Eq. (12) considers the neighborhood of each node, not taking the global graph structure into account. Diversity-based re-ranking in Eq. (13) has a greedy nature, and may not find the optimum summary in terms of centrality and diversity.","sent1: Top-ranked nodes in a graph representation of an ontology may not form the best ontology summary.
sent2: For many applications, a good summary is expected to have a good coverage of the contents of an ontology, to form a comprehensive and unbiased overview.
sent3: Accordingly, the quality of a subset of nodes forming a summary is to be assessed as a whole.
sent4: Coverage (Co) Peroni et al. [12] propose the coverage criterion which aims to show how well the selected set of classes are spread over the whole class hierarchy.
sent5: For each node v, let N + (v) be the set of nodes covered by v, including v and its neighbors, i.e., its subclasses and superclasses in the class hierarchy.
sent6: The coverage of a set of selected nodes V is defined as the proportion of nodes in the graph that are covered by V :where n is the number of nodes in the graph.
sent7: Further, Peroni et al. [12] consider an interesting measure called balance which is directly related to coverage.
sent8: It measures how balanced the selected nodes are, i.e., the degree to which each selected node contributes to the overall coverage of the set, which is characterized by standard deviation.
sent9: Diversity-based Re-ranking (Di)
sent10: In [23,22], the coverage of a summary is improved by a re-ranking step after centrality-based ranking.
sent11: In these approaches, nodes are iteratively selected to form a summary.
sent12: In each iteration, the next node to be selected may not be the top-ranked one among the remaining nodes, which will be re-ranked such that a node similar to those selected in previous iterations will be penalized.
sent13: Specifically, let score(v) be the centrality score of node v, and let sim(v, u) be the similarity between nodes v and u.
sent14: Given a set of nodes V s which are already selected into the summary and a set of candidate nodes V c , the next node to be selected from V c isZhang et al. [23,22] use this algorithm to rank RDF sentences, where two RDF sentences are similar if they share terms.
sent15: The resulting ontology summary is diversified with regard to the terms it contains.
sent16: Comments on Coverage-based Measures Coverage-based methods complement centrality-based measures, but their current implementations are suboptimal.
sent17: Coverage in Eq. (12) considers the neighborhood of each node, not taking the global graph structure into account.
sent18: Diversity-based re-ranking in Eq. (13) has a greedy nature, and may not find the optimum summary in terms of centrality and diversity.
sent19: Top-ranked nodes in a graph representation of an ontology may not form the best ontology summary.
sent20: For many applications, a good summary is expected to have a good coverage of the contents of an ontology, to form a comprehensive and unbiased overview.
sent21: Accordingly, the quality of a subset of nodes forming a summary is to be assessed as a whole.
sent22: Coverage (Co) Peroni et al. [12] propose the coverage criterion which aims to show how well the selected set of classes are spread over the whole class hierarchy.
sent23: For each node v, let N + (v) be the set of nodes covered by v, including v and its neighbors, i.e., its subclasses and superclasses in the class hierarchy.
sent24: The coverage of a set of selected nodes V is defined as the proportion of nodes in the graph that are covered by V :where n is the number of nodes in the graph.
sent25: Further, Peroni et al. [12] consider an interesting measure called balance which is directly related to coverage.
sent26: It measures how balanced the selected nodes are, i.e., the degree to which each selected node contributes to the overall coverage of the set, which is characterized by standard deviation.
sent27: Diversity-based Re-ranking (Di)
sent28: In [23,22], the coverage of a summary is improved by a re-ranking step after centrality-based ranking.
sent29: In these approaches, nodes are iteratively selected to form a summary.
sent30: In each iteration, the next node to be selected may not be the top-ranked one among the remaining nodes, which will be re-ranked such that a node similar to those selected in previous iterations will be penalized.
sent31: Specifically, let score(v) be the centrality score of node v, and let sim(v, u) be the similarity between nodes v and u.
sent32: Given a set of nodes V s which are already selected into the summary and a set of candidate nodes V c , the next node to be selected from V c isZhang et al. [23,22] use this algorithm to rank RDF sentences, where two RDF sentences are similar if they share terms.
sent33: The resulting ontology summary is diversified with regard to the terms it contains.
sent34: Comments on Coverage-based Measures Coverage-based methods complement centrality-based measures, but their current implementations are suboptimal.
sent35: Coverage in Eq. (12) considers the neighborhood of each node, not taking the global graph structure into account.
sent36: Diversity-based re-ranking in Eq. (13) has a greedy nature, and may not find the optimum summary in terms of centrality and diversity.","1. Why might top-ranked nodes in a graph representation of an ontology not form the best ontology summary? sent1
2. What is expected of a good ontology summary for many applications? sent2
3. How should the quality of a subset of nodes forming a summary be assessed? sent3
4. What is the coverage criterion proposed by Peroni et al.? sent4
    4.1. How is the coverage of a set of selected nodes defined? sent5, sent6
    4.2. What is the balance measure related to coverage? sent7, sent8
5. How is the coverage of a summary improved in diversity-based re-ranking approaches? sent9, sent10
    5.1. How are nodes selected in these approaches? sent11
    5.2. How does the re-ranking process work in these approaches? sent12, sent13
    5.3. What is the result of using this algorithm to rank RDF sentences? sent14, sent15
6. What are the comments on coverage-based measures? sent16
    6.1. What is a limitation of the coverage method in Eq. (12)? sent17
    6.2. What is a limitation of diversity-based re-ranking in Eq. (13)? sent18"
65219850,A Survey of Text Mining in Social Media: Facebook and Twitter Perspectives Establish algorithm for to determine an authorized PW error View project Further Investigations on Developing an Arabic Sentiment Lexicon View project Said Salloum British University in Dubai 3 PUBLICATIONS 2 CITATIONS SEE PROFILE,https://www.semanticscholar.org/paper/94d255e0b2a729e918a320ab99e503cfc04e1fc8,Text mining in Twitter,14,"A significant size of research has been occupied by the Twitter data analysis over the last couple of years [54]. Large spectrums of domains are using this data, some of which are using it for academic research and others for applications [55]. New improvements regarding twitter data are presented by this section. The document collection from various resources triggers the ""Text Mining"" process. A particular document would be retrieved by Text mining tool and this document is pre-processed by checking the character sets and format [56]. Subsequently, a text analysis phase would monitor the document. Semantic analysis is used to derive high-quality information from text; this is referred to ""Text analysis"". The market has a lot of text analysis techniques. Professionals can use combinations of techniques subject to the goal of the organization. Researchers tend to repeat the text analysis techniques till the time information is acquired. A management information system is capable of incorporating the resulting information, and as a result, significant knowledge is produced for the user of that information system [57]. A key issue in text mining is intricacy of natural language. The ambiguity problem is much dense in the natural language. There are multiple meanings of a single word and multiple words can possess same meaning. Ambiguity is referred to as the understanding of a word which has more than one possible meaning. Noise has emerged in extracted information as a result of this ambiguity. Since usability and flexibility are the main parts of ambiguity, it cannot be removed from the natural language. One phrase or sentence can have multiple understandings, so there is a chance we can obtain a number of meanings. The work is still undeveloped and a particular domain is correlated with the suggested approach while the experts have attempted to resolve the ambiguity problem by performing a number of research studies. As there is uncertainty/vagueness in the semantic meanings of many discovered words, so it is very difficult to answer the requirements of the user.

Scholars of [58] developed and formulated an automatic classification technique through which potentially abuseindicating user posts could be identified and evaluating the likelihood of social media usage as a source for automatic monitoring of drug medication abuse. In this regard, Twitter user posts (tweets) were collected and these were linked with three commonly abused medications (Oxycodone, Adderall, and Quetiapine). Besides interpreting a control medication (metformin), which is not the subject of abuse due to its process, nearly 6400 tweets were manually annotated, where these three medications were pointed out. The annotated data was qualitatively and quantitatively analyzed to determine as to whether or not signals of drug medication abuse are presented in Twitter posts. To sum up, Twitter's value was assessed in exploring the patterns of abuse over time and an automatic supervised classification technique was also designed, in which the purpose was to observe and separate the posts containing signals of medication abuse from those that do not. According to the findings of investigations, Twitter posts have yielded clear signals of medication abuse. As compared to the proportion for the control medication (i.e., metformin: 0.3 %), there is a very high ratio of tweets containing abuse signals for the three case medications (Adderall: 23 %, oxycodone: 12 %, quetiapine: 5.0 %). In addition, almost 82 % accuracy (medication abuse class recall: 0.51, precision: 0.41, F-measure: 0.46) has been achieved through the automatic classification approach. The Study demonstrated how the abuse patterns over time can be analyzed by using the classification data and its goal is to illustrate the effectiveness of automatic classification. As a result, it is found that abuse-related information for medications can be significantly acquired from social media, and the research indicates that natural language processing and supervised classification are the automatic approaches that have potentials for future monitoring and intervention assignments. With respect to supervised learning, the lack of sufficient training data is believed to be the largest shortcoming of the study. Both annotation and automatic classification are hindered by the lack of context and ambiguity in tweets. During the course of annotations, many ambiguous tweets were found and services of pharmacology expert were hired to address these issues. As a result of these ambiguities, the undefined situation is observed in the binary classification process and this inadequacy will continue until the time fine-tuned annotation rules could be specified by the future annotation rules.

A study by [59] applied the text mining approach on a large dataset of tweets. The complete Twitter timelines of 10 academic libraries were used to collect the dataset for this research. Nearly 23,707 tweets formed the total dataset, where there were 7625 hashtags, 17,848 mentions, and 5974 retweets. Inconsistency among academic libraries is found in the distribution of tweets. ""Open"" was the most repeated word that was used by the academic libraries in different perspectives. It was observed that ""special collections"" was the most frequent bigram (two-word sequence) in the aggregated tweets. While ""save the date"" was the most recurrent tri-gram (three-word sequence). In the semantic analysis, words such as ""insight, knowledge, and information about cultural and personal relations"" were the most frequent word categories. Moreover, ""Resources"" was the most widespread category of the tweets among all the selected academic libraries. The significance of data and text-mining approaches are reported within the study and their purpose is to gain an insight with the aggregate social data of academic libraries so that the process of decision-making and strategic planning could become facilitated for marketing of services and patron outreach. The 10 academic libraries from top global universities have undergone the text mining approach. The study aimed to illustrate their Twitter usage and to examine their tweet content.

As far as social media is concerned, decision-making is supported and user-generated text is analyzed through text mining and content analysis [60]. By employing an archiving service (twimemachine.com) in December 2014, the complete Twitter timelines of 10 academic libraries were taken into account to collect the dataset for this research. The libraries of 10 highestranking universities from the global Shanghai Ranking were chosen for that purpose. The language of the university must be English-based, which was the condition for selection and selection was restricted to only one library if there was more than one library in the university. Certain weaknesses were found in the study, for example, all of the libraries are English-language libraries in the sample and only 10 academic libraries were considered for the analysis. This gap must be filled in future by applying the analysis to a dataset from diversified academic libraries, including non-English language libraries. Consequently, a complete understanding of tweet patterns would be acknowledged. The future inquiry can also incorporate the international or crosscultural comparisons. Any discrepancy among libraries in their tweets' content affected by the number and interaction of followers could be highlighted by the analysis and its findings. The accuracy of the tweet categorization tool has yielded the inadequate findings, and the said tool needs to be substantiated through other machine-learning models along with their applications.

Researchers of [55] demonstrated in a smoking cessation nicotine patch study an innovative Twitter recruitment system that is deployed by the group. The study aimed to describe the methodology and used to address the issue of digital recruitment. Furthermore, designing a rule-based system with the provision of system specification besides representing the data mining approaches and algorithms (classification and association analysis) using Twitter data. Twitter's streaming API captured two sets of streaming tweets, which were collected for the study. Ten search terms, (i.e. quitting, quit, nicotine, smoking, smoke, patches, cig, cigarette, ecig, cigs, marijuana) were used to gather the first set. The second set of tweets contains 30 terms, in which the terms from the first set were included. Moreover, the second set is a superset of the first one. A number of studies have been conducted to review the information gathering methods. As unstructured data sets are in the textual format, the use of various procedures of text mining has been tackled by many research studies. Nonetheless, the data sets on the social networking websites are not mainly discussed by these studies. A study by [50] applied various text mining techniques. The study would describe the application of these strategies in the social networking websites. In the field of intelligent text analysis, the latest improvements would also be examined in the survey. The study focused on two key techniques pertaining to the text mining field, namely classification and clustering. Usually, they are operated for the study of the unstructured text accessible on the extensive scale frameworks. Prior to the start of World Cup, a total of approximately 30,000 tweets were used by [61]. Moreover, an algorithm was used for integrating the consensus matrix and the DBSCAN algorithm. Consequently, the concerned tweets on those prevailing topics were available to him. Afterward, the clustering analysis was applied to seek the topics discussed by the tweets. The tweets were grouped utilizing the k-means [62], Non-Negative Matrix Factorization (NMF), and a popular clustering algorithm. After that, the results were compared. Similar results were delivered by both algorithms. However, NMF became faster and the researchers could easily interpret the outcomes.

A study by [1] initiated a workflow to gain an insight into both the large-scale data mining methods and qualitative analysis. Twitter posts of engineering students were the primary concern. The basic goal was to identify their issues in their academic experiences. The study conducted a qualitative analysis of samples obtained from around 25,000 tweets that were associated with the engineering students and their college life. The encounter troubles of engineering students were discovered during the study. For example, a large volume of study, sleep deprivation and lack of social engagement. Considering these outcomes, a multi-label classification algorithm was implemented to categorize tweets in lieu of students' queries. The algorithm was applied on approximately 35,000 tweets streamed at the geo-location of Purdue University. At the first instance, the concerned authorities have addressed the experiences and issues of the students and social media data was used to expose the issues. Moreover, a study by [1] also developed a multi-label classifier so that tweets founded within the content evaluation phase could be organized. A number of renowned classifiers are significantly consumed in machine learning domain and data mining process. With Comparison to other state-of-the-art multi-label classifiers, the Naïve Bayes classifiers were found proficient on the dataset.

A study by [63] discussed the clustering technique, the execution of correlation and association analyses to social media. The investigation of insurance Twitter posts was carried out to assess this matter. Consequently, recognizing theories and keywords in the social media data has become an easy task, due to which the information by insurers and its application would be facilitated. After having a detailed analysis, client queries and the potential market would be proactively addressed with usefulness and the findings of the analysis are to be effectively implemented in suitable fields. According to this evaluation, the overall 68,370 tweets were utilized. Two additional kinds of evaluation need to be applied to the data. The first is the clustering analysis, through which the tweets depending on their similarities or dissimilarities would be merged. An Association Analysis is the second one whereas the occurrences of particular composed words were discovered.

Authors of [64] stated that sentiment analysis through social media usage has witnessed a huge interest from scholars in the last few years. In that, the authors discussed the influence of tweets' sentiment on elections and the impact of the elections' results on web sentiment.","sent1: A significant size of research has been occupied by the Twitter data analysis over the last couple of years [54].
sent2: Large spectrums of domains are using this data, some of which are using it for academic research and others for applications [55].
sent3: New improvements regarding twitter data are presented by this section.
sent4: The document collection from various resources triggers the ""Text Mining"" process.
sent5: A particular document would be retrieved by Text mining tool and this document is pre-processed by checking the character sets and format [56].
sent6: Subsequently, a text analysis phase would monitor the document.
sent7: Semantic analysis is used to derive high-quality information from text; this is referred to ""Text analysis"".
sent8: The market has a lot of text analysis techniques.
sent9: Professionals can use combinations of techniques subject to the goal of the organization.
sent10: Researchers tend to repeat the text analysis techniques till the time information is acquired.
sent11: A management information system is capable of incorporating the resulting information, and as a result, significant knowledge is produced for the user of that information system [57].
sent12: A key issue in text mining is intricacy of natural language.
sent13: The ambiguity problem is much dense in the natural language.
sent14: There are multiple meanings of a single word and multiple words can possess same meaning.
sent15: Ambiguity is referred to as the understanding of a word which has more than one possible meaning.
sent16: Noise has emerged in extracted information as a result of this ambiguity.
sent17: Since usability and flexibility are the main parts of ambiguity, it cannot be removed from the natural language.
sent18: One phrase or sentence can have multiple understandings, so there is a chance we can obtain a number of meanings.
sent19: The work is still undeveloped and a particular domain is correlated with the suggested approach while the experts have attempted to resolve the ambiguity problem by performing a number of research studies.
sent20: As there is uncertainty/vagueness in the semantic meanings of many discovered words, so it is very difficult to answer the requirements of the user.
sent21: Scholars of [58] developed and formulated an automatic classification technique through which potentially abuseindicating user posts could be identified and evaluating the likelihood of social media usage as a source for automatic monitoring of drug medication abuse.
sent22: In this regard, Twitter user posts (tweets) were collected and these were linked with three commonly abused medications (Oxycodone, Adderall, and Quetiapine).
sent23: Besides interpreting a control medication (metformin), which is not the subject of abuse due to its process, nearly 6400 tweets were manually annotated, where these three medications were pointed out.
sent24: The annotated data was qualitatively and quantitatively analyzed to determine as to whether or not signals of drug medication abuse are presented in Twitter posts.
sent25: To sum up, Twitter's value was assessed in exploring the patterns of abuse over time and an automatic supervised classification technique was also designed, in which the purpose was to observe and separate the posts containing signals of medication abuse from those that do not.
sent26: According to the findings of investigations, Twitter posts have yielded clear signals of medication abuse.
sent27: As compared to the proportion for the control medication (i.e., metformin: 0.3 %), there is a very high ratio of tweets containing abuse signals for the three case medications (Adderall: 23 %, oxycodone: 12 %, quetiapine: 5.0 %).
sent28: In addition, almost 82 % accuracy (medication abuse class recall: 0.51, precision: 0.41, F-measure: 0.46) has been achieved through the automatic classification approach.
sent29: The Study demonstrated how the abuse patterns over time can be analyzed by using the classification data and its goal is to illustrate the effectiveness of automatic classification.
sent30: As a result, it is found that abuse-related information for medications can be significantly acquired from social media, and the research indicates that natural language processing and supervised classification are the automatic approaches that have potentials for future monitoring and intervention assignments.
sent31: With respect to supervised learning, the lack of sufficient training data is believed to be the largest shortcoming of the study.
sent32: Both annotation and automatic classification are hindered by the lack of context and ambiguity in tweets.
sent33: During the course of annotations, many ambiguous tweets were found and services of pharmacology expert were hired to address these issues.
sent34: As a result of these ambiguities, the undefined situation is observed in the binary classification process and this inadequacy will continue until the time fine-tuned annotation rules could be specified by the future annotation rules.
sent35: A study by [59] applied the text mining approach on a large dataset of tweets.
sent36: The complete Twitter timelines of 10 academic libraries were used to collect the dataset for this research.
sent37: Nearly 23,707 tweets formed the total dataset, where there were 7625 hashtags, 17,848 mentions, and 5974 retweets.
sent38: Inconsistency among academic libraries is found in the distribution of tweets.
sent39: ""Open"" was the most repeated word that was used by the academic libraries in different perspectives.
sent40: It was observed that ""special collections"" was the most frequent bigram (two-word sequence) in the aggregated tweets.
sent41: While ""save the date"" was the most recurrent tri-gram (three-word sequence).
sent42: In the semantic analysis, words such as ""insight, knowledge, and information about cultural and personal relations"" were the most frequent word categories.
sent43: Moreover, ""Resources"" was the most widespread category of the tweets among all the selected academic libraries.
sent44: The significance of data and text-mining approaches are reported within the study and their purpose is to gain an insight with the aggregate social data of academic libraries so that the process of decision-making and strategic planning could become facilitated for marketing of services and patron outreach.
sent45: The 10 academic libraries from top global universities have undergone the text mining approach.
sent46: The study aimed to illustrate their Twitter usage and to examine their tweet content.
sent47: As far as social media is concerned, decision-making is supported and user-generated text is analyzed through text mining and content analysis [60].
sent48: By employing an archiving service (twimemachine.com) in December 2014, the complete Twitter timelines of 10 academic libraries were taken into account to collect the dataset for this research.
sent49: The libraries of 10 highestranking universities from the global Shanghai Ranking were chosen for that purpose.
sent50: The language of the university must be English-based, which was the condition for selection and selection was restricted to only one library if there was more than one library in the university.
sent51: Certain weaknesses were found in the study, for example, all of the libraries are English-language libraries in the sample and only 10 academic libraries were considered for the analysis.
sent52: This gap must be filled in future by applying the analysis to a dataset from diversified academic libraries, including non-English language libraries.
sent53: Consequently, a complete understanding of tweet patterns would be acknowledged.
sent54: The future inquiry can also incorporate the international or crosscultural comparisons.
sent55: Any discrepancy among libraries in their tweets' content affected by the number and interaction of followers could be highlighted by the analysis and its findings.
sent56: The accuracy of the tweet categorization tool has yielded the inadequate findings, and the said tool needs to be substantiated through other machine-learning models along with their applications.
sent57: Researchers of [55] demonstrated in a smoking cessation nicotine patch study an innovative Twitter recruitment system that is deployed by the group.
sent58: The study aimed to describe the methodology and used to address the issue of digital recruitment.
sent59: Furthermore, designing a rule-based system with the provision of system specification besides representing the data mining approaches and algorithms (classification and association analysis) using Twitter data.
sent60: Twitter's streaming API captured two sets of streaming tweets, which were collected for the study.
sent61: Ten search terms, (i.e. quitting, quit, nicotine, smoking, smoke, patches, cig, cigarette, ecig, cigs, marijuana) were used to gather the first set.
sent62: The second set of tweets contains 30 terms, in which the terms from the first set were included.
sent63: Moreover, the second set is a superset of the first one.
sent64: A number of studies have been conducted to review the information gathering methods.
sent65: As unstructured data sets are in the textual format, the use of various procedures of text mining has been tackled by many research studies.
sent66: Nonetheless, the data sets on the social networking websites are not mainly discussed by these studies.
sent67: A study by [50] applied various text mining techniques.
sent68: The study would describe the application of these strategies in the social networking websites.
sent69: In the field of intelligent text analysis, the latest improvements would also be examined in the survey.
sent70: The study focused on two key techniques pertaining to the text mining field, namely classification and clustering.
sent71: Usually, they are operated for the study of the unstructured text accessible on the extensive scale frameworks.
sent72: Prior to the start of World Cup, a total of approximately 30,000 tweets were used by [61].
sent73: Moreover, an algorithm was used for integrating the consensus matrix and the DBSCAN algorithm.
sent74: Consequently, the concerned tweets on those prevailing topics were available to him.
sent75: Afterward, the clustering analysis was applied to seek the topics discussed by the tweets.
sent76: The tweets were grouped utilizing the k-means [62], Non-Negative Matrix Factorization (NMF), and a popular clustering algorithm.
sent77: After that, the results were compared.
sent78: Similar results were delivered by both algorithms.
sent79: However, NMF became faster and the researchers could easily interpret the outcomes.
sent80: A study by [1] initiated a workflow to gain an insight into both the large-scale data mining methods and qualitative analysis.
sent81: Twitter posts of engineering students were the primary concern.
sent82: The basic goal was to identify their issues in their academic experiences.
sent83: The study conducted a qualitative analysis of samples obtained from around 25,000 tweets that were associated with the engineering students and their college life.
sent84: The encounter troubles of engineering students were discovered during the study.
sent85: For example, a large volume of study, sleep deprivation and lack of social engagement.
sent86: Considering these outcomes, a multi-label classification algorithm was implemented to categorize tweets in lieu of students' queries.
sent87: The algorithm was applied on approximately 35,000 tweets streamed at the geo-location of Purdue University.
sent88: At the first instance, the concerned authorities have addressed the experiences and issues of the students and social media data was used to expose the issues.
sent89: Moreover, a study by [1] also developed a multi-label classifier so that tweets founded within the content evaluation phase could be organized.
sent90: A number of renowned classifiers are significantly consumed in machine learning domain and data mining process.
sent91: With Comparison to other state-of-the-art multi-label classifiers, the Naïve Bayes classifiers were found proficient on the dataset.
sent92: A study by [63] discussed the clustering technique, the execution of correlation and association analyses to social media.
sent93: The investigation of insurance Twitter posts was carried out to assess this matter.
sent94: Consequently, recognizing theories and keywords in the social media data has become an easy task, due to which the information by insurers and its application would be facilitated.
sent95: After having a detailed analysis, client queries and the potential market would be proactively addressed with usefulness and the findings of the analysis are to be effectively implemented in suitable fields.
sent96: According to this evaluation, the overall 68,370 tweets were utilized.
sent97: Two additional kinds of evaluation need to be applied to the data.
sent98: The first is the clustering analysis, through which the tweets depending on their similarities or dissimilarities would be merged.
sent99: An Association Analysis is the second one whereas the occurrences of particular composed words were discovered.
sent100: Authors of [64] stated that sentiment analysis through social media usage has witnessed a huge interest from scholars in the last few years.
sent101: In that, the authors discussed the influence of tweets' sentiment on elections and the impact of the elections' results on web sentiment.","1. What has occupied a significant size of research over the last couple of years? sent1
2. What are some domains using Twitter data for? sent2
3. What does this section present regarding Twitter data? sent3
4. What triggers the ""Text Mining"" process? sent4
5. How is a particular document processed in text mining? sent5
6. What phase follows the pre-processing of a document in text mining? sent6
7. What is semantic analysis used for in text mining? sent7
8. What does the market have a lot of in terms of text analysis? sent8
9. How do professionals use text analysis techniques? sent9
10. What do researchers tend to do until information is acquired? sent10
11. What is a management information system capable of incorporating? sent11
12. What is a key issue in text mining? sent12
13. What problem is dense in natural language? sent13
14. What is ambiguity in natural language? sent14, sent15
15. What has emerged in extracted information due to ambiguity? sent16
16. Why can't ambiguity be removed from natural language? sent17
17. What can one phrase or sentence have, leading to multiple meanings? sent18
18. What is the current state of work on resolving ambiguity in text mining? sent19
19. Why is it difficult to answer user requirements in text mining? sent20
20. What did scholars develop to identify potentially abuse-indicating user posts? sent21
    20.1. What was collected and linked with three commonly abused medications? sent22
    20.2. How many tweets were manually annotated, and what was the focus? sent23
    20.3. What was analyzed to determine signals of drug medication abuse in Twitter posts? sent24
    20.4. What was the purpose of the automatic supervised classification technique? sent25
    20.5. What did the findings of investigations reveal about Twitter posts? sent26
    20.6. What is the ratio of tweets containing abuse signals for the three case medications? sent27
    20.7. What accuracy was achieved through the automatic classification approach? sent28
    20.8. What did the study demonstrate regarding abuse patterns over time? sent29
    20.9. What does the research indicate about acquiring abuse-related information from social media? sent30
21. What is believed to be the largest shortcoming of the study regarding supervised learning? sent31
22. What hinders both annotation and automatic classification in tweets? sent32
23. What was found during the course of annotations, and how were these issues addressed? sent33
24. What is observed in the binary classification process due to ambiguities? sent34
25. What approach was applied to a large dataset of tweets in a study? sent35
    25.1. What was used to collect the dataset for this research? sent36
    25.2. What formed the total dataset, and what were its components? sent37
    25.3. What inconsistency was found among academic libraries? sent38
    25.4. What was the most repeated word used by academic libraries? sent39
    25.5. What was the most frequent bigram in the aggregated tweets? sent40
    25.6. What was the most recurrent tri-gram? sent41
    25.7. What were the most frequent word categories in the semantic analysis? sent42
    25.8. What was the most widespread category of tweets among academic libraries? sent43
    25.9. What is the significance of data and text-mining approaches reported in the study? sent44
    25.10. What was the aim of the study regarding Twitter usage and tweet content? sent46
    25.11. How is decision-making supported in social media? sent47
    25.12. How was the dataset for this research collected? sent48
    25.13. What was the condition for selection of libraries in the study? sent50
    25.14. What weaknesses were found in the study? sent51
    25.15. What gap must be filled in future studies? sent52
    25.16. What can future inquiry incorporate? sent54
    25.17. What could be highlighted by the analysis and its findings? sent55
    25.18. What needs to be substantiated through other machine-learning models? sent56
26. What did researchers demonstrate in a smoking cessation nicotine patch study? sent57
    26.1. What was the aim of the study regarding digital recruitment? sent58
    26.2. What was designed besides representing data mining approaches? sent59
    26.3. How were tweets collected for the study? sent60
    26.4. What terms were used to gather the first set of tweets? sent61
    26.5. What does the second set of tweets contain? sent62
    26.6. What is the relationship between the first and second set of tweets? sent63
27. What have many research studies tackled regarding unstructured data sets? sent65
28. What is not mainly discussed by studies on social networking websites? sent66
29. What techniques were applied in a study on social networking websites? sent67
    29.1. What would the study describe regarding text mining strategies? sent68
    29.2. What would be examined in the survey regarding intelligent text analysis? sent69
    29.3. What were the key techniques focused on in the study? sent70
    29.4. What are these techniques usually operated for? sent71
30. What was used by a study prior to the start of the World Cup? sent72
    30.1. What algorithm was used for integrating the consensus matrix? sent73
    30.2. What was available to the researcher after using the algorithm? sent74
    30.3. What analysis was applied to seek topics discussed by tweets? sent75
    30.4. How were tweets grouped in the study? sent76
    30.5. What was done after grouping the tweets? sent77
    30.6. What were the results of the algorithms used? sent78
    30.7. What advantage did NMF have over other algorithms? sent79
31. What did a study initiate to gain insight into data mining methods and qualitative analysis? sent80
    31.1. What was the primary concern of the study? sent81
    31.2. What was the basic goal of the study? sent82
    31.3. What did the study conduct a qualitative analysis of? sent83
    31.4. What troubles were discovered during the study? sent84
    31.5. What was implemented to categorize tweets based on students' queries? sent86
    31.6. How was the algorithm applied in the study? sent87
    31.7. What did the concerned authorities address using social media data? sent88
    31.8. What was developed to organize tweets in the content evaluation phase? sent89
    31.9. What classifiers are significantly consumed in machine learning and data mining? sent90
    31.10. What was found proficient on the dataset compared to other classifiers? sent91
32. What did a study discuss regarding social media? sent92
    32.1. What was the investigation of insurance Twitter posts carried out to assess? sent93
    32.2. What has become an easy task due to recognizing theories and keywords in social media data? sent94
    32.3. What would be proactively addressed with the findings of the analysis? sent95
    32.4. How many tweets were utilized in the evaluation? sent96
    32.5. What two additional kinds of evaluation need to be applied to the data? sent97
    32.6. What is the first kind of evaluation? sent98
    32.7. What is the second kind of evaluation? sent99
33. What has witnessed a huge interest from scholars in recent years? sent100
    33.1. What did the authors discuss regarding tweets' sentiment? sent101"
231925325,Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition,https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23,A. RNN-based models,17,"In this section, we will discuss the literature where attention mechanism is applied for streaming speech recognition with RNN-based encoder decoder models. To work with streaming speech, it is first required to obtain the speech frame or the set of speech frames on which attention mechanism will work. A Gaussian prediction-based attention mechanism is proposed in [38] for streaming speech recognition. Instead of looking at the entire encoder hidden states, at each decoder time step, only a set of encoder hidden states are attended based on a Gaussian window. The centre and the size of window at a particular decoder time step, t are determined by its mean (µ t ) and variance (σ t ) which are predicted given the previous decoder state. Specifically, the current window centre is determined by a predicted moving forward increment ( µ t ) and last window centre. µ t = µ t + µ t−1 . A different approach compared to (5) has been considered to calculate the similarity between j th encoder state (within the current window) and i th encoder state and it is given by (20):

A hard monotonic attention mechanism is proposed in [27]. Only a single encoder hidden state h i (i represents a decoder time step and h i represents the only encoder state selected for output prediction at i th decoder time step) which scores the highest similarity with the current decoder state is selected by passing the concerned attention probabilities through a categorical function. A stochastic process is used to enable attending encoder hidden states only from left to right direction. At each decoder time step, the attention mechanism starts processing from h i−1 to the proceeding states. h i−1 is the encoder state which was attended at last decoder time step. Each calculated similarity score (e i,j ) is then sequentially passed through a logistic sigmoid function to produce selection probabilities (p i,j ) followed by a Bernoulli distribution and once it outputs 1, the attention process stops. The last attended encoder hidden state, h i at the current decoder time step is then set as the context for the current decoder time step, i.e. c i = h i . Although the encoder states within the window of boundary [h i−1 , h i ] are processed, only a single encoder state is finally selected for the current prediction.

[27] provides linear time complexity and online speech decoding, it only attends a single encoer state for each output prediction and it may cause degradation to the performance. Therefore, monotonic chunkwise attention (MoChA) is proposed in [28] where decoder attends small ""chunks"" of encoder states within a window containing a fixed number of encoder states prior to and including h i . Due to its effectiveness, MoChA is also used to develop an on-device commercialised ASR system [40]. To increase the effectiveness of the matching scores obtained to calculate the attention probabilities between the decoder state and the chunk encoder states, multi-head monotonic chunkwise attention (MTH-MoChA) is proposed in [39]. MTH-MoChA splits the encoder and decoder hidden states into K heads. K is experimentally set as 4. For each head, matching scores, attention probabilities and the context vectors are calculated to extract the dependencies between the encoder and decoder hidden states. Finally, the average context vector over all the heads takes part in decoding.

The pronunciation rate among different speakers may vary and therefore, the attention calculated over the fixed chunk size may not be effective. To overcome this, in [29] an adaptive monotonic chunkwise attention (AMoChA) was proposed where attention at current decoder time step is computed over a window whose boundary [h i−1 , h i ] is computed as in [27]. Within the window, whichever encoder states results in p i,j > 0.5 or e i,j > 0 are attended. Hence, the chunk size is adaptive instead of constant.

The input sequence or the encoder states of length L is divided equally into W in [41]. So, each block contains B = L W encoder states, while the last block may contain fewer than B encoder states. In this model, each block is responsible for a set of output predictions and attention is computed over only the concerned blocks and not the entire encoder states. Once the model has finished attending all the encoder states of a block and predicting the required outputs, it emits a special symbol called < epsilon > which marks the end of the corresponding block processing and the model proceeds to attend the next block. The effectiveness of this model has been enhanced in [42] by extending the attention span. Specifically, the attention mechanism looks at not only the current block but the k previous blocks. Experimentally, k is set as 20.

The authors of [44] have identified the latency issue in streaming attention-based models. In most streaming models, the encoder states are attended based on a local window. Computing the precise boundaries of these local windows is a computational expensive process which in turn causes a delay in the speech-to-text conversion. To overcome this issue, in [44] external hard alignments obtained from a hybrid ASR system is used for frame-wise supervision to force the MoChA model to learn accurate boundaries and alignments. In [80] performance latency is reduced by proposing a unidirectional encoder with no future dependency. Since each position does not depend on future context, the decoder hidden states are not required to be re-computed every time a new input chunk arrives and therefore, the overall delay is reduced.

In [43], attention mechanism has been incorporated in RNN-Transducer (RNN-T) [12], [13] to make streaming speech recognition more effective and efficient. RNN-T consists of three sections: (i) a RNN encoder which processes an input sequence to encoder hidden states, (ii) a RNN decoder which is analogues to a language model takes the previous predicted symbol as input and outputs decoder hidden states, and (iii) a joint network that takes encoder and decoder hidden states at the current time step to compute output logit which is responsible to predict the output symbol when passed through a softmax layer. In [43], at the encoder side, to learn contextual dependency, a multi-head self-attention layer is added on the top of RNN layers. In addition, the joint network attends a chunk of encoder hidden states instead of attending only the current hidden state at each time step.

LAS model is primarily proposed for offline speech recognition. However, it has been modified with silence modelling for working in the streaming environment in [45]. Given streamable encoder and a suitable attention mechanism (hard monotonic, chunkwise or local window-based instead of global), the main limitation of LAS model to perform in streaming environment is a long enough silence between the utterances to make decoder believe it is the end of speech. Therefore, the LAS decoder terminates the transcription process while the speaker is still active (i.e. early stopping). This limitation is addressed in [45] by incorporating reference silence tokens during the training phase to supervise the model when to output a silence token instead of terminating the process during the inference phase.","sent1: In this section, we will discuss the literature where attention mechanism is applied for streaming speech recognition with RNN-based encoder decoder models.
sent2: To work with streaming speech, it is first required to obtain the speech frame or the set of speech frames on which attention mechanism will work.
sent3: A Gaussian prediction-based attention mechanism is proposed in [38] for streaming speech recognition.
sent4: Instead of looking at the entire encoder hidden states, at each decoder time step, only a set of encoder hidden states are attended based on a Gaussian window.
sent5: The centre and the size of window at a particular decoder time step, t are determined by its mean (µ t ) and variance (σ t ) which are predicted given the previous decoder state.
sent6: Specifically, the current window centre is determined by a predicted moving forward increment ( µ t ) and last window centre.
sent7: µ t = µ t + µ t−1 . A different approach compared to (5) has been considered to calculate the similarity between j th encoder state (within the current window)
sent8: and i th encoder state and it is given by (20):A hard monotonic attention mechanism is proposed in [27].
sent9: Only a single encoder hidden state h i (i represents a decoder time step and h i represents the only encoder state selected for output prediction at i th decoder time step) which scores the highest similarity with the current decoder state is selected by passing the concerned attention probabilities through a categorical function.
sent10: A stochastic process is used to enable attending encoder hidden states only from left to right direction.
sent11: At each decoder time step, the attention mechanism starts processing from h i−1 to the proceeding states.
sent12: h i−1 is the encoder state which was attended at last decoder time step.
sent13: Each calculated similarity score (e i,j ) is then sequentially passed through a logistic sigmoid function to produce selection probabilities (p i,j ) followed by a Bernoulli distribution and once it outputs 1, the attention process stops.
sent14: The last attended encoder hidden state, h i at the current decoder time step is then set as the context for the current decoder time step, i.e. c
sent15: i = h i . Although the encoder states within the window of boundary [h i−1 , h i ] are processed, only a single encoder state is finally selected for the current prediction.
sent16: [27] provides linear time complexity and online speech decoding, it only attends a single encoer state for each output prediction and it may cause degradation to the performance.
sent17: Therefore, monotonic chunkwise attention (MoChA) is proposed in [28] where decoder attends small ""chunks"" of encoder states within a window containing a fixed number of encoder states prior to and including h i .
sent18: Due to its effectiveness, MoChA is also used to develop an on-device commercialised ASR system [40].
sent19: To increase the effectiveness of the matching scores obtained to calculate the attention probabilities between the decoder state and the chunk encoder states, multi-head monotonic chunkwise attention (MTH-MoChA) is proposed in [39].
sent20: MTH-MoChA splits the encoder and decoder hidden states into K heads.
sent21: K is experimentally set as 4. For each head, matching scores, attention probabilities and the context vectors are calculated to extract the dependencies between the encoder and decoder hidden states.
sent22: Finally, the average context vector over all the heads takes part in decoding.
sent23: The pronunciation rate among different speakers may vary and therefore, the attention calculated over the fixed chunk size may not be effective.
sent24: To overcome this, in [29] an adaptive monotonic chunkwise attention (AMoChA) was proposed where attention at current decoder time step is computed over a window whose boundary [h i−1 , h i ] is computed as in [27].
sent25: Within the window, whichever encoder states results in p i,j > 0.5 or e i,j > 0 are attended.
sent26: Hence, the chunk size is adaptive instead of constant.
sent27: The input sequence or the encoder states of length L is divided equally into W in [41].
sent28: So, each block contains B = L W encoder states, while the last block may contain fewer than B encoder states.
sent29: In this model, each block is responsible for a set of output predictions and attention is computed over only the concerned blocks and not the entire encoder states.
sent30: Once the model has finished attending all the encoder states of a block and predicting the required outputs, it emits a special symbol called < epsilon > which marks the end of the corresponding block processing and the model proceeds to attend the next block.
sent31: The effectiveness of this model has been enhanced in [42] by extending the attention span.
sent32: Specifically, the attention mechanism looks at not only the current block but the k previous blocks.
sent33: Experimentally, k is set as 20.
sent34: The authors of [44] have identified the latency issue in streaming attention-based models.
sent35: In most streaming models, the encoder states are attended based on a local window.
sent36: Computing the precise boundaries of these local windows is a computational expensive process which in turn causes a delay in the speech-to-text conversion.
sent37: To overcome this issue, in [44] external hard alignments obtained from a hybrid ASR system is used for frame-wise supervision to force the MoChA model to learn accurate boundaries and alignments.
sent38: In [80] performance latency is reduced by proposing a unidirectional encoder with no future dependency.
sent39: Since each position does not depend on future context, the decoder hidden states are not required to be re-computed every time a new input chunk arrives and therefore, the overall delay is reduced.
sent40: In [43], attention mechanism has been incorporated in RNN-Transducer (RNN-T) [12], [13] to make streaming speech recognition more effective and efficient.
sent41: RNN-T consists of three sections: (i) a RNN encoder which processes an input sequence to encoder hidden states, (ii) a RNN decoder which is analogues to a language model takes the previous predicted symbol as input and outputs decoder hidden states, and (iii) a joint network that takes encoder and decoder hidden states at the current time step to compute output logit which is responsible to predict the output symbol when passed through a softmax layer.
sent42: In [43], at the encoder side, to learn contextual dependency, a multi-head self-attention layer is added on the top of RNN layers.
sent43: In addition, the joint network attends a chunk of encoder hidden states instead of attending only the current hidden state at each time step.
sent44: LAS model is primarily proposed for offline speech recognition.
sent45: However, it has been modified with silence modelling for working in the streaming environment in [45].
sent46: Given streamable encoder and a suitable attention mechanism (hard monotonic, chunkwise or local window-based instead of global), the main limitation of LAS model to perform in streaming environment is a long enough silence between the utterances to make decoder believe it is the end of speech.
sent47: Therefore, the LAS decoder terminates the transcription process while the speaker is still active (i.e. early stopping).
sent48: This limitation is addressed in [45] by incorporating reference silence tokens during the training phase to supervise the model when to output a silence token instead of terminating the process during the inference phase.","1. What is the focus of the section regarding RNN-based models? sent1
    1.1. What is required to work with streaming speech in RNN-based models? sent2
    1.2. What attention mechanism is proposed for streaming speech recognition? sent3
        1.2.1. How does the Gaussian prediction-based attention mechanism work? sent4, sent5, sent6
        1.2.2. What is a different approach to calculate similarity between encoder states? sent7
    1.3. What is the hard monotonic attention mechanism? sent8
        1.3.1. How does the hard monotonic attention mechanism select encoder states? sent9, sent10, sent11, sent12, sent13, sent14, sent15
        1.3.2. What are the limitations of the hard monotonic attention mechanism? sent16
    1.4. What is monotonic chunkwise attention (MoChA)? sent17
        1.4.1. How is MoChA used in commercial ASR systems? sent18
        1.4.2. What is multi-head monotonic chunkwise attention (MTH-MoChA)? sent19, sent20, sent21, sent22
        1.4.3. What challenge does AMoChA address and how? sent23, sent24, sent25, sent26
    1.5. How is the input sequence divided in some models? sent27, sent28
        1.5.1. How is attention computed in block-based models? sent29, sent30
        1.5.2. How is the attention span extended in block-based models? sent31, sent32, sent33
    1.6. What issue is identified in streaming attention-based models? sent34
        1.6.1. How is the latency issue addressed in some models? sent35, sent36, sent37, sent38, sent39
    1.7. How is attention incorporated in RNN-Transducer models? sent40
        1.7.1. What are the components of RNN-T? sent41
        1.7.2. How is contextual dependency learned in RNN-T? sent42, sent43
    1.8. What is the LAS model originally proposed for? sent44
        1.8.1. How is the LAS model modified for streaming environments? sent45, sent46, sent47, sent48"
259376518,Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of Biomedical Research Articles,https://www.semanticscholar.org/paper/3bef44ef3aacddaae56d6efc9f6767e64bc65b31,Systems Overview,6,"Three teams have made in total 7 attempts for Subtask 2. A brief description of their respecitve approaches are as following: LHS712EE (Liu et al., 2023) The team carried on with the LED (Beltagy et al., 2020) model trained on the PLOS dataset from Subtask 1 to test the generalizability of their approach in generating lay summaries coupled with a pre-trained LED model for abstractive summaries.They later retrained the model using the abstract section of the dataset to improve performance in generating technical abstracts.

Pathology Dynamics (Al-Hussaini et al., 2023) As the abstract with the most salient information is no longer present in the input, to tackle the long context input, the team trained a base LSG model (Condevaux and Harispe, 2022) and truncated each article to the first 4096 tokens for generating both abstracts and lay summaries.The model was then trained on a merged dataset that uses each article twice, with one output having the lay summaries and the other having the abstract.They also reported using simplification procedures such as MUSS (Martin et al., 2022) to enhance the lay summary or other instruction-following models such as T5 with different prefix for summarisation.(Chen et al., 2023) This team made use of different models for each submission, including Primera, a PEGASUS model pre-trained on PubMed, and a BART-large Longformer model.","sent1: Three teams have made in total 7 attempts for Subtask 2.
sent2: A brief description of their respecitve approaches are as following: LHS712EE (Liu et al., 2023)
sent3: The team carried on with the LED (Beltagy et al., 2020) model trained on the PLOS dataset from Subtask 1 to test the generalizability of their approach in generating lay summaries coupled with a pre-trained LED model for abstractive summaries.
sent4: They later retrained the model using the abstract section of the dataset to improve performance in generating technical abstracts.
sent5: Pathology Dynamics (Al-Hussaini et al., 2023)
sent6: As the abstract with the most salient information is no longer present in the input, to tackle the long context input, the team trained a base LSG model (Condevaux and Harispe, 2022) and truncated each article to the first 4096 tokens for generating both abstracts and lay summaries.
sent7: The model was then trained on a merged dataset that uses each article twice, with one output having the lay summaries and the other having the abstract.
sent8: They also reported using simplification procedures such as MUSS (Martin et al., 2022) to enhance the lay summary or other instruction-following models such as T5 with different prefix for summarisation.(Chen et al., 2023)
sent9: This team made use of different models for each submission, including Primera, a PEGASUS model pre-trained on PubMed, and a BART-large Longformer model.","1. How many teams participated in Subtask 2 and how many attempts did they make? sent1
2. What approach did the LHS712EE team use for generating lay summaries? sent2, sent3
    2.1. How did the LHS712EE team improve the performance of their model? sent4
3. How did the Pathology Dynamics team handle the long context input for generating summaries? sent5, sent6
    3.1. What dataset and training strategy did the Pathology Dynamics team use? sent7
    3.2. What additional procedures did the Pathology Dynamics team use to enhance lay summaries? sent8
4. What models did the third team use for their submissions? sent9"
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,Deep features,10,"AlexNet [69] It is small network and 61M parameters to learn, but the classification accuracy is low. MobileNet [80] It's a light weight network and 4.2M parameters to learn, but it is having little high error rate. ResNet-50 [80,81,83] It is having advantage of residual block which is having skip connection from input for better learning ability. Over 25M parameters to learn. DenseNet [78,79,80] It is having connection from each layer to every other layer and hence leveraging information from other layers which learns much better compare to other networks.

features are learnable by conducting uniform partition on the conv-layer without explicitly partitioning images. The spatial consistency within-part is exploitable to refine the coarse partition provided by PCB. The improvement in the uniform partition is made by the Refined Part Pooling (RPP) network. It achieves 92.3% of rank-1 accuracy with only the PCB. Further, it improves to 93.8% of rank-1 accuracy by employing RPP with PCB network on the Market-1501 [105] dataset. The text attribute combinations exist on a large scale in a real scenario. However, a minimal amount of combination with sufficient data is available for training. Except for such combinations, others are never modelled during training. Thus, Dong et al. [83] formulate a textual attribute query-based person search problem as a zero-shot (ZSL) learning problem for the first time. The authors propose Attribute-Image Hierarchical Matching (AIHM), which matches attributes and images at multiple hierarchical levels. The algorithm achieves state-of-the-art rank-1 mAP of 43.3% on Market-1501 [105], 50.5% on DukeMTMC [107] and 31.3% on PA100K [117] datasets. Table 5 shows a comparison of handcrafted and deep feature-based methods with their advantages and disadvantages.

A query-based personal retrieval system usually provides soft biometric keywords as the input query. Improvisation in natural language processing now allows automatic extraction of the keywords from sentences. It will move the system towards full automation.","sent1: AlexNet [69] It is small network and 61M parameters to learn, but the classification accuracy is low.
sent2: MobileNet [80] It's a light weight network and 4.2M parameters to learn, but it is having little high error rate.
sent3: ResNet-50 [80,81,83] It is having advantage of residual block which is having skip connection from input for better learning ability.
sent4: Over 25M parameters to learn. DenseNet [78,79,80]
sent5: It is having connection from each layer to every other layer and hence leveraging information from other layers which learns much better compare to other networks.
sent6: features are learnable by conducting uniform partition on the conv-layer without explicitly partitioning images.
sent7: The spatial consistency within-part is exploitable to refine the coarse partition provided by PCB.
sent8: The improvement in the uniform partition is made by the Refined Part Pooling (RPP) network.
sent9: It achieves 92.3% of rank-1 accuracy with only the PCB.
sent10: Further, it improves to 93.8% of rank-1 accuracy by employing RPP with PCB network on the Market-1501 [105] dataset.
sent11: The text attribute combinations exist on a large scale in a real scenario.
sent12: However, a minimal amount of combination with sufficient data is available for training.
sent13: Except for such combinations, others are never modelled during training.
sent14: Thus, Dong et al. [83] formulate a textual attribute query-based person search problem as a zero-shot (ZSL) learning problem for the first time.
sent15: The authors propose Attribute-Image Hierarchical Matching (AIHM), which matches attributes and images at multiple hierarchical levels.
sent16: The algorithm achieves state-of-the-art rank-1 mAP of 43.3% on Market-1501 [105], 50.5% on DukeMTMC [107] and 31.3% on PA100K [117] datasets.
sent17: Table 5 shows a comparison of handcrafted and deep feature-based methods with their advantages and disadvantages.
sent18: A query-based personal retrieval system usually provides soft biometric keywords as the input query.
sent19: Improvisation in natural language processing now allows automatic extraction of the keywords from sentences.
sent20: It will move the system towards full automation.","1. What is AlexNet and what are its characteristics? sent1
2. What is MobileNet and what are its characteristics? sent2
3. What advantage does ResNet-50 have? sent3
4. What is DenseNet and how does it leverage information from other layers? sent4, sent5
5. How are features learnable in the context of conv-layer partitioning? sent6
6. How is spatial consistency within-part used to refine partitions? sent7
7. What improvement does the Refined Part Pooling (RPP) network provide? sent8
8. What rank-1 accuracy is achieved with PCB and how is it improved with RPP? sent9, sent10
9. What challenge exists with text attribute combinations in real scenarios? sent11, sent12
10. How is the textual attribute query-based person search problem formulated by Dong et al.? sent13, sent14
11. What is the Attribute-Image Hierarchical Matching (AIHM) and what are its achievements? sent15, sent16
12. What does Table 5 show? sent17
13. What does a query-based personal retrieval system provide as input? sent18
14. How does natural language processing contribute to query-based personal retrieval systems? sent19, sent20"
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,Dataset,7,"In the detection of fake news, the dataset used can be divided into single-modal and multi-modal data, as shown in Figure 5.We gathered prevalent fake news datasets from the past five years based on citations.Multimodal data has a more diverse form, typically comprising a combination of images or video text, as demonstrated in Table 3. Abbreviations for technical terms are defined upon first use.In contrast, unimodal datasets exclusively comprise text, providing a more extensive characterization.According to the dataset-construction method, data characteristics and adaptation tasks, it can be divided into three categories:

BERT model [18] has significantly improved the performance of many natural language tasks.For fake news detection in natural language processing tasks of text data, the BERT pre-training model and related improved models gradually replace the original language model, which has become the basis of current research.Jwa et al. [87] combined news data in the pre-training phase to improve fake news recognition skills; Kaliyar et al. [92] proposed a BERT-based deep convolution method (fakeBERT) to detect fake news.The advantage of the pre-trained model is that the BERT method is unique in identifying and capturing contextual meanings in sentences or texts.In the model learning process, it does not need to go deep into the details of fake news to achieve good detection performance.

After several years of development, the BERT pre-training model based on transformer structure has gradually produced many related models [93][94][95] after structural adjustment, performance optimization and retraining.These models are collectively referred to as BERTology series models and have achieved good performance in various tasks.In summary, the fake news detection method based on the pre-trained model is already a research trend in this field.However, despite the complex characteristics of fake news, fake news detection based on pre-trained models still cannot achieve good performance in practical applications, like other practical tasks.How to extract features from more complex semantic information about fake news and establish a more effective fake news detection model for the 'pre-training + fine-tuning' paradigm of the pre-training model [96] is still an urgent problem to be solved.","sent1: In the detection of fake news, the dataset used can be divided into single-modal and multi-modal data, as shown in Figure 5.We gathered prevalent fake news datasets from the past five years based on citations.
sent2: Multimodal data has a more diverse form, typically comprising a combination of images or video text, as demonstrated in Table 3.
sent3: Abbreviations for technical terms are defined upon first use.
sent4: In contrast, unimodal datasets exclusively comprise text, providing a more extensive characterization.
sent5: According to the dataset-construction method, data characteristics and adaptation tasks, it can be divided into three categories:BERT model [18] has significantly improved the performance of many natural language tasks.
sent6: For fake news detection in natural language processing tasks of text data, the BERT pre-training model and related improved models gradually replace the original language model, which has become the basis of current research.
sent7: Jwa et al. [87] combined news data in the pre-training phase to improve fake news recognition skills; Kaliyar et al. [92] proposed a BERT-based deep convolution method (fakeBERT) to detect fake news.
sent8: The advantage of the pre-trained model is that the BERT method is unique in identifying and capturing contextual meanings in sentences or texts.
sent9: In the model learning process, it does not need to go deep into the details of fake news to achieve good detection performance.
sent10: After several years of development, the BERT pre-training model based on transformer structure has gradually produced many related models [93][94][95] after structural adjustment, performance optimization and retraining.
sent11: These models are collectively referred to as BERTology series models and have achieved good performance in various tasks.
sent12: In summary, the fake news detection method based on the pre-trained model is already a research trend in this field.
sent13: However, despite the complex characteristics of fake news, fake news detection based on pre-trained models still cannot achieve good performance in practical applications, like other practical tasks.
sent14: How to extract features from more complex semantic information about fake news and establish a more effective fake news detection model for the 'pre-training + fine-tuning' paradigm of the pre-training model [96] is still an urgent problem to be solved.","1. How can datasets for fake news detection be categorized? sent1
    1.1. What does multimodal data typically comprise? sent2
    1.2. What characterizes unimodal datasets? sent4
2. How can datasets be divided according to the dataset-construction method, data characteristics, and adaptation tasks? sent5
3. What role does the BERT pre-training model play in fake news detection for NLP tasks? sent6
    3.1. How have researchers improved fake news recognition using BERT? sent7
    3.2. What is the advantage of the BERT method in identifying fake news? sent8
    3.3. What is not required in the model learning process to achieve good detection performance? sent9
    3.4. What developments have occurred in the BERT pre-training model over the years? sent10
        3.4.1. What are these models collectively referred to as, and what have they achieved? sent11
4. What is the current research trend in fake news detection methods? sent12
5. What challenges remain in fake news detection using pre-trained models? sent13
    5.1. What is an urgent problem to be solved in the 'pre-training + fine-tuning' paradigm? sent14"
201070768,Enhancing the Demand for Labour survey by including skills from online job advertisements using model-assisted calibration,https://www.semanticscholar.org/paper/b1826f1b35347fb1c190f9296b3c2fb7139b9b72,Data integration approach,4,"Enhancing probability survey with online data (i.e. non-probability sample) may be achieved by data integration.  

The goal of data integration is to estimate some quantity (e.g. mean, total) of target variables Y present only in online data. Elliott and Valliant (2017) summarised possible approaches that consider pseudo-randomization (i.e calibration) or model-based approach. In addition, Kim and Wang (2018) consider mass imputation and double robust estimation that take into account propensity score weighting.

In the paper we consider pseudo-randomization approach in which pseudo-weights from nonprobability sample are calibrated to estimated totals T X or estimated total of Y based on approach introduced by Wu and Sitter (2001) and further developed for non-probability samples by Chen (2016). Detailed description is presented in the sections below.","sent1: Enhancing probability survey with online data (i.e. non-probability sample) may be achieved by data integration.
sent2: The goal of data integration is to estimate some quantity (e.g. mean, total) of target variables Y present only in online data.
sent3: Elliott and Valliant (2017) summarised possible approaches that consider pseudo-randomization (i.e calibration) or model-based approach.
sent4: In addition, Kim and Wang (2018) consider mass imputation and double robust estimation that take into account propensity score weighting.
sent5: In the paper we consider pseudo-randomization approach in which pseudo-weights from nonprobability sample are calibrated to estimated totals T X or estimated total of Y based on approach introduced by Wu and Sitter (2001) and further developed for non-probability samples by Chen (2016).
sent6: Detailed description is presented in the sections below.","1. How can probability surveys be enhanced with online data? sent1
2. What is the goal of data integration in the context of enhancing surveys? sent2
3. What approaches are summarized by Elliott and Valliant (2017) for data integration? sent3
4. What additional methods do Kim and Wang (2018) consider for data integration? sent4
5. What approach is considered in the paper for data integration, and how is it implemented? sent5
6. Where can a detailed description of the data integration approach be found? sent6"
149117098,Article Respondent Robotics: Simulating Responses to Likert-Scale Survey Items,https://www.semanticscholar.org/paper/30d7c07ec6575bf8b1e7b132dbd2d5ae10c087ee,Estimating Item Semantics,7,"A number of algorithms exist that allow computing the similarity of the survey items. Here, we have chosen one termed ""MI"" (Mihalcea, Corley, & Strapparava, 2006;Mohler & Mihalcea, 2009). MI is chosen because it has been previously published, is well understood, and allows easy replication. The Arnulf et al. study in 2014 also showed that MI values are probably closer to everyday language than some LSA-generated values that may carry specialized domain knowledge.

The MI algorithm derives its knowledge about words from a lexical database called WordNet, containing information about 147,278 unique words that were encoded by a team of linguists between 1990 and 2007 (Leacock, Miller, & Chodorow, 1998;Miller, 1995;Poli, Healy, & Kameas, 2010). Building on knowledge about each single word in WordNet as its point of departure, MI computes a similarity measure for two candidate sentences: S1 and S2. It identifies part of speech (POS), beginning with tokenization, and POS tagging of all the words in the survey item with their respective word classes (noun, verb, adverb, adjective, and cardinal, which play a very important role in text understanding). It then calculates word similarity by measuring each word in the sentence against all the words from the other sentence. This identifies the highest semantic similarity (maxSim) from six word-similarity metrics originally created to measure concept likeness (instead of word likeness). The metrics are adapted here to compute word similarity by computing the shortest distance of given words' synsets in the WordNet hierarchy. The word-word similarity measure is directional. It begins with each word in S1 being computed against each word in S2, and then vice versa. The algorithm finally considers sentence similarity by normalizing the highest semantic similarity (maxSim) for each word in the sentences by applying ""inverse document frequency"" (IDF) to the British National Corpus to weight rare and common terms. The normalized scores are then summed up for a sentence similarity score, SimMI, as follows:

where maxSim(w, S2) is the score of the most similar word in S2 to w, and IDF (w) is the IDF of word w. The final output of MI is a numeric value between 0 and 1, where 0 indicates no semantic overlap, and numbers approaching 1 indicate identical meaning of the two sentences. These numbers serve as the input to our simulating algorithm for constructing artificial responses. Note that the information in the MI values is entirely lexical and syntactic.

It contains no knowledge about surveys, leadership, or respondent behavior. The MLQ has 45 items. This yields (45 × (45 − 1)) / 2 or 990 unique item pairs, for which we obtain MI values.

One special problem concerns the direction of signs. In the MLQ, 264 of 990 pairs of items are negatively correlated. Theory suggests that two scales, Laissez-faire and Passive Management by Exception, are likely to relate negatively to effective leadership. The problem has been treated extensively elsewhere (Arnulf et al., 2014), so we will only offer a brief explanation here. MI does not take negative values, and does not differentiate well between positive and negative statements about the same content. For two items describing how (a) a manager is unapproachable when called for and (b) that the same person uses appropriate methods of leadership, the surveyed responses correlate at -.42 in the present sample, while the MI value is .38. The chosen solution is to allow MI values to be negative for all pairs of items from Laissezfaire and Passive Management by Exception (correctly identifying 255 of the 264 negative correlations, p < .001).","sent1: A number of algorithms exist that allow computing the similarity of the survey items.
sent2: Here, we have chosen one termed ""MI"" (Mihalcea, Corley, & Strapparava, 2006;Mohler & Mihalcea, 2009). MI is chosen because it has been previously published, is well understood, and allows easy replication.
sent3: The Arnulf et al. study in 2014 also showed that MI values are probably closer to everyday language than some LSA-generated values that may carry specialized domain knowledge.
sent4: The MI algorithm derives its knowledge about words from a lexical database called WordNet, containing information about 147,278 unique words that were encoded by a team of linguists between 1990 and 2007 (Leacock, Miller, & Chodorow, 1998;Miller, 1995;Poli, Healy, & Kameas, 2010).
sent5: Building on knowledge about each single word in WordNet as its point of departure, MI computes a similarity measure for two candidate sentences: S1 and S2.
sent6: It identifies part of speech (POS), beginning with tokenization, and POS tagging of all the words in the survey item with their respective word classes (noun, verb, adverb, adjective, and cardinal, which play a very important role in text understanding).
sent7: It then calculates word similarity by measuring each word in the sentence against all the words from the other sentence.
sent8: This identifies the highest semantic similarity (maxSim) from six word-similarity metrics originally created to measure concept likeness (instead of word likeness).
sent9: The metrics are adapted here to compute word similarity by computing the shortest distance of given words' synsets in the WordNet hierarchy.
sent10: The word-word similarity measure is directional.
sent11: It begins with each word in S1 being computed against each word in S2, and then vice versa.
sent12: The algorithm finally considers sentence similarity by normalizing the highest semantic similarity (maxSim) for each word in the sentences by applying ""inverse document frequency"" (IDF) to the British National Corpus to weight rare and common terms.
sent13: The normalized scores are then summed up for a sentence similarity score, SimMI, as follows:where maxSim(w, S2) is the score of the most similar word in S2 to w, and IDF (w) is the IDF of word
sent14: w. The final output of MI is a numeric value between 0 and 1, where 0 indicates no semantic overlap, and numbers approaching 1 indicate identical meaning of the two sentences.
sent15: These numbers serve as the input to our simulating algorithm for constructing artificial responses.
sent16: Note that the information in the MI values is entirely lexical and syntactic.
sent17: It contains no knowledge about surveys, leadership, or respondent behavior.
sent18: The MLQ has 45 items. This yields (45 × (45 − 1)) /
sent19: 2 or 990 unique item pairs, for which we obtain MI values.
sent20: One special problem concerns the direction of signs.
sent21: In the MLQ, 264 of 990 pairs of items are negatively correlated.
sent22: Theory suggests that two scales, Laissez-faire and Passive Management by Exception, are likely to relate negatively to effective leadership.
sent23: The problem has been treated extensively elsewhere (Arnulf et al., 2014), so we will only offer a brief explanation here.
sent24: MI does not take negative values, and does not differentiate well between positive and negative statements about the same content.
sent25: For two items describing how (a) a manager is unapproachable when called for and (b) that the same person uses appropriate methods of leadership, the surveyed responses correlate at -.42 in the present sample, while the MI value is .38.
sent26: The chosen solution is to allow MI values to be negative for all pairs of items from Laissezfaire and Passive Management by Exception (correctly identifying 255 of the 264 negative correlations, p < .001).","1. What algorithms exist for computing the similarity of survey items? sent1
2. Which algorithm was chosen for this study and why? sent2
3. How does the MI algorithm compare to LSA-generated values according to the Arnulf et al. study? sent3
4. From where does the MI algorithm derive its knowledge about words? sent4
5. How does the MI algorithm compute a similarity measure for two sentences? sent5
6. What is the first step in the MI algorithm's process of identifying part of speech? sent6
7. How does the MI algorithm calculate word similarity? sent7
8. What does the MI algorithm identify using word-similarity metrics? sent8
9. How are the word-similarity metrics adapted in the MI algorithm? sent9
10. Is the word-word similarity measure directional in the MI algorithm? sent10
11. How does the MI algorithm compute word similarity directionally? sent11
12. How does the MI algorithm consider sentence similarity? sent12
13. What is the final output of the MI algorithm? sent14
14. What do the MI values serve as input for? sent15
15. What type of information do the MI values contain? sent16
16. What does the MI algorithm lack knowledge about? sent17
17. How many unique item pairs are there in the MLQ for which MI values are obtained? sent18, sent19
18. What special problem concerns the direction of signs in the MLQ? sent20
19. How many pairs of items in the MLQ are negatively correlated? sent21
20. Which scales are likely to relate negatively to effective leadership according to theory? sent22
21. How has the problem of direction of signs been treated previously? sent23
22. What limitation does the MI algorithm have regarding negative values? sent24
23. Can you provide an example of how MI values compare to surveyed responses? sent25
24. What solution was chosen for handling negative correlations in MI values? sent26"
5058880,False Information on Web and Social Media: A Survey,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,Textual characteristics.,10,"There is a vast literature that studies fake news in social media. False information in the form of fake news is created in such a way to invoke interest and/or be believable to consumers. Various strategies may be used to deceive these consumers. Silverman [91] found that about 13% of over 1600 news articles had incoherent headline and content body, for example, by using declarative headlines paired with bodies which are skeptical about the veracity of the information.

In a recent paper, Horne and Adali [37] studied the textual characteristics of fake news using several sources of data: Buzzfeed fake news analysis articles [92], and articles from well known satire and fake news agencies (e.g., The Onion, Ending the Fed, and others). Reputed journalistic websites were used for comparison. The authors find interesting relations by comparing fake, satirical, and real news. Below are two news article titles, one of which is fake. Can you identify the fake one? 4 1. BREAKING BOMBSHELL: NYPD Blows Whistle on New Hillary Emails: Money Laundering, Sex Crimes with Children, Child Exploitation, Pay to Play, Perjury 2. Preexisting Conditions and Republican Plans to Replace Obamacare Fake news tends to pack the main claim of the article into its title. The titles are longer but use fewer stopwords and more proper nouns and verb phrases, meaning that the creators tend to put as much information in the title as possible. The words used in the title are smaller and capitalized more often, to generate emphasis. Not surprisingly, titles of fake news and satire are very similar. In terms of the body content, fake news articles are short, repetitive, and less informative (fewer nouns and analytical words). They contain fewer technical words, more smaller words, and are generally easier to read. This allows the reader to skip reading the entire article, and instead just take information away from the title itself, which may be disparate from the rest of the content of the article. Interestingly, Rubin et al. [80] studied satire news separately from the viewpoint of misleading readers into believing it is true, and also found that satirical articles pack a lot of information in single sentences. Thus, fake news articles are more similar to satirical ones than to real news-the bodies are less wordy and contain fewer nouns, technical and analytical words. In addition, Perez et al. [74] also analyzed the textual properties of fake news using two datasets-one generated by Amazon Mechanical Turk workers and other one scraped on celebrity rumors from gossip websites. They found that fake news contains more social and positive words, is more certain, focuses more on present and future actions by using more verbs and time words.

But do people discuss false information differently from true information? To answer this, Mitra et a. [60] recently analyzed the language of how people discuss true and false information pieces using tweets of 1400 events. These events were part of their CREDBANK dataset, which used crowdsourcing to label ground truth credibility judgments [59]. Using LIWC categories [72], they found that discussions around false information are marked with increased use of more confusion, disbelief, and hedging words which indicates skepticism among readers. Surprisingly, they found while more agreement words signaled high credibility, more positive sentiment words are associated with low credibility events. The latter is because it includes words like 'ha', 'grins', 'joking' are positive sentiments, but instead mean mockery. Their findings show that in addition to the text of the tweet itself, its surrounding discussion give important information to identify false information.

Hoaxes have similar textual properties as rumors. Kumar et al. [50] compared the content of hoax articles and non-hoax articles. They found that hoaxes were surprisingly longer compared to non-hoax articles (Figure 9(a)), but they contained far fewer web and internal Wikipedia references (Figure 9(b)). This indicated that hoaxsters tried to give more information to appear more genuine, though they did not have sufficient sources to substantiate their claims.","sent1: There is a vast literature that studies fake news in social media.
sent2: False information in the form of fake news is created in such a way to invoke interest and/or be believable to consumers.
sent3: Various strategies may be used to deceive these consumers.
sent4: Silverman [91] found that about 13% of over 1600 news articles had incoherent headline and content body, for example, by using declarative headlines paired with bodies which are skeptical about the veracity of the information.
sent5: In a recent paper, Horne and Adali [37] studied the textual characteristics of fake news using several sources of data: Buzzfeed fake news analysis articles [92], and articles from well known satire and fake news agencies (e.g., The Onion, Ending the Fed, and others).
sent6: Reputed journalistic websites were used for comparison.
sent7: The authors find interesting relations by comparing fake, satirical, and real news.
sent8: Below are two news article titles, one of which is fake.
sent9: Can you identify the fake one? 4 1.
sent10: BREAKING BOMBSHELL: NYPD Blows Whistle on New Hillary Emails: Money Laundering, Sex Crimes with Children, Child Exploitation, Pay to Play, Perjury 2.
sent11: Preexisting Conditions and Republican Plans to Replace Obamacare Fake news tends to pack the main claim of the article into its title.
sent12: The titles are longer but use fewer stopwords and more proper nouns and verb phrases, meaning that the creators tend to put as much information in the title as possible.
sent13: The words used in the title are smaller and capitalized more often, to generate emphasis.
sent14: Not surprisingly, titles of fake news and satire are very similar.
sent15: In terms of the body content, fake news articles are short, repetitive, and less informative (fewer nouns and analytical words).
sent16: They contain fewer technical words, more smaller words, and are generally easier to read.
sent17: This allows the reader to skip reading the entire article, and instead just take information away from the title itself, which may be disparate from the rest of the content of the article.
sent18: Interestingly, Rubin et al. [80] studied satire news separately from the viewpoint of misleading readers into believing it is true, and also found that satirical articles pack a lot of information in single sentences.
sent19: Thus, fake news articles are more similar to satirical ones than to real news-the bodies are less wordy and contain fewer nouns, technical and analytical words.
sent20: In addition, Perez et al. [74] also analyzed the textual properties of fake news using two datasets-one generated by Amazon Mechanical Turk workers and other one scraped on celebrity rumors from gossip websites.
sent21: They found that fake news contains more social and positive words, is more certain, focuses more on present and future actions by using more verbs and time words.
sent22: But do people discuss false information differently from true information?
sent23: To answer this, Mitra et a. [60] recently analyzed the language of how people discuss true and false information pieces using tweets of 1400 events.
sent24: These events were part of their CREDBANK dataset, which used crowdsourcing to label ground truth credibility judgments [59].
sent25: Using LIWC categories [72], they found that discussions around false information are marked with increased use of more confusion, disbelief, and hedging words which indicates skepticism among readers.
sent26: Surprisingly, they found while more agreement words signaled high credibility, more positive sentiment words are associated with low credibility events.
sent27: The latter is because it includes words like 'ha', 'grins', 'joking' are positive sentiments, but instead mean mockery.
sent28: Their findings show that in addition to the text of the tweet itself, its surrounding discussion give important information to identify false information.
sent29: Hoaxes have similar textual properties as rumors.
sent30: Kumar et al. [50] compared the content of hoax articles and non-hoax articles.
sent31: They found that hoaxes were surprisingly longer compared to non-hoax articles (Figure 9(a)), but they contained far fewer web and internal Wikipedia references (Figure 9(b)).
sent32: This indicated that hoaxsters tried to give more information to appear more genuine, though they did not have sufficient sources to substantiate their claims.","1. What is the focus of the vast literature mentioned in the section? sent1
2. How is false information in the form of fake news created? sent2
3. What strategies might be used to deceive consumers of fake news? sent3
4. What did Silverman find about the coherence of headlines and content bodies in news articles? sent4
5. What did Horne and Adali study in their recent paper? sent5
    5.1. What sources of data did Horne and Adali use for their study? sent5
6. What was used for comparison in the study by Horne and Adali? sent6
7. What interesting relations did the authors find by comparing fake, satirical, and real news? sent7
8. What are some characteristics of fake news article titles? sent11, sent12, sent13
9. How are the titles of fake news and satire similar? sent14
10. What are some characteristics of the body content of fake news articles? sent15, sent16
11. How do fake news articles allow readers to skip reading the entire article? sent17
12. What did Rubin et al. study separately, and what did they find? sent18
13. How are fake news articles more similar to satirical ones than to real news? sent19
14. What did Perez et al. analyze, and what did they find about fake news? sent20, sent21
15. How do people discuss false information differently from true information? sent22, sent23
    15.1. What dataset was used by Mitra et al. to analyze discussions of true and false information? sent24
    15.2. What did Mitra et al. find about discussions around false information? sent25
    15.3. What surprising finding did Mitra et al. have regarding sentiment words and credibility? sent26, sent27
    15.4. What do Mitra et al.'s findings show about identifying false information? sent28
16. How do hoaxes compare to rumors in terms of textual properties? sent29
17. What did Kumar et al. find when comparing hoax articles to non-hoax articles? sent30, sent31
    17.1. What does the length and reference content of hoax articles indicate? sent32"
61066716,A Corpus-based Survey of Four Electronic Swahili-English Bilingual Dictionaries,https://www.semanticscholar.org/paper/b416759e1941fae9b6474541448dec676636a9fe,Monolingual corpus,4,"We used several textual sources to compute the coverage of the dictionaries.These include:

-The Helsinki Corpus of Swahili, HCS (Hurskainen 2004a) consisting of more than 9 million words.-Wikipedia in Swahili: almost 12 000 Internet pages, good for more than 1 million words.

We pre-processed the texts by uniformly converting them into UTF-8, tokenizing the data and lemmatizing them using the automatic morphological analyzer described in Section 4. The data was also part-of-speech tagged using the method described in De Pauw et al. (2006).The Helsinki Corpus of Swahili already has lemmatization and part-of-speech tag information available.We nevertheless chose to process it again using our own techniques, for reasons of annotation accuracy and consistency across the data sets.

We then proceeded to compute coverage.We used a purely quantitative approach for this, which checks for each word in the corpus whether its lemma (for the given part-of-speech) can be retrieved in the dictionaries.Table 3 displays the scores for the different dictionaries and corpora.The ILSD has the highest coverage across the board, but loses a lot of coverage for the more recent texts in the recently developed and noisier Wikipedia pages.TUKI follows the same trend, while the TeDJe-SED dictionary hardly loses coverage.The latter's smaller set of lemmas consistently covers the most frequent words in the corpus and is therefore not as vulnerable to change of register and publication date.The complete consolidated dictionary database performs quite well with an overall coverage of about 90.2%.

To study the effect of publication date, we calculated the coverage per year of the periodicals included in the Helsinki Corpus of Swahili (1990Swahili ( → 2002Swahili ( , no data for 1995Swahili ( , 1996Swahili ( , 1997)).The downward trend in coverage is visible for all four dictionaries; see Figure 5.The most frequent items not covered by the dictionaries are named entities, foreign words and IT terminology.The need to update dictionaries consistently is therefore high.The open architectures of the Freedict and ILSD projects are in this sense suitable solutions.Interestingly, the scores reported here differ significantly from those in Hurskainen (2004).Overall, coverage scores are lower than those reported in the previous survey, which may be due to differences in evaluation metrics.Stranger however is that Hurskainen (2004) observes higher recall scores for more recent documents, whereas Figure 5 shows a definite downward trend over time.These discrepancies warrant further investigation.

We also calculated the coverage of the dictionaries disregarding the frequency of the lemma.In this calculation, covering a highly frequent word like lakini 'but; however; nevertheless' scores the same as covering a hapax.Table 4 shows that in this experiment ILSD is trailing TUKI, indicating that even though ILSD contains many more entries, TUKI seems to cater for a wider range of words.

A final experiment counts for how many lemmas in the respective dictionaries evidence can be found in the corpus.The last line of Table 4 shows that TeDJe-SED has all lemmas covered by real-world data.About 30% of the entries in TUKI are not found in the data, while only two thirds of ILSD is covered by the corpus.Comparing the data in Tables 3 and 4 shows that while TUKI trails in comparison to ILSD in terms of raw coverage (Table 3), it does http://lexikos.journals.ac.za seem to strike a better balance in terms of both lexical richness and empirical evidence (Table 4).Owing to the massive amount of data, it is impossible to check whether the lemma retrieved in the dictionary is indeed the one intended in the text.It might indeed be the case that a particular lemma-tag combination retrieved in the dictionary, does not describe the correct meaning in its actual context.We manually checked a small section of the corpus (±2 000 words) and found only two occasions of such an error.We are therefore confident that our scores are reliable in the context of the comparison between the dictionaries.

In De Pauw and De Schryver ( 2008) we presented our morphological analyzer as a way to unearth undiscovered lemmas in the corpus data.Our approach indeed has the distinct advantage that it is not dependent on a preset list of roots or lemmas, and is thus capable of lemmatizing word forms for previously unseen lemmas.The experiments outlined in this section have further underlined this property, as we now have at our disposal a list of word forms and associated candidate lemmas (roughly put the remaining 10% not covered by the consolidated dictionary) that need to be lexicographically described.","sent1: We used several textual sources to compute the coverage of the dictionaries.
sent2: These include:-The Helsinki Corpus of Swahili, HCS (Hurskainen 2004a) consisting of more than 9 million words.-Wikipedia in Swahili: almost 12 000 Internet pages, good for more than 1 million words.
sent3: We pre-processed the texts by uniformly converting them into UTF-8, tokenizing the data and lemmatizing them using the automatic morphological analyzer described in Section 4.
sent4: The data was also part-of-speech tagged using the method described in De Pauw et al. (2006).The Helsinki Corpus of Swahili already has lemmatization and part-of-speech tag information available.
sent5: We nevertheless chose to process it again using our own techniques, for reasons of annotation accuracy and consistency across the data sets.
sent6: We then proceeded to compute coverage.
sent7: We used a purely quantitative approach for this, which checks for each word in the corpus whether its lemma (for the given part-of-speech) can be retrieved in the dictionaries.
sent8: Table 3 displays the scores for the different dictionaries and corpora.
sent9: The ILSD has the highest coverage across the board, but loses a lot of coverage for the more recent texts in the recently developed and noisier Wikipedia pages.
sent10: TUKI follows the same trend, while the TeDJe-SED dictionary hardly loses coverage.
sent11: The latter's smaller set of lemmas consistently covers the most frequent words in the corpus and is therefore not as vulnerable to change of register and publication date.
sent12: The complete consolidated dictionary database performs quite well with an overall coverage of about 90.2%.
sent13: To study the effect of publication date, we calculated the coverage per year of the periodicals included in the Helsinki Corpus of Swahili (1990Swahili ( → 2002Swahili ( , no data for 1995Swahili ( , 1996Swahili ( , 1997)).The downward trend in coverage is visible for all four dictionaries; see Figure 5.The most frequent items not covered by the dictionaries are named entities, foreign words and IT terminology.
sent14: The need to update dictionaries consistently is therefore high.
sent15: The open architectures of the Freedict and ILSD projects are in this sense suitable solutions.
sent16: Interestingly, the scores reported here differ significantly from those in Hurskainen (2004).Overall, coverage scores are lower than those reported in the previous survey, which may be due to differences in evaluation metrics.
sent17: Stranger however is that Hurskainen (2004) observes higher recall scores for more recent documents, whereas Figure 5 shows a definite downward trend over time.
sent18: These discrepancies warrant further investigation.
sent19: We also calculated the coverage of the dictionaries disregarding the frequency of the lemma.
sent20: In this calculation, covering a highly frequent word like lakini 'but; however; nevertheless' scores the same as covering a hapax.
sent21: Table 4 shows that in this experiment ILSD is trailing TUKI, indicating that even though ILSD contains many more entries, TUKI seems to cater for a wider range of words.
sent22: A final experiment counts for how many lemmas in the respective dictionaries evidence can be found in the corpus.
sent23: The last line of Table 4 shows that TeDJe-SED has all lemmas covered by real-world data.
sent24: About 30% of the entries in TUKI are not found in the data, while only two thirds of ILSD is covered by the corpus.
sent25: Comparing the data in Tables 3 and 4 shows that while TUKI trails in comparison to ILSD in terms of raw coverage (Table 3), it does http://lexikos.journals.ac.za seem to strike a better balance in terms of both lexical richness and empirical evidence (Table 4).Owing to the massive amount of data, it is impossible to check whether the lemma retrieved in the dictionary is indeed the one intended in the text.
sent26: It might indeed be the case that a particular lemma-tag combination retrieved in the dictionary, does not describe the correct meaning in its actual context.
sent27: We manually checked a small section of the corpus (±2 000 words) and found only two occasions of such an error.
sent28: We are therefore confident that our scores are reliable in the context of the comparison between the dictionaries.
sent29: In De Pauw and De Schryver ( 2008) we presented our morphological analyzer as a way to unearth undiscovered lemmas in the corpus data.
sent30: Our approach indeed has the distinct advantage that it is not dependent on a preset list of roots or lemmas, and is thus capable of lemmatizing word forms for previously unseen lemmas.
sent31: The experiments outlined in this section have further underlined this property, as we now have at our disposal a list of word forms and associated candidate lemmas (roughly put the remaining 10% not covered by the consolidated dictionary) that need to be lexicographically described.","1. What sources were used to compute the coverage of the dictionaries? sent1
    1.1. What are the specific textual sources included in the study? sent2
2. How were the texts pre-processed for the study? sent3
    2.1. What additional processing was done on the Helsinki Corpus of Swahili? sent4
    2.2. Why was the Helsinki Corpus of Swahili processed again using the authors' techniques? sent5
3. What approach was used to compute coverage? sent6
    3.1. How was the quantitative approach for computing coverage implemented? sent7
    3.2. What does Table 3 display? sent8
4. Which dictionary has the highest coverage, and how does it perform with more recent texts? sent9
5. How do TUKI and TeDJe-SED dictionaries compare in terms of coverage? sent10
    5.1. Why does the TeDJe-SED dictionary maintain consistent coverage? sent11
6. How does the complete consolidated dictionary database perform overall? sent12
7. What trend is observed in the coverage per year of the periodicals in the Helsinki Corpus of Swahili? sent13
    7.1. What are the most frequent items not covered by the dictionaries? sent13
8. Why is there a high need to update dictionaries consistently? sent14
    8.1. What solutions are suggested for updating dictionaries? sent15
9. How do the scores reported in this study compare to those in Hurskainen (2004)? sent16
    9.1. What discrepancy is noted between this study and Hurskainen (2004)? sent17
    9.2. What do these discrepancies warrant? sent18
10. How was the coverage of the dictionaries calculated disregarding lemma frequency? sent19
    10.1. What does Table 4 show about ILSD and TUKI in this experiment? sent21
11. What was the final experiment conducted in the study? sent22
    11.1. What does the last line of Table 4 indicate about TeDJe-SED? sent23
    11.2. What percentage of entries in TUKI and ILSD are not found in the corpus? sent24
    11.3. How does TUKI compare to ILSD in terms of lexical richness and empirical evidence? sent25
12. What challenge is noted regarding the massive amount of data? sent25
    12.1. What potential issue is identified with lemma-tag combinations? sent26
    12.2. How reliable are the scores according to the authors? sent27, sent28
13. What advantage does the authors' morphological analyzer have? sent29
    13.1. How does the approach benefit the study of undiscovered lemmas? sent30
    13.2. What has the study outlined in this section further underlined? sent31"
256900869,A Survey on Event-based News Narrative Extraction,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,Manuscript submitted to ACM,6,"Graph Representations Uramoto and Takeda [112] proposed a graph-based approach to model the relationships between news articles. In particular, they use a directed graph based on temporal ordering and event similarity. This is the earliest article that fits with our definitions of event-based narrative representations for news narratives that we found. In particular, the authors use the concepts of genus and differentia words. For adjacent articles, genus words are computed using the intersection of their word sets and represent already known information in the story. In contrast, differentia words are built from the set difference between the articles (in temporal order) and represent new knowledge in the story. Thus, differentia words are more important when trying to find coherent sequences of articles. The events are represented with a variation of TF-IDF that assigns more weight to differentia words.

Tannier and Moriceau [106] propose an approach for building multi-document event threads from news articles.

In particular, they use a supervised learning approach with a series of classifiers to define the type of relationship between news articles: same-event, continuation, or reaction. The output of this method is a temporal event graph,

where the nodes correspond to events (represented as news articles) and the edges are labeled with the corresponding relationships. In particular, the first step is to determine whether there is a connection at all between the articles. To do so, an initial classifier is implemented using a series of content similarity features (e.g., word overlap, cosine similarity, and similarity of the first sentences) to construct the initial temporal graph. However, this is not enough to find all potential relationships and a second-level classifier is included that takes into account the results from the previous classifier by using degree-based features from the temporal graph. Next, after a connection has been established, another classifier determines whether this connection is based on the articles referring to the same news event-same-event connection-or based on a continuation-when an event is a direct continuation or consequence of a previous one. This classifier relies on date-based features (e.g., differences in publication time, date references, and references between events themselves) and keyword-based features (e.g., usage of temporal words, reaction words, or opinion words). The output is fed into another classifier that leverages degree-based features again to find more relationships. Due to the transitive nature of the same-event and continuation relationships, a post-processing step takes the graph and constructs the transitive closure for these specific relations. Afterward, a final classifier uses the same features to determine whether a continuation is a reaction-a subset of continuations that relate the reactions of people (or organizations) to an event.

Hu et al. [47] propose a system to model storyline interactions from news events. Their approach generates a series of event timelines focusing on specific entities or topics and their interactions with each other. In particular, this results in a directed graph connecting multiple events. In contrast to other approaches, the underlying representation of events is based on the main event descriptors (i.e., the answers to Who, What, When, Where, Why, and How) [50] which are extracted directly from each article and represent the key elements of the event. Based on this information, a coherence graph is constructed and used to identify the storylines through a random walk. Coherence is defined by three factors:

subtopic consistency, entity relatedness, and time continuity. To measure subtopic consistency, the first step is to use a generative probabilistic mixture model to discover latent subtopics. Then, JS divergence is used to measure the distance of topic distributions between articles. Next, entity relatedness is measured by the average affinity of the entities from each pair of articles using normalized point-wise mutual information. The time continuity factor is simply defined as an exponential penalty term dependent on the temporal distance between events. The coherence graph is built by creating edges between documents that have a coherence score above a given threshold. Based on the coherence graph, a series of informative events that connect multiple storylines are identified. Specifically, a topic-sensitive PageRank algorithm [43] is used to discover these events. In turn, these events feed the storyline generation algorithm, an iterative algorithm that selects a single informative event for each story for each day.

Bögel and Gertz [14] present a temporal linking framework based on the concept of article references. In particular, they exploit the structure of news articles to construct an information network. Instead of comparing articles based on overall content similarity, they exploit the use of lead paragraphs, explanatory paragraphs, and additional information paragraphs in typical news articles. Specifically, they construct the network based on temporal expressions, keywords, and entity names. To select valid event connections, the first step is to filter based on temporal information contained in the text based on a temporal tagger. Next, connections are evaluated based on the similarity of the lead paragraph of a news article with all the other paragraphs of another news article (i.e., capturing references to the event). Similarity is computed based on the entities and keywords mentioned in each paragraph based on a weighted average of Jaccard and cosine similarity. Finally, irrelevant edges are pruned based on a user-defined threshold. However, some non-relevant edges are kept if they fulfill the role of a support path-paths that have non-relevant edges but share endpoints with fully relevant paths-that provide more evidence of two events being connected. The output is a directed graph based on references, not necessarily acyclic, as there are future temporal references in some articles.","sent1: Graph Representations Uramoto and Takeda [112] proposed a graph-based approach to model the relationships between news articles.
sent2: In particular, they use a directed graph based on temporal ordering and event similarity.
sent3: This is the earliest article that fits with our definitions of event-based narrative representations for news narratives that we found.
sent4: In particular, the authors use the concepts of genus and differentia words.
sent5: For adjacent articles, genus words are computed using the intersection of their word sets and represent already known information in the story.
sent6: In contrast, differentia words are built from the set difference between the articles (in temporal order) and represent new knowledge in the story.
sent7: Thus, differentia words are more important when trying to find coherent sequences of articles.
sent8: The events are represented with a variation of TF-IDF that assigns more weight to differentia words.
sent9: Tannier and Moriceau [106] propose an approach for building multi-document event threads from news articles.
sent10: In particular, they use a supervised learning approach with a series of classifiers to define the type of relationship between news articles: same-event, continuation, or reaction.
sent11: The output of this method is a temporal event graph,where the nodes correspond to events (represented as news articles) and the edges are labeled with the corresponding relationships.
sent12: In particular, the first step is to determine whether there is a connection at all between the articles.
sent13: To do so, an initial classifier is implemented using a series of content similarity features (e.g., word overlap, cosine similarity, and similarity of the first sentences) to construct the initial temporal graph.
sent14: However, this is not enough to find all potential relationships and a second-level classifier is included that takes into account the results from the previous classifier by using degree-based features from the temporal graph.
sent15: Next, after a connection has been established, another classifier determines whether this connection is based on the articles referring to the same news event-same-event connection-or based on a continuation-when an event is a direct continuation or consequence of a previous one.
sent16: This classifier relies on date-based features (e.g., differences in publication time, date references, and references between events themselves) and keyword-based features (e.g., usage of temporal words, reaction words, or opinion words).
sent17: The output is fed into another classifier that leverages degree-based features again to find more relationships.
sent18: Due to the transitive nature of the same-event and continuation relationships, a post-processing step takes the graph and constructs the transitive closure for these specific relations.
sent19: Afterward, a final classifier uses the same features to determine whether a continuation is a reaction-a subset of continuations that relate the reactions of people (or organizations) to an event.
sent20: Hu et al. [47] propose a system to model storyline interactions from news events.
sent21: Their approach generates a series of event timelines focusing on specific entities or topics and their interactions with each other.
sent22: In particular, this results in a directed graph connecting multiple events.
sent23: In contrast to other approaches, the underlying representation of events is based on the main event descriptors (i.e., the answers to Who, What, When, Where, Why, and How) [50] which are extracted directly from each article and represent the key elements of the event.
sent24: Based on this information, a coherence graph is constructed and used to identify the storylines through a random walk.
sent25: Coherence is defined by three factors:subtopic consistency, entity relatedness, and time continuity.
sent26: To measure subtopic consistency, the first step is to use a generative probabilistic mixture model to discover latent subtopics.
sent27: Then, JS divergence is used to measure the distance of topic distributions between articles.
sent28: Next, entity relatedness is measured by the average affinity of the entities from each pair of articles using normalized point-wise mutual information.
sent29: The time continuity factor is simply defined as an exponential penalty term dependent on the temporal distance between events.
sent30: The coherence graph is built by creating edges between documents that have a coherence score above a given threshold.
sent31: Based on the coherence graph, a series of informative events that connect multiple storylines are identified.
sent32: Specifically, a topic-sensitive PageRank algorithm [43] is used to discover these events.
sent33: In turn, these events feed the storyline generation algorithm, an iterative algorithm that selects a single informative event for each story for each day.
sent34: Bögel and Gertz [14] present a temporal linking framework based on the concept of article references.
sent35: In particular, they exploit the structure of news articles to construct an information network.
sent36: Instead of comparing articles based on overall content similarity, they exploit the use of lead paragraphs, explanatory paragraphs, and additional information paragraphs in typical news articles.
sent37: Specifically, they construct the network based on temporal expressions, keywords, and entity names.
sent38: To select valid event connections, the first step is to filter based on temporal information contained in the text based on a temporal tagger.
sent39: Next, connections are evaluated based on the similarity of the lead paragraph of a news article with all the other paragraphs of another news article (i.e., capturing references to the event).
sent40: Similarity is computed based on the entities and keywords mentioned in each paragraph based on a weighted average of Jaccard and cosine similarity.
sent41: Finally, irrelevant edges are pruned based on a user-defined threshold.
sent42: However, some non-relevant edges are kept if they fulfill the role of a support path-paths that have non-relevant edges but share endpoints with fully relevant paths-that provide more evidence of two events being connected.
sent43: The output is a directed graph based on references, not necessarily acyclic, as there are future temporal references in some articles.","1. What approach did Uramoto and Takeda propose for modeling relationships between news articles? sent1
    1.1. How do they use a directed graph in their approach? sent2
    1.2. What concepts do the authors use in their approach? sent4
        1.2.1. How are genus words computed and what do they represent? sent5
        1.2.2. How are differentia words built and what do they represent? sent6
        1.2.3. Why are differentia words important? sent7
        1.2.4. How are events represented in their approach? sent8
2. What approach do Tannier and Moriceau propose for building multi-document event threads? sent9
    2.1. What is the first step in their approach? sent12
        2.1.1. How is the initial temporal graph constructed? sent13
    2.2. What additional step is included to find potential relationships? sent14
    2.3. How is the connection type determined once a connection is established? sent15
        2.3.1. What features does this classifier rely on? sent16
    2.4. What is the purpose of the post-processing step? sent18
    2.5. How is a continuation determined to be a reaction? sent19
3. What system do Hu et al. propose for modeling storyline interactions? sent20
    3.1. What does their approach generate? sent21
    3.2. How are events represented in their approach? sent23
    3.3. How is a coherence graph constructed and used? sent24
        3.3.1. What factors define coherence? sent25
        3.3.2. How is subtopic consistency measured? sent26, sent27
        3.3.3. How is entity relatedness measured? sent28
        3.3.4. How is the time continuity factor defined? sent29
        3.3.5. How is the coherence graph built? sent30
    3.4. What is identified based on the coherence graph? sent31
    3.5. What algorithm is used to discover informative events? sent32
    3.6. How does the storyline generation algorithm work? sent33
4. What framework do Bögel and Gertz present for temporal linking? sent34
    4.1. How do they exploit the structure of news articles? sent35
    4.2. How is the network constructed in their approach? sent37
    4.3. What is the first step in selecting valid event connections? sent38
    4.4. How are connections evaluated? sent39
    4.5. How is similarity computed for evaluating connections? sent40
    4.6. How are irrelevant edges handled in their approach? sent41, sent42
    4.7. What is the output of their framework? sent43"
232116743,Use of Social Media Data in Disaster Management: A Survey,https://www.semanticscholar.org/paper/7595367176192a375585e18593d1a2bd3d5faccc,Third Parties,4,"Social media data are also collected and organised by organisations and institutions for specific purposes. Due to the benefits of Open Data, some of them have been interested in opening their collected social media data for others [20]. These organisations are considered as alternative data sources for social media data. This section discusses the different methods of accessing social media data. Third parties who provide their collected social media data are considered as alternative data sources for conducting research on social media data analytics for disaster management. This social media data are collected and organised in a specific way to be used for a specific purpose. For example, CrisisLexT26 [26] provides crisis-related tweets during emergency events, which are collected from Twitter by using crisis-specific keywords. CrowdFlower [29] provides the Figure Eight platform for free open datasets. These include tweets relevant to various kinds of disasters. Most social media data collected by third-party data sources are usually prepared using additional processes to provide higher-quality datasets. In addition to using datasets from third-party data sources for disaster management, such datasets can be used as training datasets and for evaluation in data analysis in many studies on disaster management. For example, the authors of [27] utilised datasets from CrisisLexT26 and CrowdFlower as training datasets for identifying disaster-related tweets. Table 2 depicts some existing works that use social media data sources for disaster management. Twitter is a major source of social media data, which can be accessed via the Twitter API. ","sent1: Social media data are also collected and organised by organisations and institutions for specific purposes.
sent2: Due to the benefits of Open Data, some of them have been interested in opening their collected social media data for others [20].
sent3: These organisations are considered as alternative data sources for social media data.
sent4: This section discusses the different methods of accessing social media data.
sent5: Third parties who provide their collected social media data are considered as alternative data sources for conducting research on social media data analytics for disaster management.
sent6: This social media data are collected and organised in a specific way to be used for a specific purpose.
sent7: For example, CrisisLexT26 [26] provides crisis-related tweets during emergency events, which are collected from Twitter by using crisis-specific keywords.
sent8: CrowdFlower [29] provides the Figure Eight platform for free open datasets.
sent9: These include tweets relevant to various kinds of disasters.
sent10: Most social media data collected by third-party data sources are usually prepared using additional processes to provide higher-quality datasets.
sent11: In addition to using datasets from third-party data sources for disaster management, such datasets can be used as training datasets and for evaluation in data analysis in many studies on disaster management.
sent12: For example, the authors of [27] utilised datasets from CrisisLexT26 and CrowdFlower as training datasets for identifying disaster-related tweets.
sent13: Table 2 depicts some existing works that use social media data sources for disaster management.
sent14: Twitter is a major source of social media data, which can be accessed via the Twitter API.","1. Who collects and organizes social media data for specific purposes? sent1
    1.1. Why are some organizations interested in opening their collected social media data? sent2
    1.2. How are these organizations considered in terms of data sources? sent3
2. What does this section discuss regarding social media data? sent4
3. How are third parties who provide their collected social media data viewed in the context of disaster management? sent5
4. How is social media data collected and organized by third parties? sent6
    4.1. Can you provide an example of a third-party source that provides crisis-related tweets? sent7
    4.2. What does CrowdFlower provide in terms of datasets? sent8
        4.2.1. What types of tweets are included in these datasets? sent9
5. How are most social media data collected by third-party data sources prepared? sent10
6. Besides disaster management, how else can datasets from third-party data sources be used? sent11
    6.1. Can you provide an example of how datasets from CrisisLexT26 and CrowdFlower are used? sent12
7. What does Table 2 depict? sent13
8. What is a major source of social media data and how can it be accessed? sent14"
212633493,Deep Neural Networks for Automatic Speech Processing: A Survey from Large Corpora to Limited Data,https://www.semanticscholar.org/paper/7a307b379eb714df1e38fe9a80b7ed94c0fbd64e,A. Data augmentation,5,"The first way to leverage the lack of data is to artificially augment the number of data. To do so, classic approach consists for example in adding noise or deformation. Such as in [12]. They obtain near SOTA on Librispeech (1000 hours from [3]) with an end-to-end models. Nevertheless, they obtain SOTA results on SwitchBoard (300 hours from [13]) with a WER of 6.8%/14.1% on the Switchboard/CallHome portion using shallow fusion and their data augmentation. But theses are handcrafted augmentations and some of them require additional audios (like adding noise).

Some other approaches use generative models to have new samples such as in [14], [15]. A. Chatziagapi et al. used conditional Generative Adversarial Networks (GAN) to generate new samples [14]. Conditioned GAN are GAN where we can control the mode of the generated samples. Doing so, they balanced their initial dataset and obtain better results. Y. Jiao et al. used Deep Convolutional GANs to generate dysarthric speech and improve their results [15].","sent1: The first way to leverage the lack of data is to artificially augment the number of data.
sent2: To do so, classic approach consists for example in adding noise or deformation.
sent3: Such as in [12]. They obtain near SOTA on Librispeech (1000 hours from [3]) with an end-to-end models.
sent4: Nevertheless, they obtain SOTA results on SwitchBoard (300 hours from [13]) with a WER of 6.8%/14.1% on the Switchboard/CallHome portion using shallow fusion and their data augmentation.
sent5: But theses are handcrafted augmentations and some of them require additional audios (like adding noise).
sent6: Some other approaches use generative models to have new samples such as in [14], [15]. A. Chatziagapi et al. used conditional Generative Adversarial Networks (GAN) to generate new samples [14].
sent7: Conditioned GAN are GAN where we can control the mode of the generated samples.
sent8: Doing so, they balanced their initial dataset and obtain better results.
sent9: Y. Jiao et al. used Deep Convolutional GANs to generate dysarthric speech and improve their results [15].","1. What is the first way to leverage the lack of data in automatic speech processing? sent1
    1.1. What is a classic approach to data augmentation? sent2
        1.1.1. Can you provide an example of achieving near SOTA results using data augmentation? sent3
        1.1.2. What results were obtained on SwitchBoard using data augmentation? sent4
        1.1.3. What is a limitation of handcrafted augmentations? sent5
    1.2. What other approaches use generative models for data augmentation? sent6
        1.2.1. How did A. Chatziagapi et al. use conditional GANs for data augmentation? sent7, sent8
        1.2.2. How did Y. Jiao et al. use Deep Convolutional GANs for data augmentation? sent9"
255024417,Application of named entity recognition method for Indonesian datasets: a review,https://www.semanticscholar.org/paper/941c79b384bc652cca8e8fe5b0e406ae971af713,METHOD 2.1. Systematic literature review,10,"First, this article presents a SLR of the field of NER research. A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research. Then follows a discussion of how NER has been applied to Indonesian texts. SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].

A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1). In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method. At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords. This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.  There follows the RQ that guided the following analysis: RQ. ""What are the trends in the application of NER to extract information from Indonesian online news and social media?"" In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa). According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results. In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process. These processes are sequential processes where each process aims to find the right study to be used in this research. The search and selection process are an elimination process based on the criteria specified in each process.

The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data. Some irrelevant papers were omitted in the first stage of collection based on the title and abstract. The second stage of selection articles is a full-text selection. Figure 2 illustrates the procedure of text-selection. The total number of papers obtained from the four databases was initially 241. Upon completion of the selection procedure, however, only 20 papers remained. The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset. The third stage is reporting the results and analyzing the results of this review. We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research. Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021

Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset 

First, this article presents a SLR of the field of NER research. A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research. Then follows a discussion of how NER has been applied to Indonesian texts. SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].

A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1). In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method. At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords. This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.  There follows the RQ that guided the following analysis: RQ. ""What are the trends in the application of NER to extract information from Indonesian online news and social media?"" In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa). According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results. In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process. These processes are sequential processes where each process aims to find the right study to be used in this research. The search and selection process are an elimination process based on the criteria specified in each process.

The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data. Some irrelevant papers were omitted in the first stage of collection based on the title and abstract. The second stage of selection articles is a full-text selection. Figure 2 illustrates the procedure of text-selection. The total number of papers obtained from the four databases was initially 241. Upon completion of the selection procedure, however, only 20 papers remained. The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset. The third stage is reporting the results and analyzing the results of this review. We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research. Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021

Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset ","sent1: First, this article presents a SLR of the field of NER research.
sent2: A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research.
sent3: Then follows a discussion of how NER has been applied to Indonesian texts.
sent4: SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].
sent5: A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1).
sent6: In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method.
sent7: At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords.
sent8: This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.
sent9: There follows the RQ that guided the following analysis: RQ.
sent10: ""What are the trends in the application of NER to extract information from Indonesian online news and social media?""
sent11: In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa).
sent12: According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results.
sent13: In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process.
sent14: These processes are sequential processes where each process aims to find the right study to be used in this research.
sent15: The search and selection process are an elimination process based on the criteria specified in each process.
sent16: The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data.
sent17: Some irrelevant papers were omitted in the first stage of collection based on the title and abstract.
sent18: The second stage of selection articles is a full-text selection.
sent19: Figure 2 illustrates the procedure of text-selection.
sent20: The total number of papers obtained from the four databases was initially 241.
sent21: Upon completion of the selection procedure, however, only 20 papers remained.
sent22: The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset.
sent23: The third stage is reporting the results and analyzing the results of this review.
sent24: We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research.
sent25: Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria
sent26: The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset First, this article presents a SLR of the field of NER research.
sent27: A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research.
sent28: Then follows a discussion of how NER has been applied to Indonesian texts.
sent29: SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].
sent30: A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1).
sent31: In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method.
sent32: At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords.
sent33: This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.
sent34: There follows the RQ that guided the following analysis: RQ.
sent35: ""What are the trends in the application of NER to extract information from Indonesian online news and social media?""
sent36: In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa).
sent37: According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results.
sent38: In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process.
sent39: These processes are sequential processes where each process aims to find the right study to be used in this research.
sent40: The search and selection process are an elimination process based on the criteria specified in each process.
sent41: The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data.
sent42: Some irrelevant papers were omitted in the first stage of collection based on the title and abstract.
sent43: The second stage of selection articles is a full-text selection.
sent44: Figure 2 illustrates the procedure of text-selection.
sent45: The total number of papers obtained from the four databases was initially 241.
sent46: Upon completion of the selection procedure, however, only 20 papers remained.
sent47: The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset.
sent48: The third stage is reporting the results and analyzing the results of this review.
sent49: We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research.
sent50: Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria
sent51: The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset","1. What does this article present regarding NER research? sent1
    1.1. What is the aim of a systematic literature review (SLR)? sent2
    1.2. What follows the presentation of the SLR in the article? sent3
2. In which research domains has SLR been used? sent4
3. How was the SLR carried out in this study? sent5
    3.1. What is the purpose of the planning stage in the SLR? sent6
        3.1.1. What was developed during the planning stage? sent7
        3.1.2. What strategy was used to determine the research questions (RQ)? sent8
            3.1.2.1. What research question guided the analysis? sent9, sent10
            3.1.2.2. What search string was used in the study? sent11
            3.1.2.3. What criteria were used to define the results according to the research question? sent12
    3.2. What is defined in the second stage of the SLR? sent13
        3.2.1. What is the purpose of the sequential processes in the second stage? sent14
        3.2.2. How is the search and selection process described? sent15
        3.2.3. From which databases were papers collected, and how was the data organized? sent16
        3.2.4. How were irrelevant papers handled in the first stage of collection? sent17
        3.2.5. What is the second stage of article selection? sent18
        3.2.6. What does Figure 2 illustrate? sent19
        3.2.7. How many papers were initially obtained, and how many remained after selection? sent20, sent21
        3.2.8. What challenge and opportunity does the low number of papers present? sent22
    3.3. What is the third stage of the SLR? sent23
        3.3.1. What aspects were mapped and examined in the third stage? sent24
4. What are the inclusion and exclusion criteria for the selection studies process? sent25"
259376518,Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of Biomedical Research Articles,https://www.semanticscholar.org/paper/3bef44ef3aacddaae56d6efc9f6767e64bc65b31,Evaluation,4,"For both subtasks, we evaluate summary quality according to three criteria -Relevance, Readability, and Factuality -where each criterion is composed of one or more automatic metrics:

• Relevance: ROUGE-1, 2, and L (Lin, 2004) and BERTScore (Zhang et al., 2020b).

• Readability: Flesch-Kincaid Grade Level (FKGL) and Dale-Chall Readability Score (DCRS).

• Factuality: BARTScore (Yuan et al., 2021), fine-tuned on our respective datasets (as has proven effective in recent work (Koh et al., 2022)).2

For Subtask 1, the scores calculated for each metric are the average of those calculated independently for the generated lay summaries of PLOS and eLife.The aim is to maximise the scores for Relevance and Factuality metrics and minimise scores for Readability metrics.

For Subtask 2, the scores presented for each metric are the average of those calculated independently for the generated abstracts and lay summaries.Notably, for Readability metrics in this subtask, we calculate the absolute difference between the scores of generated summary and target summary pairs (rather than simply using the scores obtained for generated summaries, as in subtask 1).The aim is to maximise the scores for Relevance and Factuality metrics and minimise the absolute difference scores calculated for Readability metrics.

Following the submission deadline for each subtask, an overall ranking is calculated based on the cumulative rank of the evaluation criteria, where a lower overall ranking equates to better overall performance).To produce a criterion ranking, we apply min-max normalisation to the scores of each metric, before averaging across metrics within each evaluation criterion.","sent1: For both subtasks, we evaluate summary quality according to three criteria -Relevance, Readability, and Factuality -where each criterion is composed of one or more automatic metrics:• Relevance: ROUGE-1, 2, and L (Lin, 2004) and BERTScore (Zhang et al., 2020b).• Readability: Flesch-Kincaid Grade Level (FKGL) and Dale-Chall Readability Score (DCRS).
sent2: • Factuality: BARTScore (Yuan et al., 2021), fine-tuned on our respective datasets (as has proven effective in recent work (Koh et al., 2022)).2For Subtask 1, the scores calculated for each metric are the average of those calculated independently for the generated lay summaries of PLOS and eLife.
sent3: The aim is to maximise the scores for Relevance and Factuality metrics and minimise scores for Readability metrics.
sent4: For Subtask 2, the scores presented for each metric are the average of those calculated independently for the generated abstracts and lay summaries.
sent5: Notably, for Readability metrics in this subtask, we calculate the absolute difference between the scores of generated summary and target summary pairs (rather than simply using the scores obtained for generated summaries, as in subtask 1).The aim is to maximise the scores for Relevance and Factuality metrics and minimise the absolute difference scores calculated for Readability metrics.
sent6: Following the submission deadline for each subtask, an overall ranking is calculated based on the cumulative rank of the evaluation criteria, where a lower overall ranking equates to better overall performance).To produce a criterion ranking, we apply min-max normalisation to the scores of each metric, before averaging across metrics within each evaluation criterion.","1. What criteria are used to evaluate summary quality in the BioLaySumm 2023 Shared Task? sent1
    1.1. What metrics are used to evaluate Relevance? sent1
    1.2. What metrics are used to evaluate Readability? sent1
    1.3. What metric is used to evaluate Factuality? sent2
2. How are scores calculated for Subtask 1? sent2
3. What is the aim regarding the scores for Relevance, Factuality, and Readability metrics? sent3
4. How are scores calculated for Subtask 2? sent4
    4.1. How are Readability metrics calculated differently in Subtask 2 compared to Subtask 1? sent5
5. How is the overall ranking determined after the submission deadline for each subtask? sent6"
259089180,A Survey on Learning Objects' Relationship for Image Captioning,https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be,Motion Relationship.,6,"Te action relationship between objects is more specifc than the positional relationship, which refects the relationship at a higher semantic level. With the diferent data structures, the motion relation can also be divided into the following two forms: (1) tensor and (2) graph. Te frst method is more intuitive. Te complexity of the motion relation makes it difcult to represent by the geometric feature. Terefore, many studies [73,74,[78][79][80][81] begin to directly mine the information from the image content, extract the features of relevant image regions, and represent them in the form of tensor. Te second method uses the graph pretrained by the upstream tasks to generate a suitable graph.","sent1: Te action relationship between objects is more specifc than the positional relationship, which refects the relationship at a higher semantic level.
sent2: With the diferent data structures, the motion relation can also be divided into the following two forms: (1) tensor and (2) graph.
sent3: Te frst method is more intuitive.
sent4: Te complexity of the motion relation makes it difcult to represent by the geometric feature.
sent5: Terefore, many studies [73,74,[78][79][80][81] begin to directly mine the information from the image content, extract the features of relevant image regions, and represent them in the form of tensor.
sent6: Te second method uses the graph pretrained by the upstream tasks to generate a suitable graph.","1. How is the action relationship between objects characterized compared to the positional relationship? sent1
2. Into what forms can the motion relation be divided based on different data structures? sent2
    2.1. What is the first method for representing motion relation? sent3
    2.2. Why is the motion relation difficult to represent by geometric features? sent4
        2.2.1. How do many studies address the complexity of motion relation representation? sent5
    2.3. What does the second method for representing motion relation involve? sent6"
195345457,Emotionally-Aware Chatbots: A Survey,https://www.semanticscholar.org/paper/c131665638feb8c11f936989ffc6187317593b41,Authors,14,"Year Focus Approach Kenneth Mark Colby [8] 1975

Designing chatbot behave like paranoid person

Rule-based approach with capability to simulate emotion. Zhou et. al. [66] 2014 Building empathetic chatbot for social interaction

Seq2seq learning with GRU-RNN model which takes into account intelligent quotient (IQ) and emotion quotien (EQ). Zhou et. al. [65] 2018 Building emotion chatting machine (ECM) for large scale conversation generation Seq2seq learning with GRU which incorporates emotion detection to capture implicit internal emotion states. Colombo et. al. [9] 2019 Developing affect-driven dialogue system which generates emotional responses in a controlled manner Seq2seq with GRU which incorporates emotion classifier, also affective re-ranking in the last step to produce the response. Li et. al. [23] 2019 Building conversational system emotional by using editing constraints to generate more meaningful and customizable emotional replies Encoder-decoder architecture with reinforcement learning approach by using asynchronous decoder which uses keyword predictor to predict the topic and emotion editor to produce emotional-embedded response. Zhang et. al. [62] 2017 Building emotional conversation systems which produces several responses for every emotion category

Multi-task seq2seq model with GRU which utilize bidirectional long short term memory (Bi-LSTMs) as emotion classifier.

Lubis et. al. [27] 2019 Building a fully data driven chatoriented dialogue system that can dynamically mimic affective human interactions

Proposing a seq2seq response generator, includes emotion encoder which is trained jointly with the entire network to encode and maintain the emotional context throughout the dialogue, but focusing only on positive emotion Ashgar et. al. [2] 2018 Building open-domain neural dialogue models by augmenting them with affective intelligence.

Encoder-decoder architecture with LSTM model using cognitively engineered affective word embedding, and also affectively diverse beam search for decoding Sun et. al. [55] 2018 Developing conversation generation which addresses the emotional factor by changing the model's input.

Encoder-decoder framework based on LSTM using three inputs, a sequence without an emotional category, a sequence with an emotional category for the input sentence, and a sequence with an emotional category for the output responses. Hu et. al. [18] 2018 Building a tone-aware chatbot for customer care on social media.

Encoder-decoder architecture with LSTM, which the decoder is modified to handle meta information by add an embedding vector as tone indicator to produce several tone responses including passionate, empathetic, and neutral. Sun et. al. [55] 2018 Building emotional human machine conversation chatbot that be able to understands the user's emotions, and consider the user's emotions then give a satisfactory response.

Model this problem as reinforcement learning task by proposing a new neural model based on Seq2Seq model which uses generative adversarial network (GAN) called EM-SeqGAN.

Shantala et. al. [50] 2018 Building neural dialogue system which addresses emotional aspect.

Seq2Seq model with LSTM by using a novel emotion embedding. Zhong et. al. [64] 2018

Building affect-rich open domain human-machine conversation system.

Extends Seq2Seq model and adopts VAD (Valence, Arousal and Dominance) affective notations. Proposed model also considers the effect of negators and intensifiers via a novel affective attention mechanism. Catania et. al. [6] 2019 Building a modular framework to facilitate and accelerate the realization and the maintenance of intelligent Conversational Agents with both rational and emotional capabilities.

The model consists of several modules including intent predictor, sentiment analysis, emotion analysis, topic analysis, and profiling module. ","sent1: Year Focus Approach Kenneth Mark Colby [8] 1975Designing chatbot behave like paranoid personRule-based approach with capability to simulate emotion.
sent2: Zhou et. al. [66] 2014 Building empathetic chatbot for social interactionSeq2seq learning with GRU-RNN model which takes into account intelligent quotient (IQ) and emotion quotien (EQ).
sent3: Zhou et. al. [65] 2018 Building emotion chatting machine (ECM) for large scale conversation generation Seq2seq learning with GRU which incorporates emotion detection to capture implicit internal emotion states.
sent4: Colombo et. al. [9] 2019 Developing affect-driven dialogue system which generates emotional responses in a controlled manner Seq2seq with GRU which incorporates emotion classifier, also affective re-ranking in the last step to produce the response.
sent5: Li et. al. [23] 2019 Building conversational system emotional by using editing constraints to generate more meaningful and customizable emotional replies Encoder-decoder architecture with reinforcement learning approach by using asynchronous decoder which uses keyword predictor to predict the topic and emotion editor to produce emotional-embedded response.
sent6: Zhang et. al. [62] 2017 Building emotional conversation systems which produces several responses for every emotion categoryMulti-task seq2seq model with GRU which utilize bidirectional long short term memory (Bi-LSTMs) as emotion classifier.
sent7: Lubis et. al. [27] 2019 Building a fully data driven chatoriented dialogue system that can dynamically mimic affective human interactionsProposing a seq2seq response generator, includes emotion encoder which is trained jointly with the entire network to encode and maintain the emotional context throughout the dialogue, but focusing only on positive emotion Ashgar et.
sent8: al. [2] 2018 Building open-domain neural dialogue models by augmenting them with affective intelligence.
sent9: Encoder-decoder architecture with LSTM model using cognitively engineered affective word embedding, and also affectively diverse beam search for decoding Sun et.
sent10: al. [55] 2018 Developing conversation generation which addresses the emotional factor by changing the model's input.Encoder-decoder framework based on LSTM using three inputs, a sequence without an emotional category, a sequence with an emotional category for the input sentence, and a sequence with an emotional category for the output responses.
sent11: Hu et. al. [18] 2018 Building a tone-aware chatbot for customer care on social media.
sent12: Encoder-decoder architecture with LSTM, which the decoder is modified to handle meta information by add an embedding vector as tone indicator to produce several tone responses including passionate, empathetic, and neutral.
sent13: Sun et. al. [55] 2018 Building emotional human machine conversation chatbot that be able to understands the user's emotions, and consider the user's emotions then give a satisfactory response.
sent14: Model this problem as reinforcement learning task by proposing a new neural model based on Seq2Seq model which uses generative adversarial network (GAN) called EM-SeqGAN.Shantala et.
sent15: al. [50] 2018 Building neural dialogue system which addresses emotional aspect.
sent16: Seq2Seq model with LSTM by using a novel emotion embedding.
sent17: Zhong et. al. [64] 2018 Building affect-rich open domain human-machine conversation system.
sent18: Extends Seq2Seq model and adopts VAD (Valence, Arousal and Dominance) affective notations.
sent19: Proposed model also considers the effect of negators and intensifiers via a novel affective attention mechanism.
sent20: Catania et. al. [6] 2019 Building a modular framework to facilitate and accelerate the realization and the maintenance of intelligent Conversational Agents with both rational and emotional capabilities.
sent21: The model consists of several modules including intent predictor, sentiment analysis, emotion analysis, topic analysis, and profiling module.","1. What was the focus and approach of Kenneth Mark Colby's work in 1975? sent1
2. What approach did Zhou et al. use in 2014 to build an empathetic chatbot? sent2
3. How did Zhou et al. enhance their emotion chatting machine (ECM) in 2018? sent3
4. What was the approach of Colombo et al. in 2019 for developing an affect-driven dialogue system? sent4
5. How did Li et al. build an emotional conversational system in 2019? sent5
6. What model did Zhang et al. use in 2017 for building emotional conversation systems? sent6
7. What was the focus of Lubis et al.'s work in 2019 on dialogue systems? sent7
8. How did Ashgar et al. augment open-domain neural dialogue models in 2018? sent8, sent9
9. What was the approach of Sun et al. in 2018 for conversation generation addressing emotional factors? sent10
10. How did Hu et al. build a tone-aware chatbot for customer care in 2018? sent11, sent12
11. What was the focus of Sun et al.'s work in 2018 on emotional human-machine conversation chatbots? sent13, sent14
12. How did Shantala et al. address the emotional aspect in neural dialogue systems in 2018? sent15, sent16
13. What approach did Zhong et al. use in 2018 for building an affect-rich open domain human-machine conversation system? sent17, sent18, sent19
14. What was the focus of Catania et al.'s work in 2019 on conversational agents? sent20, sent21"
265928847,Fake account detection in social media using machine learning methods: literature review,https://www.semanticscholar.org/paper/3ae6eeba3711501965bb004760510f1979f4b52f,Dataset,23,"Two types of datasets can be used for fake account detection: free datasets and self-made datasets.The majority of researchers chose to make their datasets that comprise fake and real accounts.One of the reasons to build their dataset is because of no available public open datasets for detecting fake accounts [24].Some researchers collected survey data using a questionnaire [21] or even hired a company to make a part of the dataset [23].For fake account classification, the dataset is collected from Facebook, Instagram, and Twitter (as shown in Tables 2 and 3).Other platforms are excluded from this literature review.1,162 accounts Gupta and Kaushal [7] 4,708 accounts Khalil et al. [19] Fake accounts: 13,000 Real accounts: 5,386 Twitter Ersahin et al. [8] Fake accounts: 501 Real accounts: 499 Cresci et al. [18] 13,101 accounts Walt and Eloff [20] 223,796 accounts Akyon and Kalfaoglu [24] Fake accounts: 700 Real accounts: 700 Bharti and Pandey [33] Real accounts: 1,103 Narayan [34] Fake accounts: 1,056 Real accounts: 1,176 Instagram Meshram et al. [14] Fake accounts: 3,231 Real accounts: 6,868 Purba et al. [15] Fake accounts: 32,869 Real accounts: 32,460 Sheikhi [1] Fake accounts: 3,132 Real accounts: 6,868 Durga and Sudhakar [40] Fake accounts: 201 Real accounts: 1,002  [29] The fake project dataset 11,737 accounts Khaled et al. [26] MIB dataset Fake accounts: 3,351 Real accounts: 1,950 Wang et al. [31] CLEF2019 dataset 7,120 accounts Bharti and Pandey [33] The fake project [18] 5.870 accounts Chakraborty et al. [36] MIB dataset Fake accounts: 3,474 Real accounts: 3,351 Kadam and Sharma [38] GitHub 2,820 accounts Instagram Kesharwani et al. [32] Fake, spammer, and genuine Instagram accounts 696 accounts Das et al. [37] Kaggle dataset 576 accounts

Various methods are used in gathering and compiling new datasets.Some of them take advantage of third-party websites [15], web data crawlers, and social media API.After data has been gathered, commonly the fake accounts and the real accounts are separated manually.There are also other methods to simplify the data-gathering process without classifying the accounts one by one.The method used by Khalil et al. [19] Bulletin of Electr Eng & Inf ISSN: 2302-9285  Fake account detection in social media using machine learning methods: … (Nalia Graciella Kerrysa) 3793 involved a university's Twitter account that has a lot of followers and verifies which accounts are real or not.Meanwhile, fake accounts are obtained by buying them from a website with affordable prices.","sent1: Two types of datasets can be used for fake account detection: free datasets and self-made datasets.
sent2: The majority of researchers chose to make their datasets that comprise fake and real accounts.
sent3: One of the reasons to build their dataset is because of no available public open datasets for detecting fake accounts [24].Some researchers collected survey data using a questionnaire [21] or even hired a company to make a part of the dataset [23].For fake account classification, the dataset is collected from Facebook, Instagram, and Twitter (as shown in Tables 2 and 3).Other platforms are excluded from this literature review.1,162 accounts Gupta and Kaushal [7] 4,708 accounts Khalil et al. [19] Fake accounts: 13,000 Real accounts: 5,386 Twitter Ersahin et al. [8] Fake accounts: 501 Real accounts: 499 Cresci et al. [18] 13,101 accounts Walt and Eloff [20] 223,796 accounts Akyon and Kalfaoglu [24] Fake accounts: 700 Real accounts: 700 Bharti and Pandey [33]
sent4: Real accounts: 1,103 Narayan [34] Fake accounts: 1,056 Real accounts: 1,176 Instagram Meshram et al. [14]
sent5: Fake accounts: 3,231 Real accounts: 6,868 Purba et al. [15] Fake accounts: 32,869 Real accounts: 32,460 Sheikhi [1] Fake
sent6: accounts: 3,132 Real accounts: 6,868 Durga and Sudhakar [40] Fake accounts: 201 Real accounts: 1,002
sent7: [29] The fake project dataset 11,737 accounts Khaled et al. [26] MIB dataset Fake accounts: 3,351 Real accounts: 1,950 Wang et al. [31] CLEF2019 dataset 7,120 accounts Bharti and Pandey [33]
sent8: The fake project [18] 5.870 accounts Chakraborty et al. [36] MIB dataset Fake accounts: 3,474 Real accounts: 3,351 Kadam and Sharma [38] GitHub 2,820 accounts Instagram Kesharwani et al. [32] Fake, spammer, and genuine Instagram accounts 696 accounts Das et al. [37] Kaggle dataset 576 accountsVarious methods are used in gathering and compiling new datasets.
sent9: Some of them take advantage of third-party websites [15], web data crawlers, and social media API.After data has been gathered, commonly the fake accounts and the real accounts are separated manually.
sent10: There are also other methods to simplify the data-gathering process without classifying the accounts one by one.
sent11: The method used by Khalil et al. [19] Bulletin of Electr Eng & Inf ISSN: 2302-9285 
sent12: Fake account detection in social media using machine learning methods: … (Nalia Graciella Kerrysa) 3793 involved a university's Twitter account that has a lot of followers and verifies which accounts are real or not.
sent13: Meanwhile, fake accounts are obtained by buying them from a website with affordable prices.","1. What types of datasets can be used for fake account detection? sent1
    1.1. What do the majority of researchers choose to do regarding datasets for fake account detection? sent2
        1.1.1. Why do researchers build their own datasets for fake account detection? sent3
            1.1.1.1. From which platforms is the dataset for fake account classification collected? sent3
    1.2. How are fake and real accounts typically separated after data gathering? sent9
    1.3. Are there methods to simplify the data-gathering process without classifying accounts individually? sent10
2. What is the method used by Khalil et al. for fake account detection? sent11
    2.1. How are fake accounts obtained according to the method used by Khalil et al.? sent13"
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,Topic 4: Epidemic Models for COVID-19 Spread,5,"A total of 10.25% (2964/28,904) of the included publications were related to this topic (eg, [39][40][41][42][43]). The 5 most prominent authors in this cluster were Gerardo Chowell (n=22), Benjamin J Cowling (n=18), Kenji Mizumoto (n=14), Shi Zhao (n=13), and Rosalind M Eggo (n=13). The most common journals and preprint servers where the articles related to this topic were published included Chaos, Solitons & Fractals (n=73), medRxiv (n=66), the International Journal of Infectious Diseases (n=36), Zhonghua liuxingbingxue zazhi (n=30), and bioRxiv (n=26). The first paper related to this topic was published on the January 19, 2020. Although there was a sharp increase in the number of weekly publications between weeks 12 and 15, the trend was almost stable from week 15 to week 22. Then, a rapid decline in the number of weekly publications was noticed (Multimedia Appendix 1). The mean number of weekly publications in this cluster was 101.6 (SD 68.2).","sent1: A total of 10.25% (2964/28,904) of the included publications were related to this topic (eg, [39][40][41][42][43]).
sent2: The 5 most prominent authors in this cluster were Gerardo Chowell (n=22), Benjamin J Cowling (n=18), Kenji Mizumoto (n=14), Shi Zhao (n=13), and Rosalind M Eggo (n=13).
sent3: The most common journals and preprint servers where the articles related to this topic were published included Chaos, Solitons & Fractals (n=73), medRxiv (n=66), the International Journal of Infectious Diseases (n=36), Zhonghua liuxingbingxue zazhi (n=30), and bioRxiv (n=26).
sent4: The first paper related to this topic was published on the January 19, 2020.
sent5: Although there was a sharp increase in the number of weekly publications between weeks 12 and 15, the trend was almost stable from week 15 to week 22.
sent6: Then, a rapid decline in the number of weekly publications was noticed (Multimedia Appendix 1).
sent7: The mean number of weekly publications in this cluster was 101.6 (SD 68.2).","1. What percentage of the included publications were related to the topic of epidemic models for COVID-19 spread? sent1
2. Who are the five most prominent authors in this cluster? sent2
3. What are the most common journals and preprint servers where articles related to this topic were published? sent3
4. When was the first paper related to this topic published? sent4
5. How did the trend in the number of weekly publications change over time? sent5, sent6
    5.1. What was the mean number of weekly publications in this cluster? sent7"
212633493,Deep Neural Networks for Automatic Speech Processing: A Survey from Large Corpora to Limited Data,https://www.semanticscholar.org/paper/7a307b379eb714df1e38fe9a80b7ed94c0fbd64e,D. Multi-task approach,4,"Multi-task models can be viewed as an extension of the Encoder-Decoder architecture where you have a decoder per task with a shared encoder (like in Figure 1). Then those tasks are trained conjointly with classic feed-forward algorithms. The goal of a multi-task learning is to have an encoder outputting sufficient information for every task. Doing so, it can potentially improve the performances of each task compared to mono task architectures. It is a way to have a more representative encoder given the same amount of data.

In emotion recognition, [22] got SOTA results over a modified version of the IEMOCAP database to have a fourclass problem. Those emotions are: angry, happy, neutral and sad. Y. Li et al. used an end-to-end multi-task system with only supervised tasks: gender identification and emotion identification [22]. The resulting model achieve an overall accuracy for the emotion task (which is the main target) of 81.6% and an average accuracy of each emotion category of 82.8%. Using such approach allows them to achieve balanced results over unbalanced data.

Nevertheless, using only supervised tasks requires multiple ground-truth for the targeted dataset. S. Pascual   a combination of self-supervised tasks combined with unsupervised tasks to tackle this problem and used the resulting encoder for transfer learning [23]. They recently improved this work in [24] where they use more tasks, a recurrent unit on top of the encoder and denoising mechanisms using multiple data augmentation on their system.","sent1: Multi-task models can be viewed as an extension of the Encoder-Decoder architecture where you have a decoder per task with a shared encoder (like in Figure 1).
sent2: Then those tasks are trained conjointly with classic feed-forward algorithms.
sent3: The goal of a multi-task learning is to have an encoder outputting sufficient information for every task.
sent4: Doing so, it can potentially improve the performances of each task compared to mono task architectures.
sent5: It is a way to have a more representative encoder given the same amount of data.
sent6: In emotion recognition, [22] got SOTA results over a modified version of the IEMOCAP database to have a fourclass problem.
sent7: Those emotions are: angry, happy, neutral and sad.
sent8: Y. Li et al. used an end-to-end multi-task system with only supervised tasks: gender identification and emotion identification [22].
sent9: The resulting model achieve an overall accuracy for the emotion task (which is the main target) of 81.6% and an average accuracy of each emotion category of 82.8%.
sent10: Using such approach allows them to achieve balanced results over unbalanced data.
sent11: Nevertheless, using only supervised tasks requires multiple ground-truth for the targeted dataset.
sent12: S. Pascual   a combination of self-supervised tasks combined with unsupervised tasks to tackle this problem and used the resulting encoder for transfer learning [23].
sent13: They recently improved this work in [24] where they use more tasks, a recurrent unit on top of the encoder and denoising mechanisms using multiple data augmentation on their system.","1. How can multi-task models be viewed in the context of the Encoder-Decoder architecture? sent1
    1.1. How are tasks trained in multi-task models? sent2
    1.2. What is the goal of multi-task learning? sent3
        1.2.1. How can multi-task learning potentially improve task performance? sent4
        1.2.2. What is the benefit of having a more representative encoder in multi-task learning? sent5
2. What results were achieved in emotion recognition using a multi-task approach? sent6
    2.1. What are the four emotion classes used in the modified IEMOCAP database? sent7
    2.2. What tasks did Y. Li et al. include in their end-to-end multi-task system? sent8
        2.2.1. What was the overall accuracy achieved for the emotion task? sent9
        2.2.2. How does the multi-task approach handle unbalanced data? sent10
3. What is a limitation of using only supervised tasks in multi-task learning? sent11
4. How did S. Pascual address the limitation of using only supervised tasks? sent12
    4.1. How was this work recently improved? sent13"
256461385,Narrative Why-Question Answering: A Review of Challenges and Datasets,https://www.semanticscholar.org/paper/98f4b43487b8386728c14aba24f848b2eadad1e2,Causality,8,"Causality is a semantic relationship between events showing that an event occurs or holds due to another event (Mostafazadeh et al., 2016b). Mostafazadeh et al. (2016b) distinguish four types of lexical causality relations: cause, enable, prevent, and cause-to-end based on the works by Wolff and Song (2003), Wolff (2007), and Khemlani et al. (2014). Moreover, causality has temporal implications such that if an event A causes/enables/prevents an event B, then A should start before B, or if an event A causes an event B to end, then B should start before A. Causality relations can hold one of the three temporal implications: before, overlaps, and during (Mostafazadeh et al., 2016b). Thus, while answering a whyquestion, the temporal relation between the events should also be taken into account in addition to the causality relation.

A causal relation is constructed from two components: cause and effect. Based on how the cause and the effect are conveyed in a text, causation can be distinguished into the following categories: explicit vs implicit, marked vs unmarked, and ambiguous vs unambiguous.

Explicit vs Implicit. Causation is explicit if both the cause and the effect are present in the text. Causation is implicit if either the cause or the effect of both are missing from the text (Blanco et al., 2008). For instance, ""She was accepted to a top university after receiving a high score in the state examination"" is explicit, while ""I did not attend the mandatory final exam."" is implicit because the effect of ""failing the course"" is not explicitly stated.

Marked vs Unmarked. Causation is marked if the text contains the causal signal words that indicate the causal relation (Blanco et al., 2008). For example, ""I was late because of traffic"" is marked, but ""Do not buy any bread. We have already got two at home"" is unmarked.

Ambiguous vs Unambiguous. If the causal relation is presented in the text with causal keywords (e.g., cause, effect, consequence) or with causal signals (e.g., because of, due to, as a result of ), it is considered unambiguous (Girju, 2003). On the other hand, if a causal relation is constructed in the form of an expression containing affect verbs (e.g., affect, change, influence) or link verbs (e.g., link, lead, depend), it is considered ambiguous. Furthermore, if a marked signal always refers to causation (e.g., because), it is unambiguous, while if a marked word occasionally signals causation (e.g., since), it is ambiguous (Blanco et al., 2008).","sent1: Causality is a semantic relationship between events showing that an event occurs or holds due to another event (Mostafazadeh et al., 2016b).
sent2: Mostafazadeh et al. (2016b) distinguish four types of lexical causality relations: cause, enable, prevent, and cause-to-end based on the works by Wolff and Song (2003), Wolff (2007), and Khemlani et al. (2014).
sent3: Moreover, causality has temporal implications such that if an event A causes/enables/prevents an event B, then A should start before B, or if an event A causes an event B to end, then B should start before A. Causality relations can hold one of the three temporal implications: before, overlaps, and during (Mostafazadeh et al., 2016b).
sent4: Thus, while answering a whyquestion, the temporal relation between the events should also be taken into account in addition to the causality relation.
sent5: A causal relation is constructed from two components: cause and effect.
sent6: Based on how the cause and the effect are conveyed in a text, causation can be distinguished into the following categories: explicit vs implicit, marked vs unmarked, and ambiguous vs unambiguous.
sent7: Explicit vs Implicit. Causation is explicit if both the cause and the effect are present in the text.
sent8: Causation is implicit if either the cause or the effect of both are missing from the text (Blanco et al., 2008).
sent9: For instance, ""She was accepted to a top university after receiving a high score in the state examination"" is explicit, while ""I did not attend the mandatory final exam.""
sent10: is implicit because the effect of ""failing the course"" is not explicitly stated.
sent11: Marked vs Unmarked. Causation is marked if the text contains the causal signal words that indicate the causal relation (Blanco et al., 2008).
sent12: For example, ""I was late because of traffic"" is marked, but ""Do not buy any bread.
sent13: We have already got two at home"" is unmarked.
sent14: Ambiguous vs Unambiguous. If the causal relation is presented in the text with causal keywords (e.g., cause, effect, consequence) or with causal signals (e.g., because of, due to, as a result of ), it is considered unambiguous (Girju, 2003).
sent15: On the other hand, if a causal relation is constructed in the form of an expression containing affect verbs (e.g., affect, change, influence) or link verbs (e.g., link, lead, depend), it is considered ambiguous.
sent16: Furthermore, if a marked signal always refers to causation (e.g., because), it is unambiguous, while if a marked word occasionally signals causation (e.g., since), it is ambiguous (Blanco et al., 2008).","1. What is causality in the context of semantic relationships between events? sent1
2. What are the four types of lexical causality relations distinguished by Mostafazadeh et al. (2016b)? sent2
3. What are the temporal implications of causality, and how do they relate to events A and B? sent3
4. Why should the temporal relation between events be considered when answering a why-question? sent4
5. What are the two components of a causal relation? sent5
6. How can causation be categorized based on how the cause and effect are conveyed in a text? sent6
    6.1. What is the difference between explicit and implicit causation? sent7, sent8
        6.1.1. Can you provide examples of explicit and implicit causation? sent9, sent10
    6.2. What distinguishes marked causation from unmarked causation? sent11
        6.2.1. Can you provide examples of marked and unmarked causation? sent12, sent13
    6.3. How is ambiguous causation different from unambiguous causation? sent14, sent15
        6.3.1. What makes a marked signal unambiguous or ambiguous? sent16"
237642905,"Aishna Gupta, Anuska Rakshit. A Technical Survey on the Modeling of Topical Bot",https://www.semanticscholar.org/paper/2846dd9c7578f680675b0d70b5f5cffe8fb8612c,Science,9,"A. NLU NLU is one of the most important segments in topical bots. It helps in selecting and producing system responses.

Segmentation of sentences: This is performed first as punctuation marks, double quotes, commas are not available in results of ASR and it helps in obtaining the appropriate units. ASR is responsible for identifying the human voice commands which include street names, landmarks, point of interests, distances, etc. Speech recognition does the speech to text conversion where the final output is the text corresponding to the recognized speech. [11] Sentiment and Emotion Recognition: To provide highquality responses, recognizing the sentiments and emotions of the user is one of the most important keys. This will lead to a meaningful and interesting conversation between the user and the device. There are 3 different types of sentiment in general; those are positive, negative, and neutral. Detecting these sentiments requires the ability to understand the difference in the tone and pitch of the user. The device should have built-in features that differentiate the normal/neutral pitch from positive, negative ones. This ability of understanding and simulating prosody is one of the most important parts of topical chat. Sentiment analysis is done by using different methods like lexicon-based methods, deep learning, or a combination of both.

Emotion plays an important role in human life. Interpersonal human communication includes not only language that is spoken, but also non-verbal cues as hand and body gestures, tone of the voice, which are used to express feeling and give feedback and most importantly through facial expression. [10] Emotion recognition includes recognizing fear, happiness, anger, sadness, disgust, surprise, and many more human emotions which can be detected through a change in tone. It is a difficult task to recognize all these emotions. And that is why the perfect topical chat device still doesn't exist. There are times when Alexa or Siri or Google assistants say that they aren't able to get us. These devices can't understand human jokes, sarcasm, doublemeaning words, slang, etc. It is difficult for AI to understand the meaning of words behind sarcasm. They give results on the basis of the literal meanings of words. Understanding these deep-down human emotions, sentiment and language is still impossible for AI and so as for topical chat devices. Even after providing the data of thousands of words, accents, jokes still people face problems in communicating with these devices efficiently.

Conversation Intent: Conversation intent was introduced in the default NLU model so that it can recognize the common natural ways of initiating the conversation like ""Hello"", ""How are you"", ""Let's talk"", ""Tell me a joke"" etc Response generation: For generating a response we have four different approaches: 1) Rule-based 2) Retrieval 3) Generative 4) Hybrid For a topical chat to give the best possible response, generally a huge amount of data from Wikipedia, online articles, and Reddit are extracted. Knowledge of adjectives, adverbs, pronouns, and slang are added to make the conversation more fun.

MELD is a multimodal multi-party conversational emotion recognition dataset. MELD contains raw videos, audio segments, and transcripts for multimodal processing. We believe this dataset will also be useful as a training corpus for both conversational emotion recognition and multimodal empathetic response generation. [5] A functional system can be a combination of these techniques, or follow a waterfall structure or use a hybrid approach with complementary modules retrieval-based modules that try to identify an appropriate response from the dataset of dialogs available. Retrieval can be performed using techniques such as entity matching, N-gram matching, or similarity-based on vectors such as TF-IDF, word or sentence embeddings, skip-thought vectors dual-encoder system, etc.

Hybrid approaches utilizing retrieval in combination with generative models are genuinely new and have shown promising outcomes in recent years, typically with sequenceto-sequence approaches with some variants. [2] Dialog Management: This is responsible for making the conversation engaging and finding the results related to the conversation. Once response generation is done, the dialog manager works in selecting the right response and the context. It also helps in smoothly changing or transiting the topics which the user can find interesting Dialog Management: This is responsible for making the conversation engaging and finding the results related to the conversation. Once response generation is done, the dialog manager works in selecting the right response and the context. It also helps in smoothly changing or transiting the topics which the user can find interesting.

B. SPEECH RECOGNITION BY CONVERSATIONAL AUTOMATIC Speech recognition systems can be separated in several different classes by describing what types of utterances they have the ability to recognize, these classes are classified as the following: Isolated Words, Connected Words, Continuous Speech, Spontaneous Speech [13]. Automatic Speech Recognition (ASR) is very important for voice-based assistants like google assistant. It's the advanced technology that helps us (humans) in talking to computer-based interface and to an extent the device recognize human conversations. ASR is based on or revolves around NLP (Natural Language Processing). NLP's work is to comprehend human speech and reply accordingly. Although having an accuracy of more than 95%, it still can't have a smooth interactive conversation between the device and the human. One of the majors is, even after having thousands of words, dictionaries, informal words, slangs it still is not enough when comes to having a comparison with humans. NLP enables computers to perform a wide range of natural language related tasks at all levels, ranging from parsing and part-of-speech (POS) tagging, to machine translation and dialogue systems. [9] C. CONVERSATIONAL DATASETS AND COMMON-SENSE REASONING Open source and pre-processed datasets can be used to get the information about trending topics, culture, preparing long and short-term memory, and integrating them within their dialog manager to make the responses seem as natural as possible. The term ""Social Bot"" is a superordinate concept which summarizes different types of (semi-) automatic agents. These agents are designed to fulfill a specific purpose by means of one-or many-sided communication in online media [17]. Moreover, to complement common sense reasoning, user satisfaction modules can be included to improve engagement along with the coherence of conversations.

D. CONTEXT AND DIALOG MODELLING The most powerful component of a conversational agent is a robust system that can handle dialogs effectively. Tasks that can be accomplished by the system include:

1) Help break down the complexity of the open domain problem to a manageable set of interaction modes, and 2) Be able to scale the topic based on its diversity and breadth differs. A common strategy is using a hierarchical architecture dialog modeling with the main Dialog Manager (DM) and multiple smaller DMs corresponding to specific tasks, topics, or contexts. The focus should be made not just on response generation but also on customer experience, and conversational strategies to increase engagement. Social bots received an obfuscated user hash code to enable personalization for repeat users.

E. CONVERSATIONAL TOPIC TRACKER For detecting the conversation topics, Deep Average Networks (DAN) can be adopted and train a topic classifier on interaction data categorized into multiple matters. The conversational topic tracker was identified for various purposes such as sentiment analysis, conversation evaluation, entity extraction, response generation, and many more. A novel extension was made by adding topic-word attention to formulate an attention-based DAN (ADAN) that allows the system to jointly capture topic keywords in an utterance and perform topic classification. It was observed that a user's satisfaction correlates well with long and coherent on-topic conversations, while metrics of topic breadth may provide complementary information to user ratings, as the repetitiveness of topics is hardly captured in user ratings due to the intrinsic limitations of live user data collection [12].

F. SELECTION AND RANKING TECHNIQUES In the case of rule-based rankers, the ranker chooses a response from the candidate responses obtained from submodules based on some logic. For model-based strategies, a supervised or reinforcement learning approach can be applied, trained on user ratings or on pre-defined large-scale dialog datasets such as Yahoo Answers, Reddit commentaries, OpenSubtitles, and much more. Higher scores are provided to the ranker to correct responses (e.g., followup comments on Quora are considered correct responses) while ignoring the incorrect or non-coherent responses obtained by sampling. To categorize responses and choose the best one, social bots need mechanisms to achieve the goal of having coherent and engaging conversations. When a reinforcement learning approach is used developed frameworks where the agent is a ranker, the actions are the candidate responses obtained from sub-modules, and the agent is trying to maximize the trade-off between satisfying the customer immediately versus taking into account the long-term reward of selecting a certain response. [3] G","sent1: A. NLU NLU is one of the most important segments in topical bots.
sent2: It helps in selecting and producing system responses.
sent3: Segmentation of sentences: This is performed first as punctuation marks, double quotes, commas are not available in results of ASR and it helps in obtaining the appropriate units.
sent4: ASR is responsible for identifying the human voice commands which include street names, landmarks, point of interests, distances, etc.
sent5: Speech recognition does the speech to text conversion where the final output is the text corresponding to the recognized speech.
sent6: [11] Sentiment and Emotion Recognition: To provide highquality responses, recognizing the sentiments and emotions of the user is one of the most important keys.
sent7: This will lead to a meaningful and interesting conversation between the user and the device.
sent8: There are 3 different types of sentiment in general; those are positive, negative, and neutral.
sent9: Detecting these sentiments requires the ability to understand the difference in the tone and pitch of the user.
sent10: The device should have built-in features that differentiate the normal/neutral pitch from positive, negative ones.
sent11: This ability of understanding and simulating prosody is one of the most important parts of topical chat.
sent12: Sentiment analysis is done by using different methods like lexicon-based methods, deep learning, or a combination of both.
sent13: Emotion plays an important role in human life.
sent14: Interpersonal human communication includes not only language that is spoken, but also non-verbal cues as hand and body gestures, tone of the voice, which are used to express feeling and give feedback and most importantly through facial expression.
sent15: [10] Emotion recognition includes recognizing fear, happiness, anger, sadness, disgust, surprise, and many more human emotions which can be detected through a change in tone.
sent16: It is a difficult task to recognize all these emotions.
sent17: And that is why the perfect topical chat device still doesn't exist.
sent18: There are times when Alexa or Siri or Google assistants say that they aren't able to get us.
sent19: These devices can't understand human jokes, sarcasm, doublemeaning words, slang, etc.
sent20: It is difficult for AI to understand the meaning of words behind sarcasm.
sent21: They give results on the basis of the literal meanings of words.
sent22: Understanding these deep-down human emotions, sentiment and language is still impossible for AI and so as for topical chat devices.
sent23: Even after providing the data of thousands of words, accents, jokes still people face problems in communicating with these devices efficiently.
sent24: Conversation Intent: Conversation intent was introduced in the default NLU model so that it can recognize the common natural ways of initiating the conversation like ""Hello"", ""How are you"", ""Let's talk"", ""Tell me a joke"" etc Response generation: For generating a response we have four different approaches: 1) Rule-based 2) Retrieval 3) Generative 4) Hybrid For a topical chat to give the best possible response, generally a huge amount of data from Wikipedia, online articles, and Reddit are extracted.
sent25: Knowledge of adjectives, adverbs, pronouns, and slang are added to make the conversation more fun.
sent26: MELD is a multimodal multi-party conversational emotion recognition dataset.
sent27: MELD contains raw videos, audio segments, and transcripts for multimodal processing.
sent28: We believe this dataset will also be useful as a training corpus for both conversational emotion recognition and multimodal empathetic response generation.
sent29: [5] A functional system can be a combination of these techniques, or follow a waterfall structure or use a hybrid approach with complementary modules retrieval-based modules that try to identify an appropriate response from the dataset of dialogs available.
sent30: Retrieval can be performed using techniques such as entity matching, N-gram matching, or similarity-based on vectors such as TF-IDF, word or sentence embeddings, skip-thought vectors dual-encoder system, etc.
sent31: Hybrid approaches utilizing retrieval in combination with generative models are genuinely new and have shown promising outcomes in recent years, typically with sequenceto-sequence approaches with some variants.
sent32: [2] Dialog Management: This is responsible for making the conversation engaging and finding the results related to the conversation.
sent33: Once response generation is done, the dialog manager works in selecting the right response and the context.
sent34: It also helps in smoothly changing or transiting the topics which the user can find interesting Dialog Management: This is responsible for making the conversation engaging and finding the results related to the conversation.
sent35: Once response generation is done, the dialog manager works in selecting the right response and the context.
sent36: It also helps in smoothly changing or transiting the topics which the user can find interesting.
sent37: B. SPEECH RECOGNITION BY CONVERSATIONAL AUTOMATIC Speech recognition systems can be separated in several different classes by describing what types of utterances they have the ability to recognize, these classes are classified as the following: Isolated Words, Connected Words, Continuous Speech, Spontaneous Speech [13].
sent38: Automatic Speech Recognition (ASR) is very important for voice-based assistants like google assistant.
sent39: It's the advanced technology that helps us (humans) in talking to computer-based interface and to an extent the device recognize human conversations.
sent40: ASR is based on or revolves around NLP (Natural Language Processing).
sent41: NLP's work is to comprehend human speech and reply accordingly.
sent42: Although having an accuracy of more than 95%, it still can't have a smooth interactive conversation between the device and the human.
sent43: One of the majors is, even after having thousands of words, dictionaries, informal words, slangs it still is not enough when comes to having a comparison with humans.
sent44: NLP enables computers to perform a wide range of natural language related tasks at all levels, ranging from parsing and part-of-speech (POS) tagging, to machine translation and dialogue systems.
sent45: [9] C. CONVERSATIONAL DATASETS AND COMMON-SENSE REASONING Open source and pre-processed datasets can be used to get the information about trending topics, culture, preparing long and short-term memory, and integrating them within their dialog manager to make the responses seem as natural as possible.
sent46: The term ""Social Bot"" is a superordinate concept which summarizes different types of (semi-) automatic agents.
sent47: These agents are designed to fulfill a specific purpose by means of one-or many-sided communication in online media [17].
sent48: Moreover, to complement common sense reasoning, user satisfaction modules can be included to improve engagement along with the coherence of conversations.
sent49: D. CONTEXT AND DIALOG MODELLING The most powerful component of a conversational agent is a robust system that can handle dialogs effectively.
sent50: Tasks that can be accomplished by the system include:1) Help break down the complexity of the open domain problem to a manageable set of interaction modes, and 2) Be able to scale the topic based on its diversity and breadth differs.
sent51: A common strategy is using a hierarchical architecture dialog modeling with the main Dialog Manager (DM) and multiple smaller DMs corresponding to specific tasks, topics, or contexts.
sent52: The focus should be made not just on response generation but also on customer experience, and conversational strategies to increase engagement.
sent53: Social bots received an obfuscated user hash code to enable personalization for repeat users.
sent54: E. CONVERSATIONAL TOPIC TRACKER For detecting the conversation topics, Deep Average Networks (DAN) can be adopted and train a topic classifier on interaction data categorized into multiple matters.
sent55: The conversational topic tracker was identified for various purposes such as sentiment analysis, conversation evaluation, entity extraction, response generation, and many more.
sent56: A novel extension was made by adding topic-word attention to formulate an attention-based DAN (ADAN) that allows the system to jointly capture topic keywords in an utterance and perform topic classification.
sent57: It was observed that a user's satisfaction correlates well with long and coherent on-topic conversations, while metrics of topic breadth may provide complementary information to user ratings, as the repetitiveness of topics is hardly captured in user ratings due to the intrinsic limitations of live user data collection [12].
sent58: F. SELECTION AND RANKING TECHNIQUES
sent59: In the case of rule-based rankers, the ranker chooses a response from the candidate responses obtained from submodules based on some logic.
sent60: For model-based strategies, a supervised or reinforcement learning approach can be applied, trained on user ratings or on pre-defined large-scale dialog datasets such as Yahoo Answers, Reddit commentaries, OpenSubtitles, and much more.
sent61: Higher scores are provided to the ranker to correct responses (e.g., followup comments on Quora are considered correct responses) while ignoring the incorrect or non-coherent responses obtained by sampling.
sent62: To categorize responses and choose the best one, social bots need mechanisms to achieve the goal of having coherent and engaging conversations.
sent63: When a reinforcement learning approach is used developed frameworks where the agent is a ranker, the actions are the candidate responses obtained from sub-modules, and the agent is trying to maximize the trade-off between satisfying the customer immediately versus taking into account the long-term reward of selecting a certain response.
sent64: [3] G","1. What is NLU and why is it important in topical bots? sent1
    1.1. How does NLU help in system response generation? sent2
2. What is the first step in sentence segmentation and why is it necessary? sent3
3. What is ASR responsible for in the context of topical bots? sent4
4. What is the final output of speech recognition? sent5
5. Why is sentiment and emotion recognition important in topical bots? sent6
    5.1. How does recognizing sentiments and emotions affect the conversation? sent7
    5.2. What are the three different types of sentiment? sent8
    5.3. What is required to detect these sentiments? sent9
    5.4. What features should a device have to differentiate pitch? sent10
    5.5. Why is understanding and simulating prosody important in topical chat? sent11
    5.6. What methods are used for sentiment analysis? sent12
6. What role does emotion play in human communication? sent13
    6.1. What non-verbal cues are included in interpersonal human communication? sent14
    6.2. What emotions are recognized in emotion recognition? sent15
    6.3. Why is it difficult to recognize all human emotions? sent16
    6.4. Why doesn't a perfect topical chat device exist yet? sent17
7. What are some limitations of current voice assistants like Alexa or Siri? sent18
    7.1. What types of language do these devices struggle to understand? sent19
    7.2. Why is it difficult for AI to understand sarcasm? sent20
    7.3. How do these devices interpret words? sent21
    7.4. What is still impossible for AI and topical chat devices? sent22
    7.5. What problems do people face when communicating with these devices? sent23
8. What is conversation intent and how is it used in NLU models? sent24
    8.1. What approaches are used for response generation in topical chat? sent24
    8.2. What knowledge is added to make conversations more fun? sent25
9. What is MELD and what does it contain? sent26, sent27
    9.1. How can MELD be useful? sent28
10. What techniques are used in retrieval-based modules for response generation? sent29, sent30
    10.1. What are hybrid approaches in response generation? sent31
11. What is the role of dialog management in conversational systems? sent32
    11.1. What does the dialog manager do after response generation? sent33
    11.2. How does dialog management help in topic transition? sent34, sent35, sent36
12. How are speech recognition systems classified? sent37
13. Why is ASR important for voice-based assistants? sent38
    13.1. What does ASR help humans do? sent39
    13.2. What is ASR based on? sent40
    13.3. What is the role of NLP in ASR? sent41
    13.4. What is a major limitation of ASR in interactive conversations? sent42
    13.5. Why is the vocabulary of ASR systems still insufficient? sent43
14. What tasks does NLP enable computers to perform? sent44
15. How can open source and pre-processed datasets be used in conversational systems? sent45
16. What is a ""Social Bot""? sent46
    16.1. What is the purpose of these agents? sent47
17. How can user satisfaction modules improve conversational systems? sent48
18. What is the most powerful component of a conversational agent? sent49
    18.1. What tasks can this system accomplish? sent50
    18.2. What is a common strategy in dialog modeling? sent51
    18.3. What should be the focus in conversational strategies? sent52
19. How do social bots use user hash codes? sent53
20. What is the purpose of a conversational topic tracker? sent54
    20.1. What was the novel extension made to the topic tracker? sent56
    20.2. How does user satisfaction correlate with conversation quality? sent57
21. What are selection and ranking techniques used for in conversational systems? sent58
    21.1. How do rule-based rankers choose responses? sent59
    21.2. What approach can be used in model-based strategies? sent60
    21.3. How are higher scores used in ranking? sent61
    21.4. What mechanisms do social bots need for coherent conversations? sent62
    21.5. How does a reinforcement learning approach work in response ranking? sent63"
231603122,Persuasive Natural Language Generation -A Literature Review,https://www.semanticscholar.org/paper/e9ac35f68ef4290d74551bafdfb09a4ea0983940,Benevolence,15,"Determinants that aim at creating value for the persuadee are subsumed in this category (DeMers 2016, Voss & Raz 2016). In line with Cognitive Dissonance Theory (Festinger 1957) the identified eight determinants relate to altering a persuadee's perceived benevolence through dissonant or consonant measures (summarized in Table 3). An implementation in a persuasive NLG AI can be facilitated through identifying their absence or impact (Hunter et al 2019, Zarouali et al. 2020. The benevolence determinants are ordered alphabetically to not imply a specific order. The first column presents the determinants, the second column concisely defines each, and the third provides examples for all determinants that were identified. The last column states the corresponding citations. Linguistic Appropriacy

This category subsumes fourteen determinants that facilitate an individual's stylome and aim at matching this with linguistic appropriacy. Such a stylome can be quantified and identified linguistically (Zarouali et al. 2020). Aforementioned Language Expectation Theory identifies written or spoken language as a rule-based system through which persuadees develop expectations (Burgoon & Miller 1985). The reason for profiling the stylome of an individual is to match these expectations (Park et al. 2015). Once implemented, a persuasive NLG AI can achieve congruence between a persuasive message and the persuadee, and thus generate persuasiveness. Table 2 introduces fourteen determinants of linguistic appropriacy in alphabetical order, provides a synopsis (i.e., brief summary, column two), an example for each determinant (column three), and the corresponding academic citation (in column four).  Quirk et al. 1985, Brewer 1980 Evidence Words Tendency to approve or disapprove something. Words such as: according to. Quirk et al. 1985 Familiarity Degree of familiarity of a word or how easily a word can be recognized by an adult. Word Frequency Indication of how often used words occur in a given language.

More uncommon words reflect that the writer possesses larger vocabulary. Baayen et al. 1995 Logical Argumentation

Previous academic works unveil that arguments with consistent logic in persuasive acts increase persuasiveness (Cialdini & Goldstein 2002, Walton et al. 2008, Block et al. 2019. In line with the theory of Probabilistic Models (McGuire 1981, Wyer 1970, it is assumed that conclusive statements lead to a persuadee's expectation that a conclusion will follow. Technical implementations of logical argumentation or logical meaning representations occur as first order logic or semantic argumentation graphs (Moens 2018, Block et al. 2019). The first column of Table 4 enumerates our fourteen logical argumentation determinants, while the second provides a synopsis. Column three provides an example, and column four the corresponding citation in which the factor was identified. As in previous tables, the determinants are merely sorted alphabetically. ","sent1: Determinants that aim at creating value for the persuadee are subsumed in this category (DeMers 2016, Voss & Raz 2016).
sent2: In line with Cognitive Dissonance Theory (Festinger 1957)
sent3: the identified eight determinants relate to altering a persuadee's perceived benevolence through dissonant or consonant measures (summarized in Table 3).
sent4: An implementation in a persuasive NLG AI can be facilitated through identifying their absence or impact (Hunter et al 2019, Zarouali et al. 2020.
sent5: The benevolence determinants are ordered alphabetically to not imply a specific order.
sent6: The first column presents the determinants, the second column concisely defines each, and the third provides examples for all determinants that were identified.
sent7: The last column states the corresponding citations.
sent8: Linguistic AppropriacyThis category subsumes fourteen determinants that facilitate an individual's stylome and aim at matching this with linguistic appropriacy.
sent9: Such a stylome can be quantified and identified linguistically (Zarouali et al. 2020).
sent10: Aforementioned Language Expectation Theory identifies written or spoken language as a rule-based system through which persuadees develop expectations (Burgoon & Miller 1985).
sent11: The reason for profiling the stylome of an individual is to match these expectations (Park et al. 2015).
sent12: Once implemented, a persuasive NLG AI can achieve congruence between a persuasive message and the persuadee, and thus generate persuasiveness.
sent13: Table 2 introduces fourteen determinants of linguistic appropriacy in alphabetical order, provides a synopsis (i.e., brief summary, column two), an example for each determinant (column three), and the corresponding academic citation (in column four).
sent14: Quirk et al. 1985, Brewer 1980 Evidence Words Tendency to approve or disapprove something.
sent15: Words such as: according to. Quirk et al. 1985 Familiarity Degree of familiarity of a word or how easily a word can be recognized by an adult.
sent16: Word Frequency Indication of how often used words occur in a given language.
sent17: More uncommon words reflect that the writer possesses larger vocabulary.
sent18: Baayen et al. 1995 Logical ArgumentationPrevious academic works unveil that arguments with consistent logic in persuasive acts increase persuasiveness (Cialdini & Goldstein 2002, Walton et al. 2008, Block et al. 2019. In line with the theory of Probabilistic Models (McGuire 1981, Wyer 1970, it is assumed that conclusive statements lead to a persuadee's expectation that a conclusion will follow. Technical implementations of logical argumentation or logical meaning representations occur as first order logic or semantic argumentation graphs (Moens 2018, Block et al. 2019).
sent19: The first column of Table 4 enumerates our fourteen logical argumentation determinants, while the second provides a synopsis.
sent20: Column three provides an example, and column four the corresponding citation in which the factor was identified.
sent21: As in previous tables, the determinants are merely sorted alphabetically.","1. What are the determinants aimed at creating value for the persuadee in the category of benevolence? sent1
    1.1. How do the eight identified determinants relate to altering a persuadee's perceived benevolence? sent3
    1.2. How can the implementation of benevolence determinants in a persuasive NLG AI be facilitated? sent4
    1.3. How are the benevolence determinants organized in the table? sent5
    1.4. What information is provided in the columns of the table for benevolence determinants? sent6, sent7
2. What does the category of linguistic appropriacy subsume? sent8
    2.1. How can a stylome be quantified and identified? sent9
    2.2. What does the Language Expectation Theory identify about language? sent10
    2.3. Why is profiling the stylome of an individual important? sent11
    2.4. What can a persuasive NLG AI achieve once linguistic appropriacy is implemented? sent12
    2.5. What information is provided in the columns of the table for linguistic appropriacy determinants? sent13
3. What is the significance of evidence words in persuasive language? sent14
4. How is familiarity defined in the context of persuasive language? sent15
5. What does word frequency indicate in a given language? sent16
6. What do more uncommon words reflect about the writer? sent17
7. How does logical argumentation increase persuasiveness according to previous academic works? sent18
    7.1. What technical implementations are used for logical argumentation? sent18
    7.2. What information is provided in the columns of the table for logical argumentation determinants? sent19, sent20
    7.3. How are the logical argumentation determinants organized in the table? sent21"
265928847,Fake account detection in social media using machine learning methods: literature review,https://www.semanticscholar.org/paper/3ae6eeba3711501965bb004760510f1979f4b52f,11,4,"Chakraborty et al. [36] Number of friends, followers, status, favorites, listed count, language count, geo-enabled.7

Durga and Sudhakar [40] Numbers of followers, followings, media, biography count, profile picture, private account, username digit count, username length, biography emoji count.9

Pashwan and Ravi [35] Numbers of followers, friends, favorites, tweets, tweet frequency, location, verified account.7

Shreya et al. [39] User age, gender, account age, link in the description, status, friends count, location, location IP, status.9","sent1: Chakraborty et al. [36] Number of friends, followers, status, favorites, listed count, language count, geo-enabled.7Durga and Sudhakar [40] Numbers of followers, followings, media, biography count, profile picture, private account, username digit count, username length, biography emoji count.9Pashwan and Ravi [35] Numbers of followers, friends, favorites, tweets, tweet frequency, location, verified account.7Shreya et al.
sent2: [39] User age, gender, account age, link in the description, status, friends count, location, location IP, status.9","1. What features are considered by Chakraborty et al. for fake account detection? sent1
2. What features are considered by Durga and Sudhakar for fake account detection? sent1
3. What features are considered by Pashwan and Ravi for fake account detection? sent1
4. What features are considered by Shreya et al. for fake account detection? sent2"
264832783,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825,Interpreting Emergent Abilities from Micro Perspective,12,"From a micro perspective, research predominantly emphasizes empirical probing, focusing on the factors that influence the emergent abilities of LLMs.These factors encompass variations in results across downstream tasks, driven by aspects such as the quality of pre-training data (Chan et al., 2022;Razeghi et al., 2022;Shin et al., 2022;Razeghi et al., 2022;Power et al., 2022), the quality of the provided examples (Lu et al., 2022;Liu et al., 2022;Wang et al., 2022;Turpin et al., 2023), and mappings of demonstration labels (Min et al., 2022;Kossen et al., 2023;Wei et al., 2023;Yoo et al., 2022).","sent1: From a micro perspective, research predominantly emphasizes empirical probing, focusing on the factors that influence the emergent abilities of LLMs.
sent2: These factors encompass variations in results across downstream tasks, driven by aspects such as the quality of pre-training data (Chan et al., 2022;Razeghi et al., 2022;Shin et al., 2022;Razeghi et al., 2022;Power et al., 2022), the quality of the provided examples (Lu et al., 2022;Liu et al., 2022;Wang et al., 2022;Turpin et al., 2023), and mappings of demonstration labels (Min et al., 2022;Kossen et al., 2023;Wei et al., 2023;Yoo et al., 2022).","1. What is the focus of research from a micro perspective on the emergent abilities of LLMs? sent1
    1.1. What factors influence the emergent abilities of LLMs? sent2"
264833081,Emotion Detection for Misinformation: A Review,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,Multimodality,4,"Although rumors and fake news were traditionally spread through face-to-face communication, the emergence of social media resulted in their primary means of dissemination switching to text.However, continual advances in technology have led to an increasing shift towards multimodality.For example, people now frequently augment textual post content with images or videos, while on platforms like YouTube or TikTok, videos are the predominant means of sharing information.Accordingly, it is becoming increasingly important to explore methods that can address the challenges of multimodality [216], and that are able to adapt to the ever-changing characteristics of social media communication.While we have reviewed a number of approaches that combine text and image-based information, recent advanced multi-modal models that integrate language and visual understanding provide considerable scope for further research in this area.For example, GPT-4 has a certain level of visual understanding capability, although its implementation details have not been publicly disclosed.Inspired by the success of LLMs, some studies have started to focus on large multi-modal models, such as LLaVA [217,218], an end-to-end large multimodal model that connects a visual encoder and a large language model to achieve general visual and language understanding.Additionally, MiniGPT-5 [219] introduces a novel interleaved vision-and-language generation technique, with a focus on non-descriptive multimodal generation.Exploring the integration of these large multimodal models within misinformation detection methods is an interesting and promising research direction.","sent1: Although rumors and fake news were traditionally spread through face-to-face communication, the emergence of social media resulted in their primary means of dissemination switching to text.
sent2: However, continual advances in technology have led to an increasing shift towards multimodality.
sent3: For example, people now frequently augment textual post content with images or videos, while on platforms like YouTube or TikTok, videos are the predominant means of sharing information.
sent4: Accordingly, it is becoming increasingly important to explore methods that can address the challenges of multimodality [216], and that are able to adapt to the ever-changing characteristics of social media communication.
sent5: While we have reviewed a number of approaches that combine text and image-based information, recent advanced multi-modal models that integrate language and visual understanding provide considerable scope for further research in this area.
sent6: For example, GPT-4 has a certain level of visual understanding capability, although its implementation details have not been publicly disclosed.
sent7: Inspired by the success of LLMs, some studies have started to focus on large multi-modal models, such as LLaVA [217,218], an end-to-end large multimodal model that connects a visual encoder and a large language model to achieve general visual and language understanding.
sent8: Additionally, MiniGPT-5 [219] introduces a novel interleaved vision-and-language generation technique, with a focus on non-descriptive multimodal generation.
sent9: Exploring the integration of these large multimodal models within misinformation detection methods is an interesting and promising research direction.","1. How has the primary means of dissemination for rumors and fake news changed with the emergence of social media? sent1
2. What technological advances have led to an increasing shift towards multimodality? sent2
3. How do people augment textual post content in the context of multimodality? sent3
4. Why is it important to explore methods that address the challenges of multimodality? sent4
5. What recent developments provide scope for further research in combining text and image-based information? sent5
    5.1. What is an example of a model with visual understanding capability? sent6
    5.2. What are some studies focusing on large multi-modal models inspired by LLMs? sent7
    5.3. What novel technique does MiniGPT-5 introduce? sent8
6. Why is exploring the integration of large multimodal models within misinformation detection methods considered promising? sent9"
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,Source(s) Target,5,"Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","sent1: Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011)
sent2: Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b)
sent3: Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article *
sent4: 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '
sent5: 17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection.
sent6: #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds.
sent7: * the article's body is summarised.
sent8: Sources: Twitter, ǌ News, ɀikipedia, Reddit.
sent9: Evidence: Single, Multiple, Thread.
sent10: 2 What is Stance? In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process.
sent11: Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition.
sent12: Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc.
sent13: Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither.
sent14: Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016).
sent15: Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).
sent16: Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","1. What is the purpose of the section titled ""Source(s) Target""? sent1
    1.1. What are some key characteristics of stance detection datasets for mis-and disinformation detection? sent5
        1.1.1. What does ""#Instances"" denote in the context of these datasets? sent6
        1.1.2. What is noted about the article's body in these datasets? sent7
    1.2. What are the sources of data for stance detection datasets? sent8
    1.3. What types of evidence are used in stance detection datasets? sent9
2. What is stance in the context of stance detection? sent10
    2.1. How do Biber and Finegan (1988) define stance? sent11
    2.2. How does Du Bois (2007) define stance and the stance-taking process? sent12
    2.3. What is the general definition of stance detection adopted by Küçük and Can (2020)? sent13
        2.3.1. What additional category label is occasionally added to stance categories? sent14
        2.3.2. How do stance detection definitions and label inventories vary? sent15
3. How is stance detection distinguished from other NLP tasks? sent16"
237532483,A Survey on Temporal Sentence Grounding in Videos,https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453,4.2.1,4,"Large-scale video corpus moment retrieval. Large-scale video corpus moment retrieval (VCMR) is a research direction extended from TSGV that has been explored over the past few years [15,32,77,79]. It has more application value since it can retrieve the target segment semantically corresponding to a given text query from a large-scale video corpus (i.e., a collection of untrimmed and unsegmented videos) rather than from a single video. As compared with TSGV, VCMR has higher efficiency requirements since it not only needs to retrieve a specific segment from one single video but also locates the target video from a video corpus. Escorcia et al. [15] first extend TSGV to VCMR, introducing a model named Clip Alignment with Language (CAL) to align the query feature with a sequence of uniformly partitioned clips for moment composing. Lei et al. [32] introduce a new dataset for VCMR called TVR, which is comprised of videos and their associated subtitle texts. A Cross-modal Moment Localization (XML) network with a novel convolutional start-end detector module is also proposed to produce moment predictions in a late fusion manner. Zhang et al. [77] present a hierarchical multi-modal encoder (HAMMER) to capture both coarse-and fine-grained semantic information from the videos and train the model with three sub-tasks (i.e., video retrieval, segment temporal localization, and masked language modeling). Zhang et al. [79] introduce contrastive learning for VCMR, designing a retrieval and localization network with contrastive learning (ReLoCLNet).","sent1: Large-scale video corpus moment retrieval.
sent2: Large-scale video corpus moment retrieval (VCMR) is a research direction extended from TSGV that has been explored over the past few years [15,32,77,79].
sent3: It has more application value since it can retrieve the target segment semantically corresponding to a given text query from a large-scale video corpus (i.e., a collection of untrimmed and unsegmented videos) rather than from a single video.
sent4: As compared with TSGV, VCMR has higher efficiency requirements since it not only needs to retrieve a specific segment from one single video but also locates the target video from a video corpus.
sent5: Escorcia et al. [15] first extend TSGV to VCMR, introducing a model named Clip Alignment with Language (CAL) to align the query feature with a sequence of uniformly partitioned clips for moment composing.
sent6: Lei et al. [32] introduce a new dataset for VCMR called TVR, which is comprised of videos and their associated subtitle texts.
sent7: A Cross-modal Moment Localization (XML) network with a novel convolutional start-end detector module is also proposed to produce moment predictions in a late fusion manner.
sent8: Zhang et al. [77] present a hierarchical multi-modal encoder (HAMMER) to capture both coarse-and fine-grained semantic information from the videos and train the model with three sub-tasks (i.e., video retrieval, segment temporal localization, and masked language modeling).
sent9: Zhang et al. [79] introduce contrastive learning for VCMR, designing a retrieval and localization network with contrastive learning (ReLoCLNet).","1. What is large-scale video corpus moment retrieval (VCMR)? sent1, sent2
    1.1. Why does VCMR have more application value compared to TSGV? sent3
    1.2. What are the efficiency requirements of VCMR compared to TSGV? sent4
2. Who first extended TSGV to VCMR and what model did they introduce? sent5
3. What new dataset for VCMR did Lei et al. introduce, and what does it comprise? sent6
4. What network did Lei et al. propose for VCMR, and what is its function? sent7
5. What did Zhang et al. present to capture semantic information in VCMR, and how is the model trained? sent8
6. What approach did Zhang et al. introduce for VCMR, and what is the design focus of their network? sent9"
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,Explainable Model Behavior,15,"Explainable model behavior, that is, explainable analysis of the results of model prediction behavior, provides the basis for prediction results.Behavioral analysis typically involves comprehending the foundation of a model's anticipated behavior.Since deep learning algorithms consist of nonlinear structures, these successful models are commonly obscure and have difficulty revealing the rationale of their forecasts in a format that humans can grasp.The absence of transparency and intelligibility regarding a model's forecasts can lead to grave consequences.Shu et al. [55] used the sentence-comment joint attention sub-network to improve the performance of fake news detection, aiming to capture the inherent interpretability of news phrases and user comments.The dEFEND algorithm module facilitates search functionalities for searching news dissemination networks, trending news, top statements and related news.Moreover, it presents test results and explanations.In a similar vein, Lu et al. [151] utilized the graph-aware common attention network (GCAN) to assess the authenticity of source tweets on social media while providing explanations for the results.GCAN uses the attention mechanism to capture three aspects of the algorithm results: highlighting key words in source tweets, identifying characteristics of retweet propagation paths and understanding the behavior of retweeters.Chi et al. [152] proposed an automated explainable decision-making system (QA-AXDS) based on quantitative argumentation.This system can detect fake news and explain the results to users.It automatically captures human-level knowledge, constructs an interpretation model based on a dialogue tree and employs natural language to help users understand the reasoning process within the system.Notably, QA-AXDS is fully automated and does not require expert experience as pre-input, which enhances the robustness of the system.Ni et al. [153] studied the use of a multi-view attention mechanism network (MVAN) [154] to detect fake news in social networks and provide explanations for the results.MVAN incorporates a dual attention mechanism, encompassing text semantic attention and propagation structure attention, to capture clues in source tweets and propagation structures.It identifies crucial keywords and generates explainable detection results.Raha et al. [155] proposed a neural model for factual inconsistency classification with explanations.By training four neural models, they can predict the inconsistency type and provide explanations for a given sentence.However, Bhattarai et al. [156] introduced an explainable fake news detection framework based on the Tsetlin Machine (TM) [157].By capturing lexical and semantic features of true and fake news texts, this framework achieves accurate detection of fake news and the credibility score is used to provide interpretability.

Fu et al. [158] introduced a comprehensive and explainable false information detection framework called DISCO, as depicted in Figure 8.This framework addresses the challenge of detecting false information by leveraging the heterogeneity of false information and offering explanations for the detection results.Their approach demonstrates commendable accuracy and interpretability in a real-world fake news detection task.

within the system.Notably, QA-AXDS is fully automated and does not require expert experience as pre-input, which enhances the robustness of the system.Ni et al. [153] studied the use of a multi-view attention mechanism network (MVAN) [154] to detect fake news in social networks and provide explanations for the results.MVAN incorporates a dual attention mechanism, encompassing text semantic attention and propagation structure attention, to capture clues in source tweets and propagation structures.It identifies crucial keywords and generates explainable detection results.Raha et al. [155] proposed a neural model for factual inconsistency classification with explanations.By training four neural models, they can predict the inconsistency type and provide explanations for a given sentence.However, Bhattarai et al. [156] introduced an explainable fake news detection framework based on the Tsetlin Machine (TM) [157].By capturing lexical and semantic features of true and fake news texts, this framework achieves accurate detection of fake news and the credibility score is used to provide interpretability.

Fu et al. [158] introduced a comprehensive and explainable false information detection framework called DISCO, as depicted in Figure 8.This framework addresses the challenge of detecting false information by leveraging the heterogeneity of false information and offering explanations for the detection results.Their approach demonstrates commendable accuracy and interpretability in a real-world fake news detection task.We summarize the explainable model behavior analysis methods in Table 8, including the main techniques they use, datasets and accuracy performance.We summarize the explainable model behavior analysis methods in Table 8, including the main techniques they use, datasets and accuracy performance.","sent1: Explainable model behavior, that is, explainable analysis of the results of model prediction behavior, provides the basis for prediction results.
sent2: Behavioral analysis typically involves comprehending the foundation of a model's anticipated behavior.
sent3: Since deep learning algorithms consist of nonlinear structures, these successful models are commonly obscure and have difficulty revealing the rationale of their forecasts in a format that humans can grasp.
sent4: The absence of transparency and intelligibility regarding a model's forecasts can lead to grave consequences.
sent5: Shu et al. [55] used the sentence-comment joint attention sub-network to improve the performance of fake news detection, aiming to capture the inherent interpretability of news phrases and user comments.
sent6: The dEFEND algorithm module facilitates search functionalities for searching news dissemination networks, trending news, top statements and related news.
sent7: Moreover, it presents test results and explanations.
sent8: In a similar vein, Lu et al. [151] utilized the graph-aware common attention network (GCAN) to assess the authenticity of source tweets on social media while providing explanations for the results.
sent9: GCAN uses the attention mechanism to capture three aspects of the algorithm results: highlighting key words in source tweets, identifying characteristics of retweet propagation paths and understanding the behavior of retweeters.
sent10: Chi et al. [152] proposed an automated explainable decision-making system (QA-AXDS) based on quantitative argumentation.
sent11: This system can detect fake news and explain the results to users.
sent12: It automatically captures human-level knowledge, constructs an interpretation model based on a dialogue tree and employs natural language to help users understand the reasoning process within the system.
sent13: Notably, QA-AXDS is fully automated and does not require expert experience as pre-input, which enhances the robustness of the system.
sent14: Ni et al. [153] studied the use of a multi-view attention mechanism network (MVAN) [154] to detect fake news in social networks and provide explanations for the results.
sent15: MVAN incorporates a dual attention mechanism, encompassing text semantic attention and propagation structure attention, to capture clues in source tweets and propagation structures.
sent16: It identifies crucial keywords and generates explainable detection results.
sent17: Raha et al. [155] proposed a neural model for factual inconsistency classification with explanations.
sent18: By training four neural models, they can predict the inconsistency type and provide explanations for a given sentence.
sent19: However, Bhattarai et al. [156] introduced an explainable fake news detection framework based on the Tsetlin Machine (TM) [157].By capturing lexical and semantic features of true and fake news texts, this framework achieves accurate detection of fake news and the credibility score is used to provide interpretability.
sent20: Fu et al. [158] introduced a comprehensive and explainable false information detection framework called DISCO, as depicted in Figure
sent21: 8.This framework addresses the challenge of detecting false information by leveraging the heterogeneity of false information and offering explanations for the detection results.
sent22: Their approach demonstrates commendable accuracy and interpretability in a real-world fake news detection task.within the system.
sent23: Notably, QA-AXDS is fully automated and does not require expert experience as pre-input, which enhances the robustness of the system.
sent24: Ni et al. [153] studied the use of a multi-view attention mechanism network (MVAN) [154] to detect fake news in social networks and provide explanations for the results.
sent25: MVAN incorporates a dual attention mechanism, encompassing text semantic attention and propagation structure attention, to capture clues in source tweets and propagation structures.
sent26: It identifies crucial keywords and generates explainable detection results.
sent27: Raha et al. [155] proposed a neural model for factual inconsistency classification with explanations.
sent28: By training four neural models, they can predict the inconsistency type and provide explanations for a given sentence.
sent29: However, Bhattarai et al. [156] introduced an explainable fake news detection framework based on the Tsetlin Machine (TM) [157].By capturing lexical and semantic features of true and fake news texts, this framework achieves accurate detection of fake news and the credibility score is used to provide interpretability.
sent30: Fu et al. [158] introduced a comprehensive and explainable false information detection framework called DISCO, as depicted in Figure
sent31: 8.This framework addresses the challenge of detecting false information by leveraging the heterogeneity of false information and offering explanations for the detection results.
sent32: Their approach demonstrates commendable accuracy and interpretability in a real-world fake news detection task.
sent33: We summarize the explainable model behavior analysis methods in Table 8, including the main techniques they use, datasets and accuracy performance.
sent34: We summarize the explainable model behavior analysis methods in Table 8, including the main techniques they use, datasets and accuracy performance.","1. What is explainable model behavior in the context of model prediction? sent1
2. What does behavioral analysis typically involve? sent2
3. Why are deep learning algorithms often obscure in revealing the rationale of their forecasts? sent3
4. What can the absence of transparency and intelligibility in a model's forecasts lead to? sent4
5. How did Shu et al. improve the performance of fake news detection? sent5
6. What functionalities does the dEFEND algorithm module facilitate? sent6, sent7
7. How does Lu et al.'s GCAN assess the authenticity of source tweets? sent8
    7.1. What aspects does GCAN capture using the attention mechanism? sent9
8. What system did Chi et al. propose for explainable decision-making? sent10
    8.1. How does QA-AXDS help users understand the reasoning process? sent11, sent12
    8.2. What enhances the robustness of QA-AXDS? sent13
9. What did Ni et al. study regarding fake news detection? sent14
    9.1. How does MVAN capture clues in source tweets and propagation structures? sent15
    9.2. What does MVAN identify and generate? sent16
10. What model did Raha et al. propose for factual inconsistency classification? sent17
    10.1. How do the neural models predict inconsistency type and provide explanations? sent18
11. What framework did Bhattarai et al. introduce for fake news detection? sent19
    11.1. How does this framework achieve accurate detection and provide interpretability? sent19
12. What is the DISCO framework introduced by Fu et al.? sent20, sent21
    12.1. What does their approach demonstrate in a real-world fake news detection task? sent22
13. Where can a summary of explainable model behavior analysis methods be found? sent33"
5058880,False Information on Web and Social Media: A Survey,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,User characteristics.,4,"Several studies have shown that the characteristics of creators of false information are different from those of true information creators. Kumar et al. [50] found that the creators of hoaxes have typically more recently registered accounts and less editing experience (Figure 9(c)), suggesting the use of ""throw-away"" accounts. Surprisingly, non-hoax articles that are wrongly assumed to be hoaxes were also created by similar editors, meaning that they lack the skills to create well-written articles, which leads to others believing that the article is a hoax.

In cases of rumors, Bessi et al. [13] studied over 20.7 million tweets related to US presidential election, and identified users involved in tweeting as bots or honest users using a classification tool produced by Davis et al. [21]. Their analysis found that about one-fifth of content created and spread was by bots, showing that rumors are spread by automated accounts in short-bursts of time. Shao et al. [87] came to similar conclusions in their experiments.","sent1: Several studies have shown that the characteristics of creators of false information are different from those of true information creators.
sent2: Kumar et al. [50] found that the creators of hoaxes have typically more recently registered accounts and less editing experience (Figure 9(c)), suggesting the use of ""throw-away"" accounts.
sent3: Surprisingly, non-hoax articles that are wrongly assumed to be hoaxes were also created by similar editors, meaning that they lack the skills to create well-written articles, which leads to others believing that the article is a hoax.
sent4: In cases of rumors, Bessi et al. [13] studied over 20.7 million tweets related to US presidential election, and identified users involved in tweeting as bots or honest users using a classification tool produced by Davis et al. [21].
sent5: Their analysis found that about one-fifth of content created and spread was by bots, showing that rumors are spread by automated accounts in short-bursts of time.
sent6: Shao et al. [87] came to similar conclusions in their experiments.","1. How do the characteristics of creators of false information differ from those of true information creators? sent1
    1.1. What did Kumar et al. find about the creators of hoaxes? sent2
    1.2. What is surprising about non-hoax articles wrongly assumed to be hoaxes? sent3
2. What did Bessi et al. study in relation to rumors and the US presidential election? sent4
    2.1. What did their analysis reveal about the spread of rumors? sent5
    2.2. Did Shao et al. reach similar conclusions in their experiments? sent6"
231603122,Persuasive Natural Language Generation -A Literature Review,https://www.semanticscholar.org/paper/e9ac35f68ef4290d74551bafdfb09a4ea0983940,Method,23,"This paper's methodology follows a framework proposed by vom Brocke et al. (2009) which is based on a screening of the review literature itself and especially highlights the need for comprehensively documenting the process of literature search in such an article (Duerr et al. 2016). The framework is structured into the following five steps: (1) definition of review scope, (2) conceptualization of topic, (3) literature search, (4) literature analysis and synthesis, and (5) research agenda. Each of the steps will be briefly explained, when it will be addressed in the course of this work.

The first step is the definition of the review scope of this literature review. It is summarized in Figure 1 (categories applicable to this review on Persuasion and NLG research are highlighted) which is based on the taxonomy adapted by vom Brocke et al. (2009).  Brocke et al. 2009) This literature review focuses on outcomes of applied research in the domains of Persuasion and Natural Language Generation. The goal is to integrate findings with respect to five categories concluded from the business problem of persuading individuals through textual exchange (DeMers 2016). These categories were chosen as they address the psychological and technical aspects of persuasion and are a prerequisite to creating a persuasive NLG artificial intelligence. We selected this field because the artificial generation of persuasion through NLG has, as this literature review reveals, not been addressed in seminal articles following our structured research approach. Persuasion is already commonly studied in psychology (Quirk et al. 1985, Marwell & Schmitt 1967. Also, numerous studies in natural language processing focus on identifying and classifying persuasion in an automated way (Li et al. 2020, Rocklage et al. 2018, Iyer & Sycara 2019). This paper is organized along a conceptual structure. We did not take a particular perspective to provide a neutral representation of the results. As an audience of this review specialized scholars having an interest in the field of persuasion and the artificial generation of it were chosen. For coverage, our literature review can be categorized as representative as our research has been limited to certain journals, but does not consider the totality of the literature. The second step is conceptualization of the topic .

It addresses the point that an author of a review article must begin with a topic in need of review, a broad conception of what is known about the topic, and potential areas where new knowledge may be needed. In the following, we conceptualize persuasion, and embed it into a business context. Furthermore, we introduce related theories, and conclude a categorization for the successive literature review.

In persuasion, the persuader induces a particular kind of mental state in the persuadee , e.g., through threats, but unlike an expression of sentiment, persuasion intends a change in the mental state of the persuadee (Iyer & Sycara 2019). Contemporary psychology and communication science (Rocklage et al. 2018, Park et al. 2015, Hunter et al. 2019 require the persuader to be acting intentionally, that is, the persuasive act. In the context of NLG, we usually refer to messages generated or augmented by an artificial intelligence, if we use the term persuasive act.

In his seminal work ' On Rhetoric ', Aristotle introduced his well-known ontology for persuasive acts. Accordingly, persuasion depends on multiple facets: emotions ( pathos ), logical structure of the argument ( logos ), the context ( cairos ), and on the speaker ( ethos ) (Schiappa & Nordin 2013).

Likewise, contemporary business literature conceptualizes persuasive acts through ""principles of persuasion"" (DeMers 2016). The author concludes that six interventions help at achieving persuasiveness. The first is being confident and remaining confident during the entirety of an appeal. Next, the introduction of logical argumentation fosters persuasiveness, since individuals are more inclined to be persuaded by logic. Additionally, making an appeal seem beneficial to the other party, by demonstrating the value of an appeal, choosing words carefully (i.e., selecting from a vocabulary that may be more persuasive), and using flattery (i.e., finding appropriate compliments) are recommended. Lastly, DeMers (2016) reveals that being patient and persistent (i.e., not to greatly alter one's approach) strengthens a persuader's persuasiveness. Next, we embed the presented ""principles of persuasion"" into related theories on persuasion (Cameron 2009).

Festinger's Theory of Cognitive Dissonance (1957) focuses on the relationships among cognitive elements, which include beliefs, opinions, attitudes, or knowledge (O'Keefe 1990). This relates most to Aristotle's cairos , and the need to create benevolence for the persuadee. Evaluating the 'business principles', this theory resonates best with what DeMers (2016) defines as 'making [the cognition] appealing to the other party'. However cognitions, and thus, a persuadee's perceived benevolence, can be dissonant, consonant, or irrelevant to each other. If a persuadee is presented with a sufficiently vast cognitive inconsistency, then they will perceive psychological discomfort, leading to an attempt to restore their cognitive balance through a reduction or elimination of the inconsistency (Stiff 1994, Harmon-Jones 2002. The magnitude of dissonance determines one's motivation to reduce it (Stiff 1994, Festinger 1957. Approaches towards reducing dissonance are: changing terms to make them more consonant, adding additional consonant percipience, or altering the magnitude of the percipience (Harmon-Jones 2002, Stiff 1994, O'Keefe 1990.

In 'principles of persuasion', DeMers (2016) contends that appropriate flattering and the usage of so-called high value words contribute to persuasive acts in business contexts (cf. Aristotle's ethos ). Accordingly, Language Expectancy Theory (LET) identifies written or spoken language as a rule-based system through which persuadees develop expectations and preferences towards ''appropriate'' linguistic usage of words in varying situations (Burgoon & Miller 1985). Such expectations are frequently consistent with sociological and cultural norms, while preferences tend to relate to societal standards and cultural values (Burgoon & Miller 1985, Burgoon et al. 2002. Positive expectations that facilitate a persuasive act are, for instance, if a persuader stylizes a behavior that is perceived as more preferred than expected by the persuadee. In contrast, negative ones are inhibiting persuasion, e.g., when the persuader makes use of language that is considered to be socially unacceptable (Burgoon & Miller 1985, Burgoon et al. 2002.

Next, the 'principles of persuasion' confer that an argument based on logic is persuasive (DeMers 2016). What Aristotle terms logos is consistent with the theory of probabilistic models. Probabilistic models (McGuire 1981, Wyer 1970) are based on the rules of formal logic and probability, and predict beliefs regarding the conclusion of reasoning. These predictions are based on mathematical probability, and as such this theory is consistent with what Aristotle defines as logos . An exemplary belief syllogism (McGuire 1981) is composed of two premises that lead to a logical conclusion. The theorists (McGuire 1981, Wyer 1970 explain that believing in the premises leads to the expectation that the identified conclusion will follow. However, rather than solely thinking in all-or-nothing scenarios, beliefs can be ascertained through subjective probabilities: one's judgment of the probability that each of the premises is true (McGuire 1981, Wyer 1970. Furthermore, if a message evokes a perceptual change in the truth of the premise, or additional premises are supplemented, following this theory, a change in perceiving the conclusion is expected.

Last, Balance Theory focuses on the triadic relationship involving two individuals (e.g., persuader and persuadee), the persuadee's attitude toward the persuader (Aristotle's pathos ), and their attitudes toward an attitudinal object (Heider 1958). The resulting triad can be balanced or unbalanced: This triad is in balance if all three relationships are positive, or one is positive and two are negative. If all three relationships are negative, or one is negative and two are positive, an unbalanced triad results, often motivating one to alter one of the three relationships (Heider 1958). Building on this theory, we aim to identify relational determinants that relate to improving the relationship between the persuader and the persuadee towards the attitudinal object (Heider 1958). In the business framework, we relate those determinants towards the pattern of 'trustworthiness', that represent the persuadee's attitude towards the persuader. In his persuasion attempt, the persuader wants the persuadee to have a positive attitude.","sent1: This paper's methodology follows a framework proposed by vom Brocke et al. (2009) which is based on a screening of the review literature itself and especially highlights the need for comprehensively documenting the process of literature search in such an article (Duerr et al. 2016).
sent2: The framework is structured into the following five steps: (1) definition of review scope, (2) conceptualization of topic, (3) literature search, (4) literature analysis and synthesis, and (5) research agenda.
sent3: Each of the steps will be briefly explained, when it will be addressed in the course of this work.
sent4: The first step is the definition of the review scope of this literature review.
sent5: It is summarized in Figure 1 (categories applicable to this review on Persuasion and NLG research are highlighted) which is based on the taxonomy adapted by vom Brocke et al. (2009).
sent6: Brocke et al. 2009) This literature review focuses on outcomes of applied research in the domains of Persuasion and Natural Language Generation.
sent7: The goal is to integrate findings with respect to five categories concluded from the business problem of persuading individuals through textual exchange (DeMers 2016).
sent8: These categories were chosen as they address the psychological and technical aspects of persuasion and are a prerequisite to creating a persuasive NLG artificial intelligence.
sent9: We selected this field because the artificial generation of persuasion through NLG has, as this literature review reveals, not been addressed in seminal articles following our structured research approach.
sent10: Persuasion is already commonly studied in psychology (Quirk et al. 1985, Marwell & Schmitt 1967. Also, numerous studies in natural language processing focus on identifying and classifying persuasion in an automated way (Li et al. 2020, Rocklage et al. 2018, Iyer & Sycara 2019).
sent11: This paper is organized along a conceptual structure.
sent12: We did not take a particular perspective to provide a neutral representation of the results.
sent13: As an audience of this review specialized scholars having an interest in the field of persuasion and the artificial generation of it were chosen.
sent14: For coverage, our literature review can be categorized as representative as our research has been limited to certain journals, but does not consider the totality of the literature.
sent15: The second step is conceptualization of the topic .
sent16: It addresses the point that an author of a review article must begin with a topic in need of review, a broad conception of what is known about the topic, and potential areas where new knowledge may be needed.
sent17: In the following, we conceptualize persuasion, and embed it into a business context.
sent18: Furthermore, we introduce related theories, and conclude a categorization for the successive literature review.
sent19: In persuasion, the persuader induces a particular kind of mental state in the persuadee , e.g., through threats, but unlike an expression of sentiment, persuasion intends a change in the mental state of the persuadee (Iyer & Sycara 2019).
sent20: Contemporary psychology and communication science (Rocklage et al. 2018, Park et al. 2015, Hunter et al. 2019 require the persuader to be acting intentionally, that is, the persuasive act.
sent21: In the context of NLG, we usually refer to messages generated or augmented by an artificial intelligence, if we use the term persuasive act.
sent22: In his seminal work ' On Rhetoric ', Aristotle introduced his well-known ontology for persuasive acts.
sent23: Accordingly, persuasion depends on multiple facets: emotions ( pathos ), logical structure of the argument ( logos ), the context ( cairos ), and on the speaker ( ethos ) (Schiappa & Nordin 2013).
sent24: Likewise, contemporary business literature conceptualizes persuasive acts through ""principles of persuasion"" (DeMers 2016).
sent25: The author concludes that six interventions help at achieving persuasiveness.
sent26: The first is being confident and remaining confident during the entirety of an appeal.
sent27: Next, the introduction of logical argumentation fosters persuasiveness, since individuals are more inclined to be persuaded by logic.
sent28: Additionally, making an appeal seem beneficial to the other party, by demonstrating the value of an appeal, choosing words carefully (i.e., selecting from a vocabulary that may be more persuasive), and using flattery (i.e., finding appropriate compliments) are recommended.
sent29: Lastly, DeMers (2016) reveals that being patient and persistent (i.e., not to greatly alter one's approach) strengthens a persuader's persuasiveness.
sent30: Next, we embed the presented ""principles of persuasion"" into related theories on persuasion (Cameron 2009).
sent31: Festinger's Theory of Cognitive Dissonance (1957) focuses on the relationships among cognitive elements, which include beliefs, opinions, attitudes, or knowledge (O'Keefe 1990).
sent32: This relates most to Aristotle's cairos , and the need to create benevolence for the persuadee.
sent33: Evaluating the 'business principles', this theory resonates best with what DeMers (2016) defines as 'making [the cognition] appealing to the other party'.
sent34: However cognitions, and thus, a persuadee's perceived benevolence, can be dissonant, consonant, or irrelevant to each other.
sent35: If a persuadee is presented with a sufficiently vast cognitive inconsistency, then they will perceive psychological discomfort, leading to an attempt to restore their cognitive balance through a reduction or elimination of the inconsistency (Stiff 1994, Harmon-Jones 2002.
sent36: The magnitude of dissonance determines one's motivation to reduce it (Stiff 1994, Festinger 1957.
sent37: Approaches towards reducing dissonance are: changing terms to make them more consonant, adding additional consonant percipience, or altering the magnitude of the percipience (Harmon-Jones 2002, Stiff 1994, O'Keefe 1990.
sent38: In 'principles of persuasion', DeMers (2016) contends that appropriate flattering and the usage of so-called high value words contribute to persuasive acts in business contexts (cf. Aristotle's ethos ).
sent39: Accordingly, Language Expectancy Theory (LET) identifies written or spoken language as a rule-based system through which persuadees develop expectations and preferences towards ''appropriate'' linguistic usage of words in varying situations (Burgoon & Miller 1985).
sent40: Such expectations are frequently consistent with sociological and cultural norms, while preferences tend to relate to societal standards and cultural values (Burgoon & Miller 1985, Burgoon et al. 2002.
sent41: Positive expectations that facilitate a persuasive act are, for instance, if a persuader stylizes a behavior that is perceived as more preferred than expected by the persuadee.
sent42: In contrast, negative ones are inhibiting persuasion, e.g., when the persuader makes use of language that is considered to be socially unacceptable (Burgoon & Miller 1985, Burgoon et al. 2002.
sent43: Next, the 'principles of persuasion' confer that an argument based on logic is persuasive (DeMers 2016).
sent44: What Aristotle terms logos is consistent with the theory of probabilistic models.
sent45: Probabilistic models (McGuire 1981, Wyer 1970) are based on the rules of formal logic and probability, and predict beliefs regarding the conclusion of reasoning.
sent46: These predictions are based on mathematical probability, and as such this theory is consistent with what Aristotle defines as logos .
sent47: An exemplary belief syllogism (McGuire 1981) is composed of two premises that lead to a logical conclusion.
sent48: The theorists (McGuire 1981, Wyer 1970 explain that believing in the premises leads to the expectation that the identified conclusion will follow.
sent49: However, rather than solely thinking in all-or-nothing scenarios, beliefs can be ascertained through subjective probabilities: one's judgment of the probability that each of the premises is true (McGuire 1981, Wyer 1970.
sent50: Furthermore, if a message evokes a perceptual change in the truth of the premise, or additional premises are supplemented, following this theory, a change in perceiving the conclusion is expected.
sent51: Last, Balance Theory focuses on the triadic relationship involving two individuals (e.g., persuader and persuadee), the persuadee's attitude toward the persuader (Aristotle's pathos ), and their attitudes toward an attitudinal object (Heider 1958).
sent52: The resulting triad can be balanced or unbalanced: This triad is in balance if all three relationships are positive, or one is positive and two are negative.
sent53: If all three relationships are negative, or one is negative and two are positive, an unbalanced triad results, often motivating one to alter one of the three relationships (Heider 1958).
sent54: Building on this theory, we aim to identify relational determinants that relate to improving the relationship between the persuader and the persuadee towards the attitudinal object (Heider 1958).
sent55: In the business framework, we relate those determinants towards the pattern of 'trustworthiness', that represent the persuadee's attitude towards the persuader.
sent56: In his persuasion attempt, the persuader wants the persuadee to have a positive attitude.","1. What framework does the paper's methodology follow? sent1
    1.1. What are the five steps of the framework? sent2
    1.2. How will each step be addressed in the paper? sent3
2. What is the first step in the framework? sent4
    2.1. How is the review scope summarized? sent5
    2.2. What is the focus of this literature review? sent6
        2.2.1. What is the goal of the literature review? sent7
        2.2.2. Why were the categories chosen for the review? sent8
        2.2.3. Why was this field selected for the review? sent9
3. How is persuasion commonly studied? sent10
4. How is the paper organized? sent11
5. What perspective does the paper take? sent12
6. Who is the intended audience of the review? sent13
7. How is the coverage of the literature review categorized? sent14
8. What is the second step in the framework? sent15
    8.1. What does the conceptualization of the topic address? sent16
    8.2. How is persuasion conceptualized in the paper? sent17
    8.3. What related theories are introduced? sent18
9. How is persuasion defined in the context of the paper? sent19
10. What do contemporary psychology and communication science require for a persuasive act? sent20
11. How is a persuasive act defined in the context of NLG? sent21
12. What did Aristotle introduce in his work 'On Rhetoric'? sent22
    12.1. What facets does persuasion depend on according to Aristotle? sent23
13. How does contemporary business literature conceptualize persuasive acts? sent24
    13.1. What interventions help achieve persuasiveness according to the author? sent25
        13.1.1. What is the first intervention for achieving persuasiveness? sent26
        13.1.2. How does logical argumentation foster persuasiveness? sent27
        13.1.3. What other recommendations are made for making an appeal persuasive? sent28
        13.1.4. How does being patient and persistent contribute to persuasiveness? sent29
14. How are the ""principles of persuasion"" embedded into related theories? sent30
15. What does Festinger's Theory of Cognitive Dissonance focus on? sent31
    15.1. How does this theory relate to Aristotle's cairos? sent32
    15.2. How does this theory resonate with DeMers' definition of making cognition appealing? sent33
    15.3. What are the possible states of cognitions according to the theory? sent34
    15.4. What happens when a persuadee perceives cognitive inconsistency? sent35
    15.5. What determines one's motivation to reduce dissonance? sent36
    15.6. What are the approaches towards reducing dissonance? sent37
16. How does DeMers (2016) relate to Aristotle's ethos in persuasive acts? sent38
17. What does Language Expectancy Theory (LET) identify? sent39
    17.1. How are expectations and preferences related to sociological and cultural norms? sent40
    17.2. What are positive and negative expectations in persuasion? sent41, sent42
18. How is an argument based on logic considered persuasive? sent43
    18.1. How does Aristotle's logos relate to probabilistic models? sent44
    18.2. What are probabilistic models based on? sent45
    18.3. How are predictions made in probabilistic models? sent46
    18.4. What is an exemplary belief syllogism? sent47
    18.5. How do theorists explain belief in premises and conclusions? sent48
    18.6. How can beliefs be ascertained according to the theory? sent49
    18.7. What happens if a message changes the truth of a premise? sent50
19. What does Balance Theory focus on? sent51
    19.1. When is a triad balanced or unbalanced according to the theory? sent52, sent53
    19.2. What is the aim of building on Balance Theory in the paper? sent54
    19.3. How are relational determinants related to trustworthiness in a business framework? sent55
    19.4. What is the persuader's goal in a persuasion attempt? sent56"
212633633,"第 ** 卷 第 * 期 中文信息学报 Overview of the CCKS 2019 Knowledge Graph Evaluation Track: Entity, Rela- tion, Event and QA",https://www.semanticscholar.org/paper/81374b14e36bd993f20d92b9fca0a822c47e382a,3 典型模型与系统,4,"自从 BERT [12] 发布后， 它很快超过 ELMo [21] [22] 和 GloVe [23] 

正如前面提到的那样， 预测结果中是否有 NA 关系不会影响 % 值的计算，并且评测数据中各关 系的分类也是不平衡的。这就导致，如果一个关 系抽取系统对其中几个大类的预测有非常好的性 能时，在整体数据上的性能也同样会很好。在测 试集数据中， ""现夫"" 、 ""现妻"" 、 ""生父"" 、 ""生母"" 、 ""儿子"" 、 ""恋人"" 、 ""老师""这七种关系占据了大 多数。 在 Sent-Track 中， 2300 个标准答案中有 2000 个属于这七类；在 Bag-Track 中，740 个标准答案  ","sent1: 自从 BERT [12] 发布后， 它很快超过 ELMo [21] [22] 和 GloVe [23] 正如前面提到的那样， 预测结果中是否有 NA 关系不会影响 % 值的计算，并且评测数据中各关 系的分类也是不平衡的。这就导致，如果一个关 系抽取系统对其中几个大类的预测有非常好的性 能时，在整体数据上的性能也同样会很好。在测 试集数据中， ""现夫"" 、 ""现妻"" 、 ""生父"" 、 ""生母"" 、 ""儿子"" 、 ""恋人"" 、 ""老师""这七种关系占据了大 多数。 在 Sent-Track 中， 2300 个标准答案中有 2000 个属于这七类；在 Bag-Track 中，740 个标准答案","1. What impact did the release of BERT have on previous models like ELMo and GloVe? sent1
2. How does the presence of NA relationships affect the calculation of percentage values in prediction results? sent1
3. What is the effect of class imbalance in the evaluation data on the performance of relationship extraction systems? sent1
4. Which seven types of relationships dominate the test set data? sent1
5. How many standard answers belong to these seven types in the Sent-Track and Bag-Track datasets? sent1"
265607982,Overview of PragTag-2023: Low-Resource Multi-Domain Pragmatic Tagging of Peer Reviews,https://www.semanticscholar.org/paper/a6d2fd7c874c0074135ffc09a81f7c409e5b1ae1,Submissions,12,"Out of over 20 teams that signed up for the competition, five teams have made it to the final submission.The submitted systems explore a wide range of techniques and architectures for multi-domain pragmatic tagging in low-resource scenarios.We summarize the main ideas behind each submission below and refer to the system papers for details.

CATALPA_EduNLP (Ding et al., 2023) investigated a wide array of approaches.For the full-and low-data setting, this includes supervised sentence labeling via RoBERTa (Liu et al., 2019) augmented with additional features (domain, position, context, word normalization), as well as IOB-style sequence tagging using long-document Transformers and nearest-neighbor-based labeling using SBERT (Reimers and Gurevych, 2019).In the zero-shot setting, the team experimented with labeling test instances based on their similarity to class defini- tions from the shared task description, as well as with prompting via GPT3.5.The participants used the ARR-22 auxiliary data, addressing the gap in label distribution between ARR and the core data via subsampling, and explored data augmentation based on F1000raw auxiliary data.By ensembling best per-domain configurations selected on the validation set, they found that a BERT-based model with additional features outperforms sequence tagging and nearest-neighbor labeling on the full data, while a BERT-based model augmented with additional data performs best in the low-data setting.Prompting GPT3.5 in the zero-shot setting was shown vastly superior to SBERT-based classification based on task definitions -yet, following the PragTag rules, GPT3.5 result was not used for the leaderboard submission.

DeepBlueAI (Luo et al., 2023) focused their approach on increasing the robustness of pre-trained models in the sentence labeling setting.The experiments were conducted using three models -RoBERTa, DeBERTa (He et al., 2023) and XLM-RoBERTa (Conneau et al., 2020).The participants augmented the model via max pooling and attention pooling, introduced adversarial training via fast gradient method, and reported comparative performance of the models trained under different settings via cross-fold validation, showing that the modifications lead to variable performance gains.The authors report that the DeBERTa model consistently outperforms the other two models on the task.To tackle the secret test set in the final phase of the competition, the authors used a voting approach combining a range of models trained in different configurations and selecting the label with the maximum vote, stressing the benefits of fusing different types of models for prediction.

NUS-IDS (Gollapalli et al., 2023) explored multiple approaches to the task for each experimental condition.In the zero-shot no-data condition, the participants proposed two methods: a questionanswering model that selects passages from the peer review based on a set of questions derived from peer reviewing guidelines of NLP conferences, and a prompting-based approach based on the Flan-T5 (Chung et al., 2022) model.For the low-and full-data setting, the participants experimented with fine-tuning pre-trained language models, additionally exploring ensembling and data augmentation techniques by tentatively labeling the auxiliary shared task data.The results indicate that prompting via Flan-T5 outperforms questionanswering based approach in the no-data setting; in low-and full-data data, fine-tuning a T5 model (Raffel et al., 2019) on tentatively labeled auxiliary data followed by fine-tuning on the core task data performs best.

MILAB (Lee et al., 2023) approached the problem of data scarcity and domain shift via data augmentation.In particular, to compensate for the lack of data, the team applied an ensemble of RoBERTa-based classifiers to label auxiliary data from F1000raw and ARR-22.Apart from majority labeling, the authors explored a novel recall labeling technique: the models assign tentative labels to the unlabeled instances in the decreasing order of recall on a validation set, while labeling the residual instances as Other.Additionally, the authors experimented with diversifying the data by applying off-the-shelf synonym generation followed by BERTScore filtering (Zhang et al., 2020).The results indicate that the proposed data augmentation techniques combined with ensembling improve the model performance on the task, especially in the no-data condition.

SuryaKiran (Suri et al., 2023) explored the use of unsupervised pre-training on F1000raw auxiliary data to increase domain robustness of the pragmatic tag classifier.In particular, the participants pre-trained the DeBERTa model on F1000raw using masked language modeling objective (Devlin et al., 2019), and later used an ensemble of five models further fine-tuned on different training data splits to make the test set prediction.Their results demonstrate that pre-training via masked language modeling leads to improved performance only in some cases; the authors attribute this to the vocabulary discrepancies between the domains.The team submitted their system only to the final evaluation.","sent1: Out of over 20 teams that signed up for the competition, five teams have made it to the final submission.
sent2: The submitted systems explore a wide range of techniques and architectures for multi-domain pragmatic tagging in low-resource scenarios.
sent3: We summarize the main ideas behind each submission below and refer to the system papers for details.
sent4: CATALPA_EduNLP (Ding et al., 2023) investigated a wide array of approaches.
sent5: For the full-and low-data setting, this includes supervised sentence labeling via RoBERTa (Liu et al., 2019) augmented with additional features (domain, position, context, word normalization), as well as IOB-style sequence tagging using long-document Transformers and nearest-neighbor-based labeling using SBERT (Reimers and Gurevych, 2019).In the zero-shot setting, the team experimented with labeling test instances based on their similarity to class defini- tions from the shared task description, as well as with prompting via GPT3.5.The participants used the ARR-22 auxiliary data, addressing the gap in label distribution between ARR and the core data via subsampling, and explored data augmentation based on F1000raw auxiliary data.
sent6: By ensembling best per-domain configurations selected on the validation set, they found that a BERT-based model with additional features outperforms sequence tagging and nearest-neighbor labeling on the full data, while a BERT-based model augmented with additional data performs best in the low-data setting.
sent7: Prompting GPT3.5 in the zero-shot setting was shown vastly superior to SBERT-based classification based on task definitions -yet, following the PragTag rules, GPT3.5 result was not used for the leaderboard submission.
sent8: DeepBlueAI (Luo et al., 2023) focused their approach on increasing the robustness of pre-trained models in the sentence labeling setting.
sent9: The experiments were conducted using three models -RoBERTa, DeBERTa (He et al., 2023) and XLM-RoBERTa (Conneau et al., 2020).The participants augmented the model via max pooling and attention pooling, introduced adversarial training via fast gradient method, and reported comparative performance of the models trained under different settings via cross-fold validation, showing that the modifications lead to variable performance gains.
sent10: The authors report that the DeBERTa model consistently outperforms the other two models on the task.
sent11: To tackle the secret test set in the final phase of the competition, the authors used a voting approach combining a range of models trained in different configurations and selecting the label with the maximum vote, stressing the benefits of fusing different types of models for prediction.
sent12: NUS-IDS (Gollapalli et al., 2023) explored multiple approaches to the task for each experimental condition.
sent13: In the zero-shot no-data condition, the participants proposed two methods: a questionanswering model that selects passages from the peer review based on a set of questions derived from peer reviewing guidelines of NLP conferences, and a prompting-based approach based on the Flan-T5 (Chung et al., 2022) model.
sent14: For the low-and full-data setting, the participants experimented with fine-tuning pre-trained language models, additionally exploring ensembling and data augmentation techniques by tentatively labeling the auxiliary shared task data.
sent15: The results indicate that prompting via Flan-T5 outperforms questionanswering based approach in the no-data setting; in low-and full-data data, fine-tuning a T5 model (Raffel et al., 2019) on tentatively labeled auxiliary data followed by fine-tuning on the core task data performs best.
sent16: MILAB (Lee et al., 2023) approached the problem of data scarcity and domain shift via data augmentation.
sent17: In particular, to compensate for the lack of data, the team applied an ensemble of RoBERTa-based classifiers to label auxiliary data from F1000raw and ARR-22.Apart from majority labeling, the authors explored a novel recall labeling technique: the models assign tentative labels to the unlabeled instances in the decreasing order of recall on a validation set, while labeling the residual instances as Other.
sent18: Additionally, the authors experimented with diversifying the data by applying off-the-shelf synonym generation followed by BERTScore filtering (Zhang et al., 2020).The results indicate that the proposed data augmentation techniques combined with ensembling improve the model performance on the task, especially in the no-data condition.
sent19: SuryaKiran (Suri et al., 2023) explored the use of unsupervised pre-training on F1000raw auxiliary data to increase domain robustness of the pragmatic tag classifier.
sent20: In particular, the participants pre-trained the DeBERTa model on F1000raw using masked language modeling objective (Devlin et al., 2019), and later used an ensemble of five models further fine-tuned on different training data splits to make the test set prediction.
sent21: Their results demonstrate that pre-training via masked language modeling leads to improved performance only in some cases; the authors attribute this to the vocabulary discrepancies between the domains.
sent22: The team submitted their system only to the final evaluation.","1. How many teams made it to the final submission in the PragTag-2023 competition? sent1
2. What techniques and architectures do the submitted systems explore? sent2
3. What is summarized in the section regarding the submissions? sent3
4. What approaches did CATALPA_EduNLP investigate? sent4
    4.1. What techniques did CATALPA_EduNLP use in the full-and low-data setting? sent5
    4.2. How did CATALPA_EduNLP perform in the zero-shot setting? sent7
5. What was DeepBlueAI's focus in their approach? sent8
    5.1. Which models did DeepBlueAI use in their experiments? sent9
    5.2. Which model consistently outperformed the others according to DeepBlueAI? sent10
    5.3. How did DeepBlueAI tackle the secret test set in the final phase? sent11
6. What approaches did NUS-IDS explore for the task? sent12
    6.1. What methods did NUS-IDS propose in the zero-shot no-data condition? sent13
    6.2. What techniques did NUS-IDS experiment with in the low-and full-data setting? sent14
7. How did MILAB approach the problem of data scarcity and domain shift? sent16
    7.1. What data augmentation techniques did MILAB apply? sent17
    7.2. What additional experiment did MILAB conduct to diversify the data? sent18
8. What did SuryaKiran explore to increase domain robustness? sent19
    8.1. How did SuryaKiran pre-train the DeBERTa model? sent20
    8.2. What were the results of SuryaKiran's pre-training approach? sent21
9. When did SuryaKiran submit their system for evaluation? sent22"
257913422,Self-Supervised Multimodal Learning: A Survey,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,Remote Sensing,7,"In remote sensing, different sensors can provide complementary information for earth observations, including hyperspectral data, multispectral data, light detection and ranging (LiDAR), synthetic aperture radar (SAR) data, etc. [165], [166]. For example, hyperspectral images provide information on land-cover categories based on spectral signatures, while SAR images provide dielectric properties and are not affected by clouds. To integrate SAR and optical images, [167] use a multimodal contrastive loss at the image and super-pixel level. To fuse multispectral and SAR images, SSCL [168] presents a unified framework of contrastive learning and masked prediction applicable to images with any number of channels. Change detection is also an important problem in remote sensing, which is used for damage assessment, environmental monitoring, etc. [169] uses a pseudo-Siamese network to regress the output between its two branches for cross-sensor image pairs. The feature distance between the outputs of the two branches is used to define a change measure. Additionally, [170] combines clustering and contrastive learning for change detection in a bi-temporal scene where paired images are captured by the optical sensor and SAR sensor, respectively. Geo-tagged audio recordings can also be used with contrastive learning to learn the correspondence with image data [171].","sent1: In remote sensing, different sensors can provide complementary information for earth observations, including hyperspectral data, multispectral data, light detection and ranging (LiDAR), synthetic aperture radar (SAR) data, etc. [165], [166].
sent2: For example, hyperspectral images provide information on land-cover categories based on spectral signatures, while SAR images provide dielectric properties and are not affected by clouds.
sent3: To integrate SAR and optical images, [167] use a multimodal contrastive loss at the image and super-pixel level.
sent4: To fuse multispectral and SAR images, SSCL [168] presents a unified framework of contrastive learning and masked prediction applicable to images with any number of channels.
sent5: Change detection is also an important problem in remote sensing, which is used for damage assessment, environmental monitoring, etc. [169] uses a pseudo-Siamese network to regress the output between its two branches for cross-sensor image pairs.
sent6: The feature distance between the outputs of the two branches is used to define a change measure.
sent7: Additionally, [170] combines clustering and contrastive learning for change detection in a bi-temporal scene where paired images are captured by the optical sensor and SAR sensor, respectively.
sent8: Geo-tagged audio recordings can also be used with contrastive learning to learn the correspondence with image data [171].","1. What types of complementary information can different sensors provide in remote sensing? sent1
    1.1. Can you provide examples of the information provided by hyperspectral and SAR images? sent2
2. How is SAR and optical image integration achieved in remote sensing? sent3
3. What framework is used to fuse multispectral and SAR images? sent4
4. What is change detection used for in remote sensing, and how is it achieved? sent5
    4.1. How is the feature distance used in change detection? sent6
    4.2. How are clustering and contrastive learning combined for change detection? sent7
5. How can geo-tagged audio recordings be used in remote sensing? sent8"
227061,Detection of User Demographics on Social Media: A Review of Methods and Recommendations for Best Practices,https://www.semanticscholar.org/paper/43b55cc08eba13fb309e77a1033da0ad60bd7a5e,Data Extraction,14,"We queried Google Scholar during October of 2016 using combinations of methodological terms: ""prediction,"" ""classification,"" and ""machine learning""; terms related to user traits: ""age,"" ""race,"" ""ethnicity,"" ""gender"", and platform-related terms: ""social media"" and ""Twitter"" and ""blog."" On average, each search returned 121,120 articles. Within the first 20 pages of each search (ordered by relevance), an average of approximately 16 articles appeared relevant based on title and summary text. We then read through the abstracts of the potentially relevant articles and identified 60 studies that proposed methods for inferring or predicting users' demographic characteristics. All identified studies were published between 2006 to 2016. For each study, we extracted the year of publication, a brief description of the methods used, if and how the researchers obtained ground truth data, which metadata fields were used for classification, the precise features extracted from these metadata fields, the intended audience of the paper (i.e. computer science, social science, etc.) and the demographic characteristic detected (i.e. age, race/ethnicity or gender) (See Appendix Table 1). The 60 selected studies focused on different social media platforms; 39 (65%) studies focused on  Twitter, two on Facebook, two on Livejournal, one on Yelp, one on YouTube, and one on Pinterest. The remaining studies focused on other social media sites (e.g., Netlog, Fotolog) and blogs. Additionally, 44 (73%) studies explored supervised or semi-supervised machine learning methods, one (2%) used unsupervised learning, three (5%) used facial evaluation (human or automated), and 11 (18%) used raw or adjusted data matching. The machine learning methods considered include support vector machines [35]- [45], naïve Bayes [43], [46], modified balanced winnow [47], neural networks [48], maximum entropy [49], naïve Bayes decision tree hybrid [50], [51], gradient boosted decision trees [52], [53], as well as regularized linear, logistic or log-linear regression [43], [54]- [59]. Additional details on these methods can be found in the cited articles and in [60].","sent1: We queried Google Scholar during October of 2016 using combinations of methodological terms: ""prediction,"" ""classification,"" and ""machine learning""; terms related to user traits: ""age,"" ""race,"" ""ethnicity,"" ""gender"", and platform-related terms: ""social media"" and ""Twitter"" and ""blog.""
sent2: On average, each search returned 121,120 articles.
sent3: Within the first 20 pages of each search (ordered by relevance), an average of approximately 16 articles appeared relevant based on title and summary text.
sent4: We then read through the abstracts of the potentially relevant articles and identified 60 studies that proposed methods for inferring or predicting users' demographic characteristics.
sent5: All identified studies were published between 2006 to 2016.
sent6: For each study, we extracted the year of publication, a brief description of the methods used, if and how the researchers obtained ground truth data, which metadata fields were used for classification, the precise features extracted from these metadata fields, the intended audience of the paper (i.e. computer science, social science, etc.) and the demographic characteristic detected (i.e. age, race/ethnicity or gender) (See Appendix Table 1).
sent7: The 60 selected studies focused on different social media platforms; 39 (65%) studies focused on  Twitter, two on Facebook, two on Livejournal, one on Yelp, one on YouTube, and one on Pinterest.
sent8: The remaining studies focused on other social media sites (e.g., Netlog, Fotolog) and blogs.
sent9: Additionally, 44 (73%) studies explored supervised or semi-supervised machine learning methods, one (2%) used unsupervised learning, three (5%) used facial evaluation (human or automated), and 11 (18%) used raw or adjusted data matching.
sent10: The machine learning methods considered include support vector machines [35]- [45], naïve Bayes [43], [46], modified balanced winnow [47], neural networks [48], maximum entropy [49], naïve Bayes decision tree hybrid [50], [51], gradient boosted decision trees [52], [53], as well as regularized linear, logistic or log-linear regression [43], [54]- [59].
sent11: Additional details on these methods can be found in the cited articles and in [60].","1. What was the method used for querying articles in the study? sent1
    1.1. How many articles were returned on average from each search? sent2
    1.2. How many articles appeared relevant based on title and summary text? sent3
    1.3. How many studies were identified as proposing methods for inferring or predicting users' demographic characteristics? sent4
    1.4. What is the publication range of the identified studies? sent5
    1.5. What information was extracted from each study? sent6
2. What social media platforms were the focus of the selected studies? sent7
    2.1. What other social media sites and platforms were included in the remaining studies? sent8
3. What methods were explored in the studies for detecting user demographics? sent9
    3.1. What machine learning methods were considered in the studies? sent10
    3.2. Where can additional details on these methods be found? sent11"
149117098,Article Respondent Robotics: Simulating Responses to Likert-Scale Survey Items,https://www.semanticscholar.org/paper/30d7c07ec6575bf8b1e7b132dbd2d5ae10c087ee,Simulating Responses,8,"Based on the consideration above, it is possible to hypothesize that a given respondent's responses are not free to vary. Once the respondent has chosen a response to the initial items, the subsequent responses should be determined by the semantic relationships of the items (Arnulf et al., 2014;Nimon et al., 2016) and the structure of the survey, most notably the response categories (Maul, 2017;Slaney, 2017) and the unfolding patterns following from expected negative correlations (Michell, 1994;Roysamb & Strype, 2002;van Schuur & Kiers, 1994).

Ideally, it should be possible to predict any given response based on the knowledge of the semantic matrix and a minimum of initial responses. In our simulations, we can see that any response in the MLQ is predictable by using other known responses and knowledge about the distances between items. The R 2 s of these predictions are in the range of .86 to .94.  Step 2) Were Regressed on the Average Score Differences (N = 990).

Step 1

Step As the semantic MI values correlate at -.79 and predict the distances significantly (R 2 = .63), it should theoretically be possible to substitute the distances with the semantic values, and thus predict later responses with a minimum of initial responses.

The perfect formula is yet to be found, but we have created a preliminary algorithm that can possibly mimic real responses to the MLQ. The present approach is explicitly aiming at reproducing existing responses as this gives us the best opportunity to compare simulated with real responses.

The rationale for the algorithm combines semantics and unfolding theory as follows: x -0.79. However, the distances were computed as absolute measures; that is, the absolute distance from 3 to 5 = 2, but so is 5 to 3. In practice, though, the algorithm may need to predict a high number from a low number or vice versa. The constant will therefore not ""anchor"" the distance at the right point in the scale. 4. We therefore need to tie the estimated point to the value of Item A. We have tested several approaches to this, and the formula that seems to work best for calculating any response B is to simply replace the constant with the value for Item A, thus Value(Item B) = Value(Item A) + (MI for Item A and Item B) x − 0.79. 5. This formula does impose the structure of semantic values on the subsequent numbers. It also seems counterintuitive because if MI increases (indicating higher similarity), the term will grow in absolute numbers. However, the beta is negative, and the resulting number will be smaller. The impact on the ensuing calculations now comes from the unfolding operations, depending on whether Response B is higher or lower than A. To comply with predictions from unfolding theory, the formula above keeps its positive form if the respondent's first three responses indicate a positive evaluation (biasing the item distances in a positive direction) but should be negative if the unfolding pattern appears to be negative. This information is picked up by comparing the responses of Items 1, 2, and 3. While Items 1 and 2 are descriptions of positive leadership, Item 3 contains a negative appreciation. 6. In the case that the Items A and B are assumed to be negatively related (this was discussed in the explanation of MI values above), the same relationship between MI and distances hold. However, the estimated value should logically be at the other end of the Likert-type scale (in a perfect negative correlation, a score of 5 on A indicates that the score for B is 1). So in the case of expected negative correlations, the direction of the algorithm formula is reversed within the 5-point Likert-type scale, such that In this way, our algorithm is based on the complete pattern of semantic distances for every item with all other items, as well as a hypothesis on the direction of scale unfolding based on the initial three responses. It is admittedly explorative and based on an incomplete understanding of the issues involved, and our intention is to invite criticism and improvements from others. One questionable feature of this algorithm is the tendency for positive evaluations to escalate positively and vice versa, probably due to a deficiency of the formula in

Step 4. In the course of all 990 iterations however, these tendencies seem to balance each other out, and fix the averaged responses as dictated by the mutual pattern of semantic distances. We have also checked that this formula performs better than simply using averages of the known values instead of semantics, thus substantiating the use of semantics in the formula. A further contrasting procedure will be described below. The MLQ has 45 items. Of these, 36 measure different types of leadership behaviors, and the nine last items measure how well the rated person's work group does, commonly treated as ""outcome"" variables. The Arnulf et al. (2014) study found the ""outcome"" variables to be determined by the responses to the preceding items. We will therefore start by trying to predict the individual cases of these by deleting them from real response sets. By deleting progressive numbers of items, we will then explore how well the semantics will perform to predict the missing responses.

Therefore, our first simulated step will be concerned with predicting outcomes training the algorithm on the first 36 items. In the next steps, we simply subtract remaining half of the survey until all real responses are deleted, offering the algorithm diminishing amounts of training information. In this way, we can evaluate the degree to which the computed values still bear resemblance to the original values.

Contrast validation procedure. Algorithms like this may create artificial structures that are not due to the semantic MI values but simply artifacts created by the algorithm procedures themselves. To control for this, we have created similar sets of responses with the same numbers of missing values, where the MI values in the algorithm are replaced by randomly generated values in the same range as the MI values (from −1 to +1). If similarities between artificial and real responses are created by biases in the algorithmic procedure and not by semantics, the output of randomly generated numbers should also be able to reproduce numbers resembling the original scores. The difference between the output of random and semantically created numbers expresses the value of (present-day) semantics in predicting real responses.","sent1: Based on the consideration above, it is possible to hypothesize that a given respondent's responses are not free to vary.
sent2: Once the respondent has chosen a response to the initial items, the subsequent responses should be determined by the semantic relationships of the items (Arnulf et al., 2014;Nimon et al., 2016) and the structure of the survey, most notably the response categories (Maul, 2017;Slaney, 2017) and the unfolding patterns following from expected negative correlations (Michell, 1994;Roysamb & Strype, 2002;van Schuur & Kiers, 1994).
sent3: Ideally, it should be possible to predict any given response based on the knowledge of the semantic matrix and a minimum of initial responses.
sent4: In our simulations, we can see that any response in the MLQ is predictable by using other known responses and knowledge about the distances between items.
sent5: The R 2 s of these predictions are in the range of .86 to .94.
sent6: Step 2) Were Regressed on the Average Score Differences (N = 990).
sent7: Step 1Step As the semantic MI values correlate at -.79 and predict the distances significantly (R 2 = .63), it should theoretically be possible to substitute the distances with the semantic values, and thus predict later responses with a minimum of initial responses.
sent8: The perfect formula is yet to be found, but we have created a preliminary algorithm that can possibly mimic real responses to the MLQ.
sent9: The present approach is explicitly aiming at reproducing existing responses as this gives us the best opportunity to compare simulated with real responses.
sent10: The rationale for the algorithm combines semantics and unfolding theory as follows: x -0.79.
sent11: However, the distances were computed as absolute measures; that is, the absolute distance from 3 to 5 = 2, but so is 5 to 3.
sent12: In practice, though, the algorithm may need to predict a high number from a low number or vice versa.
sent13: The constant will therefore not ""anchor"" the distance at the right point in the scale.
sent14: 4. We therefore need to tie the estimated point to the value of Item A. We have tested several approaches to this, and the formula that seems to work best for calculating any response B is to simply replace the constant with the value for Item A, thus Value(Item B) = Value(Item A) + (MI for Item A and Item B) x − 0.79. 5.
sent15: This formula does impose the structure of semantic values on the subsequent numbers.
sent16: It also seems counterintuitive because if MI increases (indicating higher similarity), the term will grow in absolute numbers.
sent17: However, the beta is negative, and the resulting number will be smaller.
sent18: The impact on the ensuing calculations now comes from the unfolding operations, depending on whether Response B is higher or lower than A. To comply with predictions from unfolding theory, the formula above keeps its positive form if the respondent's first three responses indicate a positive evaluation (biasing the item distances in a positive direction) but should be negative if the unfolding pattern appears to be negative.
sent19: This information is picked up by comparing the responses of Items 1, 2, and 3.
sent20: While Items 1 and 2 are descriptions of positive leadership, Item 3 contains a negative appreciation.
sent21: 6. In the case that the Items A and B are assumed to be negatively related (this was discussed in the explanation of MI values above), the same relationship between MI and distances hold.
sent22: However, the estimated value should logically be at the other end of the Likert-type scale (in a perfect negative correlation, a score of 5 on A indicates that the score for B is 1).
sent23: So in the case of expected negative correlations, the direction of the algorithm formula is reversed within the 5-point Likert-type scale, such that In this way, our algorithm is based on the complete pattern of semantic distances for every item with all other items, as well as a hypothesis on the direction of scale unfolding based on the initial three responses.
sent24: It is admittedly explorative and based on an incomplete understanding of the issues involved, and our intention is to invite criticism and improvements from others.
sent25: One questionable feature of this algorithm is the tendency for positive evaluations to escalate positively and vice versa, probably due to a deficiency of the formula inStep 4.
sent26: In the course of all 990 iterations however, these tendencies seem to balance each other out, and fix the averaged responses as dictated by the mutual pattern of semantic distances.
sent27: We have also checked that this formula performs better than simply using averages of the known values instead of semantics, thus substantiating the use of semantics in the formula.
sent28: A further contrasting procedure will be described below.
sent29: The MLQ has 45 items. Of these, 36 measure different types of leadership behaviors, and the nine last items measure how well the rated person's work group does, commonly treated as ""outcome"" variables.
sent30: The Arnulf et al. (2014) study found the ""outcome"" variables to be determined by the responses to the preceding items.
sent31: We will therefore start by trying to predict the individual cases of these by deleting them from real response sets.
sent32: By deleting progressive numbers of items, we will then explore how well the semantics will perform to predict the missing responses.
sent33: Therefore, our first simulated step will be concerned with predicting outcomes training the algorithm on the first 36 items.
sent34: In the next steps, we simply subtract remaining half of the survey until all real responses are deleted, offering the algorithm diminishing amounts of training information.
sent35: In this way, we can evaluate the degree to which the computed values still bear resemblance to the original values.
sent36: Contrast validation procedure. Algorithms like this may create artificial structures that are not due to the semantic MI values but simply artifacts created by the algorithm procedures themselves.
sent37: To control for this, we have created similar sets of responses with the same numbers of missing values, where the MI values in the algorithm are replaced by randomly generated values in the same range as the MI values (from −1 to +1).
sent38: If similarities between artificial and real responses are created by biases in the algorithmic procedure and not by semantics, the output of randomly generated numbers should also be able to reproduce numbers resembling the original scores.
sent39: The difference between the output of random and semantically created numbers expresses the value of (present-day) semantics in predicting real responses.","1. What hypothesis can be made about a respondent's responses based on the initial items? sent1
2. What factors determine the subsequent responses of a respondent? sent2
3. How can any given response be predicted according to the study? sent3
4. What do the simulations show about predictability in the MLQ? sent4
5. What is the range of R² values for these predictions? sent5
6. What is the relationship between semantic MI values and distances, and how does it affect predictions? sent7
7. What is the current status of the algorithm for mimicking real responses to the MLQ? sent8
8. What is the rationale behind the algorithm's approach? sent9
9. How are distances computed in the algorithm? sent11
10. What challenge does the algorithm face in predicting values? sent12
11. How is the estimated point tied to the value of Item A in the algorithm? sent14
12. How does the formula impose the structure of semantic values on subsequent numbers? sent15
13. What is the impact of MI increases on the resulting number? sent16
14. How does the unfolding theory affect the calculations in the algorithm? sent18
15. How are Items 1, 2, and 3 used to determine the unfolding pattern? sent19
16. What happens when Items A and B are assumed to be negatively related? sent21
17. How is the algorithm adjusted for expected negative correlations? sent23
18. What is the intention behind the explorative nature of the algorithm? sent24
19. What is a questionable feature of the algorithm, and how is it addressed? sent25, sent26
20. How does the formula perform compared to using averages of known values? sent27
21. What does the MLQ measure, and how are the items categorized? sent29
22. What did the Arnulf et al. (2014) study find about ""outcome"" variables? sent30
23. How will the study attempt to predict individual cases of ""outcome"" variables? sent31
24. What is the procedure for exploring the performance of semantics in predicting missing responses? sent32
25. What is the first simulated step in predicting outcomes? sent33
26. How is the algorithm tested with diminishing amounts of training information? sent34
27. What is the purpose of the contrast validation procedure? sent36
28. How is the control for artificial structures in the algorithm achieved? sent37
29. What would indicate that similarities are created by biases in the algorithmic procedure? sent38
30. What does the difference between random and semantically created numbers express? sent39"
232116743,Use of Social Media Data in Disaster Management: A Survey,https://www.semanticscholar.org/paper/7595367176192a375585e18593d1a2bd3d5faccc,Social Media Platforms,6,"Generally, social media data are directly accessible on social media platforms (e.g., Facebook, Twitter, and Instagram). These platforms are considered as major data sources for social media data analytics in disaster management. Most social media platforms provide HTTP-based Application Programming Interfaces (APIs) for data consumers to access their social media services (e.g., data service and analytics services). Data consumers can use their tools to communicate with the respective APIs to collect and store social media data for their purposes [28]. For example, Twitter provides search APIs that enable consumers to find historical or real-time data by using keywords or hashtags. Much research on using social media data for disaster management utilises such APIs to access social media data directly from the social media platforms [22][23][24][25]. Due to the unstructured characteristics of social media data and the indeterminacy of the sources, the quality and trustworthiness of the collected social media data become significant issues [20]. Based on this, additional processes (e.g., data filtering, data classification, and data extraction) for data preparation are required. Due to privacy concerns, some of the social media platforms (e.g., Facebook and Twitter) have put several restrictions on data access.","sent1: Generally, social media data are directly accessible on social media platforms (e.g., Facebook, Twitter, and Instagram).
sent2: These platforms are considered as major data sources for social media data analytics in disaster management.
sent3: Most social media platforms provide HTTP-based Application Programming Interfaces (APIs) for data consumers to access their social media services (e.g., data service and analytics services).
sent4: Data consumers can use their tools to communicate with the respective APIs to collect and store social media data for their purposes [28].
sent5: For example, Twitter provides search APIs that enable consumers to find historical or real-time data by using keywords or hashtags.
sent6: Much research on using social media data for disaster management utilises such APIs to access social media data directly from the social media platforms [22][23][24][25].
sent7: Due to the unstructured characteristics of social media data and the indeterminacy of the sources, the quality and trustworthiness of the collected social media data become significant issues [20].
sent8: Based on this, additional processes (e.g., data filtering, data classification, and data extraction) for data preparation are required.
sent9: Due to privacy concerns, some of the social media platforms (e.g., Facebook and Twitter) have put several restrictions on data access.","1. How is social media data generally accessed on social media platforms? sent1
2. Why are social media platforms considered major data sources in disaster management? sent2
3. How do most social media platforms provide access to their services? sent3
4. How can data consumers collect and store social media data? sent4
5. What example is given of a social media platform providing APIs for data access? sent5
6. How is research on social media data for disaster management typically conducted? sent6
7. What are significant issues related to the quality and trustworthiness of social media data? sent7
8. What additional processes are required for data preparation due to the nature of social media data? sent8
9. What restrictions have some social media platforms implemented due to privacy concerns? sent9"
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,Evaluation of 24 recently developed NLIs,4,"In this section, we provide a systematic analysis of the major NLIs. We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity. Each system category, based on its technical approach, has its own strengths and weaknesses. There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.

We evaluate the systems based on what is reported in the papers. If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3. If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L). If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with . If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3. In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1). This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them. Therefore, they do not expect any complexer questions like Q4 or Q7. Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7). However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10). For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery. Trigger words are not sufficient to identify the range of each subquery.

Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR). This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified. Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.

Grammar-based systems offer the possibility to guide the users during the formulation of their questions. Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language. The huge disadvantage of grammar-based systems is that they need handcrafted rules. There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL). The general rules can be used for other domains and therefore increase the adaptability of the system. Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).

We will now analyze how well the different systems can handle the ten sample questions. The first question is a basic filter question and can be solved by all NLIs as shown in Table 3. The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others). Complex questions (e.g., aggregations) cannot be phrased with keywords only. Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences. In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords. Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible. They adapted their systems so that they can handle different forms of user input. Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution. This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors. Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms. This approach can be used to answer questions formulated as keywords or as complete sentences. Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR). The identification of possible subqueries is necessary to answer questions like Q9 and Q10.

Based on the sample questions, SQUALL, SPARK-LIS, NaLIR and ATHENA are the systems that per- 

In this section, we provide a systematic analysis of the major NLIs. We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity. Each system category, based on its technical approach, has its own strengths and weaknesses. There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.

We evaluate the systems based on what is reported in the papers. If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3. If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L). If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with . If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3. In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1). This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them. Therefore, they do not expect any complexer questions like Q4 or Q7. Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7). However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10). For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery. Trigger words are not sufficient to identify the range of each subquery.

Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR). This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified. Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.

Grammar-based systems offer the possibility to guide the users during the formulation of their questions. Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language. The huge disadvantage of grammar-based systems is that they need handcrafted rules. There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL). The general rules can be used for other domains and therefore increase the adaptability of the system. Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).

We will now analyze how well the different systems can handle the ten sample questions. The first question is a basic filter question and can be solved by all NLIs as shown in Table 3. The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others). Complex questions (e.g., aggregations) cannot be phrased with keywords only. Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences. In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords. Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible. They adapted their systems so that they can handle different forms of user input. Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution. This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors. Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms. This approach can be used to answer questions formulated as keywords or as complete sentences. Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR). The identification of possible subqueries is necessary to answer questions like Q9 and Q10.

Based on the sample questions, SQUALL, SPARK-LIS, NaLIR and ATHENA are the systems that per- ","sent1: In this section, we provide a systematic analysis of the major NLIs.
sent2: We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity.
sent3: Each system category, based on its technical approach, has its own strengths and weaknesses.
sent4: There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.
sent5: We evaluate the systems based on what is reported in the papers.
sent6: If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3.
sent7: If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L).
sent8: If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with .
sent9: If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3.
sent10: In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1).
sent11: This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them.
sent12: Therefore, they do not expect any complexer questions like Q4 or Q7.
sent13: Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7).
sent14: However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10).
sent15: For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery.
sent16: Trigger words are not sufficient to identify the range of each subquery.
sent17: Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR).
sent18: This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified.
sent19: Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.
sent20: Grammar-based systems offer the possibility to guide the users during the formulation of their questions.
sent21: Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language.
sent22: The huge disadvantage of grammar-based systems is that they need handcrafted rules.
sent23: There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL).
sent24: The general rules can be used for other domains and therefore increase the adaptability of the system.
sent25: Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).
sent26: We will now analyze how well the different systems can handle the ten sample questions.
sent27: The first question is a basic filter question and can be solved by all NLIs as shown in Table 3.
sent28: The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others).
sent29: Complex questions (e.g., aggregations) cannot be phrased with keywords only.
sent30: Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences.
sent31: In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords.
sent32: Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible.
sent33: They adapted their systems so that they can handle different forms of user input.
sent34: Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution.
sent35: This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors.
sent36: Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms.
sent37: This approach can be used to answer questions formulated as keywords or as complete sentences.
sent38: Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR).
sent39: The identification of possible subqueries is necessary to answer questions like Q9 and Q10.
sent40: Based on the sample questions, SQUALL, SPARK-LIS, NaLIR and ATHENA are the systems that per-
sent41: In this section, we provide a systematic analysis of the major NLIs.
sent42: We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity.
sent43: Each system category, based on its technical approach, has its own strengths and weaknesses.
sent44: There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.
sent45: We evaluate the systems based on what is reported in the papers.
sent46: If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3.
sent47: If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L).
sent48: If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with .
sent49: If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3.
sent50: In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1).
sent51: This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them.
sent52: Therefore, they do not expect any complexer questions like Q4 or Q7.
sent53: Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7).
sent54: However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10).
sent55: For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery.
sent56: Trigger words are not sufficient to identify the range of each subquery.
sent57: Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR).
sent58: This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified.
sent59: Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.
sent60: Grammar-based systems offer the possibility to guide the users during the formulation of their questions.
sent61: Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language.
sent62: The huge disadvantage of grammar-based systems is that they need handcrafted rules.
sent63: There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL).
sent64: The general rules can be used for other domains and therefore increase the adaptability of the system.
sent65: Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).
sent66: We will now analyze how well the different systems can handle the ten sample questions.
sent67: The first question is a basic filter question and can be solved by all NLIs as shown in Table 3.
sent68: The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others).
sent69: Complex questions (e.g., aggregations) cannot be phrased with keywords only.
sent70: Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences.
sent71: In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords.
sent72: Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible.
sent73: They adapted their systems so that they can handle different forms of user input.
sent74: Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution.
sent75: This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors.
sent76: Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms.
sent77: This approach can be used to answer questions formulated as keywords or as complete sentences.
sent78: Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR).
sent79: The identification of possible subqueries is necessary to answer questions like Q9 and Q10.
sent80: Based on the sample questions, SQUALL, SPARK-LIS, NaLIR and ATHENA are the systems that per-","1. What is the focus of this section? sent1
2. How were the 24 recently developed NLIs categorized and analyzed? sent2
3. What are the strengths and weaknesses of each system category based on? sent3
4. What aspects are not considered in this paper when evaluating the systems? sent4
5. How are the systems evaluated in this study? sent5
6. How are questions labeled in Table 3 if there is an example or a clear statement in the paper? sent6
7. How are questions labeled if they need to be asked in a strict syntactical way or if the answer is partially correct? sent7
8. How are questions labeled if something is not implemented? sent8
9. How are questions labeled if it is unclear whether a system can answer them? sent9
10. What is the general capability of keyword-based NLIs according to Table 3? sent10
11. What is the limitation of keyword-based systems? sent11
12. What type of questions do keyword-based systems not expect? sent12
13. How do pattern-based NLIs extend keyword-based systems? sent13
14. What type of questions can pattern-based NLIs not answer? sent14
15. What is the difficulty with questions including subqueries? sent15
16. Why are trigger words insufficient for identifying subqueries? sent16
17. How do parsing-based NLIs handle questions with subqueries? sent17
18. How do parsing-based NLIs identify and group input questions? sent18
19. What problem do some parsing-based systems share with pattern-based NLIs? sent19
20. What do grammar-based systems offer to users? sent20
21. How do grammar-based systems ensure input questions are translatable into formal language? sent21
22. What is a major disadvantage of grammar-based systems? sent22
23. How do some NLIs use general and domain-specific rules? sent23
24. How do general rules benefit the adaptability of the system? sent24
25. How do some systems reduce the number of handcrafted rules? sent25
26. What will be analyzed next in this section? sent26
27. What type of question can be solved by all NLIs? sent27
28. Which questions are the most difficult and which systems can answer them? sent28
29. Why can't complex questions be phrased with keywords only? sent29
30. How do users phrase more complicated questions? sent30
31. How can simple questions be asked? sent31
32. What phenomenon do Waltinger et al. (2013) and Lawrence and Riezler (2016) describe? sent32
33. How did Waltinger et al. (2013) and Lawrence and Riezler (2016) adapt their systems? sent33
34. What caution does Waltinger et al. (2013) suggest regarding parse trees? sent34
35. What is the approach of Zheng et al. (2017) regarding NLP technologies? sent35
36. What approach does Walter et al. (2012) propose for processing questions? sent36
37. How can Walter et al.'s (2012) approach be used? sent37
38. What are parse trees useful for, and under what condition? sent38
39. Why is the identification of subqueries necessary? sent39"
248987614,Deep Learning for Visual Speech Analysis: A Survey,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,Temporal backend network,20,"The temporal backend network built upon visual features aims to further aggregate context information. In traditional VSR, classical statistical models (e.g., Hidden Markov Model, HMM) are commonly used for temporal information aggregation.

RNN-based Architectures. In the field of deep learning, Recurrent Neural Networks (RNNs) are representative network structures used to learn sequence data. The typical RNN structures (e.g., LSTM and GRU) are shown in Fig. 6, and their basic structures are similar to that of HMM, in which the dependencies between the observed state sequences are described by the transformation of the hidden state sequence. Compared to HMM, RNNs have a more powerful representation ability due to the nonlinear transformation during hidden state transitions. Bidirectional RNNs (BiRNNs) are variations of basic RNNs, which attempt to aggregate context information from previous timesteps as well as future timesteps. Many works [28,50,55,56,127,132,133,134,135] have adopted RNN-based (BiLSTM or BiGRU) network architectures as the temporal backend network in VSR. Beyond above fundamental RNN structures, various modifications [58,136] have been made to improve feature learning for VSR. For example, Wang et al. [58] utilized BiConvLSTM [137] as temporal backend network. ConvLSTM is a convolutional counterpart of conventional fully connected LSTM, which models temporal dependency while preserving spatial information. Wang et al.integrated the attention mechanism into the model to further improve the BiConvLSTM architecture.

Transformer-based Architectures. Compared to RNN-based architectures, Transformers [138] have significant advantages in long-term dependency and parallel computation. However, transformers usually suffer from some drawbacks. First, transformers are more prone to overfitting than RNNs and TCNs in small-scale datasets. Second, transformers are limited in some specific tasks (e.g., word-level VSR tasks) with shortterm context. Therefore, transformers are more suitable for sentence-level VSR tasks, rather than word-level VSR tasks. [6] is the first work introducing transformers to VSR. Based on the basic transformer architecture, the authors proposed two types of backend models: TM-seq2seq and TM-CTC. The difference between the two models lies in the training target. The experiments clearly showed that the transformer-based backend network performs much better than the RNN-based backend network in the sentence-level VSR task. Since the basic transformer pays no extra attention to short-term dependency, Zhang et al. [61] proposed multiple Temporal Focal blocks (TF-blocks), helping features to look around their neighbors and capturing more short-term temporal dependencies. The results demonstrated that the short-term dependency is as crucial as the long-term dependency in sentence-level VSR.

TCN-based Architectures. In the context of deep sequence models, RNNs and Transformers have a high demand for memory and computation ability. Temporal Convolutional Networks (TCNs) are another type of deep sequence model, and various improvements have been applied to the basic TCN to make them more appropriate for VSR. For example, Afouras et al. [60] used depth-wise separable convolution (DS-TCN) for sentence-level VSR. However, the performance of DS-TCN does not work as well as transformers, as TCN-based models have poor ability on capturing long-term dependency. To enable temporal backend network capturing multi-scale temporal patterns, Martinez [59] proposed to utilize multi-scale TCN (MS-TCN) structure, which achieved SOTA results (87.9% Acc) on the word-level LRW dataset. Table. 2 summarizes the general pros and cons of various visual frontend networks and temporal backend networks, and the available inputs of the corresponding visual frontend network. As we know, most of the existing VSR models are derived from general backbone models used in other fields (e.g., action recognition [139,140], audio speech recognition [137], etc.), and few are designed explicitly for VSR. Therefore, more attention should be paid to the particular structure adaptive to the properties of VSR in the future.","sent1: The temporal backend network built upon visual features aims to further aggregate context information.
sent2: In traditional VSR, classical statistical models (e.g., Hidden Markov Model, HMM) are commonly used for temporal information aggregation.
sent3: RNN-based Architectures. In the field of deep learning, Recurrent Neural Networks (RNNs) are representative network structures used to learn sequence data.
sent4: The typical RNN structures (e.g., LSTM and GRU) are shown in Fig. 6, and their basic structures are similar to that of HMM, in which the dependencies between the observed state sequences are described by the transformation of the hidden state sequence.
sent5: Compared to HMM, RNNs have a more powerful representation ability due to the nonlinear transformation during hidden state transitions.
sent6: Bidirectional RNNs (BiRNNs) are variations of basic RNNs, which attempt to aggregate context information from previous timesteps as well as future timesteps.
sent7: Many works [28,50,55,56,127,132,133,134,135] have adopted RNN-based (BiLSTM or BiGRU) network architectures as the temporal backend network in VSR.
sent8: Beyond above fundamental RNN structures, various modifications [58,136] have been made to improve feature learning for VSR.
sent9: For example, Wang et al. [58] utilized BiConvLSTM [137] as temporal backend network.
sent10: ConvLSTM is a convolutional counterpart of conventional fully connected LSTM, which models temporal dependency while preserving spatial information.
sent11: Wang et al.integrated the attention mechanism into the model to further improve the BiConvLSTM architecture.
sent12: Transformer-based Architectures.
sent13: Compared to RNN-based architectures, Transformers [138] have significant advantages in long-term dependency and parallel computation.
sent14: However, transformers usually suffer from some drawbacks.
sent15: First, transformers are more prone to overfitting than RNNs and TCNs in small-scale datasets.
sent16: Second, transformers are limited in some specific tasks (e.g., word-level VSR tasks) with shortterm context.
sent17: Therefore, transformers are more suitable for sentence-level VSR tasks, rather than word-level VSR tasks.
sent18: [6] is the first work introducing transformers to VSR.
sent19: Based on the basic transformer architecture, the authors proposed two types of backend models: TM-seq2seq and TM-CTC.
sent20: The difference between the two models lies in the training target.
sent21: The experiments clearly showed that the transformer-based backend network performs much better than the RNN-based backend network in the sentence-level VSR task.
sent22: Since the basic transformer pays no extra attention to short-term dependency, Zhang et al. [61] proposed multiple Temporal Focal blocks (TF-blocks), helping features to look around their neighbors and capturing more short-term temporal dependencies.
sent23: The results demonstrated that the short-term dependency is as crucial as the long-term dependency in sentence-level VSR.
sent24: TCN-based Architectures. In the context of deep sequence models, RNNs and Transformers have a high demand for memory and computation ability.
sent25: Temporal Convolutional Networks (TCNs) are another type of deep sequence model, and various improvements have been applied to the basic TCN to make them more appropriate for VSR.
sent26: For example, Afouras et al. [60] used depth-wise separable convolution (DS-TCN) for sentence-level VSR.
sent27: However, the performance of DS-TCN does not work as well as transformers, as TCN-based models have poor ability on capturing long-term dependency.
sent28: To enable temporal backend network capturing multi-scale temporal patterns, Martinez [59] proposed to utilize multi-scale TCN (MS-TCN) structure, which achieved SOTA results (87.9% Acc) on the word-level LRW dataset.
sent29: Table. 2 summarizes the general pros and cons of various visual frontend networks and temporal backend networks, and the available inputs of the corresponding visual frontend network.
sent30: As we know, most of the existing VSR models are derived from general backbone models used in other fields (e.g., action recognition
sent31: [139,140], audio speech recognition [137], etc.), and few are designed explicitly for VSR.
sent32: Therefore, more attention should be paid to the particular structure adaptive to the properties of VSR in the future.","1. What is the purpose of the temporal backend network in visual speech recognition (VSR)? sent1
2. What models are traditionally used for temporal information aggregation in VSR? sent2
3. What are RNN-based architectures, and how are they used in VSR? sent3
    3.1. What are the typical structures of RNNs, and how do they compare to HMMs? sent4, sent5
    3.2. What are Bidirectional RNNs (BiRNNs), and what is their purpose? sent6
    3.3. How have RNN-based architectures been adopted in VSR? sent7
    3.4. What modifications have been made to RNN structures to improve feature learning for VSR? sent8
        3.4.1. Can you provide an example of a modified RNN structure used in VSR? sent9, sent10
        3.4.2. How was the attention mechanism integrated into the BiConvLSTM architecture? sent11
4. What are the advantages and drawbacks of Transformer-based architectures compared to RNNs? sent12, sent13, sent14
    4.1. What are the specific drawbacks of transformers in VSR tasks? sent15, sent16
    4.2. For which type of VSR tasks are transformers more suitable? sent17
    4.3. What was the first work to introduce transformers to VSR, and what models were proposed? sent18, sent19
        4.3.1. What is the difference between the TM-seq2seq and TM-CTC models? sent20
        4.3.2. How did transformer-based networks perform compared to RNN-based networks in sentence-level VSR tasks? sent21
    4.4. What modifications were proposed to address short-term dependency in transformers? sent22
        4.4.1. What did the results demonstrate about short-term dependency in sentence-level VSR? sent23
5. What are TCN-based architectures, and how do they compare to RNNs and Transformers? sent24
    5.1. What improvements have been applied to TCNs for VSR? sent25
        5.1.1. Can you provide an example of a TCN-based model used in VSR? sent26
        5.1.2. How does the performance of DS-TCN compare to transformers? sent27
    5.2. What structure was proposed to enable capturing multi-scale temporal patterns, and what were the results? sent28
6. What does Table 2 summarize in the context of VSR models? sent29
7. What is the current state of VSR models in terms of their derivation and design? sent30, sent31
8. What should future research focus on regarding VSR models? sent32"
259089180,A Survey on Learning Objects' Relationship for Image Captioning,https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be,Semantic Graph.,5,"Te graph method use pretrained relationship detection networks in visual relation detection to extract action relations between objects and construct corresponding scene graphs. Specifcally, Yao et al. [72] used the abovementioned method to build the graph, as shown in Figure 5. Te pretrained model predicts the action relationship and uses the relationship category as the edge label. In each relational tuple <subject-predicate-object>, the subject and object are the 2048-dimensional attribute feature from the object detection network's RoI pooling. Te image region feature corresponding initializes the feature of the predicate to the minimum circumscribing moment of two bounding boxes belonging to the subject and object. Te above features are concatenated together and then input to the subsequent classifcation layer for obtaining the relationship category of the predicate. Te N × (N − 1) relational tuples are input into (excluding self-relations) the Computational Intelligence and Neuroscience 7 relational classifcation network. Edges with a probability larger than 0.5 are kept to form an action graph, as shown in Figure 6(b). Yang et al. [73] constructed scene graphs based on reference sentences in the training phase to reconstruct the sentence to accomplish the auto-encode training. Te scene graph divides its nodes into three categories: object nodes, relational nodes, and attribute nodes. For each <subjectpredicate-object> tuple, the subject and object correspond to the object node o i and o j . Te l attribute of the object corresponds to the attribute node a i,l , and the relationship between the two objects i, j corresponds to the relationship node r ij . Each node in the scene graph is represented by a feature vector of e o , e a , e r ∈ R d , respectively. Te object node o i and all of its attribute nodes a i,l have connections by an edge from the object node to the attribute node. If there is a relationship node, the subject-object node o i will frst connect to the relationship node r ij , and then the relationship node r ij will connect to the object object node o j . Te constructed graph is shown in Figure 6(c). In terms of implementation, they adopt the scene graph constructor used in [83] frst to convert sentences into syntactically independent trees and then convert the trees into scene graphs according to the rules mentioned in [75].

Chen et al. [74] designed a customized captioning model to generate sentences according to an abstract graph. Te abstract graph is a scene graph customized according to the user's wish. Te diferent forms of description graphs determine the level of detail in the generated caption. Specifcally, the abstract graph is constructed by the combination of three types of nodes: (1) object nodes, (2) attribute nodes (representing a specifc attribute of an object node), and (3) relationship nodes. Te construction of the abstract graph is to add the nodes and edges into the graph according to the user's interests. Specifcally, given all N object boxes of an image, if the user wants to know the content of the i object box, the object node o i is added to the abstract graph. At the same time, if the user wants to know about the attribute characteristics contained in the object node o i , l attribute nodes are added, and each attribute node corresponds to a path from o i to a i,l directed edges. If the user wants to describe the relationship between two objects, add the corresponding relationship node r i,j in the abstract graph, and build the edge connection between the subject and the object. Te subject-object node o i points to the relationship node r i,j , and then the relationship node r i,j points to the object object node o j . Te features corresponding to the object nodes and attribute nodes in the abstract graph adopt the visual features of the corresponding object bounding box. Te extraction method for the relational node is mainly used to extract the union frame features of two objects. Te result of its construction is shown in Figure 6(d).

In summary, the graph method represents more complex action relationships between objects than the tensor method. At the same time, some unnecessary relationship information is also eliminated, which can better retain important relationship content. Tere has also been a more signifcant improvement in computational cost and model performance. But the disadvantage is that it depends on the efectiveness of the relationship detection network and relies on training additional relationship information, which increases the complexity of the entire process. In the geometric graph, each edge represents a certain orientation. But in the semantic graph, each edge directly corresponds to a relational category. Tis more detailed representation of the relationship makes the semantic graph more efective to model the alignment of relational words. However, the limited number of relational categories also limits the variety of generated relational words. At the same time, the semantic similarity between diferent categories is also eliminated due to the classifcation operation.","sent1: Te graph method use pretrained relationship detection networks in visual relation detection to extract action relations between objects and construct corresponding scene graphs.
sent2: Specifcally, Yao et al. [72] used the abovementioned method to build the graph, as shown in Figure 5.
sent3: Te pretrained model predicts the action relationship and uses the relationship category as the edge label.
sent4: In each relational tuple <subject-predicate-object>, the subject and object are the 2048-dimensional attribute feature from the object detection network's RoI pooling.
sent5: Te image region feature corresponding initializes the feature of the predicate to the minimum circumscribing moment of two bounding boxes belonging to the subject and object.
sent6: Te above features are concatenated together and then input to the subsequent classifcation layer for obtaining the relationship category of the predicate.
sent7: Te N × (N − 1) relational tuples are input into (excluding self-relations) the Computational Intelligence and Neuroscience 7 relational classifcation network.
sent8: Edges with a probability larger than 0.5 are kept to form an action graph, as shown in Figure 6(b).
sent9: Yang et al. [73] constructed scene graphs based on reference sentences in the training phase to reconstruct the sentence to accomplish the auto-encode training.
sent10: Te scene graph divides its nodes into three categories: object nodes, relational nodes, and attribute nodes.
sent11: For each <subjectpredicate-object> tuple, the subject and object correspond to the object node o i and o j .
sent12: Te l attribute of the object corresponds to the attribute node a i,l , and the relationship between the two objects i, j corresponds to the relationship node r ij .
sent13: Each node in the scene graph is represented by a feature vector of e o , e a , e r ∈ R d , respectively.
sent14: Te object node o i and all of its attribute nodes a i,l have connections by an edge from the object node to the attribute node.
sent15: If there is a relationship node, the subject-object node o i will frst connect to the relationship node r ij , and then the relationship node r ij will connect to the object object node o j .
sent16: Te constructed graph is shown in Figure 6(c).
sent17: In terms of implementation, they adopt the scene graph constructor used in [83] frst to convert sentences into syntactically independent trees and then convert the trees into scene graphs according to the rules mentioned in [75].
sent18: Chen et al. [74] designed a customized captioning model to generate sentences according to an abstract graph.
sent19: Te abstract graph is a scene graph customized according to the user's wish.
sent20: Te diferent forms of description graphs determine the level of detail in the generated caption.
sent21: Specifcally, the abstract graph is constructed by the combination of three types of nodes: (1) object nodes, (2) attribute nodes (representing a specifc attribute of an object node), and (3) relationship nodes.
sent22: Te construction of the abstract graph is to add the nodes and edges into the graph according to the user's interests.
sent23: Specifcally, given all N object boxes of an image, if the user wants to know the content of the i object box, the object node o i is added to the abstract graph.
sent24: At the same time, if the user wants to know about the attribute characteristics contained in the object node o
sent25: i , l attribute nodes are added, and each attribute node corresponds to a path from o i to a i,l directed edges.
sent26: If the user wants to describe the relationship between two objects, add the corresponding relationship node r i,j in the abstract graph, and build the edge connection between the subject and the object.
sent27: Te subject-object node o i points to the relationship node r i,j , and then the relationship node r i,j points to the object object node o j .
sent28: Te features corresponding to the object nodes and attribute nodes in the abstract graph adopt the visual features of the corresponding object bounding box.
sent29: Te extraction method for the relational node is mainly used to extract the union frame features of two objects.
sent30: Te result of its construction is shown in Figure 6(d).
sent31: In summary, the graph method represents more complex action relationships between objects than the tensor method.
sent32: At the same time, some unnecessary relationship information is also eliminated, which can better retain important relationship content.
sent33: Tere has also been a more signifcant improvement in computational cost and model performance.
sent34: But the disadvantage is that it depends on the efectiveness of the relationship detection network and relies on training additional relationship information, which increases the complexity of the entire process.
sent35: In the geometric graph, each edge represents a certain orientation.
sent36: But in the semantic graph, each edge directly corresponds to a relational category.
sent37: Tis more detailed representation of the relationship makes the semantic graph more efective to model the alignment of relational words.
sent38: However, the limited number of relational categories also limits the variety of generated relational words.
sent39: At the same time, the semantic similarity between diferent categories is also eliminated due to the classifcation operation.","1. What method is used to extract action relations between objects in the graph method? sent1
    1.1. How is the graph constructed using the pretrained model? sent2, sent3
    1.2. What are the components of each relational tuple in the graph method? sent4
    1.3. How is the feature of the predicate initialized in the graph method? sent5
    1.4. How are the features processed to obtain the relationship category of the predicate? sent6
    1.5. How are relational tuples processed in the relational classification network? sent7
    1.6. What criteria are used to form an action graph? sent8
2. How did Yang et al. construct scene graphs for auto-encode training? sent9
    2.1. How are nodes categorized in the scene graph? sent10
    2.2. How are subject and object nodes represented in the scene graph? sent11
    2.3. How are attribute and relationship nodes represented in the scene graph? sent12
    2.4. What feature vectors represent each node in the scene graph? sent13
    2.5. How are connections made between object nodes and attribute nodes? sent14
    2.6. How are connections made if there is a relationship node? sent15
    2.7. How is the scene graph constructed from sentences? sent17
3. What did Chen et al. design to generate sentences from an abstract graph? sent18
    3.1. What is an abstract graph and how is it customized? sent19, sent20
    3.2. How is the abstract graph constructed based on user interests? sent21, sent22
    3.3. How are object nodes added to the abstract graph? sent23
    3.4. How are attribute nodes added to the abstract graph? sent24, sent25
    3.5. How are relationship nodes added to the abstract graph? sent26, sent27
    3.6. What features are used for object and attribute nodes in the abstract graph? sent28
    3.7. How are relational node features extracted in the abstract graph? sent29
4. What are the advantages and disadvantages of the graph method compared to the tensor method? sent31, sent32, sent33, sent34
5. How do geometric and semantic graphs differ in terms of edge representation? sent35, sent36
6. What are the benefits and limitations of the semantic graph's detailed relationship representation? sent37, sent38, sent39"
195345457,Emotionally-Aware Chatbots: A Survey,https://www.semanticscholar.org/paper/c131665638feb8c11f936989ffc6187317593b41,HISTORY OF EMOTIONALLY-AWARE CHATBOT,14,"The early development of chatbot was inspired by Turing test in 1950 [57]. Eliza was the first publicly known chatbot, built by using simple hand-crafted script [58]. Parry [8] was another chatbot which successfully passed the Turing test. Similar to Eliza, Parry still uses a rule-based approach but with a better understanding, including the mental model that can stimulate emotion. Therefore, Parry is the first chatbot which involving emotion in its development. Also, worth to be mentioned is ALICE (Artificial Linguistic Internet Computer Entity), a customizable chatbot by using Artificial Intelligence Markup Language (AIML). Therefore, ALICE still also use a rule-based approach by executing a pattern-matcher recursively to obtain the response. Then in May 2014, Microsoft introduced XiaoIce [66], an empathetic social chatbot which is able to recognize users' emotional needs. XiaoIce can provide an engaging interpersonal communication by giving encouragement or other affective messages, so that can hold human attention during communication.

Nowadays, most of chatbots technologies were built by using neural-based approach. Emotional Chatting Machine (ECM) [65] was the first works which exploiting deep learning approach in building a large-scale emotionally-aware conversational bot. Then several studies were proposed to deal with this research area by introducing emotion embedding representation [2,9,50] or modeling as reinforcement learning problem [23,55]. Most of these studies used encoder-decoder architecture, specifically sequence to sequence (seq2seq) learning. Some works also tried to introduce a new dataset in order to have a better gold standard and improve system performance. [45] introduce EMPATHETICDIA-LOGUES dataset, a novel dataset containing 25k conversations include emotional contexts information to facilitate training and evaluating the textual conversational system. Then, work from [18] produce a dataset containing 1.5 million Twitter conversation, gathered by using Twitter API from customer care account of 62 brands across several industries. This dataset was used to build tone-aware customer care chatbot. Finally, [27] tried to enhance SEMAINE corpus [29] by using crowdsourcing scenario to obtain a human judgement for deciding which response that elicits positive emotion. Their dataset was used to develop a chatbot which captures human emotional states and elicits positive emotion during the conversation.","sent1: The early development of chatbot was inspired by Turing test in 1950 [57].
sent2: Eliza was the first publicly known chatbot, built by using simple hand-crafted script [58].
sent3: Parry [8] was another chatbot which successfully passed the Turing test.
sent4: Similar to Eliza, Parry still uses a rule-based approach but with a better understanding, including the mental model that can stimulate emotion.
sent5: Therefore, Parry is the first chatbot which involving emotion in its development.
sent6: Also, worth to be mentioned is ALICE (Artificial Linguistic Internet Computer Entity), a customizable chatbot by using Artificial Intelligence Markup Language (AIML).
sent7: Therefore, ALICE still also use a rule-based approach by executing a pattern-matcher recursively to obtain the response.
sent8: Then in May 2014, Microsoft introduced XiaoIce [66], an empathetic social chatbot which is able to recognize users' emotional needs.
sent9: XiaoIce can provide an engaging interpersonal communication by giving encouragement or other affective messages, so that can hold human attention during communication.
sent10: Nowadays, most of chatbots technologies were built by using neural-based approach.
sent11: Emotional Chatting Machine (ECM)
sent12: [65] was the first works which exploiting deep learning approach in building a large-scale emotionally-aware conversational bot.
sent13: Then several studies were proposed to deal with this research area by introducing emotion embedding representation [2,9,50] or modeling as reinforcement learning problem [23,55].
sent14: Most of these studies used encoder-decoder architecture, specifically sequence to sequence (seq2seq) learning.
sent15: Some works also tried to introduce a new dataset in order to have a better gold standard and improve system performance.
sent16: [45] introduce EMPATHETICDIA-LOGUES dataset, a novel dataset containing 25k conversations include emotional contexts information to facilitate training and evaluating the textual conversational system.
sent17: Then, work from [18] produce a dataset containing 1.5 million Twitter conversation, gathered by using Twitter API from customer care account of 62 brands across several industries.
sent18: This dataset was used to build tone-aware customer care chatbot.
sent19: Finally, [27] tried to enhance SEMAINE corpus [29] by using crowdsourcing scenario to obtain a human judgement for deciding which response that elicits positive emotion.
sent20: Their dataset was used to develop a chatbot which captures human emotional states and elicits positive emotion during the conversation.","1. What inspired the early development of chatbots? sent1
    1.1. What was the first publicly known chatbot and how was it built? sent2
    1.2. What was Parry and how did it differ from Eliza? sent3, sent4
        1.2.1. What is significant about Parry in the context of emotion in chatbots? sent5
    1.3. What is ALICE and how does it function? sent6, sent7
2. What was introduced by Microsoft in May 2014 in the field of chatbots? sent8
    2.1. How does XiaoIce engage in interpersonal communication? sent9
3. What approach is predominantly used in modern chatbot technologies? sent10
    3.1. What was the first work to exploit deep learning for emotionally-aware chatbots? sent11, sent12
    3.2. What research areas have been explored in emotionally-aware chatbots? sent13
        3.2.1. What architecture is commonly used in these studies? sent14
        3.2.2. What efforts have been made to improve datasets for emotionally-aware chatbots? sent15
            3.2.2.1. What is the EMPATHETICDIALOGUES dataset? sent16
            3.2.2.2. What dataset was produced from Twitter conversations and for what purpose? sent17, sent18
            3.2.2.3. How was the SEMAINE corpus enhanced and for what purpose? sent19, sent20"
237532483,A Survey on Temporal Sentence Grounding in Videos,https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453,End-to-end method,24,"The end-to-end model follows one single-pass pattern. We divide it into two types, i.e., anchor-based and anchor-free, based on whether the method uses anchors (i.e., proposals) to make predictions.

2.2.1 anchor-based. The representative anchor-based works include TGN [5], CMIN [27], SCDM [73], MAN [78], CBP [60], CSMGAN [36], 2D-TAN [83], FIAN [46], SMIN [59] and Zhang et al. [82].

TGN [5] is one typical end-to-end deep architecture, which can localize the target moment in one single pass without handling heavily overlapped pre-segmented candidate moments. As shown in Fig. 5, TGN dynamically matches the sentence and video units via a sequential LSTM grounder with fine-grained frame-by-word interaction, and at each time step, the grounder would simultaneously score a group of candidate segments with different temporal scales ending at this time step.

CMIN [84] sequentially scores a set of candidate moments of multi-scale anchors like TGN but with a sequential BiGRU network, and refines the candidate moments with boundary regression. To further enhance the cross-modal matching, it devises a novel cross-modal interaction network (CMIN), which first leverages a syntactic GCN to model the syntactic structure of queries, and captures long-range temporal dependencies of video context with a multi-head self-attention, then employs the fine-grained cross-modal multi-stage interaction module to produce the cross-modal features for following sequentially scoring.

Similarly, CBP [60] builds a single-stream model with sequential LSTM, which jointly predicts temporal anchors and boundaries at each time step for yield precise localization. Furthermore, to better detect semantic boundaries, CBP devises a self attention based module to collect contextual clues instead of simply concatenating the contextual features like [16,18,23]. Based on interaction output of both language and video, it explicitly measures the contributions from different contextual elements.

CSMGAN [36] also adopts such a single-pass scheme. It builds a joint graph for modelling the cross-/self-modal relations via iterative message passing, to capture the high-order interactions between two modalities effectively. Each node of the graph aggregates the messages from its neighbor nodes in an edge-weighted manner and updates its state with both aggregated message and current state through ConvGRU.

Qu et al. [46] present a fine-grained iterative attention network (FIAN), which devises a contentoriented strategy to generate candidate moments differing from the anchor-based methods with sequential RNNs mentioned above. FIAN employs a refined cross-modal guided attention (CGA) block to capture the detailed cross-modal interactions, and further adopts a symmetrical iterative attention to generate both sentence-aware video and video-aware sentence representations, where the latter are explicitly facilitated to enhance the former and finally both parts contribute to a robust cross-modal feature.

TGN establishes the temporal grounding architecture through a sequential LSTM network, while Yuan et al. [73] propose SCDM, which exploits a hierarchical temporal convolutional network to conduct target segment localization, and couples it with a semantics-conditioned dynamic modulation to fully leverage sentence semantics to compose the sentence-related video contents over time. As shown in Fig. 6, the multimodal fusion module fuses the entire sentence and each video clip in a fine-grained manner. The fused representation is formulated as:

With such fused representations as inputs, the semantic modulated temporal convolution module further correlates sentence-related video contents in a temporal convolution procedure, dynamically modulating the temporal feature maps concerning the sentence. Specifically, for each temporal convolutional layer, the feature map is denoted as A = {a }. The feature unit a will be modulated based on the modulation vectors and :

where the modulation vectors are computed based on the sentence representation S = {s } =1 :

Finally, the position prediction module outputs the location offsets and overlap scores of candidate video segments based on the modulated features. MAN [78] also leverages temporal convolutional network to address the TSGV task, where the sentence query is integrated as dynamic filters into the convolutional process. Specifically, as shown in Fig. 7, MAN encodes the entire video stream using a hierarchical convolutional network to produce multi-scale candidate moment representations. The textual features are encoded as dynamic filters and convolved with such visual representations. Additionally, MAN exploits the graph-structured moment relation modelling adapted from Graph Convolution Network (GCN) [30] for temporal reasoning to further improve the moment representations.

Both SCDM and MAN only consider 1D temporal feature maps, while the 2D-TAN [83] network models the temporal relations of video segments via a two-dimensional map. As shown in Fig. 8, it firstly divides the video into evenly spaced video clips with duration . The ( , )-th location on the 2D temporal map represents a candidate moment (or anchor) from the time to ( + 1) . This kind of 2D temporal map covers diverse video moments with different lengths, while representing their adjacent relations. The proposed temporal adjacent network fuses the sentence representation with each of the candidate moment feature and then leverages convolutional neural network to embed the video context information, and finally predicts the confidence score of each candidate to be the final target segment. 2D-TAN adopts a binary cross entropy loss with a the scaled IoU as the supervision signal. The scaled IoU is controlled by two thresholds and as:

where is the temporal IoU between one candidate moment and the groundtruth moment. Thus, the loss function can be expressed as:

where is the predicted confidence score of a moment. Wang et al. [59] propose a structured multi-level interaction network (SMIN), which makes further modifications on the 2D temporal feature map as its proposal generation module. SMIN explores the inherent structure of moment, which can be disentangled into visual content and positional boundary parts for fine-grained cross-modal and intra-moment interaction. Zhang et al. [82] also adopts the same proposal generation approach as that of 2D-TAN, designing a visuallanguage transformer backbone followed by a multi-stage aggregation module to get discriminative moment representations for more accurate moment localization.

Despite the superior performance anchor-based methods have achieved, the performance is sensitive with the heuristic rules manually designed (i.e., the number and scales of anchors). As a result, such anchor-based methods can not adapt to the situation with variable video length. Meanwhile, although the pre-segmentation like two-stage methods is not required, it still essentially depends on the ranking of proposal candidates, which will also influence its efficiency.","sent1: The end-to-end model follows one single-pass pattern.
sent2: We divide it into two types, i.e., anchor-based and anchor-free, based on whether the method uses anchors (i.e., proposals) to make predictions.
sent3: 2.2.1 anchor-based. The representative anchor-based works include TGN [5], CMIN [27], SCDM [73], MAN [78], CBP [60], CSMGAN [36], 2D-TAN [83], FIAN [46], SMIN [59] and Zhang et al. [82].
sent4: TGN [5] is one typical end-to-end deep architecture, which can localize the target moment in one single pass without handling heavily overlapped pre-segmented candidate moments.
sent5: As shown in Fig. 5, TGN dynamically matches the sentence and video units via a sequential LSTM grounder with fine-grained frame-by-word interaction, and at each time step, the grounder would simultaneously score a group of candidate segments with different temporal scales ending at this time step.
sent6: CMIN [84] sequentially scores a set of candidate moments of multi-scale anchors like TGN but with a sequential BiGRU network, and refines the candidate moments with boundary regression.
sent7: To further enhance the cross-modal matching, it devises a novel cross-modal interaction network (CMIN), which first leverages a syntactic GCN to model the syntactic structure of queries, and captures long-range temporal dependencies of video context with a multi-head self-attention, then employs the fine-grained cross-modal multi-stage interaction module to produce the cross-modal features for following sequentially scoring.
sent8: Similarly, CBP [60] builds a single-stream model with sequential LSTM, which jointly predicts temporal anchors and boundaries at each time step for yield precise localization.
sent9: Furthermore, to better detect semantic boundaries, CBP devises a self attention based module to collect contextual clues instead of simply concatenating the contextual features like [16,18,23].
sent10: Based on interaction output of both language and video, it explicitly measures the contributions from different contextual elements.
sent11: CSMGAN [36] also adopts such a single-pass scheme.
sent12: It builds a joint graph for modelling the cross-/self-modal relations via iterative message passing, to capture the high-order interactions between two modalities effectively.
sent13: Each node of the graph aggregates the messages from its neighbor nodes in an edge-weighted manner and updates its state with both aggregated message and current state through ConvGRU.
sent14: Qu et al. [46] present a fine-grained iterative attention network (FIAN), which devises a contentoriented strategy to generate candidate moments differing from the anchor-based methods with sequential RNNs mentioned above.
sent15: FIAN employs a refined cross-modal guided attention (CGA) block to capture the detailed cross-modal interactions, and further adopts a symmetrical iterative attention to generate both sentence-aware video and video-aware sentence representations, where the latter are explicitly facilitated to enhance the former and finally both parts contribute to a robust cross-modal feature.
sent16: TGN establishes the temporal grounding architecture through a sequential LSTM network, while Yuan et al. [73] propose SCDM, which exploits a hierarchical temporal convolutional network to conduct target segment localization, and couples it with a semantics-conditioned dynamic modulation to fully leverage sentence semantics to compose the sentence-related video contents over time.
sent17: As shown in Fig. 6, the multimodal fusion module fuses the entire sentence and each video clip in a fine-grained manner.
sent18: The fused representation is formulated as:With such fused representations as inputs, the semantic modulated temporal convolution module further correlates sentence-related video contents in a temporal convolution procedure, dynamically modulating the temporal feature maps concerning the sentence.
sent19: Specifically, for each temporal convolutional layer, the feature map is denoted as A = {a }.
sent20: The feature unit a will be modulated based on the modulation vectors and :where the modulation vectors are computed based on the sentence representation S = {s } =1 :Finally, the position prediction module outputs the location offsets and overlap scores of candidate video segments based on the modulated features.
sent21: MAN [78] also leverages temporal convolutional network to address the TSGV task, where the sentence query is integrated as dynamic filters into the convolutional process.
sent22: Specifically, as shown in Fig. 7, MAN encodes the entire video stream using a hierarchical convolutional network to produce multi-scale candidate moment representations.
sent23: The textual features are encoded as dynamic filters and convolved with such visual representations.
sent24: Additionally, MAN exploits the graph-structured moment relation modelling adapted from Graph Convolution Network (GCN) [30] for temporal reasoning to further improve the moment representations.
sent25: Both SCDM and MAN only consider 1D temporal feature maps, while the 2D-TAN [83] network models the temporal relations of video segments via a two-dimensional map.
sent26: As shown in Fig. 8, it firstly divides the video into evenly spaced video clips with duration .
sent27: The ( , )-th location on the 2D temporal map represents a candidate moment (or anchor) from the time to ( + 1) .
sent28: This kind of 2D temporal map covers diverse video moments with different lengths, while representing their adjacent relations.
sent29: The proposed temporal adjacent network fuses the sentence representation with each of the candidate moment feature and then leverages convolutional neural network to embed the video context information, and finally predicts the confidence score of each candidate to be the final target segment.
sent30: 2D-TAN adopts a binary cross entropy loss with a the scaled IoU as the supervision signal.
sent31: The scaled IoU is controlled by two thresholds and as:where is the temporal IoU between one candidate moment and the groundtruth moment.
sent32: Thus, the loss function can be expressed as:where is the predicted confidence score of a moment.
sent33: Wang et al. [59] propose a structured multi-level interaction network (SMIN), which makes further modifications on the 2D temporal feature map as its proposal generation module.
sent34: SMIN explores the inherent structure of moment, which can be disentangled into visual content and positional boundary parts for fine-grained cross-modal and intra-moment interaction.
sent35: Zhang et al. [82] also adopts the same proposal generation approach as that of 2D-TAN, designing a visuallanguage transformer backbone followed by a multi-stage aggregation module to get discriminative moment representations for more accurate moment localization.
sent36: Despite the superior performance anchor-based methods have achieved, the performance is sensitive with the heuristic rules manually designed (i.e., the number and scales of anchors).
sent37: As a result, such anchor-based methods can not adapt to the situation with variable video length.
sent38: Meanwhile, although the pre-segmentation like two-stage methods is not required, it still essentially depends on the ranking of proposal candidates, which will also influence its efficiency.","1. What is the pattern followed by the end-to-end model in temporal sentence grounding in videos? sent1
    1.1. How is the end-to-end model divided? sent2
2. What are some representative anchor-based works in temporal sentence grounding? sent3
    2.1. What is TGN and how does it function? sent4, sent5
    2.2. How does CMIN enhance cross-modal matching? sent6, sent7
    2.3. How does CBP improve semantic boundary detection? sent8, sent9, sent10
    2.4. How does CSMGAN model cross-/self-modal relations? sent11, sent12, sent13
    2.5. What is the approach of FIAN in generating candidate moments? sent14, sent15
    2.6. How does SCDM utilize sentence semantics for video content composition? sent16, sent17, sent18, sent19, sent20
    2.7. How does MAN integrate sentence queries into the convolutional process? sent21, sent22, sent23, sent24
    2.8. How does 2D-TAN model temporal relations of video segments? sent25, sent26, sent27, sent28, sent29, sent30, sent31, sent32
    2.9. What modifications does SMIN make on the 2D temporal feature map? sent33, sent34
    2.10. How does Zhang et al. approach moment localization? sent35
3. What are the limitations of anchor-based methods? sent36, sent37, sent38"
248987614,Deep Learning for Visual Speech Analysis: A Survey,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,Evaluation Metrics on VSG,5,"Appropriate evaluation for VSG continues to be an open problem, and many recent works have explored various evaluation metrics on VSG. We categorize those metrics based on three learning targets, i.e., identity preservation, visual quality, audio-visual semantic consistency.

Identity Preservation. One of the most important goals of VSG is to preserve the target identity as much as possible during video generation, as humans are quite sensitive to subtle appearance changes in synthesized videos. Since identity is a semantic concept, direct evaluation is not feasible. To evaluate how well the generated video preserves the target identity, existing works usually use the embedding distance of the generated video frames and the ground truth image to measure the identity-preserving performance. For example, Vougioukas et al. [40] adopted the average content distance (ACD) [111] to measure the average Euclidean distance of target image representation, obtained using OpenFace [112], and the representations of generated frames. Besides, Zakharov et al. [113] used the cosine similarity between embedding vectors of the ArcFace network [114] for measuring identity mismatch.","sent1: Appropriate evaluation for VSG continues to be an open problem, and many recent works have explored various evaluation metrics on VSG.
sent2: We categorize those metrics based on three learning targets, i.e., identity preservation, visual quality, audio-visual semantic consistency.
sent3: Identity Preservation. One of the most important goals of VSG is to preserve the target identity as much as possible during video generation, as humans are quite sensitive to subtle appearance changes in synthesized videos.
sent4: Since identity is a semantic concept, direct evaluation is not feasible.
sent5: To evaluate how well the generated video preserves the target identity, existing works usually use the embedding distance of the generated video frames and the ground truth image to measure the identity-preserving performance.
sent6: For example, Vougioukas et al. [40] adopted the average content distance (ACD) [111] to measure the average Euclidean distance of target image representation, obtained using OpenFace [112], and the representations of generated frames.
sent7: Besides, Zakharov et al. [113] used the cosine similarity between embedding vectors of the ArcFace network [114] for measuring identity mismatch.","1. What is the current state of evaluation for VSG? sent1
    1.1. How are evaluation metrics for VSG categorized? sent2
2. What is one of the most important goals of VSG? sent3
    2.1. Why is direct evaluation of identity preservation not feasible? sent4
    2.2. How is identity-preserving performance typically evaluated in VSG? sent5
        2.2.1. What method did Vougioukas et al. use to measure identity-preserving performance? sent6
        2.2.2. How did Zakharov et al. measure identity mismatch? sent7"
264832783,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825,Bayesian Inference,5,"In their work, Xie et al. ( 2022) first provided an interpretation of ICL through the lens of Bayesian inference, proposing that LLMs have the capability to perform implicit Bayesian inference via ICL.Specifically, they synthesized a small-scale dataset to examine how ICL emerges in LSTM and Transformer models during pretraining on text with extended coherence.Their findings revealed that both models are capable of inferring latent concepts to generate coherent subsequent tokens during pretraining.Additionally, these models were shown to perform ICL by identifying a shared latent concept among examples during the inference process.Their theoretical analysis confirms that this phenomenon persists even when there is a distribution mismatch between the examples and the data used for pretraining, particularly in settings where the pretraining distribution is derived from a mixture of Hidden Markov Models (HMMs) (Baum and Petrie, 1966).Furthermore, Xie et al. ( 2022) observed that the ICL error decreases as the length of each example increases, emphasizing the significance of the inherent information within inputs.This goes beyond mere input-label correlations and highlights the roles of intrinsic input characteristics in facilitating ICL.

Following on, Wang et al. (2023b) expanded the investigation of ICL by relaxing the assumptions made by Xie et al. (2022) and posited that ICL in LLMs essentially operates as a form of topic modeling that implicitly extracts task-relevant information from examples to aid in inference.Wang et al. (2023b) grounded their theoretical analysis in a setting with a finite number of demonstrations, and under a more general language generation process.Specifically, they characterized the data generation process using a causal graph with three variables and imposed no constraints on the distribution or quantity of samples.Their empirical and theoretical investigations revealed that ICL can approximate the Bayes optimal predictor when a finite number of samples are chosen based on the latent concept variable.Moreover, Wang et al. (2023b) devised an effective practical algorithm for demonstration selection tailored to real-world LLMs.

At the same time, Jiang (2023) also introduced a novel latent space theory extending the idea of Xie et al. ( 2022) to explain emergent abilities in LLMs.Instead of focusing on specific data distributions generated by HMMs, they delved into general sparse data distributions and employed LLMs as a universal density approximator for the marginal distribution, allowing them to probe these sparse structures more broadly.Jiang (2023) demonstrated that ICL, CoT, and instruction-following abilities in LLMs can be ascribed to Bayesian inference operating on the broader sparse joint distribution of languages.To shed light on the significance of the attention mechanism for ICL from a Bayesian view, Zhang et al. (2023) defined ICL as the task of predicting a response that aligns with a given covariate based on examples derived from a latent variable model.They established that ICL implicitly implements the Bayesian Model Averaging (BMA) algorithm, which is approximated by the attention mechanism.Furthermore, they demonstrated that certain attention mechanisms converge towards the conventional softmax attention as the number of examples goes to infinity.These attentions, due to their encoding of BMA within their structure, empower the Transformer model to perform ICL.

Although their conclusions are insightful, there is a room for improvement.Their findings might be influenced by various factors, such as the formats of the examples, the nature of tasks, and the choice of evaluation metrics.Additionally, many of these studies are based on analyses conducted using small synthetic datasets, potentially restricting their relevance and applicability to real-world scenarios.","sent1: In their work, Xie et al. ( 2022) first provided an interpretation of ICL through the lens of Bayesian inference, proposing that LLMs have the capability to perform implicit Bayesian inference via ICL.Specifically, they synthesized a small-scale dataset to examine how ICL emerges in LSTM and Transformer models during pretraining on text with extended coherence.
sent2: Their findings revealed that both models are capable of inferring latent concepts to generate coherent subsequent tokens during pretraining.
sent3: Additionally, these models were shown to perform ICL by identifying a shared latent concept among examples during the inference process.
sent4: Their theoretical analysis confirms that this phenomenon persists even when there is a distribution mismatch between the examples and the data used for pretraining, particularly in settings where the pretraining distribution is derived from a mixture of Hidden Markov Models (HMMs) (Baum and Petrie, 1966).Furthermore, Xie et al. ( 2022) observed that the ICL error decreases as the length of each example increases, emphasizing the significance of the inherent information within inputs.
sent5: This goes beyond mere input-label correlations and highlights the roles of intrinsic input characteristics in facilitating ICL.
sent6: Following on, Wang et al. (2023b) expanded the investigation of ICL by relaxing the assumptions made by Xie et al. (2022) and posited that ICL in LLMs essentially operates as a form of topic modeling that implicitly extracts task-relevant information from examples to aid in inference.
sent7: Wang et al. (2023b) grounded their theoretical analysis in a setting with a finite number of demonstrations, and under a more general language generation process.
sent8: Specifically, they characterized the data generation process using a causal graph with three variables and imposed no constraints on the distribution or quantity of samples.
sent9: Their empirical and theoretical investigations revealed that ICL can approximate the Bayes optimal predictor when a finite number of samples are chosen based on the latent concept variable.
sent10: Moreover, Wang et al. (2023b) devised an effective practical algorithm for demonstration selection tailored to real-world LLMs.
sent11: At the same time, Jiang (2023) also introduced a novel latent space theory extending the idea of Xie et al. ( 2022) to explain emergent abilities in LLMs.
sent12: Instead of focusing on specific data distributions generated by HMMs, they delved into general sparse data distributions and employed LLMs as a universal density approximator for the marginal distribution, allowing them to probe these sparse structures more broadly.
sent13: Jiang (2023) demonstrated that ICL, CoT, and instruction-following abilities in LLMs can be ascribed to Bayesian inference operating on the broader sparse joint distribution of languages.
sent14: To shed light on the significance of the attention mechanism for ICL from a Bayesian view, Zhang et al. (2023) defined ICL as the task of predicting a response that aligns with a given covariate based on examples derived from a latent variable model.
sent15: They established that ICL implicitly implements the Bayesian Model Averaging (BMA) algorithm, which is approximated by the attention mechanism.
sent16: Furthermore, they demonstrated that certain attention mechanisms converge towards the conventional softmax attention as the number of examples goes to infinity.
sent17: These attentions, due to their encoding of BMA within their structure, empower the Transformer model to perform ICL.
sent18: Although their conclusions are insightful, there is a room for improvement.
sent19: Their findings might be influenced by various factors, such as the formats of the examples, the nature of tasks, and the choice of evaluation metrics.
sent20: Additionally, many of these studies are based on analyses conducted using small synthetic datasets, potentially restricting their relevance and applicability to real-world scenarios.","1. How did Xie et al. (2022) interpret ICL through Bayesian inference in LLMs? sent1
    1.1. What did Xie et al. (2022) find about the models' capabilities during pretraining? sent2
    1.2. How do models perform ICL according to Xie et al. (2022)? sent3
    1.3. What does the theoretical analysis by Xie et al. (2022) confirm about ICL? sent4
        1.3.1. What significance does the length of examples have in ICL according to Xie et al. (2022)? sent4
    1.4. What roles do intrinsic input characteristics play in ICL? sent5
2. How did Wang et al. (2023b) expand the investigation of ICL? sent6
    2.1. What setting did Wang et al. (2023b) use for their theoretical analysis? sent7
    2.2. How did Wang et al. (2023b) characterize the data generation process? sent8
    2.3. What did Wang et al. (2023b) reveal about ICL and the Bayes optimal predictor? sent9
    2.4. What practical algorithm did Wang et al. (2023b) devise? sent10
3. What novel theory did Jiang (2023) introduce regarding LLMs? sent11
    3.1. How did Jiang (2023) extend the idea of Xie et al. (2022)? sent12
    3.2. What abilities in LLMs did Jiang (2023) ascribe to Bayesian inference? sent13
4. How did Zhang et al. (2023) define ICL from a Bayesian perspective? sent14
    4.1. What algorithm does ICL implicitly implement according to Zhang et al. (2023)? sent15
    4.2. What did Zhang et al. (2023) demonstrate about attention mechanisms? sent16
        4.2.1. How do attention mechanisms empower the Transformer model according to Zhang et al. (2023)? sent17
5. What limitations and factors might influence the findings of these studies? sent18, sent19
6. What potential restriction do these studies face regarding real-world applicability? sent20"
256461385,Narrative Why-Question Answering: A Review of Challenges and Datasets,https://www.semanticscholar.org/paper/98f4b43487b8386728c14aba24f848b2eadad1e2,Short vs Long narratives,6,"All reviewed datasets have short narratives as their context. The NarrativeQA short texts have a more complex narrative structure than other datasets, since the short context versions of the NarrativeQA are summaries of the larger narratives, and not single scenes from the long narratives. In short narratives, if there is a common lexical pattern between the question and a part of the narrative, or a large lexical overlap between the answer and the narrative, sophisticated models can treat free-form QA as an extractive task. For example, models trained on the TellMeWhy dataset generally try to find the answer span in the text and copy a part of the narrative as an answer (Lal et al., 2021).

The NarrativeQA dataset is the only dataset that has long narratives as its context. Linking narrative elements to answer questions in large narratives is harder than in short narratives (see section 3.2). Typically, in order to reason about long narratives, the parts relevant to reasoning are retrieved first (Kočiský et al., 2018;Tay et al., 2019;Frermann, 2019;Mou et al., 2020Mou et al., , 2021. The retrieval is difficult even with the state-of-the-art models due to the characteristics of narratives and the necessity of high-level narrative comprehension (Mou et al., 2021).","sent1: All reviewed datasets have short narratives as their context.
sent2: The NarrativeQA short texts have a more complex narrative structure than other datasets, since the short context versions of the NarrativeQA are summaries of the larger narratives, and not single scenes from the long narratives.
sent3: In short narratives, if there is a common lexical pattern between the question and a part of the narrative, or a large lexical overlap between the answer and the narrative, sophisticated models can treat free-form QA as an extractive task.
sent4: For example, models trained on the TellMeWhy dataset generally try to find the answer span in the text and copy a part of the narrative as an answer (Lal et al., 2021).
sent5: The NarrativeQA dataset is the only dataset that has long narratives as its context.
sent6: Linking narrative elements to answer questions in large narratives is harder than in short narratives (see section 3.2).
sent7: Typically, in order to reason about long narratives, the parts relevant to reasoning are retrieved first (Kočiský et al., 2018;Tay et al., 2019;Frermann, 2019;Mou et al., 2020Mou et al., , 2021. The retrieval is difficult even with the state-of-the-art models due to the characteristics of narratives and the necessity of high-level narrative comprehension (Mou et al., 2021).","1. What type of narratives do all reviewed datasets have as their context? sent1
2. How do the NarrativeQA short texts differ from other datasets in terms of narrative structure? sent2
3. How can sophisticated models treat free-form QA in short narratives? sent3
    3.1. Can you provide an example of how models handle free-form QA in short narratives? sent4
4. Which dataset is the only one that includes long narratives as its context? sent5
5. Why is linking narrative elements to answer questions harder in long narratives compared to short narratives? sent6
6. What is typically required to reason about long narratives, and what challenges are associated with this process? sent7"
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,Text heatmap.,4,"The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].

The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].","sent1: The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient.
sent2: In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output.
sent3: With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.
sent4: Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example.
sent5: However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3.
sent6: Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works.
sent7: Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].
sent8: The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient.
sent9: In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output.
sent10: With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.
sent11: Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example.
sent12: However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3.
sent13: Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works.
sent14: Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].","1. What does the model proposed by Huang et al. receive as input, and what does it indicate? sent1
2. How does the architecture handle input text in a similar fashion to input image cases? sent2
3. What is the purpose of the feature importance map in the model? sent3
4. What are the types of local explanations provided by the explainability approaches? sent4
5. How do most works treat interpretability improvement, and what is lacking in their evaluation? sent5
6. Why is the aspect of explainability considered understudied in the medical report generation task? sent6
7. What additional techniques and approaches could be explored for explainability in medical report generation? sent7"
252724722,A Review of the Research on the Evaluation Metrics for Automatic Grammatical Error Correction System,https://www.semanticscholar.org/paper/4241a0cb51c3780c57cae8b4f4d28341ccba2176,3),4,"Taking the sentence Our baseline system feed into PB-SMT pipeline as an example, if the manually modified set g � {feed ⟶ feeds, feed ⟶ feed a word}, the modified set e � {feed ⟶ feeds}, |e i ∩ g i | is 1, then the precision of the system is 100% and the recall rate is 50%.

In an ideal evaluation, both P and R values are expected to be as high as possible. But in practice, the precision rate and the recall rate are inconsistent under certain circumstances. Suppose that the optimal editing set g � 10, the modified set e 1 � 5, |e 1 ∩ g| � 3 in system 1 and the modified set e 2 � 8, |e 2 ∩ g | � 4 of system 2. Under these conditions, the P value of system 1 is 0.6, R value is 0.3, and system 2 has a P value of 0.5 and an R value of 0.4. e results show that the P value of system 1 is higher than that of system 2, but the R value is lower than that of system 2. erefore, for the convenience of comparison, the F (F-measure or F-score) [14] is introduced to evaluate the P value and the R value as a whole. e F value is the weighted harmonic average of the P and R values, as shown in formula (4):

In the M 2 evaluation, the value of α is usually set to 1, and the calculation formula of F 1 is shown in formula (5).

Later, CoNLL-2014 shared mission [15] was introduced and M 2 was used as the official evaluation indicator for the mission evaluation. But because of the nature of the data set, the researchers changed the value of weight α from 1 to 0.5. Because in the shared task evaluation process, the task recognizes that the precision rate weighs more than the recall rate and thus gives it a higher weight, twice as much weight as before, and twice as much precision, f 0.5 as shown in formula (6).

Currently, M 2 is the evaluation index most frequently used in the field of GEC, largely as a result of CoNLL-2014's shared task's promotion.

is tool has evolved into the standard for figuring out the GEC system's precision and recall rate. ere is a strong association between this evaluation metric and manual error correction . [16].","sent1: Taking the sentence Our baseline system feed into PB-SMT pipeline as an example, if the manually modified set g � {feed ⟶ feeds, feed ⟶ feed a word}, the modified set e � {feed ⟶ feeds}, |e i ∩ g i | is 1, then the precision of the system is 100% and the recall rate is 50%.
sent2: In an ideal evaluation, both P and R values are expected to be as high as possible.
sent3: But in practice, the precision rate and the recall rate are inconsistent under certain circumstances.
sent4: Suppose that the optimal editing set g � 10, the modified set e 1 � 5, |e 1 ∩ g| � 3 in system 1 and the modified set e 2 � 8, |e 2 ∩ g | � 4 of system 2.
sent5: Under these conditions, the P value of system 1 is 0.6, R value is 0.3, and system 2 has a P value of 0.5 and an R value of 0.4.
sent6: e results show that the P value of system 1 is higher than that of system 2, but the R value is lower than that of system 2.
sent7: erefore, for the convenience of comparison, the F (F-measure or F-score) [14] is introduced to evaluate the P value and the R value as a whole.
sent8: e F value is the weighted harmonic average of the P and R values, as shown in formula (4):
sent9: In the M 2 evaluation, the value of α is usually set to 1, and the calculation formula of F 1 is shown in formula (5).
sent10: Later, CoNLL-2014 shared mission [15] was introduced and M 2 was used as the official evaluation indicator for the mission evaluation.
sent11: But because of the nature of the data set, the researchers changed the value of weight α from 1 to 0.5.
sent12: Because in the shared task evaluation process, the task recognizes that the precision rate weighs more than the recall rate and thus gives it a higher weight, twice as much weight as before, and twice as much precision, f 0.5 as shown in formula (6).
sent13: Currently, M 2 is the evaluation index most frequently used in the field of GEC, largely as a result of CoNLL-2014's shared task's promotion.
sent14: is tool has evolved into the standard for figuring out the GEC system's precision and recall rate.
sent15: ere is a strong association between this evaluation metric and manual error correction .[16].","1. What is an example of evaluating precision and recall in a grammatical error correction system? sent1
2. What is expected in an ideal evaluation of precision and recall values? sent2
3. How do precision and recall rates behave in practice? sent3
4. What are the precision and recall values for systems 1 and 2 under certain conditions? sent4, sent5
    4.1. How do the precision and recall values of system 1 compare to those of system 2? sent6
5. What is introduced for the convenience of comparing precision and recall values? sent7
6. How is the F value calculated? sent8
7. What is the usual value of α in the M2 evaluation, and how is F1 calculated? sent9
8. What was introduced in the CoNLL-2014 shared mission, and what was used as the official evaluation indicator? sent10
9. Why was the weight α changed from 1 to 0.5 in the shared task evaluation process? sent11, sent12
10. What is the most frequently used evaluation index in the field of GEC, and why? sent13
11. What has the M2 tool evolved into regarding the GEC system's precision and recall rate? sent14
12. What is the association between the evaluation metric and manual error correction? sent15"
248987614,Deep Learning for Visual Speech Analysis: A Survey,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,Supervised learning for VSR,8,"There are two mainstream VSR tasks: word-level and sentencelevel. With a limited number of word categories, the former is to recognize isolated words from the input videos (i.e., talking face video classification), usually trained with multi-classification cross-entropy loss. The latter is to make unconstrained sentencelevel sequence prediction. However, due to the unconstrained word categories and video frame length, it is much more complicated than the word-level VSR task.

Supervised learning of end-to-end sentence-level VSR tasks (sentence prediction) can be divided into two types. Given the input sequence, the first type uses a neural network as an emission model, which outputs the likelihood of each output symbol (e.g., phonemes, characters, words). These methods generally employ a second phase of decoding using HMM. A popular version of this variant is the Contortionist Temporal Classification (CTC) [141], where the model predicts framewise labels and then looks for the optimal alignment between the frame-wise predictions and the output sequence. The main weakness of CTC is that the output labels are not conditioned on each other (it assumes each unit is conditional independent), and hence a language model is needed as a post-processing step. Different from the basic CTC, Xu et al. [62] proposed LCANet that feeds the encoded spatio-temporal features into a cascaded attention CTC decoder. The introduction of an attention mechanism improves the defect of the conditional independence assumption CTC in hidden neural layers. Another assumption of this approach is that it assumes a monotonic ordering between input and output sequences, which is suitable for VSR but not for machine translation.

The second type is sequence-to-sequence (seq2seq) models that first read the whole input sequence before predicting the output sentence. A number of works adopted this approach for speech recognition [142]. Chan et al. [143] proposed an elegant seq2seq method to transcribe audio signal to characters. Seq2seq models decode an output symbol at time t (e.g., phonemes, characters, words) conditioned on previous outputs 1, ..., t − 1. Thus, unlike CTC-based models, the model implicitly learns a language model over output symbols, and no further processing is required. However, it has been shown [144] that it is beneficial to incorporate an external language model in the decoding of seq2seq models as well. Chung et al. [28] proposed the WAS (Watch, Attend and Spell) model, which is a classical seq2seq VSR model. With the help of attention mechanism, WAS model is more capable of capturing long-term dependency.

Based on the transformer backbone architecture, Afouras et al. [6] have deeply analyzed the pros and cons of the CTC model and the seq2seq model for VSR. Generally, the seq2seq model performs well than the CTC model in the sentence-level VSR task. But, the seq2seq model needs more training time and inference time. Besides, the CTC model generalizes better and adapts faster as the sequence lengths are increased. Besides the above label-level supervised learning paradigms, feature-level supervised learning is also widely explored in VSR. Knowledge distillation [145] ","sent1: There are two mainstream VSR tasks: word-level and sentencelevel.
sent2: With a limited number of word categories, the former is to recognize isolated words from the input videos (i.e., talking face video classification), usually trained with multi-classification cross-entropy loss.
sent3: The latter is to make unconstrained sentencelevel sequence prediction.
sent4: However, due to the unconstrained word categories and video frame length, it is much more complicated than the word-level VSR task.
sent5: Supervised learning of end-to-end sentence-level VSR tasks (sentence prediction) can be divided into two types.
sent6: Given the input sequence, the first type uses a neural network as an emission model, which outputs the likelihood of each output symbol (e.g., phonemes, characters, words).
sent7: These methods generally employ a second phase of decoding using HMM.
sent8: A popular version of this variant is the Contortionist Temporal Classification (CTC) [141], where the model predicts framewise labels and then looks for the optimal alignment between the frame-wise predictions and the output sequence.
sent9: The main weakness of CTC is that the output labels are not conditioned on each other (it assumes each unit is conditional independent), and hence a language model is needed as a post-processing step.
sent10: Different from the basic CTC, Xu et al. [62] proposed LCANet that feeds the encoded spatio-temporal features into a cascaded attention CTC decoder.
sent11: The introduction of an attention mechanism improves the defect of the conditional independence assumption CTC in hidden neural layers.
sent12: Another assumption of this approach is that it assumes a monotonic ordering between input and output sequences, which is suitable for VSR but not for machine translation.
sent13: The second type is sequence-to-sequence (seq2seq) models that first read the whole input sequence before predicting the output sentence.
sent14: A number of works adopted this approach for speech recognition [142].
sent15: Chan et al. [143] proposed an elegant seq2seq method to transcribe audio signal to characters.
sent16: Seq2seq models decode an output symbol at time t (e.g., phonemes, characters, words) conditioned on previous outputs 1, ..., t − 1.
sent17: Thus, unlike CTC-based models, the model implicitly learns a language model over output symbols, and no further processing is required.
sent18: However, it has been shown [144] that it is beneficial to incorporate an external language model in the decoding of seq2seq models as well.
sent19: Chung et al. [28] proposed the WAS (Watch, Attend and Spell) model, which is a classical seq2seq VSR model.
sent20: With the help of attention mechanism, WAS model is more capable of capturing long-term dependency.
sent21: Based on the transformer backbone architecture, Afouras et al. [6] have deeply analyzed the pros and cons of the CTC model and the seq2seq model for VSR.
sent22: Generally, the seq2seq model performs well than the CTC model in the sentence-level VSR task.
sent23: But, the seq2seq model needs more training time and inference time.
sent24: Besides, the CTC model generalizes better and adapts faster as the sequence lengths are increased.
sent25: Besides the above label-level supervised learning paradigms, feature-level supervised learning is also widely explored in VSR.
sent26: Knowledge distillation [145]","1. What are the two mainstream VSR tasks? sent1
    1.1. What is the word-level VSR task and how is it typically trained? sent2
    1.2. What is the sentence-level VSR task and why is it more complicated than the word-level task? sent3, sent4
2. How can supervised learning of end-to-end sentence-level VSR tasks be divided? sent5
    2.1. What is the first type of supervised learning approach for sentence-level VSR tasks? sent6
        2.1.1. What is a popular version of this approach and how does it work? sent7, sent8
        2.1.2. What is the main weakness of CTC and how is it addressed by LCANet? sent9, sent10, sent11
        2.1.3. What is another assumption of the CTC approach? sent12
    2.2. What is the second type of supervised learning approach for sentence-level VSR tasks? sent13
        2.2.1. How do seq2seq models differ from CTC-based models? sent14, sent15, sent16, sent17
        2.2.2. Is it beneficial to incorporate an external language model in seq2seq models? sent18
        2.2.3. What is the WAS model and what advantage does it have? sent19, sent20
        2.2.4. How do the CTC and seq2seq models compare in terms of performance and requirements? sent21, sent22, sent23, sent24
3. Besides label-level supervised learning, what other type of supervised learning is explored in VSR? sent25"
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,Travelers' behavior analysis,8,"The analysis of people's transportation information is considered an important issue in various fields, such as city planning, architectural planning, car navigation, sightseeing administration, crime prevention, and tracing the spread of infection of epidemics. In this section, we focus on the analysis of travelers' behavior.

Ishino et al. [15] proposed a method to extract people's transportation information from automatically identified travel blogs written in Japanese [25]. They used machine learning to extract information, such as ""departure place"", ""destination"", or ""transportation device"", from travel blog entries. First, the tags used in their examination are defined.

• FROM tag indicates the departure place.

• TO tag indicates the destination.

• VIA tag indicates the route.

• METHOD tag indicates the transportation device.

• TIME tag indicates the time of transportation.

The following is a tagged example.

It took <TIME>five hours< /TIME> to travel from <FROM>Hiroshima< /FROM> to<TO>Osaka< /TO> by <METHOD>bus< /METHOD>.

They formulated the task of identifying the class of each word in a given sentence and solved it using machine learning. For the machine learning method, they used CRF [20], in the same way as Nanba et al. [25], which we mentioned in Section 2.2. The CRF-based method identifies the class of each entry. Features and tags are used in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry. They used the value k = 4 7 , which was determined via a pilot study. They used the following features for machine learning.

• A word.

• The part of speech to which the word belongs (noun, verb, adjective, etc.).

• Whether the word is a quotation mark.

• Whether the word is a cue phrase.

The details of cue phrases, together with the number of cue phrases of the given type, are shown as follows.

The analysis of people's transportation information is considered an important issue in various fields, such as city planning, architectural planning, car navigation, sightseeing administration, crime prevention, and tracing the spread of infection of epidemics. In this section, we focus on the analysis of travelers' behavior.

Ishino et al. [15] proposed a method to extract people's transportation information from automatically identified travel blogs written in Japanese [25]. They used machine learning to extract information, such as ""departure place"", ""destination"", or ""transportation device"", from travel blog entries. First, the tags used in their examination are defined.

• FROM tag indicates the departure place.

• TO tag indicates the destination.

• VIA tag indicates the route.

• METHOD tag indicates the transportation device.

• TIME tag indicates the time of transportation.

The following is a tagged example.

It took <TIME>five hours< /TIME> to travel from <FROM>Hiroshima< /FROM> to<TO>Osaka< /TO> by <METHOD>bus< /METHOD>.

They formulated the task of identifying the class of each word in a given sentence and solved it using machine learning. For the machine learning method, they used CRF [20], in the same way as Nanba et al. [25], which we mentioned in Section 2.2. The CRF-based method identifies the class of each entry. Features and tags are used in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry. They used the value k = 4 7 , which was determined via a pilot study. They used the following features for machine learning.

• A word.

• The part of speech to which the word belongs (noun, verb, adjective, etc.).

• Whether the word is a quotation mark.

• Whether the word is a cue phrase.

The details of cue phrases, together with the number of cue phrases of the given type, are shown as follows.","sent1: The analysis of people's transportation information is considered an important issue in various fields, such as city planning, architectural planning, car navigation, sightseeing administration, crime prevention, and tracing the spread of infection of epidemics.
sent2: In this section, we focus on the analysis of travelers' behavior.
sent3: Ishino et al. [15] proposed a method to extract people's transportation information from automatically identified travel blogs written in Japanese [25].
sent4: They used machine learning to extract information, such as ""departure place"", ""destination"", or ""transportation device"", from travel blog entries.
sent5: First, the tags used in their examination are defined.
sent6: • FROM tag indicates the departure place.
sent7: • TO tag indicates the destination.
sent8: • VIA tag indicates the route. • METHOD tag indicates the transportation device.
sent9: • TIME tag indicates the time of transportation.
sent10: The following is a tagged example.
sent11: It took <TIME>five hours< /TIME> to travel from <FROM>Hiroshima< /FROM> to<TO>Osaka< /TO>
sent12: by <METHOD>bus< /METHOD>. They formulated the task of identifying the class of each word in a given sentence and solved it using machine learning.
sent13: For the machine learning method, they used CRF [20], in the same way as Nanba et al. [25], which we mentioned in Section 2.2.
sent14: The CRF-based method identifies the class of each entry.
sent15: Features and tags are used in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry.
sent16: They used the value k = 4 7 , which was determined via a pilot study.
sent17: They used the following features for machine learning.
sent18: • A word. • The part of speech to which the word belongs (noun, verb, adjective, etc.).• Whether the word is a quotation mark.
sent19: • Whether the word is a cue phrase.
sent20: The details of cue phrases, together with the number of cue phrases of the given type, are shown as follows.
sent21: The analysis of people's transportation information is considered an important issue in various fields, such as city planning, architectural planning, car navigation, sightseeing administration, crime prevention, and tracing the spread of infection of epidemics.
sent22: In this section, we focus on the analysis of travelers' behavior.
sent23: Ishino et al. [15] proposed a method to extract people's transportation information from automatically identified travel blogs written in Japanese [25].
sent24: They used machine learning to extract information, such as ""departure place"", ""destination"", or ""transportation device"", from travel blog entries.
sent25: First, the tags used in their examination are defined.
sent26: • FROM tag indicates the departure place.
sent27: • TO tag indicates the destination.
sent28: • VIA tag indicates the route. • METHOD tag indicates the transportation device.
sent29: • TIME tag indicates the time of transportation.
sent30: The following is a tagged example.
sent31: It took <TIME>five hours< /TIME> to travel from <FROM>Hiroshima< /FROM> to<TO>Osaka< /TO>
sent32: by <METHOD>bus< /METHOD>. They formulated the task of identifying the class of each word in a given sentence and solved it using machine learning.
sent33: For the machine learning method, they used CRF [20], in the same way as Nanba et al. [25], which we mentioned in Section 2.2.
sent34: The CRF-based method identifies the class of each entry.
sent35: Features and tags are used in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry.
sent36: They used the value k = 4 7 , which was determined via a pilot study.
sent37: They used the following features for machine learning.
sent38: • A word. • The part of speech to which the word belongs (noun, verb, adjective, etc.).• Whether the word is a quotation mark.
sent39: • Whether the word is a cue phrase.
sent40: The details of cue phrases, together with the number of cue phrases of the given type, are shown as follows.","1. Why is the analysis of people's transportation information considered important? sent1
2. What is the focus of this section? sent2
3. What method did Ishino et al. propose for extracting transportation information? sent3
4. What information do they extract from travel blog entries? sent4
5. How are the tags used in their examination defined? sent5
    5.1. What does the FROM tag indicate? sent6
    5.2. What does the TO tag indicate? sent7
    5.3. What do the VIA and METHOD tags indicate? sent8
    5.4. What does the TIME tag indicate? sent9
6. Can you provide a tagged example of the extracted information? sent10, sent11, sent12
7. What machine learning method did they use to identify the class of each word? sent13
8. How does the CRF-based method identify the class of each entry? sent14
9. What features and tags are used in the CRF method? sent15
10. What value of k was used, and how was it determined? sent16
11. What features were used for machine learning? sent17, sent18, sent19
12. What details are provided about cue phrases? sent20"
237642905,"Aishna Gupta, Anuska Rakshit. A Technical Survey on the Modeling of Topical Bot",https://www.semanticscholar.org/paper/2846dd9c7578f680675b0d70b5f5cffe8fb8612c,3) An intermediate representation -A method has been,8,"proposed to use more flexible intermediate representations rather than encoding X ⊕ C using a fixed-size vector. This enhances the representation capability to address the one-to-many issue in the dialog system and to improve the interpretability of the representation to more readily control the response generation. 4) An encoder that encodes user input and dialog context -It encodes richer information and generates more informative responses. Moreover, it can be used to extract topic words using LDA and encodes such words in a topic-aware model. Dialog Models based on grounded knowledge: Data stored in Wikipedia or Freebase are referred to as Knowledge facts.

A knowledge grounded open-domain dialog system can identify the entities and topics mentioned in user input further linking them into real-world facts, retrieve related background information, and thereby respond to users in a proactive way that is by recommending new, related topics to discuss [4].

A knowledge-grounded model assists to generate a response by integrating few retrieved posts that are relevant to the input. The generation model (one of the systems in which the knowledge-grounded model is used) can generate a word in response from the context or the knowledge base. The DAN performs competitively with more complicated neural networks that explicitly model semantic and syntactic compositionality. [7] Then decoding is done which includes a graph attention mechanism in which the model initially takes care of a knowledge graph and then the decoder chooses a word to generate from either the graph or the common lexis. The plan is to provide the model with appropriate long-form text as a input source of external knowledge. A reading comprehension is performed on this text in response to each conversational turn, thereby resulting in a more focused integration of external knowledge than prior approaches. Most of the studies focus on two challenges: 1) Knowledge-aware generation -providing the required knowledge into a generated response. 2) Knowledge selection -selecting appropriate knowledge that can be incorporated in the next response given in the dialog context and previously-selected knowledge. Consistency: While searching for blogs on a specific topic, it has been observed that information seekers prefer blogs that place a central focus on that topic rather than mentioning the topic in a diffused format [4]. Therefore, to be able to focus on a particular topic consistency is needed. A topical dialog system needs to encapsulate consistent behaviours so that it can gain the user's confidence and trust. However, there are three major consistency issues which include.

Persona Consistency: Can be grouped into two categories that address the dialog models:

1) Implicit personalization -the persona is implicitly represented by a persona vector-like proposing a ranking-based approach to integrate a personal knowledge base and user interests in a dialogue system. It utilizes learned user persona features to capture userlevel consistency implicitly. 2) Explicit personalization-to generate personalitycoherent responses given a pre-specified user profile. This explicit persona model controls the conversation generation using explicitly defined user profiles. The chatbot's persona is defined by a key-value table which consists of name, gender, age, occupation, and many more. During generation, firstly the model chooses a keyvalue from the profile and then a response is decoded from the chosen key-value pair forward and backward.

Stylistic Response Generation: It is a form of personalization in conversation. It is closely related to domain adaptation and transfer learning. There are two main challenges:

1) To construct training data having pairs of responses that are of the same content but are usually in different approach. 2) To extract content and style in representation. In order to solve the above problems an idea was proposed to train a general conversation model on a large corpus in the source domain then to transfer the model to a new speaker or target domain using small amounts of personalized (or stylistic) data in the target domain [4]. Some of the other proposed models include:

1) A two-phase transfer learning approach i.e., initialization then adaptation which can be used to generate personalized responses. Furthermore, a quasi-Turing test method can be implemented to evaluate the performance of the generated responses. 2) A multi-task learning approach where the response generation and utterance representation are treated as two sub-tasks for speaker role adaptation. Contextual Consistency: Earlier work focused on representing better dialog contexts using hierarchical models, this was viewed as implicit modelling of contextual consistency. Recently, contextual consistency is noticed as a natural language inference (NLI) problem.

Interactiveness: Interactiveness refers to the system's capacity to execute complex social objectives such as entertainment and conforming by optimizing its behaviours and applying dialog strategies in multi-turn conversation. From a technical viewpoint, interactiveness mainly involves sentiment and emotion detection, dialog state tracking, topic detection and recommendation, dialog policy learning, and controllable response generation. However, optimizing the behaviours and strategies of a dialog system to maximize long-term user engagement and accomplish long-term, complex goals is still a major issue. To overcome this and improve interactiveness, it is important to understand the user's emotion and optimize the system's behaviour and interaction strategy in multi-turn conversations.

User Emotion Model: Emotion perception and expression is an important factor in building a human-like machine. To generate emotional responses, we have Emotional Chatting Machine (ECM) ECM consists of three components: 1) While decoding the internal emotional state gradually decays and finally reach zero. 2) Each decoding position is fed with an emotion category embedding. 3) An external memory that permits the model to select emotional or generic words. Patterns have been observed in human-human conversations such as empathy and comfort, which would inspire a more delicate but firm design of emotional communication between humans and machines.

Another method of affective response generation was developed which consists of three components:

1) For searching effective responses -the effective beam search algorithm. 2) To increase or decrease the affective consistency between a post and a response -the effective loss functions. 3) To supplement word vectors -the effective vectors based on Dominance dimensions. To generate the reviews of a particular polarity, a multiclass generative adversarial network was proposed which consists of generators for multiple polarities and discriminators. A challenging issue in this is emotion representation.

With the massive increase in social interactions on online social networks, there has also been an increase of hateful activities that exploit such infrastructure. Detecting such hateful speech is important for analyzing public sentiment of a group of users towards another group, and for discouraging associated wrongful activities. [6] Emotional representation is also a very major issue in the existing model. One of the simple approaches is to project implicit subtle emotion label to a vector. However, this approach failed to explicitly model the user's emotional changeover during a conversation. it would try to cheer the user up through e.g., shifting to new discussion that are more comfortable for both parties.

This model is crucial for a dialog system to establish a long-term connection with a user because the user is more interested to engage with the system if the system can always detect a negative transformation in her emotion during the conversation it would try to cheer the user up through e.g., shifting to new discussion that are more comfortable for both parties. Strategy of Conversation Behaviour: A framework was built to capture the abstract emotions such as politeness strategies freedom concepts that are used to start a conversation and examined their relation. When is framework is applied in a controlled environment it is possible to detect early warning signs of antisocial behaviour in online conversation. Firstly, to detect signs of deadlock a retrieval-based method is proposed and then the response is received containing the entities related to the input.

A proactive suggestion method was proposed for the user that would provide a look-ahead post in addition to the system response, circumstances, and prior generated response. The user can use the generated post directly or type a new one during the conversation. However, asking upright questions in conversation can be shown as an important proactive behaviour. Thus, a typed decoder is proposed to generate meaningful questions by predicting a type distribution over topic words at each decoding position. The final output distribution was modelled by the type distribution, leading to a strong control over the question to be generated. In addition, a dataset of clarification questions was evaluated and a neural network model was built for ranking clarification questions. The most important drift for future research on this field are:

1) The comprehensive investigation of conversation behaviours in the human-human dialog.

2) The second is to create a more sophisticated real-world dialog setting for system development and evaluation [4]. J. SPEAKER GENDER ANALYSIS Gender detection systems based on Gaussian Mixture Models, i-vectors and Convolutional Neural Networks (CNN) were trained using an internal database of 2,284 French speakers and evaluated using REPERE challenge corpus out of which the CNN system obtained the best performance with a frame-level gender detection F-measure of 96.52 and a hourly women speaking time percentage error below 0.6% [15]. The data is extracted from SwDA and analysis is done on the speaker gender information to check whether latent modes of unsupervised learning in the dynamic speaker model could pick up some gender language variations. The process involves gathering the latent mode association scores for every 32 modes i.e. the no of modes differs according to the input data, which was computed in Latent Model Analyzer.

Then to test the associate score distributions of male vs female utterances group mean tests for individual modes are carried out. The strength of the difference is measured using the most effective way to measure effect size -The cohen-d score.

Cohen's D Formula is computed as:

D= (M1-M2) / SP Where, M1 and M2 denotes the sample means for groups 1 and 2 SP denotes the pooled estimated population standard deviation.

Lastly, the p-value is computed using the Mann-Whitney U test. The Mann-Whitney U test is generally used to work around the underlying assumption of normality in parametric tests. However, this method is used when the validity of the assumptions of the t-test is not certain thus this test has wider applicability. Cohen's d relies on the pooled standard deviation (the denominator of equation) to standardize the measure of the ES; it assumes the groups having (roughly) equal size and variance [16].

Therefore, the group mean tests are carried out on the following three sets: 1) All conversations, 2) Conversations involving either males or females, and 3) Conversations involving both genders. The Cohen's D scores for the group mean tests are shown below graphs.

Previously in Telephone Conversations, numerous researches were done to analyse the difference between genders. The main motive in analysing the gender linguistic differences in genders was of two folds.

1) From the scientific perspective, it can increase the understanding of language production. 2) From the engineering perspective, the performance of a number of natural language processing tasks, such as text classification, machine translation, or automatic speech recognition by training better language models can be improved. The methods which are used for characterizing the differences between genders and gender pairs are similar to the processes used for text classification. Studies have shown that the most effective ways for characterizing the differences between gender categories. Firstly, the transcript of each speaker has to be classified i.e., based on the appropriate gender category. And the second approach involves the application of feature selection methods, this would reveal the most characteristic features for each gender.

From each set, the most female-like mode (with the most positive Cohen-d score) and the most male-like mode (with the most negative Cohen's-d score) are identified. The significant advantage of using gender information for automatic speech recognition is that it can be robustly detected using acoustic features.","sent1: proposed to use more flexible intermediate representations rather than encoding X ⊕ C using a fixed-size vector.
sent2: This enhances the representation capability to address the one-to-many issue in the dialog system and to improve the interpretability of the representation to more readily control the response generation.
sent3: 4) An encoder that encodes user input and dialog context -It encodes richer information and generates more informative responses.
sent4: Moreover, it can be used to extract topic words using LDA and encodes such words in a topic-aware model.
sent5: Dialog Models based on grounded knowledge: Data stored in Wikipedia or Freebase are referred to as Knowledge facts.
sent6: A knowledge grounded open-domain dialog system can identify the entities and topics mentioned in user input further linking them into real-world facts, retrieve related background information, and thereby respond to users in a proactive way that is by recommending new, related topics to discuss [4].
sent7: A knowledge-grounded model assists to generate a response by integrating few retrieved posts that are relevant to the input.
sent8: The generation model (one of the systems in which the knowledge-grounded model is used) can generate a word in response from the context or the knowledge base.
sent9: The DAN performs competitively with more complicated neural networks that explicitly model semantic and syntactic compositionality.
sent10: [7] Then decoding is done which includes a graph attention mechanism in which the model initially takes care of a knowledge graph and then the decoder chooses a word to generate from either the graph or the common lexis.
sent11: The plan is to provide the model with appropriate long-form text as a input source of external knowledge.
sent12: A reading comprehension is performed on this text in response to each conversational turn, thereby resulting in a more focused integration of external knowledge than prior approaches.
sent13: Most of the studies focus on two challenges: 1) Knowledge-aware generation -providing the required knowledge into a generated response.
sent14: 2) Knowledge selection -selecting appropriate knowledge that can be incorporated in the next response given in the dialog context and previously-selected knowledge.
sent15: Consistency: While searching for blogs on a specific topic, it has been observed that information seekers prefer blogs that place a central focus on that topic rather than mentioning the topic in a diffused format [4].
sent16: Therefore, to be able to focus on a particular topic consistency is needed.
sent17: A topical dialog system needs to encapsulate consistent behaviours so that it can gain the user's confidence and trust.
sent18: However, there are three major consistency issues which include.
sent19: Persona Consistency: Can be grouped into two categories that address the dialog models:1) Implicit personalization -the persona is implicitly represented by a persona vector-like proposing a ranking-based approach to integrate a personal knowledge base and user interests in a dialogue system.
sent20: It utilizes learned user persona features to capture userlevel consistency implicitly.
sent21: 2) Explicit personalization-to generate personalitycoherent responses given a pre-specified user profile.
sent22: This explicit persona model controls the conversation generation using explicitly defined user profiles.
sent23: The chatbot's persona is defined by a key-value table which consists of name, gender, age, occupation, and many more.
sent24: During generation, firstly the model chooses a keyvalue from the profile and then a response is decoded from the chosen key-value pair forward and backward.
sent25: Stylistic Response Generation: It is a form of personalization in conversation.
sent26: It is closely related to domain adaptation and transfer learning.
sent27: There are two main challenges:1) To construct training data having pairs of responses that are of the same content but are usually in different approach.
sent28: 2) To extract content and style in representation.
sent29: In order to solve the above problems an idea was proposed to train a general conversation model on a large corpus in the source domain then to transfer the model to a new speaker or target domain using small amounts of personalized (or stylistic) data in the target domain [4].
sent30: Some of the other proposed models include:1)
sent31: A two-phase transfer learning approach i.e., initialization then adaptation which can be used to generate personalized responses.
sent32: Furthermore, a quasi-Turing test method can be implemented to evaluate the performance of the generated responses.
sent33: 2) A multi-task learning approach where the response generation and utterance representation are treated as two sub-tasks for speaker role adaptation.
sent34: Contextual Consistency: Earlier work focused on representing better dialog contexts using hierarchical models, this was viewed as implicit modelling of contextual consistency.
sent35: Recently, contextual consistency is noticed as a natural language inference (NLI) problem.
sent36: Interactiveness: Interactiveness refers to the system's capacity to execute complex social objectives such as entertainment and conforming by optimizing its behaviours and applying dialog strategies in multi-turn conversation.
sent37: From a technical viewpoint, interactiveness mainly involves sentiment and emotion detection, dialog state tracking, topic detection and recommendation, dialog policy learning, and controllable response generation.
sent38: However, optimizing the behaviours and strategies of a dialog system to maximize long-term user engagement and accomplish long-term, complex goals is still a major issue.
sent39: To overcome this and improve interactiveness, it is important to understand the user's emotion and optimize the system's behaviour and interaction strategy in multi-turn conversations.
sent40: User Emotion Model: Emotion perception and expression is an important factor in building a human-like machine.
sent41: To generate emotional responses, we have Emotional Chatting Machine (ECM) ECM consists of three components: 1) While decoding the internal emotional state gradually decays and finally reach zero.
sent42: 2) Each decoding position is fed with an emotion category embedding.
sent43: 3) An external memory that permits the model to select emotional or generic words.
sent44: Patterns have been observed in human-human conversations such as empathy and comfort, which would inspire a more delicate but firm design of emotional communication between humans and machines.
sent45: Another method of affective response generation was developed which consists of three components:1) For searching effective responses -the effective beam search algorithm.
sent46: 2) To increase or decrease the affective consistency between a post and a response -the effective loss functions.
sent47: 3) To supplement word vectors -the effective vectors based on Dominance dimensions.
sent48: To generate the reviews of a particular polarity, a multiclass generative adversarial network was proposed which consists of generators for multiple polarities and discriminators.
sent49: A challenging issue in this is emotion representation.
sent50: With the massive increase in social interactions on online social networks, there has also been an increase of hateful activities that exploit such infrastructure.
sent51: Detecting such hateful speech is important for analyzing public sentiment of a group of users towards another group, and for discouraging associated wrongful activities.
sent52: [6] Emotional representation is also a very major issue in the existing model.
sent53: One of the simple approaches is to project implicit subtle emotion label to a vector.
sent54: However, this approach failed to explicitly model the user's emotional changeover during a conversation.
sent55: it would try to cheer the user up through e.g., shifting to new discussion that are more comfortable for both parties.
sent56: This model is crucial for a dialog system to establish a long-term connection with a user because the user is more interested to engage with the system if the system can always detect a negative transformation in her emotion during the conversation it would try to cheer the user up through e.g., shifting to new discussion that are more comfortable for both parties.
sent57: Strategy of Conversation Behaviour: A framework was built to capture the abstract emotions such as politeness strategies freedom concepts that are used to start a conversation and examined their relation.
sent58: When is framework is applied in a controlled environment it is possible to detect early warning signs of antisocial behaviour in online conversation.
sent59: Firstly, to detect signs of deadlock a retrieval-based method is proposed and then the response is received containing the entities related to the input.
sent60: A proactive suggestion method was proposed for the user that would provide a look-ahead post in addition to the system response, circumstances, and prior generated response.
sent61: The user can use the generated post directly or type a new one during the conversation.
sent62: However, asking upright questions in conversation can be shown as an important proactive behaviour.
sent63: Thus, a typed decoder is proposed to generate meaningful questions by predicting a type distribution over topic words at each decoding position.
sent64: The final output distribution was modelled by the type distribution, leading to a strong control over the question to be generated.
sent65: In addition, a dataset of clarification questions was evaluated and a neural network model was built for ranking clarification questions.
sent66: The most important drift for future research on this field are:1)
sent67: The comprehensive investigation of conversation behaviours in the human-human dialog.2) The second is to create a more sophisticated real-world dialog setting for system development and evaluation [4].
sent68: J. SPEAKER GENDER ANALYSIS Gender detection systems based on Gaussian Mixture Models, i-vectors and Convolutional Neural Networks (CNN) were trained using an internal database of 2,284 French speakers and evaluated using REPERE challenge corpus out of which the CNN system obtained the best performance with a frame-level gender detection F-measure of 96.52 and a hourly women speaking time percentage error below 0.6% [15].
sent69: The data is extracted from SwDA and analysis is done on the speaker gender information to check whether latent modes of unsupervised learning in the dynamic speaker model could pick up some gender language variations.
sent70: The process involves gathering the latent mode association scores for every 32 modes i.e. the no of modes differs according to the input data, which was computed in Latent Model Analyzer.
sent71: Then to test the associate score distributions of male vs female utterances group mean tests for individual modes are carried out.
sent72: The strength of the difference is measured using the most effective way to measure effect size -The cohen-d score.
sent73: Cohen's D Formula is computed as:D= (M1-M2) / SP Where, M1 and M2 denotes the sample means for groups 1 and 2 SP denotes the pooled estimated population standard deviation.
sent74: Lastly, the p-value is computed using the Mann-Whitney U test.
sent75: The Mann-Whitney U test is generally used to work around the underlying assumption of normality in parametric tests.
sent76: However, this method is used when the validity of the assumptions of the t-test is not certain thus this test has wider applicability.
sent77: Cohen's d relies on the pooled standard deviation (the denominator of equation) to standardize the measure of the ES; it assumes the groups having (roughly) equal size and variance [16].
sent78: Therefore, the group mean tests are carried out on the following three sets: 1) All conversations, 2) Conversations involving either males or females, and 3) Conversations involving both genders.
sent79: The Cohen's D scores for the group mean tests are shown below graphs.
sent80: Previously in Telephone Conversations, numerous researches were done to analyse the difference between genders.
sent81: The main motive in analysing the gender linguistic differences in genders was of two folds.
sent82: 1) From the scientific perspective, it can increase the understanding of language production.
sent83: 2) From the engineering perspective, the performance of a number of natural language processing tasks, such as text classification, machine translation, or automatic speech recognition by training better language models can be improved.
sent84: The methods which are used for characterizing the differences between genders and gender pairs are similar to the processes used for text classification.
sent85: Studies have shown that the most effective ways for characterizing the differences between gender categories.
sent86: Firstly, the transcript of each speaker has to be classified i.e., based on the appropriate gender category.
sent87: And the second approach involves the application of feature selection methods, this would reveal the most characteristic features for each gender.
sent88: From each set, the most female-like mode (with the most positive Cohen-d score) and the most male-like mode (with the most negative Cohen's-d score) are identified.
sent89: The significant advantage of using gender information for automatic speech recognition is that it can be robustly detected using acoustic features.","1. What method has been proposed to enhance representation capability in dialog systems? sent1
    1.1. What issues does this method address in dialog systems? sent2
2. What is the role of an encoder in dialog systems? sent3
    2.1. How can topic words be extracted and encoded in a topic-aware model? sent4
3. What are knowledge facts in dialog models based on grounded knowledge? sent5
    3.1. How does a knowledge-grounded open-domain dialog system function? sent6
    3.2. How does a knowledge-grounded model assist in response generation? sent7
        3.2.1. How does the generation model use context or knowledge base for response generation? sent8
4. How does the DAN perform compared to other neural networks? sent9
5. What mechanism is involved in decoding with a knowledge graph? sent10
6. What is the plan for providing models with external knowledge? sent11
    6.1. How is reading comprehension performed in response to conversational turns? sent12
7. What are the two main challenges in knowledge-aware generation? sent13, sent14
8. Why is consistency important in topical dialog systems? sent15, sent16
    8.1. What are the three major consistency issues in dialog systems? sent18
        8.1.1. What is persona consistency and how is it addressed? sent19, sent20
        8.1.2. How is explicit personalization achieved in dialog systems? sent21, sent22
            8.1.2.1. How is a chatbot's persona defined and used during generation? sent23, sent24
9. What is stylistic response generation in conversation? sent25
    9.1. What are the challenges in constructing training data for stylistic response generation? sent27, sent28
    9.2. What idea was proposed to solve the challenges in stylistic response generation? sent29
        9.2.1. What are some other proposed models for personalized response generation? sent30, sent31
            9.2.1.1. How can the performance of generated responses be evaluated? sent32
            9.2.1.2. What is the multi-task learning approach in response generation? sent33
10. How was contextual consistency viewed in earlier work? sent34
    10.1. How is contextual consistency currently perceived? sent35
11. What does interactiveness refer to in dialog systems? sent36
    11.1. What technical aspects are involved in interactiveness? sent37
    11.2. What is a major issue in optimizing dialog system behaviors? sent38
        11.2.1. How can interactiveness be improved in dialog systems? sent39
12. What is the role of a user emotion model in dialog systems? sent40
    12.1. What components make up the Emotional Chatting Machine (ECM)? sent41, sent42, sent43
    12.2. What patterns have been observed in human-human conversations? sent44
13. What method was developed for affective response generation? sent45
    13.1. What components are involved in this method? sent46, sent47
14. What was proposed to generate reviews of a particular polarity? sent48
15. What is a challenging issue in emotion representation? sent49
16. Why is detecting hateful speech important? sent51
17. What is a simple approach to emotion representation? sent53
    17.1. What is a limitation of this approach? sent54
18. What is crucial for a dialog system to establish a long-term connection with a user? sent56
19. What framework was built to capture abstract emotions in conversation? sent57
    19.1. What can be detected when this framework is applied in a controlled environment? sent58
20. What method is proposed to detect signs of deadlock in conversation? sent59
    20.1. What is a proactive suggestion method for users? sent60
        20.1.1. How can users interact with the generated post? sent61
21. What is an important proactive behavior in conversation? sent62
    21.1. How is a typed decoder used in generating questions? sent63
        21.1.1. How is the final output distribution modeled? sent64
22. What was evaluated to rank clarification questions? sent65
23. What are the important future research directions in this field? sent66, sent67
24. What systems were used for gender detection in speaker analysis? sent68
    24.1. What data was used for analysis in speaker gender information? sent69
    24.2. How are group mean tests conducted in speaker analysis? sent71
        24.2.1. How is the strength of the difference measured? sent72
        24.2.2. What is the formula for Cohen's D? sent73
        24.2.3. How is the p-value computed? sent74
25. What is the advantage of using gender information in automatic speech recognition? sent89"
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,Dataset,18,"Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9. Additional data contained in each dataset. MeSH [115] and RadLex [81] are sets of medical concepts. MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools. Manual means manually annotated by experts. In all cases, the localization information was manually annotated by experts.    Table 11. Summary of optimization strategies used in the literature.

Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9. Additional data contained in each dataset. MeSH [115] and RadLex [81] are sets of medical concepts. MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools. Manual means manually annotated by experts. In all cases, the localization information was manually annotated by experts.    Table 11. Summary of optimization strategies used in the literature.","sent1: Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9.
sent2: Additional data contained in each dataset.
sent3: MeSH [115] and RadLex [81] are sets of medical concepts.
sent4: MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools.
sent5: Manual means manually annotated by experts.
sent6: In all cases, the localization information was manually annotated by experts.
sent7: Table 11. Summary of optimization strategies used in the literature.
sent8: Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9.
sent9: Additional data contained in each dataset.
sent10: MeSH [115] and RadLex [81] are sets of medical concepts.
sent11: MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools.
sent12: Manual means manually annotated by experts.
sent13: In all cases, the localization information was manually annotated by experts.
sent14: Table 11. Summary of optimization strategies used in the literature.","1. What are MeSH and RadLex? sent3
2. What are some examples of automatic labeler tools mentioned in the section? sent4
3. How is the term ""Manual"" defined in the context of this section? sent5
4. How was the localization information annotated in all cases? sent6"
195345457,Emotionally-Aware Chatbots: A Survey,https://www.semanticscholar.org/paper/c131665638feb8c11f936989ffc6187317593b41,BUILDING EMOTIONALLY-AWARE CHATBOT (EAC),5,"As we mentioned before that emotion is an essential aspect of building humanize chatbot. The rise of the emotionally-aware chatbot is started by Parry [8] in early 1975. Now, most of EAC development exploits neural-based model. In this section, we will try to review previous works which focus on EAC development. Table 1 summarizes this information includes the objective and exploited approach of each work. In early development, EAC is designed by using a rule-based approach. However, in recent years mostly EAC exploit neural-based approach. Studies in EAC development become a hot topic start from 2017, noted by the first shared task in Emotion Generation Challenge on NLPCC 2017 [20]. Based on Table 1 this research line continues to gain massive attention from scholars in the latest years. Based on Table 1, we can see that most of all recent EAC was built by using encoder-decoder architecture with sequence-to-sequence learning. These seq2seq learning models maximize the likelihood of response and are prepared to incorporate rich data to generate an appropriate answer. Basic seq2seq architecture structured of two recurrent neural networks (RNNs), one as an encoder processing the input and one as a decoder generating the response. long short term memory (LSTM) or gated recurrent unit (GRU) was the most dominant variant of RNNs which used to learn the conversational dataset in these models. Some studies also tried to model this task as a reinforcement learning task, in order to get more generic responses and let the chatbot able to achieve successful long-term conversation. Attention mechanism was also introduced in this report 6 . This mechanism will allow the decoder to focus only on some important parts in the input at every decoding step.

Another vital part of building EAC is emotion classifier to detect emotion contained in the text to produce a more meaningful response. Emotion detection is a well-established task in natural language processing research area. This task was promoted in two latest series of SemEval-2018 (Task 1) and SemEval-2019 (Task 3). Some tasks were focusing on classifying utterance into several categories of emotion [34]. However, there is also a task which trying to predict the emotion intensities contained in the text [33]. In the early development of emotion classifier, most of the studies proposed to use traditional machine-learning approach. However, the neural-based approach is able to gain better performance, which leads more scholars to exploit it to deal with this task. In chatbot, the system will generate several responses based on several emotion categories. Then the system will respond with the most appropriate emotion based on emotion detected on posted utterance by emotion classifier. Based on Table 1, studies have different emotion categories based on their focus and objective in building chatbots.","sent1: As we mentioned before that emotion is an essential aspect of building humanize chatbot.
sent2: The rise of the emotionally-aware chatbot is started by Parry [8] in early 1975.
sent3: Now, most of EAC development exploits neural-based model.
sent4: In this section, we will try to review previous works which focus on EAC development.
sent5: Table 1 summarizes this information includes the objective and exploited approach of each work.
sent6: In early development, EAC is designed by using a rule-based approach.
sent7: However, in recent years mostly EAC exploit neural-based approach.
sent8: Studies in EAC development become a hot topic start from 2017, noted by the first shared task in Emotion Generation Challenge on NLPCC 2017 [20].
sent9: Based on Table 1 this research line continues to gain massive attention from scholars in the latest years.
sent10: Based on Table 1, we can see that most of all recent EAC was built by using encoder-decoder architecture with sequence-to-sequence learning.
sent11: These seq2seq learning models maximize the likelihood of response and are prepared to incorporate rich data to generate an appropriate answer.
sent12: Basic seq2seq architecture structured of two recurrent neural networks (RNNs), one as an encoder processing the input and one as a decoder generating the response.
sent13: long short term memory (LSTM) or gated recurrent unit (GRU) was the most dominant variant of RNNs which used to learn the conversational dataset in these models.
sent14: Some studies also tried to model this task as a reinforcement learning task, in order to get more generic responses and let the chatbot able to achieve successful long-term conversation.
sent15: Attention mechanism was also introduced in this report 6 .
sent16: This mechanism will allow the decoder to focus only on some important parts in the input at every decoding step.
sent17: Another vital part of building EAC is emotion classifier to detect emotion contained in the text to produce a more meaningful response.
sent18: Emotion detection is a well-established task in natural language processing research area.
sent19: This task was promoted in two latest series of SemEval-2018 (Task 1) and SemEval-2019 (Task 3).
sent20: Some tasks were focusing on classifying utterance into several categories of emotion [34].
sent21: However, there is also a task which trying to predict the emotion intensities contained in the text [33].
sent22: In the early development of emotion classifier, most of the studies proposed to use traditional machine-learning approach.
sent23: However, the neural-based approach is able to gain better performance, which leads more scholars to exploit it to deal with this task.
sent24: In chatbot, the system will generate several responses based on several emotion categories.
sent25: Then the system will respond with the most appropriate emotion based on emotion detected on posted utterance by emotion classifier.
sent26: Based on Table 1, studies have different emotion categories based on their focus and objective in building chatbots.","1. Why is emotion considered an essential aspect of building humanized chatbots? sent1
2. When did the development of emotionally-aware chatbots begin, and who initiated it? sent2
3. What approach is predominantly used in the development of emotionally-aware chatbots (EAC) today? sent3
4. What does Table 1 summarize in the context of EAC development? sent5
5. How were early emotionally-aware chatbots designed? sent6
6. What approach has become more common in recent years for EAC development? sent7
7. When did studies in EAC development become a hot topic, and what event marked this? sent8
8. What trend is observed in recent EAC development according to Table 1? sent9
9. What architecture is most commonly used in recent EAC models, and what learning method do they employ? sent10
    9.1. What is the purpose of seq2seq learning models in EAC? sent11
    9.2. What is the basic structure of seq2seq architecture? sent12
    9.3. Which variants of RNNs are most commonly used in these models? sent13
10. How have some studies modeled the EAC task differently to achieve more generic responses? sent14
11. What mechanism was introduced to improve the focus of the decoder in seq2seq models? sent15
    11.1. How does the attention mechanism benefit the decoding process? sent16
12. What is another vital component of building EAC, and what is its purpose? sent17
13. How is emotion detection characterized in the field of natural language processing? sent18
14. What events promoted the task of emotion detection? sent19
15. What are some tasks related to emotion classification in text? sent20
16. What alternative task is mentioned regarding emotion prediction? sent21
17. How were early emotion classifiers developed, and what approach has shown better performance? sent22, sent23
18. How does a chatbot system utilize emotion detection to generate responses? sent24, sent25
19. How do studies differ in their approach to emotion categories in chatbots? sent26"
259089180,A Survey on Learning Objects' Relationship for Image Captioning,https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be,Evaluation.,5,"Te evaluation standard of relational captioning is consistent with the standard evaluation used in natural language processing to evaluate the similarity between the generated sentence and the ground-truth sentence. Te evaluation metrics: BLEU [88], METEOR [89], ROUGE [90], CIDEr [91], and SPICE [92]. For the fve metrics, BLEU and METEOR are used for machine translation, ROUGE for automatic translation summaries, and CIDEr and SPICE for image captioning. In principle, the abovementioned evaluation metrics measure the n-gram consistency between generated sentences and reference sentences and are also afected by the importance and rarity of n-grams in the corpus.","sent1: Te evaluation standard of relational captioning is consistent with the standard evaluation used in natural language processing to evaluate the similarity between the generated sentence and the ground-truth sentence.
sent2: Te evaluation metrics: BLEU [88], METEOR [89], ROUGE [90], CIDEr [91], and SPICE [92].
sent3: For the fve metrics, BLEU and METEOR are used for machine translation, ROUGE for automatic translation summaries, and CIDEr and SPICE for image captioning.
sent4: In principle, the abovementioned evaluation metrics measure the n-gram consistency between generated sentences and reference sentences and are also afected by the importance and rarity of n-grams in the corpus.","1. What is the evaluation standard for relational captioning in image captioning? sent1
2. What are the evaluation metrics used in relational captioning? sent2
    2.1. For what purposes are BLEU and METEOR used? sent3
    2.2. What is the purpose of ROUGE? sent3
    2.3. For what purpose are CIDEr and SPICE used? sent3
3. What do the evaluation metrics measure in principle? sent4"
232116743,Use of Social Media Data in Disaster Management: A Survey,https://www.semanticscholar.org/paper/7595367176192a375585e18593d1a2bd3d5faccc,Spatial,8,"Although spatial representations of social media data, such as geo-location, play an essential part in social-media-based event detection or event analysis, there are few social media data that provide information about users' locations [34]. Furthermore, there is a variety of location information represented in social media, ranging from a very precise location using geographic coordinates (e.g., longitude and latitude) to a very fuzzy location using descriptive language (e.g., city name).

Geographical information is represented in several ways on social media, as shown in   The approach can be applied to disseminate disaster early warning messages to social media users whose home locations are at risk [36] spatial pattern analysis spatial pattern analysis -

The analysis of patterns between the distance from the epicentre and time using social media can enhance situational awareness. However, the proposed method requires a sufficient number of tweets to avoid data quality issues.

[37] spatial pattern analysis spatial pattern analysis spatial pattern analysis This approach combines social media and sensor data for statistical analysis, which provides a more precise assessment of tweets' spatial patterns in a hazardous area.

[38]

Kernel density estimation (KDE) -based clustering for social media analysis from Hurricane Sandy KDE-based clustering for social media analysis from Hurricane Sandy KDE-based clustering for social media analysis from Hurricane Sandy

Associating social media data with hurricane damage data for spatial pattern analysis enabled qualified assessment of rapid damage.

[3] -Hot-spot detection Hot-spot detection

The proposed method for spatial analysis enables timely decision-making for emergency response and full awareness of public concern.

Several approaches have been proposed in the literature to address the issues of spatial representation in social media. Here, Table 4 presents the state-of-the-art methods for identifying and analysing the spatial information of social media data.

Geo-location identification: The research of [34] shows that 0.42% of all tweets use the latitude and longitude function to tag their geo-location, and out of 1 million Twitter users, only 26% have listed a city name. In most cases, general expressions (such as California) or nonsensical expressions (such as wonderland) are used. This research aimed to detect the locations of tweets that do not clearly mention geographic information. To this end, the authors proposed a method of computing the probability that a word is linked to a city. In order to improve the accuracy, the authors introduced a model of spatial variation for analysing the geographic distribution of words in tweets. The authors of [35] mentioned that geo-location information in tweet data may have noisy signals. For example, a user in the UK could tweet about a Houston Rockets game or their vacation in India. To overcome this, the authors integrate two types of signals (user's friend and user's tweet's nearby location) from social networks to predict a user's location.

Geo-location analytics: The authors of [36] discussed the effect of an earthquake on the East Coast of the United States (US) on August 23, 2011 by analysing the collected tweet data. The main finding of the paper was the patterns between the distance from the epicentre and the time after the earthquake. The authors of [37] used sensor data to identify flood-affected regions. The authors performed some statistical analyses of the collected data to find the general spatial patterns and to explore the differences between the spatial patterns among the relevant tweets. On the other hand, methods such as kernel density estimation (KDE) have been widely used for clustering of the activities during Hurricane Sandy [38] and spatial hotspot detection during the 2012 Beijing rainstorm [3].","sent1: Although spatial representations of social media data, such as geo-location, play an essential part in social-media-based event detection or event analysis, there are few social media data that provide information about users' locations [34].
sent2: Furthermore, there is a variety of location information represented in social media, ranging from a very precise location using geographic coordinates (e.g., longitude and latitude) to a very fuzzy location using descriptive language (e.g., city name).
sent3: Geographical information is represented in several ways on social media, as shown in   The approach can be applied to disseminate disaster early warning messages to social media users whose home locations are at risk [36] spatial pattern analysis spatial pattern analysis -
sent4: The analysis of patterns between the distance from the epicentre and time using social media can enhance situational awareness.
sent5: However, the proposed method requires a sufficient number of tweets to avoid data quality issues.
sent6: [37] spatial pattern analysis spatial pattern analysis spatial pattern analysis This approach combines social media and sensor data for statistical analysis, which provides a more precise assessment of tweets' spatial patterns in a hazardous area.
sent7: [38]Kernel density estimation (KDE) -based clustering for social media analysis from Hurricane Sandy KDE-based clustering for social media analysis from Hurricane Sandy KDE-based clustering for social media analysis from Hurricane SandyAssociating social media data with hurricane damage data for spatial pattern analysis enabled qualified assessment of rapid damage.
sent8: [3] -Hot-spot detection Hot-spot detectionThe proposed method for spatial analysis enables timely decision-making for emergency response and full awareness of public concern.
sent9: Several approaches have been proposed in the literature to address the issues of spatial representation in social media.
sent10: Here, Table 4 presents the state-of-the-art methods for identifying and analysing the spatial information of social media data.
sent11: Geo-location identification: The research of [34] shows that 0.42% of all tweets use the latitude and longitude function to tag their geo-location, and out of 1 million Twitter users, only 26% have listed a city name.
sent12: In most cases, general expressions (such as California) or nonsensical expressions (such as wonderland) are used.
sent13: This research aimed to detect the locations of tweets that do not clearly mention geographic information.
sent14: To this end, the authors proposed a method of computing the probability that a word is linked to a city.
sent15: In order to improve the accuracy, the authors introduced a model of spatial variation for analysing the geographic distribution of words in tweets.
sent16: The authors of [35] mentioned that geo-location information in tweet data may have noisy signals.
sent17: For example, a user in the UK could tweet about a Houston Rockets game or their vacation in India.
sent18: To overcome this, the authors integrate two types of signals (user's friend and user's tweet's nearby location) from social networks to predict a user's location.
sent19: Geo-location analytics: The authors of [36] discussed the effect of an earthquake on the East Coast of the United States (US) on August 23, 2011 by analysing the collected tweet data.
sent20: The main finding of the paper was the patterns between the distance from the epicentre and the time after the earthquake.
sent21: The authors of [37] used sensor data to identify flood-affected regions.
sent22: The authors performed some statistical analyses of the collected data to find the general spatial patterns and to explore the differences between the spatial patterns among the relevant tweets.
sent23: On the other hand, methods such as kernel density estimation (KDE) have been widely used for clustering of the activities during Hurricane Sandy [38] and spatial hotspot detection during the 2012 Beijing rainstorm [3].","1. What role do spatial representations of social media data play in event detection or analysis? sent1
    1.1. What types of location information are represented in social media? sent2
    1.2. How is geographical information represented and used in social media for disaster management? sent3
        1.2.1. How can the analysis of patterns between distance from the epicenter and time enhance situational awareness? sent4
        1.2.2. What is required for the proposed method to avoid data quality issues? sent5
        1.2.3. How does combining social media and sensor data improve spatial pattern analysis? sent6
        1.2.4. How is KDE-based clustering used in social media analysis for disaster events like Hurricane Sandy? sent7
        1.2.5. What does the proposed method for spatial analysis enable in terms of emergency response? sent8
2. What challenges exist in spatial representation in social media, and what approaches have been proposed to address them? sent9
    2.1. Where can one find state-of-the-art methods for identifying and analyzing spatial information of social media data? sent10
    2.2. What does the research on geo-location identification reveal about the use of geographic tagging in tweets? sent11
        2.2.1. What types of expressions are commonly used in tweets instead of precise geographic information? sent12
        2.2.2. What was the aim of the research regarding tweets that do not clearly mention geographic information? sent13
            2.2.2.1. What method did the authors propose to detect tweet locations without clear geographic information? sent14
            2.2.2.2. How did the authors aim to improve the accuracy of geographic distribution analysis in tweets? sent15
    2.3. What issues are associated with geo-location information in tweet data, and how can they be addressed? sent16
        2.3.1. Can you provide an example of noisy geo-location signals in tweets? sent17
        2.3.2. How do the authors propose to predict a user's location despite noisy signals? sent18
3. What findings were discussed regarding geo-location analytics in the context of an earthquake on the East Coast of the US? sent19
    3.1. What was the main finding related to the patterns between distance from the epicenter and time after the earthquake? sent20
4. How was sensor data used to identify flood-affected regions, and what analyses were performed? sent21
    4.1. What statistical analyses were performed to explore spatial patterns in tweets? sent22
5. How have methods like kernel density estimation (KDE) been applied in disaster events? sent23"
65219850,A Survey of Text Mining in Social Media: Facebook and Twitter Perspectives Establish algorithm for to determine an authorized PW error View project Further Investigations on Developing an Arabic Sentiment Lexicon View project Said Salloum British University in Dubai 3 PUBLICATIONS 2 CITATIONS SEE PROFILE,https://www.semanticscholar.org/paper/94d255e0b2a729e918a320ab99e503cfc04e1fc8,Text mining in Facebook,9,"The social networks are growing at a rapid rate without a break. Most importantly, the unstructured data is being stored on these networks as they act as a large pool and this data pertains to a host of domains containing governments, businesses, and health. Data mining techniques tend to transform the unstructured data for its placement within a systematic arrangement [47]. Nowadays, Facebook is one of the most popular social media. This media is used by a large number of people on earth for expressing their ideas, thoughts, sorrows, pleasures and poems [48]. Researchers had chosen a number of Facebook variables that were expected to develop the right situation for carrying out our investigations. The valuable statistics of user's personality is provided by the Facebook profiles and activities, which exposes the actual objects instead of projected or idealized character [49]. The digital data has currently witnessed an enormous growth. The key area of interest among professionals is now data mining and knowledge discovery. Moreover, a strong need has been felt to transform such data into useful knowledge and information. A number of applications like business management and market analysis have realized the benefits from the information and knowledge extracted out of large scale data. Information is stored in text form across various applications so one of the up-to-date areas for research is text mining. The hard issue is extracting the user required information. The knowledge discovery process has an important step which is believed to be the Text Mining. The hidden information is extracted from unstructured to semistructured data in this process. Extracting information from a number of written resources and its automatic discovery is called as Text mining. Moreover, computers are also used for the needful and to meet this goal.

Scholars of [50] illustrated the text mining techniques, methods, and challenges. These successful techniques would be described to give usefulness over information acquisition during text mining. The study discussed the situations where each technology could be beneficial for a different number of users. A number of business organizations would be examined by mining data that has been exposed by their employees on LinkedIn, Facebook, and other openly available sources. A network of informal social connections among employees is extracted through web crawler developed for this purpose. According to the findings, leadership roles can be identified within the organization and this could be achieved absolutely by using machine learning techniques besides centrality analysis. Clustering the social network of an organization and collecting available information within each cluster can result in the valuable non-trivial perceptions. A key asset or a considerable threat to the primary organization can be the knowledge about the network of informal relationships. Besides analyzing social networks of the organizations, algorithms and methods used to gather data from freely available sources would be presented by this paper. A web crawler was developed to obtain profiles of employees from six targeted organizations and this was done by collecting the Facebook data. A social network topology was created for each organization, and machine-learning algorithms and centrality measures were implemented so that the hidden leadership positions within each company could be discovered. Moreover, the social community clusters inside these organizations were also revealed by the algorithms, which gave us understanding about the communication network of each company in addition to the structure of the organization.

According to a study by [51], it has become clear that social media data is simply susceptible to misuse. The scheme encompasses structured approach and its application. Furthermore, it entails performing a statistical cluster analysis in addition to the comprehensive analysis of social media comments so that researchers could determine the inter-relationships among key factors. The qualitative social media data can be quantified by these schemes and subsequently cluster them based on their similar features, and then they can be used as decision-making tools. The SAMSUNG Mobile Facebook page, where Samsung smartphones were introduced, was used for the data acquisition process. The comment published by Facebook users on the captioned Facebook page is referred to as the ""Data"". In a period of 3 months, almost 128371 comments were downloaded. The English comments only were undergone through the analysis process. Afterward, the conceptual analysis was used by the content analysis and ultimately statistical cluster analysis was performed by carrying out relational analysis. Hence, social media data is integrated by applying the statistical cluster analysis and it is performed based on the output of the conceptual analysis. The researchers are consequently enabled to categorize a large dataset into many subsets, at times, referred to as objects. One of the disciplines of its application is marketing. Factors that can be manageable in some cases are also minimized by these types of techniques.

A study by [52] explored the social data as a systematic data mining architecture. Findings indicated that Facebook as a social networking site is the major source of data. Besides this approach, information on ""my wall"" post regarding myself, age and comments from the Facebook all are emphasized by the author. It has been taken as a raw data, which is applied later to study and monitor the analytical tactics. In addition, the study investigated images for the advertisement of their products and for the decisionmaking process. A number of data mining techniques precede the coercion of intellectual knowledge from social data. Mainly, it organizes the key information and other applied activities in which users are attributed regarding their colleagues on social networking sites (i.e. Facebook). For the recovery on Facebook user database, Facebook API performs Application Secret key and Facebook API Key are executed by Facebook API. As a result, WEKA files and data mining techniques are supported to collect certain data into the secondary database, while the text data is represented by the detached data.

Researchers of [41] explored the applicability of representing user's personality based on the extracted features from the Facebook data. The classification techniques and their utilities were completely analyzed with regard to the inspirational research outcomes. A sample of 250 user instances from Facebook formed the research study and this sample was from about 10,000 status updates, which was delivered by the My Personality project [53]. The study has the following two interconnected objectives: (1) having knowledge about the pertinent personality-correlated indicators that presents user data implicitly or explicitly in Facebook, and (2) identifying the feasibility of prognostic character demonstration so that upcoming intelligent systems could be supported. The study emphasized on the promotion of pertinent features in a model, through which the enhanced output of the classifiers under evaluation could be observed.","sent1: The social networks are growing at a rapid rate without a break.
sent2: Most importantly, the unstructured data is being stored on these networks as they act as a large pool and this data pertains to a host of domains containing governments, businesses, and health.
sent3: Data mining techniques tend to transform the unstructured data for its placement within a systematic arrangement [47].
sent4: Nowadays, Facebook is one of the most popular social media.
sent5: This media is used by a large number of people on earth for expressing their ideas, thoughts, sorrows, pleasures and poems [48].
sent6: Researchers had chosen a number of Facebook variables that were expected to develop the right situation for carrying out our investigations.
sent7: The valuable statistics of user's personality is provided by the Facebook profiles and activities, which exposes the actual objects instead of projected or idealized character [49].
sent8: The digital data has currently witnessed an enormous growth.
sent9: The key area of interest among professionals is now data mining and knowledge discovery.
sent10: Moreover, a strong need has been felt to transform such data into useful knowledge and information.
sent11: A number of applications like business management and market analysis have realized the benefits from the information and knowledge extracted out of large scale data.
sent12: Information is stored in text form across various applications so one of the up-to-date areas for research is text mining.
sent13: The hard issue is extracting the user required information.
sent14: The knowledge discovery process has an important step which is believed to be the Text Mining.
sent15: The hidden information is extracted from unstructured to semistructured data in this process.
sent16: Extracting information from a number of written resources and its automatic discovery is called as Text mining.
sent17: Moreover, computers are also used for the needful and to meet this goal.
sent18: Scholars of [50] illustrated the text mining techniques, methods, and challenges.
sent19: These successful techniques would be described to give usefulness over information acquisition during text mining.
sent20: The study discussed the situations where each technology could be beneficial for a different number of users.
sent21: A number of business organizations would be examined by mining data that has been exposed by their employees on LinkedIn, Facebook, and other openly available sources.
sent22: A network of informal social connections among employees is extracted through web crawler developed for this purpose.
sent23: According to the findings, leadership roles can be identified within the organization and this could be achieved absolutely by using machine learning techniques besides centrality analysis.
sent24: Clustering the social network of an organization and collecting available information within each cluster can result in the valuable non-trivial perceptions.
sent25: A key asset or a considerable threat to the primary organization can be the knowledge about the network of informal relationships.
sent26: Besides analyzing social networks of the organizations, algorithms and methods used to gather data from freely available sources would be presented by this paper.
sent27: A web crawler was developed to obtain profiles of employees from six targeted organizations and this was done by collecting the Facebook data.
sent28: A social network topology was created for each organization, and machine-learning algorithms and centrality measures were implemented so that the hidden leadership positions within each company could be discovered.
sent29: Moreover, the social community clusters inside these organizations were also revealed by the algorithms, which gave us understanding about the communication network of each company in addition to the structure of the organization.
sent30: According to a study by [51], it has become clear that social media data is simply susceptible to misuse.
sent31: The scheme encompasses structured approach and its application.
sent32: Furthermore, it entails performing a statistical cluster analysis in addition to the comprehensive analysis of social media comments so that researchers could determine the inter-relationships among key factors.
sent33: The qualitative social media data can be quantified by these schemes and subsequently cluster them based on their similar features, and then they can be used as decision-making tools.
sent34: The SAMSUNG Mobile Facebook page, where Samsung smartphones were introduced, was used for the data acquisition process.
sent35: The comment published by Facebook users on the captioned Facebook page is referred to as the ""Data"".
sent36: In a period of 3 months, almost 128371 comments were downloaded.
sent37: The English comments only were undergone through the analysis process.
sent38: Afterward, the conceptual analysis was used by the content analysis and ultimately statistical cluster analysis was performed by carrying out relational analysis.
sent39: Hence, social media data is integrated by applying the statistical cluster analysis and it is performed based on the output of the conceptual analysis.
sent40: The researchers are consequently enabled to categorize a large dataset into many subsets, at times, referred to as objects.
sent41: One of the disciplines of its application is marketing.
sent42: Factors that can be manageable in some cases are also minimized by these types of techniques.
sent43: A study by [52] explored the social data as a systematic data mining architecture.
sent44: Findings indicated that Facebook as a social networking site is the major source of data.
sent45: Besides this approach, information on ""my wall"" post regarding myself, age and comments from the Facebook all are emphasized by the author.
sent46: It has been taken as a raw data, which is applied later to study and monitor the analytical tactics.
sent47: In addition, the study investigated images for the advertisement of their products and for the decisionmaking process.
sent48: A number of data mining techniques precede the coercion of intellectual knowledge from social data.
sent49: Mainly, it organizes the key information and other applied activities in which users are attributed regarding their colleagues on social networking sites (i.e. Facebook).
sent50: For the recovery on Facebook user database, Facebook API performs Application Secret key and Facebook API Key are executed by Facebook API.
sent51: As a result, WEKA files and data mining techniques are supported to collect certain data into the secondary database, while the text data is represented by the detached data.
sent52: Researchers of [41] explored the applicability of representing user's personality based on the extracted features from the Facebook data.
sent53: The classification techniques and their utilities were completely analyzed with regard to the inspirational research outcomes.
sent54: A sample of 250 user instances from Facebook formed the research study and this sample was from about 10,000 status updates, which was delivered by the My Personality project [53].
sent55: The study has the following two interconnected objectives: (1) having knowledge about the pertinent personality-correlated indicators that presents user data implicitly or explicitly in Facebook, and (2) identifying the feasibility of prognostic character demonstration so that upcoming intelligent systems could be supported.
sent56: The study emphasized on the promotion of pertinent features in a model, through which the enhanced output of the classifiers under evaluation could be observed.","1. How are social networks growing, and what type of data do they store? sent1, sent2
    1.1. How do data mining techniques transform unstructured data? sent3
2. What is the popularity and usage of Facebook as a social media platform? sent4, sent5
3. What variables have researchers chosen for investigations on Facebook? sent6
4. What valuable information do Facebook profiles and activities provide? sent7
5. What is the current trend in digital data growth, and what is the key area of interest among professionals? sent8, sent9
    5.1. Why is there a strong need to transform data into useful knowledge and information? sent10
    5.2. What applications have realized the benefits of information and knowledge extracted from large-scale data? sent11
6. What is text mining, and why is it an important area for research? sent12, sent13
    6.1. What is the role of text mining in the knowledge discovery process? sent14
    6.2. How is hidden information extracted in text mining? sent15
    6.3. What is the definition of text mining? sent16
    6.4. How are computers used in text mining? sent17
7. What did scholars illustrate about text mining techniques, methods, and challenges? sent18
    7.1. How do successful text mining techniques provide usefulness? sent19
    7.2. In what situations could each text mining technology be beneficial? sent20
8. How can business organizations be examined through data mining on social media? sent21
    8.1. How is a network of informal social connections among employees extracted? sent22
    8.2. What findings can be achieved using machine learning techniques and centrality analysis? sent23
    8.3. What can result from clustering the social network of an organization? sent24
    8.4. How can knowledge about informal relationships be an asset or threat to an organization? sent25
9. What does the paper present regarding data gathering from freely available sources? sent26
    9.1. How was a web crawler used to obtain employee profiles from organizations? sent27
    9.2. How were social network topologies and leadership positions discovered? sent28
    9.3. What understanding was gained about communication networks and organizational structure? sent29
10. What does a study reveal about the susceptibility of social media data to misuse? sent30
11. What does the scheme encompass and entail? sent31, sent32
    11.1. How can qualitative social media data be quantified and used? sent33
12. How was data acquired from the SAMSUNG Mobile Facebook page? sent34, sent35
    12.1. How many comments were downloaded, and what was the analysis process? sent36, sent37, sent38
    12.2. How is social media data integrated using statistical cluster analysis? sent39
    12.3. How are researchers enabled to categorize large datasets? sent40
13. What is one of the disciplines of application for text mining techniques? sent41
    13.1. How do these techniques minimize manageable factors? sent42
14. What did a study explore regarding social data and data mining architecture? sent43
    14.1. What findings were indicated about Facebook as a data source? sent44
    14.2. What information is emphasized by the author regarding Facebook posts? sent45
    14.3. How is raw data applied for analytical tactics? sent46
    14.4. What did the study investigate regarding images and decision-making? sent47
15. How do data mining techniques extract intellectual knowledge from social data? sent48
    15.1. How is key information organized on social networking sites? sent49
16. How does Facebook API support data recovery and collection? sent50, sent51
17. What did researchers explore about representing user personality from Facebook data? sent52
    17.1. How were classification techniques analyzed in the study? sent53
    17.2. What was the sample size and source for the research study? sent54
    17.3. What were the two interconnected objectives of the study? sent55
    17.4. What did the study emphasize regarding model features and classifier output? sent56"
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,Topic 6: Host Immune Response to 19-nCoV,5,"This topic was discussed in about 6.36% (1837/28,904) of the publications (eg, [48][49][50][51][52]). Authors who had the highest number of publications related to this topic were Alessandro Sette (n=7), Stanley Perlman (n=6), Nima Rezaei (n=6), Irfan Rahman (n=5), and Akiko Iwasaki (n=6). The top 5 journals and preprint servers in terms of publishing articles related to this topic were bioRxiv (n=199), Medical Hypotheses (n=50), the Journal of Medical Virology (n=49), Frontiers in Immunology (n=22), and the British Journal of Haematology (n=19). The earliest article related to this topic was published on January 2, 2020. From that date until week 14, there was a slight increase in the number of weekly publications before it increased markedly, peaking in week 25 (n=155) (Multimedia Appendix 1). The mean number of weekly publications in this cluster was 62.8 (SD 54.6).","sent1: This topic was discussed in about 6.36% (1837/28,904) of the publications (eg, [48][49][50][51][52]).
sent2: Authors who had the highest number of publications related to this topic were Alessandro Sette (n=7), Stanley Perlman (n=6), Nima Rezaei (n=6), Irfan Rahman (n=5), and Akiko Iwasaki (n=6).
sent3: The top 5 journals and preprint servers in terms of publishing articles related to this topic were bioRxiv (n=199), Medical Hypotheses (n=50), the Journal of Medical Virology (n=49), Frontiers in Immunology (n=22), and the British Journal of Haematology (n=19).
sent4: The earliest article related to this topic was published on January 2, 2020.
sent5: From that date until week 14, there was a slight increase in the number of weekly publications before it increased markedly, peaking in week 25 (n=155) (Multimedia Appendix 1).
sent6: The mean number of weekly publications in this cluster was 62.8 (SD 54.6).","1. What percentage of publications discussed the topic of host immune response to 19-nCoV? sent1
2. Who were the authors with the highest number of publications related to this topic? sent2
3. Which journals and preprint servers published the most articles related to this topic? sent3
4. When was the earliest article related to this topic published? sent4
5. How did the number of weekly publications change over time, and when did it peak? sent5
6. What was the mean number of weekly publications in this cluster? sent6"
56657817,Analysis Methods in Neural Language Processing: A Survey,https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,Visualization,16,"Visualization is a valuable tool for analyzing neural networks in the language domain and beyond. Early work visualized hidden unit activations in RNNs trained on an artificial language modeling task, and observed how they correspond to certain grammatical relations such as agreement (Elman, 1991). Much recent work has focused on visualizing activations on specific examples in modern neural networks for language (Karpathy et al., 2015;Kádár et al., 2017;Qian et al., 2016a;Liu et al., 2018) and speech (Wu and King, 2016;Nagamine et al., 2015;Wang et al., 2017b). Figure 1 shows an example visualization of a neuron that captures position of words in a sentence. The heatmap uses blue and red colors for negative and positive activation values, respectively, enabling the user to quickly grasp the function of this neuron.

The attention mechanism that originated in work on NMT (Bahdanau et al., 2014) also lends itself to a natural visualization. The alignments obtained via different attention mechanisms have produced visualizations ranging from tasks like NLI (Rocktäschel et al., 2016;Yin et al., 2016), summarization (Rush et al., 2015), MT post-editing (Jauregi Unanue et al., 2018), and morphological inflection (Aharoni and Goldberg, 2017) to matching users on social media (Tay et al., 2018). Figure 2 reproduces a visualization of attention alignments from the original work by Bahdanau et al. Here grayscale values correspond to the weight of the attention between words in an English source sentence (columns) and its French translation (rows). As Bahdanau et al. explain, this visualization demonstrates that the NMT model learned a soft alignment between source and target words. Some aspects of word order may also be  Godin et al., 2018). Saliency can also be computed with respect to intermediate values, rather than input features (Ghaeini et al., 2018). 7 An instructive visualization technique is to cluster neural network activations and compare them to some linguistic property. Early work clustered RNN activations, showing that they organize in lexical categories (Elman, 1989(Elman, , 1990. Similar techniques have been followed by others. Recent examples include clustering of sentence embeddings in an RNN encoder trained in a multitask learning scenario (Brunner et al., 2017), and phoneme clusters in a joint audio-visual RNN model (Alishahi et al., 2017).

A few online tools for visualizing neural networks have recently become available. LSTMVis (Strobelt et al., 2018b) visualizes RNN activations, focusing on tracing hidden state dynamics. 8 Seq2Seq-Vis (Strobelt et al., 2018a) visualizes different modules in attention-based seq2seq models, with the goal of examining model decisions and testing alternative decisions. Another tool focused on comparing attention alignments was proposed by Rikters (2018). It also provides translation confidence scores based on the distribution of attention weights. NeuroX (Dalvi et al., 2019b) is a tool for finding and analyzing individual neurons, focusing on machine translation.

Evaluation As in much work on interpretability, evaluating visualization quality is difficult and often limited to qualitative examples. A few notable exceptions report human evaluations of visualization quality. Singh et al. (2018) showed human raters hierarchical clusterings of input words generated by two interpretation methods, and asked them to evaluate which method is more accurate, or in which method they trust more. Others reported human evaluations for attention visualization in conversation modeling (Freeman et al., 2018) and medical code prediction tasks (Mullenbach et al., 2018).

The availability of open-source tools of the sort described above will hopefully encourage users to utilize visualization in their regular research and development cycle. However, it remains to be seen how useful visualizations turn out to be.","sent1: Visualization is a valuable tool for analyzing neural networks in the language domain and beyond.
sent2: Early work visualized hidden unit activations in RNNs trained on an artificial language modeling task, and observed how they correspond to certain grammatical relations such as agreement (Elman, 1991).
sent3: Much recent work has focused on visualizing activations on specific examples in modern neural networks for language (Karpathy et al., 2015;Kádár et al., 2017;Qian et al., 2016a;Liu et al., 2018) and speech (Wu and King, 2016;Nagamine et al., 2015;Wang et al., 2017b).
sent4: Figure 1 shows an example visualization of a neuron that captures position of words in a sentence.
sent5: The heatmap uses blue and red colors for negative and positive activation values, respectively, enabling the user to quickly grasp the function of this neuron.
sent6: The attention mechanism that originated in work on NMT (Bahdanau et al., 2014) also lends itself to a natural visualization.
sent7: The alignments obtained via different attention mechanisms have produced visualizations ranging from tasks like NLI (Rocktäschel et al., 2016;Yin et al., 2016), summarization (Rush et al., 2015), MT post-editing (Jauregi Unanue et al., 2018), and morphological inflection (Aharoni and Goldberg, 2017) to matching users on social media (Tay et al., 2018).
sent8: Figure 2 reproduces a visualization of attention alignments from the original work by Bahdanau et al.
sent9: Here grayscale values correspond to the weight of the attention between words in an English source sentence (columns) and its French translation (rows).
sent10: As Bahdanau et al. explain, this visualization demonstrates that the NMT model learned a soft alignment between source and target words.
sent11: Some aspects of word order may also be  Godin et al., 2018).
sent12: Saliency can also be computed with respect to intermediate values, rather than input features (Ghaeini et al., 2018). 7
sent13: An instructive visualization technique is to cluster neural network activations and compare them to some linguistic property.
sent14: Early work clustered RNN activations, showing that they organize in lexical categories (Elman, 1989(Elman, , 1990. Similar techniques have been followed by others. Recent examples include clustering of sentence embeddings in an RNN encoder trained in a multitask learning scenario (Brunner et al., 2017), and phoneme clusters in a joint audio-visual RNN model (Alishahi et al., 2017).
sent15: A few online tools for visualizing neural networks have recently become available.
sent16: LSTMVis (Strobelt et al., 2018b) visualizes RNN activations, focusing on tracing hidden state dynamics.
sent17: 8 Seq2Seq-Vis (Strobelt et al., 2018a) visualizes different modules in attention-based seq2seq models, with the goal of examining model decisions and testing alternative decisions.
sent18: Another tool focused on comparing attention alignments was proposed by Rikters (2018).
sent19: It also provides translation confidence scores based on the distribution of attention weights.
sent20: NeuroX (Dalvi et al., 2019b) is a tool for finding and analyzing individual neurons, focusing on machine translation.
sent21: Evaluation As in much work on interpretability, evaluating visualization quality is difficult and often limited to qualitative examples.
sent22: A few notable exceptions report human evaluations of visualization quality.
sent23: Singh et al. (2018) showed human raters hierarchical clusterings of input words generated by two interpretation methods, and asked them to evaluate which method is more accurate, or in which method they trust more.
sent24: Others reported human evaluations for attention visualization in conversation modeling (Freeman et al., 2018) and medical code prediction tasks (Mullenbach et al., 2018).
sent25: The availability of open-source tools of the sort described above will hopefully encourage users to utilize visualization in their regular research and development cycle.
sent26: However, it remains to be seen how useful visualizations turn out to be.","1. Why is visualization considered a valuable tool in neural language processing? sent1
2. What did early work on visualizing hidden unit activations in RNNs reveal? sent2
3. What has recent work focused on in terms of visualizing neural network activations? sent3
4. What does Figure 1 illustrate in the context of visualization? sent4
5. How does the heatmap in Figure 1 help users understand neuron functions? sent5
6. How is the attention mechanism related to visualization? sent6
    6.1. What tasks have produced visualizations using attention mechanisms? sent7
    6.2. What does Figure 2 demonstrate about attention alignments? sent8, sent9, sent10
7. What is an instructive visualization technique involving clustering? sent13
    7.1. What did early work on clustering RNN activations show? sent14
8. What online tools are available for visualizing neural networks? sent15
    8.1. What does LSTMVis focus on visualizing? sent16
    8.2. What is the goal of Seq2Seq-Vis? sent17
    8.3. What does the tool proposed by Rikters focus on? sent18, sent19
    8.4. What is the focus of NeuroX? sent20
9. What challenges exist in evaluating visualization quality? sent21
    9.1. What are some exceptions that report human evaluations of visualization quality? sent22
        9.1.1. What did Singh et al. (2018) evaluate using human raters? sent23
        9.1.2. What other tasks have reported human evaluations for attention visualization? sent24
10. What is the potential impact of open-source visualization tools on research and development? sent25
11. What remains uncertain about the usefulness of visualizations? sent26"
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,Automatic extraction of travel information from texts,8,"Nakatoh et al. [24] proposed a method for extracting names of local culinary dishes from travel blogs written in Japanese, which were identified when the blog entry included both the name of a sightseeing destination and the word ""tourism"". They extracted local dishes by gathering nouns that are dependent on the verb ""'ßy‹"" (eat). Tsai and Chou [32] also proposed a method for extracting dish names from restaurant review blogs written in Chinese using a machine learning (CRF) technique.  In the following, we explain the detail of the bootstrapping-based and machine learning-based information extraction approaches based on Nanba's work [25]. Nanba et al. extracted pairs comprising a location name and a local product from travel blogs written in Japanese, which were identified using the method described in Section 2.1. For the efficient extraction of travel information, they employed a bootstrapping method.

First, they prepared 482 pairs as seeds for the bootstrapping. These pairs were obtained automatically from a ""Web Japanese N-gram"" database provided by Google, Inc. The database comprises N-grams (N = 1-7) extracted from 20 billion Japanese sentences on the Web. They applied the pattern ""[0 ] i [ i] "" ([slot of ""location name""] local product [slot of ""local product""] ) to the database, and extracted location names and local products from each corresponding slot, thereby obtaining the 482 pairs. Second, they applied a machine learning-based information extraction technique to the travel blogs identified in the previous step, and obtained new pairs. In this step, they prepared training data for the machine learning in the following three steps.

1. Select 200 sentences that contain both a location name and a local product from the 482 pairs. Then automatically create 200 tagged sentences, to which both ""location"" and ""product"" tags are assigned. 5 2. Prepare another 200 sentences that contain only a location name. Then create 200 tagged sentences, to which the ""location"" tag is assigned.

3. Apply machine learning to the 400 tagged sentences, and obtain a system that automatically allocates ""location"" and ""product"" tags to given sentences.

As a machine learning method, they used CRF. The CRF-based method identifies the class of each word in a given sentence. Features and tags are given in the CRF method as follows: (1) k tags occur before a target word; (2) k features occur before a target word; and (3) k features follow a target word. They used the value of k = 2, which was determined in a pilot study. They used the following six features for machine learning.

• Word.

• The part of speech to which the word belongs (noun, verb, adjective, etc.)

• Whether the word is a quotation mark.

• Whether the word is a cue word, such as "" i"", "" #"", ""y#"" (local product), ""˜Ó"" (famous confection), or "" #"" (souvenir).

• Whether the word is a surface case.

• Whether the word is frequently used in the names of local products or souvenirs, such as ""cake"" or ""noodle"".

Nakatoh et al. [24] proposed a method for extracting names of local culinary dishes from travel blogs written in Japanese, which were identified when the blog entry included both the name of a sightseeing destination and the word ""tourism"". They extracted local dishes by gathering nouns that are dependent on the verb ""'ßy‹"" (eat). Tsai and Chou [32] also proposed a method for extracting dish names from restaurant review blogs written in Chinese using a machine learning (CRF) technique.  In the following, we explain the detail of the bootstrapping-based and machine learning-based information extraction approaches based on Nanba's work [25]. Nanba et al. extracted pairs comprising a location name and a local product from travel blogs written in Japanese, which were identified using the method described in Section 2.1. For the efficient extraction of travel information, they employed a bootstrapping method.

First, they prepared 482 pairs as seeds for the bootstrapping. These pairs were obtained automatically from a ""Web Japanese N-gram"" database provided by Google, Inc. The database comprises N-grams (N = 1-7) extracted from 20 billion Japanese sentences on the Web. They applied the pattern ""[0 ] i [ i] "" ([slot of ""location name""] local product [slot of ""local product""] ) to the database, and extracted location names and local products from each corresponding slot, thereby obtaining the 482 pairs. Second, they applied a machine learning-based information extraction technique to the travel blogs identified in the previous step, and obtained new pairs. In this step, they prepared training data for the machine learning in the following three steps.

1. Select 200 sentences that contain both a location name and a local product from the 482 pairs. Then automatically create 200 tagged sentences, to which both ""location"" and ""product"" tags are assigned. 5 2. Prepare another 200 sentences that contain only a location name. Then create 200 tagged sentences, to which the ""location"" tag is assigned.

3. Apply machine learning to the 400 tagged sentences, and obtain a system that automatically allocates ""location"" and ""product"" tags to given sentences.

As a machine learning method, they used CRF. The CRF-based method identifies the class of each word in a given sentence. Features and tags are given in the CRF method as follows: (1) k tags occur before a target word; (2) k features occur before a target word; and (3) k features follow a target word. They used the value of k = 2, which was determined in a pilot study. They used the following six features for machine learning.

• Word.

• The part of speech to which the word belongs (noun, verb, adjective, etc.)

• Whether the word is a quotation mark.

• Whether the word is a cue word, such as "" i"", "" #"", ""y#"" (local product), ""˜Ó"" (famous confection), or "" #"" (souvenir).

• Whether the word is a surface case.

• Whether the word is frequently used in the names of local products or souvenirs, such as ""cake"" or ""noodle"".","sent1: Nakatoh et al. [24] proposed a method for extracting names of local culinary dishes from travel blogs written in Japanese, which were identified when the blog entry included both the name of a sightseeing destination and the word ""tourism"".
sent2: They extracted local dishes by gathering nouns that are dependent on the verb ""'ßy‹"" (eat).
sent3: Tsai and Chou [32] also proposed a method for extracting dish names from restaurant review blogs written in Chinese using a machine learning (CRF) technique.
sent4: In the following, we explain the detail of the bootstrapping-based and machine learning-based information extraction approaches based on Nanba's work [25].
sent5: Nanba et al. extracted pairs comprising a location name and a local product from travel blogs written in Japanese, which were identified using the method described in Section 2.1.
sent6: For the efficient extraction of travel information, they employed a bootstrapping method.
sent7: First, they prepared 482 pairs as seeds for the bootstrapping.
sent8: These pairs were obtained automatically from a ""Web Japanese N-gram"" database provided by Google, Inc.
sent9: The database comprises N-grams (N = 1-7) extracted from 20 billion Japanese sentences on the Web.
sent10: They applied the pattern ""[0 ] i [ i] "" ([slot of ""location name""] local product [slot of ""local product""] ) to the database, and extracted location names and local products from each corresponding slot, thereby obtaining the 482 pairs.
sent11: Second, they applied a machine learning-based information extraction technique to the travel blogs identified in the previous step, and obtained new pairs.
sent12: In this step, they prepared training data for the machine learning in the following three steps.
sent13: 1. Select 200 sentences that contain both a location name and a local product from the 482 pairs.
sent14: Then automatically create 200 tagged sentences, to which both ""location"" and ""product"" tags are assigned.
sent15: 5 2. Prepare another 200 sentences that contain only a location name.
sent16: Then create 200 tagged sentences, to which the ""location"" tag is assigned.
sent17: 3. Apply machine learning to the 400 tagged sentences, and obtain a system that automatically allocates ""location"" and ""product"" tags to given sentences.
sent18: As a machine learning method, they used CRF.
sent19: The CRF-based method identifies the class of each word in a given sentence.
sent20: Features and tags are given in the CRF method as follows: (1) k tags occur before a target word; (2) k features occur before a target word; and (3) k features follow a target word.
sent21: They used the value of k = 2, which was determined in a pilot study.
sent22: They used the following six features for machine learning.
sent23: • Word. • The part of speech to which the word belongs (noun, verb, adjective, etc.)• Whether the word is a quotation mark.
sent24: • Whether the word is a cue word, such as "" i"", "" #"", ""y#"" (local product), ""˜Ó"" (famous confection), or "" #"" (souvenir).
sent25: • Whether the word is a surface case.
sent26: • Whether the word is frequently used in the names of local products or souvenirs, such as ""cake"" or ""noodle"".
sent27: Nakatoh et al. [24] proposed a method for extracting names of local culinary dishes from travel blogs written in Japanese, which were identified when the blog entry included both the name of a sightseeing destination and the word ""tourism"".
sent28: They extracted local dishes by gathering nouns that are dependent on the verb ""'ßy‹"" (eat).
sent29: Tsai and Chou [32] also proposed a method for extracting dish names from restaurant review blogs written in Chinese using a machine learning (CRF) technique.
sent30: In the following, we explain the detail of the bootstrapping-based and machine learning-based information extraction approaches based on Nanba's work [25].
sent31: Nanba et al. extracted pairs comprising a location name and a local product from travel blogs written in Japanese, which were identified using the method described in Section 2.1.
sent32: For the efficient extraction of travel information, they employed a bootstrapping method.
sent33: First, they prepared 482 pairs as seeds for the bootstrapping.
sent34: These pairs were obtained automatically from a ""Web Japanese N-gram"" database provided by Google, Inc.
sent35: The database comprises N-grams (N = 1-7) extracted from 20 billion Japanese sentences on the Web.
sent36: They applied the pattern ""[0 ] i [ i] "" ([slot of ""location name""] local product [slot of ""local product""] ) to the database, and extracted location names and local products from each corresponding slot, thereby obtaining the 482 pairs.
sent37: Second, they applied a machine learning-based information extraction technique to the travel blogs identified in the previous step, and obtained new pairs.
sent38: In this step, they prepared training data for the machine learning in the following three steps.
sent39: 1. Select 200 sentences that contain both a location name and a local product from the 482 pairs.
sent40: Then automatically create 200 tagged sentences, to which both ""location"" and ""product"" tags are assigned.
sent41: 5 2. Prepare another 200 sentences that contain only a location name.
sent42: Then create 200 tagged sentences, to which the ""location"" tag is assigned.
sent43: 3. Apply machine learning to the 400 tagged sentences, and obtain a system that automatically allocates ""location"" and ""product"" tags to given sentences.
sent44: As a machine learning method, they used CRF.
sent45: The CRF-based method identifies the class of each word in a given sentence.
sent46: Features and tags are given in the CRF method as follows: (1) k tags occur before a target word; (2) k features occur before a target word; and (3) k features follow a target word.
sent47: They used the value of k = 2, which was determined in a pilot study.
sent48: They used the following six features for machine learning.
sent49: • Word. • The part of speech to which the word belongs (noun, verb, adjective, etc.)• Whether the word is a quotation mark.
sent50: • Whether the word is a cue word, such as "" i"", "" #"", ""y#"" (local product), ""˜Ó"" (famous confection), or "" #"" (souvenir).
sent51: • Whether the word is a surface case.
sent52: • Whether the word is frequently used in the names of local products or souvenirs, such as ""cake"" or ""noodle"".","1. What method did Nakatoh et al. propose for extracting names of local culinary dishes from travel blogs? sent1
    1.1. How did they extract local dishes? sent2
2. What method did Tsai and Chou propose for extracting dish names from restaurant review blogs? sent3
3. What approaches are explained in the following section based on Nanba's work? sent4
4. What did Nanba et al. extract from travel blogs, and how were they identified? sent5
5. What method did they employ for the efficient extraction of travel information? sent6
    5.1. What was the first step in their bootstrapping method? sent7
        5.1.1. How were the 482 pairs obtained? sent8
        5.1.2. What does the ""Web Japanese N-gram"" database comprise? sent9
        5.1.3. How did they apply the pattern to the database to obtain the 482 pairs? sent10
    5.2. What was the second step in their method? sent11
        5.2.1. How did they prepare training data for machine learning? sent12
            5.2.1.1. What was the first step in preparing training data? sent13
            5.2.1.2. What was done after selecting 200 sentences? sent14
            5.2.1.3. What was the second step in preparing training data? sent15
            5.2.1.4. What was done after preparing another 200 sentences? sent16
            5.2.1.5. What was the third step in preparing training data? sent17
        5.2.2. What machine learning method did they use? sent18
            5.2.2.1. What does the CRF-based method identify? sent19
            5.2.2.2. How are features and tags given in the CRF method? sent20
            5.2.2.3. What value of k was used, and how was it determined? sent21
            5.2.2.4. What features were used for machine learning? sent22
                5.2.2.4.1. What are the specific features used in machine learning? sent23, sent24, sent25, sent26"
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,Text quality metrics.,136,"The methods in this category measure general quality aspects of the generated text, and are originated from translation, summarizing or captioning tasks. The most widely used metrics in the papers reviewed are BLEU [102], ROUGE-L [90], METEOR [12,82] and CIDEr [140], which measure the similarity of a target text (also referred to as candidate), against one or more reference texts (ground truth). These metrics are mainly based on counting n-gram matchings between the candidate and the ground truth. BLEU is precision-oriented, ROUGE-L and METEOR are F1-scores that can be biased towards precision or recall with a given parameter, and CIDEr attempts to capture both precision and recall through a TF-IDF score. Most of these metrics have variants and parameters for their calculation: ROUGE is a set of multiple metrics, being ROUGE-L the only one used in this task; METEOR has variants presented by the same authors [30][31][32]; and CIDEr was presented with the CIDEr-D variant to prevent gameability effects. SPICE [9] is a metric designed for the image captioning task, and evaluates the underlying meaning of the sentences describing the image scene, partially disregarding fluency or grammatical aspects. Specifically, the text is parsed as a graph, capturing the objects, their described characteristics and relations, which are then measured against the ground truth using an F1-score. Even though SPICE attempts to assess the semantic information in a caption, we believe it is not suitable for medical reports, as the graph parsing is designed for general domain objects. Nonetheless, Zhang et al. [162] presented the medical correctness metric MIRQI, applying a similar idea in a specific medical domain, which we will discuss in the next subsection (5.4.2).

Besides standard captioning metrics, we identified two other approaches to measure text quality. First, Alsharid et al. [7] used Grammar Bot 3 , a rule and statistics based automated system that counts the grammatical errors in sentences. Second, Harzig et al. 2019a [48] measured the sentence variability, by counting the different sentences in a set of reports. They argue that the sentences indicating abnormalities occur very rarely in the dataset, while the ones indicating normality are the most frequent. Hence, a certain level of variability is desired, and a system generating reports with low variability may indicate that not all medical conditions are being captured.

Lastly, both works from Li et al. [85,86] performed human evaluation with non-expert users via Amazon Mechanical Turk (AMT), following the same procedure. The authors presented two reports to the AMT participants, one generated with the proposed model and one generated with the CoAtt model [68] as baseline, and asked them to choose the most similar with the ground truth in terms of fluency, abnormalities correctness and content coverage. The results shown that their report was preferred around 50-60% of the cases, while the baseline around 20-30% (for the rest, none or both were preferred). We categorize this evaluation as a text quality metric, as the participants are not experts, and their answers are not fine-grained (i.e., did not specify what failed: fluency, correctness or coverage; or by how much they failed). 5.4.2 Medical correctness metrics. While the most common purpose of the text quality metrics is to measure the similarity between the generated report and a ground truth, they do not necessarily capture the medical facts in the reports [11,17,92,107,108,161]. For example, the sentences ""effusion is observed"" and ""effusion is not observed"" are very similar, thus may present a very high score for any metric based on n-gram matching, though the medical facts are the exact opposite. Therefore, an evaluation directly measuring the reports correctness is required, not necessarily taking into account fluency, grammatical rules or text quality in general. From the literature reviewed, in ten works [7,16,61,67,85,92,98,148,154,162] the authors presented an automatic metric to address this issue, four works [7,38,126,131] did a formal expert evaluation, and multiple works [49, 76, 86, 89, 94, 120, 126, 131, 144, 155-158, 162, 163] evaluated medical correctness indirectly from auxiliary tasks. The methods are listed in Table 6 and are further discussed next.

In several works the authors presented a method that detects concepts in the generated and ground truth reports, and compare the results using common classification metrics, such as accuracy, F1-score, and more. The main difference between these methods lies in how the concepts are automatically detected in the reports. The simplest approaches are keyword-based, which consists in reporting the ratio of a set of keywords found between the generated report and ground truth, like MeSH Accuracy [61] that uses MeSH terms, and Keyword Accuracy that uses 438 MTI terms (presented by Xue et al. [154] and used in A3FN [150]). Similarly, Medical Abnormality Terminology Detection [85] calculates precision and false positive rate of the 10 most frequent abnormalityrelated terms in the dataset; and Wu et al. [148] calculated accuracy, sensitivity and specificity for a set of keywords.

Other approaches are abnormality-based, which attempt to directly classify abnormalities from the report by different means: Abnormality Detection [67] uses manually designed patterns; Medical Abnormality Detection [92] uses the CheXpert labeler tool [63]; Biswal et al. [16] used a character-level CNN [160] that classifies multiple CheXpert labels [63]; and Moradi et al. [98] used a proprietary software to extract semantic descriptors. Lastly, Anatomical Relevance Score (ARS) [7] is a body-part-based approach, which detects the anatomical elements mentioned in a report considering the vocabulary used. Though these methods may be useful for measuring medical correctness to a certain degree, there is no consensus or standard, and there is no formal evaluation of the correlation with expert judgement. From the discussed techniques, Alsharid et al. [7] are the only authors that also performed an expert evaluation of the generated reports, though they did not conduct a correlation or similar analysis to validate the ARS method.

Zhang et al. [162] went further with the concept extraction and presented Medical Image Report Quality Index (MIRQI), which works in a similar fashion as the SPICE [9] metric presented in the text quality subsection (5.4.1). MIRQI applies ideas from NegBio [106] and the CheXpert labeler [63] to identify diseases or medical conditions in the reports, considering synonyms and negations, and uses the Stanford parser [23] to obtain semantic dependencies and finer-grained attributes from each sentence, such as severity, size, shape, body parts, etc. With this information, an abnormality graph is built for each report, where each node is a disease with its attributes, and the nodes are connected if they belong to the same organ or tissue. Lastly, the graphs from the ground truth and generated reports are matched node-wise, and MIRQI-p (precision), MIRQI-r (recall) and MIRQI-F1 (F1-score) are computed. Compared to the formerly discussed correctness metrics, we believe this approach seems more robust to assess the medical facts in the reports, as it attempts to capture the attributes and relations, opposed to the concepts only. However, the authors did not present an evaluation against expert judgement, so we cannot determine if this metric is sufficient.

Considering human evaluation, only a few works [7,38,126,131] present a formal expert medical correctness assessment. In the work by Alsharid et al. [7] a medical professional assessed the reports on a Likert Scale from 0 to 2 in four different aspects: accurately describes the image, presents no incorrect information, is grammatically correct and is relevant for the image; the results were further separated for samples from different body parts, showing averages between 0.5 and 1. Gale et al. [38] asked a radiologist to evaluate the correctness of the hip fractures description, finding that the fracture's character was properly described 98% of the cases, while the fracture location only for 90%. In the work by Tian et al. [131] a medical expert evaluated 30 randomly selected reports with a rating from 1 (definite accept) to 5 (definite reject), scoring an average of 2.33. Lastly, Spinks and Moens [126] asked four questions to three experts regarding the generated reports, where the third and fourth questions measured correctness: ""Do you agree with the proposed diagnosis?"", answering 0 (no) or 1 (yes) and ""How certain are you about your final diagnosis?"", from 1 (not sure) to 4 (very sure). The average scores were high (0.88 and 3.75), showing agreement with the model's diagnosis, and certainty on the experts' diagnoses. The other questions concerned explainability aspects, and are detailed in the next subsection (5.4.3). So far, there is no standard approach to perform an expert evaluation, though we believe the first two approaches provide finer-grained information than the latter two, and hence should be more useful for determining in which cases the models are failing and for designing improvements. The certainty question should also be very useful, as diagnoses may be susceptible to human judgement.

Lastly, multiple papers [49,86,89,94,120,131,144,155,156,158,162,163] evaluated the performance of the auxiliary tasks with ROC-AUC, accuracy and other typical classification or segmentation metrics, as shown in Table 6. Note that in any of these cases, the task is a previous or intermediary step of the process and is not derived from the report. In consequence, even if the classification has great performance, the language component could be performing poorly, and the generated reports still may be inaccurate. Accordingly, we believe this type of measure should not be used as the primary report correctness evaluation, unless it can be proven that the report reproduces exactly the classification made (e.g. by a template-filling process).

The methods in this category measure general quality aspects of the generated text, and are originated from translation, summarizing or captioning tasks. The most widely used metrics in the papers reviewed are BLEU [102], ROUGE-L [90], METEOR [12,82] and CIDEr [140], which measure the similarity of a target text (also referred to as candidate), against one or more reference texts (ground truth). These metrics are mainly based on counting n-gram matchings between the candidate and the ground truth. BLEU is precision-oriented, ROUGE-L and METEOR are F1-scores that can be biased towards precision or recall with a given parameter, and CIDEr attempts to capture both precision and recall through a TF-IDF score. Most of these metrics have variants and parameters for their calculation: ROUGE is a set of multiple metrics, being ROUGE-L the only one used in this task; METEOR has variants presented by the same authors [30][31][32]; and CIDEr was presented with the CIDEr-D variant to prevent gameability effects. SPICE [9] is a metric designed for the image captioning task, and evaluates the underlying meaning of the sentences describing the image scene, partially disregarding fluency or grammatical aspects. Specifically, the text is parsed as a graph, capturing the objects, their described characteristics and relations, which are then measured against the ground truth using an F1-score. Even though SPICE attempts to assess the semantic information in a caption, we believe it is not suitable for medical reports, as the graph parsing is designed for general domain objects. Nonetheless, Zhang et al. [162] presented the medical correctness metric MIRQI, applying a similar idea in a specific medical domain, which we will discuss in the next subsection (5.4.2).

Besides standard captioning metrics, we identified two other approaches to measure text quality. First, Alsharid et al. [7] used Grammar Bot 3 , a rule and statistics based automated system that counts the grammatical errors in sentences. Second, Harzig et al. 2019a [48] measured the sentence variability, by counting the different sentences in a set of reports. They argue that the sentences indicating abnormalities occur very rarely in the dataset, while the ones indicating normality are the most frequent. Hence, a certain level of variability is desired, and a system generating reports with low variability may indicate that not all medical conditions are being captured.

Lastly, both works from Li et al. [85,86] performed human evaluation with non-expert users via Amazon Mechanical Turk (AMT), following the same procedure. The authors presented two reports to the AMT participants, one generated with the proposed model and one generated with the CoAtt model [68] as baseline, and asked them to choose the most similar with the ground truth in terms of fluency, abnormalities correctness and content coverage. The results shown that their report was preferred around 50-60% of the cases, while the baseline around 20-30% (for the rest, none or both were preferred). We categorize this evaluation as a text quality metric, as the participants are not experts, and their answers are not fine-grained (i.e., did not specify what failed: fluency, correctness or coverage; or by how much they failed). 5.4.2 Medical correctness metrics. While the most common purpose of the text quality metrics is to measure the similarity between the generated report and a ground truth, they do not necessarily capture the medical facts in the reports [11,17,92,107,108,161]. For example, the sentences ""effusion is observed"" and ""effusion is not observed"" are very similar, thus may present a very high score for any metric based on n-gram matching, though the medical facts are the exact opposite. Therefore, an evaluation directly measuring the reports correctness is required, not necessarily taking into account fluency, grammatical rules or text quality in general. From the literature reviewed, in ten works [7,16,61,67,85,92,98,148,154,162] the authors presented an automatic metric to address this issue, four works [7,38,126,131] did a formal expert evaluation, and multiple works [49, 76, 86, 89, 94, 120, 126, 131, 144, 155-158, 162, 163] evaluated medical correctness indirectly from auxiliary tasks. The methods are listed in Table 6 and are further discussed next.

In several works the authors presented a method that detects concepts in the generated and ground truth reports, and compare the results using common classification metrics, such as accuracy, F1-score, and more. The main difference between these methods lies in how the concepts are automatically detected in the reports. The simplest approaches are keyword-based, which consists in reporting the ratio of a set of keywords found between the generated report and ground truth, like MeSH Accuracy [61] that uses MeSH terms, and Keyword Accuracy that uses 438 MTI terms (presented by Xue et al. [154] and used in A3FN [150]). Similarly, Medical Abnormality Terminology Detection [85] calculates precision and false positive rate of the 10 most frequent abnormalityrelated terms in the dataset; and Wu et al. [148] calculated accuracy, sensitivity and specificity for a set of keywords.

Other approaches are abnormality-based, which attempt to directly classify abnormalities from the report by different means: Abnormality Detection [67] uses manually designed patterns; Medical Abnormality Detection [92] uses the CheXpert labeler tool [63]; Biswal et al. [16] used a character-level CNN [160] that classifies multiple CheXpert labels [63]; and Moradi et al. [98] used a proprietary software to extract semantic descriptors. Lastly, Anatomical Relevance Score (ARS) [7] is a body-part-based approach, which detects the anatomical elements mentioned in a report considering the vocabulary used. Though these methods may be useful for measuring medical correctness to a certain degree, there is no consensus or standard, and there is no formal evaluation of the correlation with expert judgement. From the discussed techniques, Alsharid et al. [7] are the only authors that also performed an expert evaluation of the generated reports, though they did not conduct a correlation or similar analysis to validate the ARS method.

Zhang et al. [162] went further with the concept extraction and presented Medical Image Report Quality Index (MIRQI), which works in a similar fashion as the SPICE [9] metric presented in the text quality subsection (5.4.1). MIRQI applies ideas from NegBio [106] and the CheXpert labeler [63] to identify diseases or medical conditions in the reports, considering synonyms and negations, and uses the Stanford parser [23] to obtain semantic dependencies and finer-grained attributes from each sentence, such as severity, size, shape, body parts, etc. With this information, an abnormality graph is built for each report, where each node is a disease with its attributes, and the nodes are connected if they belong to the same organ or tissue. Lastly, the graphs from the ground truth and generated reports are matched node-wise, and MIRQI-p (precision), MIRQI-r (recall) and MIRQI-F1 (F1-score) are computed. Compared to the formerly discussed correctness metrics, we believe this approach seems more robust to assess the medical facts in the reports, as it attempts to capture the attributes and relations, opposed to the concepts only. However, the authors did not present an evaluation against expert judgement, so we cannot determine if this metric is sufficient.

Considering human evaluation, only a few works [7,38,126,131] present a formal expert medical correctness assessment. In the work by Alsharid et al. [7] a medical professional assessed the reports on a Likert Scale from 0 to 2 in four different aspects: accurately describes the image, presents no incorrect information, is grammatically correct and is relevant for the image; the results were further separated for samples from different body parts, showing averages between 0.5 and 1. Gale et al. [38] asked a radiologist to evaluate the correctness of the hip fractures description, finding that the fracture's character was properly described 98% of the cases, while the fracture location only for 90%. In the work by Tian et al. [131] a medical expert evaluated 30 randomly selected reports with a rating from 1 (definite accept) to 5 (definite reject), scoring an average of 2.33. Lastly, Spinks and Moens [126] asked four questions to three experts regarding the generated reports, where the third and fourth questions measured correctness: ""Do you agree with the proposed diagnosis?"", answering 0 (no) or 1 (yes) and ""How certain are you about your final diagnosis?"", from 1 (not sure) to 4 (very sure). The average scores were high (0.88 and 3.75), showing agreement with the model's diagnosis, and certainty on the experts' diagnoses. The other questions concerned explainability aspects, and are detailed in the next subsection (5.4.3). So far, there is no standard approach to perform an expert evaluation, though we believe the first two approaches provide finer-grained information than the latter two, and hence should be more useful for determining in which cases the models are failing and for designing improvements. The certainty question should also be very useful, as diagnoses may be susceptible to human judgement.

Lastly, multiple papers [49,86,89,94,120,131,144,155,156,158,162,163] evaluated the performance of the auxiliary tasks with ROC-AUC, accuracy and other typical classification or segmentation metrics, as shown in Table 6. Note that in any of these cases, the task is a previous or intermediary step of the process and is not derived from the report. In consequence, even if the classification has great performance, the language component could be performing poorly, and the generated reports still may be inaccurate. Accordingly, we believe this type of measure should not be used as the primary report correctness evaluation, unless it can be proven that the report reproduces exactly the classification made (e.g. by a template-filling process).","sent1: The methods in this category measure general quality aspects of the generated text, and are originated from translation, summarizing or captioning tasks.
sent2: The most widely used metrics in the papers reviewed are BLEU [102], ROUGE-L [90], METEOR [12,82] and CIDEr [140], which measure the similarity of a target text (also referred to as candidate), against one or more reference texts (ground truth).
sent3: These metrics are mainly based on counting n-gram matchings between the candidate and the ground truth.
sent4: BLEU is precision-oriented, ROUGE-L and METEOR are F1-scores that can be biased towards precision or recall with a given parameter, and CIDEr attempts to capture both precision and recall through a TF-IDF score.
sent5: Most of these metrics have variants and parameters for their calculation: ROUGE is a set of multiple metrics, being ROUGE-L the only one used in this task; METEOR has variants presented by the same authors [30][31][32]; and CIDEr was presented with the CIDEr-D variant to prevent gameability effects.
sent6: SPICE [9] is a metric designed for the image captioning task, and evaluates the underlying meaning of the sentences describing the image scene, partially disregarding fluency or grammatical aspects.
sent7: Specifically, the text is parsed as a graph, capturing the objects, their described characteristics and relations, which are then measured against the ground truth using an F1-score.
sent8: Even though SPICE attempts to assess the semantic information in a caption, we believe it is not suitable for medical reports, as the graph parsing is designed for general domain objects.
sent9: Nonetheless, Zhang et al. [162] presented the medical correctness metric MIRQI, applying a similar idea in a specific medical domain, which we will discuss in the next subsection (5.4.2).
sent10: Besides standard captioning metrics, we identified two other approaches to measure text quality.
sent11: First, Alsharid et al. [7] used Grammar Bot 3 , a rule and statistics based automated system that counts the grammatical errors in sentences.
sent12: Second, Harzig et al. 2019a [48] measured the sentence variability, by counting the different sentences in a set of reports.
sent13: They argue that the sentences indicating abnormalities occur very rarely in the dataset, while the ones indicating normality are the most frequent.
sent14: Hence, a certain level of variability is desired, and a system generating reports with low variability may indicate that not all medical conditions are being captured.
sent15: Lastly, both works from Li et al. [85,86] performed human evaluation with non-expert users via Amazon Mechanical Turk (AMT), following the same procedure.
sent16: The authors presented two reports to the AMT participants, one generated with the proposed model and one generated with the CoAtt model [68] as baseline, and asked them to choose the most similar with the ground truth in terms of fluency, abnormalities correctness and content coverage.
sent17: The results shown that their report was preferred around 50-60% of the cases, while the baseline around 20-30% (for the rest, none or both were preferred).
sent18: We categorize this evaluation as a text quality metric, as the participants are not experts, and their answers are not fine-grained (i.e., did not specify what failed: fluency, correctness or coverage; or by how much they failed).
sent19: 5.4.2 Medical correctness metrics.
sent20: While the most common purpose of the text quality metrics is to measure the similarity between the generated report and a ground truth, they do not necessarily capture the medical facts in the reports [11,17,92,107,108,161].
sent21: For example, the sentences ""effusion is observed"" and ""effusion is not observed"" are very similar, thus may present a very high score for any metric based on n-gram matching, though the medical facts are the exact opposite.
sent22: Therefore, an evaluation directly measuring the reports correctness is required, not necessarily taking into account fluency, grammatical rules or text quality in general.
sent23: From the literature reviewed, in ten works [7,16,61,67,85,92,98,148,154,162] the authors presented an automatic metric to address this issue, four works [7,38,126,131] did a formal expert evaluation, and multiple works [49, 76, 86, 89, 94, 120, 126, 131, 144, 155-158, 162, 163] evaluated medical correctness indirectly from auxiliary tasks.
sent24: The methods are listed in Table 6 and are further discussed next.
sent25: In several works the authors presented a method that detects concepts in the generated and ground truth reports, and compare the results using common classification metrics, such as accuracy, F1-score, and more.
sent26: The main difference between these methods lies in how the concepts are automatically detected in the reports.
sent27: The simplest approaches are keyword-based, which consists in reporting the ratio of a set of keywords found between the generated report and ground truth, like MeSH Accuracy [61] that uses MeSH terms, and Keyword Accuracy that uses 438 MTI terms (presented by Xue et al. [154] and used in A3FN [150]).
sent28: Similarly, Medical Abnormality Terminology Detection [85] calculates precision and false positive rate of the 10 most frequent abnormalityrelated terms in the dataset; and Wu et al. [148] calculated accuracy, sensitivity and specificity for a set of keywords.
sent29: Other approaches are abnormality-based, which attempt to directly classify abnormalities from the report by different means: Abnormality Detection [67] uses manually designed patterns; Medical Abnormality Detection [92] uses the CheXpert labeler tool [63]; Biswal et al. [16] used a character-level CNN [160] that classifies multiple CheXpert labels [63]; and Moradi et al. [98] used a proprietary software to extract semantic descriptors.
sent30: Lastly, Anatomical Relevance Score (ARS) [7] is a body-part-based approach, which detects the anatomical elements mentioned in a report considering the vocabulary used.
sent31: Though these methods may be useful for measuring medical correctness to a certain degree, there is no consensus or standard, and there is no formal evaluation of the correlation with expert judgement.
sent32: From the discussed techniques, Alsharid et al. [7] are the only authors that also performed an expert evaluation of the generated reports, though they did not conduct a correlation or similar analysis to validate the ARS method.
sent33: Zhang et al. [162] went further with the concept extraction and presented Medical Image Report Quality Index (MIRQI), which works in a similar fashion as the SPICE [9] metric presented in the text quality subsection (5.4.1).
sent34: MIRQI applies ideas from NegBio [106] and the CheXpert labeler [63] to identify diseases or medical conditions in the reports, considering synonyms and negations, and uses the Stanford parser [23] to obtain semantic dependencies and finer-grained attributes from each sentence, such as severity, size, shape, body parts, etc.
sent35: With this information, an abnormality graph is built for each report, where each node is a disease with its attributes, and the nodes are connected if they belong to the same organ or tissue.
sent36: Lastly, the graphs from the ground truth and generated reports are matched node-wise, and MIRQI-p (precision), MIRQI-r (recall) and MIRQI-F1 (F1-score) are computed.
sent37: Compared to the formerly discussed correctness metrics, we believe this approach seems more robust to assess the medical facts in the reports, as it attempts to capture the attributes and relations, opposed to the concepts only.
sent38: However, the authors did not present an evaluation against expert judgement, so we cannot determine if this metric is sufficient.
sent39: Considering human evaluation, only a few works [7,38,126,131] present a formal expert medical correctness assessment.
sent40: In the work by Alsharid et al. [7] a medical professional assessed the reports on a Likert Scale from 0 to 2 in four different aspects: accurately describes the image, presents no incorrect information, is grammatically correct and is relevant for the image; the results were further separated for samples from different body parts, showing averages between 0.5 and 1.
sent41: Gale et al. [38] asked a radiologist to evaluate the correctness of the hip fractures description, finding that the fracture's character was properly described 98% of the cases, while the fracture location only for 90%.
sent42: In the work by Tian et al. [131] a medical expert evaluated 30 randomly selected reports with a rating from 1 (definite accept) to 5 (definite reject), scoring an average of 2.33.
sent43: Lastly, Spinks and Moens [126] asked four questions to three experts regarding the generated reports, where the third and fourth questions measured correctness: ""Do you agree with the proposed diagnosis?"", answering 0 (no) or 1 (yes) and ""How certain are you about your final diagnosis?"", from 1 (not sure) to 4 (very sure).
sent44: The average scores were high (0.88 and 3.75), showing agreement with the model's diagnosis, and certainty on the experts' diagnoses.
sent45: The other questions concerned explainability aspects, and are detailed in the next subsection (5.4.3).
sent46: So far, there is no standard approach to perform an expert evaluation, though we believe the first two approaches provide finer-grained information than the latter two, and hence should be more useful for determining in which cases the models are failing and for designing improvements.
sent47: The certainty question should also be very useful, as diagnoses may be susceptible to human judgement.
sent48: Lastly, multiple papers [49,86,89,94,120,131,144,155,156,158,162,163] evaluated the performance of the auxiliary tasks with ROC-AUC, accuracy and other typical classification or segmentation metrics, as shown in Table 6.
sent49: Note that in any of these cases, the task is a previous or intermediary step of the process and is not derived from the report.
sent50: In consequence, even if the classification has great performance, the language component could be performing poorly, and the generated reports still may be inaccurate.
sent51: Accordingly, we believe this type of measure should not be used as the primary report correctness evaluation, unless it can be proven that the report reproduces exactly the classification made (e.g. by a template-filling process).
sent52: The methods in this category measure general quality aspects of the generated text, and are originated from translation, summarizing or captioning tasks.
sent53: The most widely used metrics in the papers reviewed are BLEU [102], ROUGE-L [90], METEOR [12,82] and CIDEr [140], which measure the similarity of a target text (also referred to as candidate), against one or more reference texts (ground truth).
sent54: These metrics are mainly based on counting n-gram matchings between the candidate and the ground truth.
sent55: BLEU is precision-oriented, ROUGE-L and METEOR are F1-scores that can be biased towards precision or recall with a given parameter, and CIDEr attempts to capture both precision and recall through a TF-IDF score.
sent56: Most of these metrics have variants and parameters for their calculation: ROUGE is a set of multiple metrics, being ROUGE-L the only one used in this task; METEOR has variants presented by the same authors [30][31][32]; and CIDEr was presented with the CIDEr-D variant to prevent gameability effects.
sent57: SPICE [9] is a metric designed for the image captioning task, and evaluates the underlying meaning of the sentences describing the image scene, partially disregarding fluency or grammatical aspects.
sent58: Specifically, the text is parsed as a graph, capturing the objects, their described characteristics and relations, which are then measured against the ground truth using an F1-score.
sent59: Even though SPICE attempts to assess the semantic information in a caption, we believe it is not suitable for medical reports, as the graph parsing is designed for general domain objects.
sent60: Nonetheless, Zhang et al. [162] presented the medical correctness metric MIRQI, applying a similar idea in a specific medical domain, which we will discuss in the next subsection (5.4.2).
sent61: Besides standard captioning metrics, we identified two other approaches to measure text quality.
sent62: First, Alsharid et al. [7] used Grammar Bot 3 , a rule and statistics based automated system that counts the grammatical errors in sentences.
sent63: Second, Harzig et al. 2019a [48] measured the sentence variability, by counting the different sentences in a set of reports.
sent64: They argue that the sentences indicating abnormalities occur very rarely in the dataset, while the ones indicating normality are the most frequent.
sent65: Hence, a certain level of variability is desired, and a system generating reports with low variability may indicate that not all medical conditions are being captured.
sent66: Lastly, both works from Li et al. [85,86] performed human evaluation with non-expert users via Amazon Mechanical Turk (AMT), following the same procedure.
sent67: The authors presented two reports to the AMT participants, one generated with the proposed model and one generated with the CoAtt model [68] as baseline, and asked them to choose the most similar with the ground truth in terms of fluency, abnormalities correctness and content coverage.
sent68: The results shown that their report was preferred around 50-60% of the cases, while the baseline around 20-30% (for the rest, none or both were preferred).
sent69: We categorize this evaluation as a text quality metric, as the participants are not experts, and their answers are not fine-grained (i.e., did not specify what failed: fluency, correctness or coverage; or by how much they failed).
sent70: 5.4.2 Medical correctness metrics.
sent71: While the most common purpose of the text quality metrics is to measure the similarity between the generated report and a ground truth, they do not necessarily capture the medical facts in the reports [11,17,92,107,108,161].
sent72: For example, the sentences ""effusion is observed"" and ""effusion is not observed"" are very similar, thus may present a very high score for any metric based on n-gram matching, though the medical facts are the exact opposite.
sent73: Therefore, an evaluation directly measuring the reports correctness is required, not necessarily taking into account fluency, grammatical rules or text quality in general.
sent74: From the literature reviewed, in ten works [7,16,61,67,85,92,98,148,154,162] the authors presented an automatic metric to address this issue, four works [7,38,126,131] did a formal expert evaluation, and multiple works [49, 76, 86, 89, 94, 120, 126, 131, 144, 155-158, 162, 163] evaluated medical correctness indirectly from auxiliary tasks.
sent75: The methods are listed in Table 6 and are further discussed next.
sent76: In several works the authors presented a method that detects concepts in the generated and ground truth reports, and compare the results using common classification metrics, such as accuracy, F1-score, and more.
sent77: The main difference between these methods lies in how the concepts are automatically detected in the reports.
sent78: The simplest approaches are keyword-based, which consists in reporting the ratio of a set of keywords found between the generated report and ground truth, like MeSH Accuracy [61] that uses MeSH terms, and Keyword Accuracy that uses 438 MTI terms (presented by Xue et al. [154] and used in A3FN [150]).
sent79: Similarly, Medical Abnormality Terminology Detection [85] calculates precision and false positive rate of the 10 most frequent abnormalityrelated terms in the dataset; and Wu et al. [148] calculated accuracy, sensitivity and specificity for a set of keywords.
sent80: Other approaches are abnormality-based, which attempt to directly classify abnormalities from the report by different means: Abnormality Detection [67] uses manually designed patterns; Medical Abnormality Detection [92] uses the CheXpert labeler tool [63]; Biswal et al. [16] used a character-level CNN [160] that classifies multiple CheXpert labels [63]; and Moradi et al. [98] used a proprietary software to extract semantic descriptors.
sent81: Lastly, Anatomical Relevance Score (ARS) [7] is a body-part-based approach, which detects the anatomical elements mentioned in a report considering the vocabulary used.
sent82: Though these methods may be useful for measuring medical correctness to a certain degree, there is no consensus or standard, and there is no formal evaluation of the correlation with expert judgement.
sent83: From the discussed techniques, Alsharid et al. [7] are the only authors that also performed an expert evaluation of the generated reports, though they did not conduct a correlation or similar analysis to validate the ARS method.
sent84: Zhang et al. [162] went further with the concept extraction and presented Medical Image Report Quality Index (MIRQI), which works in a similar fashion as the SPICE [9] metric presented in the text quality subsection (5.4.1).
sent85: MIRQI applies ideas from NegBio [106] and the CheXpert labeler [63] to identify diseases or medical conditions in the reports, considering synonyms and negations, and uses the Stanford parser [23] to obtain semantic dependencies and finer-grained attributes from each sentence, such as severity, size, shape, body parts, etc.
sent86: With this information, an abnormality graph is built for each report, where each node is a disease with its attributes, and the nodes are connected if they belong to the same organ or tissue.
sent87: Lastly, the graphs from the ground truth and generated reports are matched node-wise, and MIRQI-p (precision), MIRQI-r (recall) and MIRQI-F1 (F1-score) are computed.
sent88: Compared to the formerly discussed correctness metrics, we believe this approach seems more robust to assess the medical facts in the reports, as it attempts to capture the attributes and relations, opposed to the concepts only.
sent89: However, the authors did not present an evaluation against expert judgement, so we cannot determine if this metric is sufficient.
sent90: Considering human evaluation, only a few works [7,38,126,131] present a formal expert medical correctness assessment.
sent91: In the work by Alsharid et al. [7] a medical professional assessed the reports on a Likert Scale from 0 to 2 in four different aspects: accurately describes the image, presents no incorrect information, is grammatically correct and is relevant for the image; the results were further separated for samples from different body parts, showing averages between 0.5 and 1.
sent92: Gale et al. [38] asked a radiologist to evaluate the correctness of the hip fractures description, finding that the fracture's character was properly described 98% of the cases, while the fracture location only for 90%.
sent93: In the work by Tian et al. [131] a medical expert evaluated 30 randomly selected reports with a rating from 1 (definite accept) to 5 (definite reject), scoring an average of 2.33.
sent94: Lastly, Spinks and Moens [126] asked four questions to three experts regarding the generated reports, where the third and fourth questions measured correctness: ""Do you agree with the proposed diagnosis?"", answering 0 (no) or 1 (yes) and ""How certain are you about your final diagnosis?"", from 1 (not sure) to 4 (very sure).
sent95: The average scores were high (0.88 and 3.75), showing agreement with the model's diagnosis, and certainty on the experts' diagnoses.
sent96: The other questions concerned explainability aspects, and are detailed in the next subsection (5.4.3).
sent97: So far, there is no standard approach to perform an expert evaluation, though we believe the first two approaches provide finer-grained information than the latter two, and hence should be more useful for determining in which cases the models are failing and for designing improvements.
sent98: The certainty question should also be very useful, as diagnoses may be susceptible to human judgement.
sent99: Lastly, multiple papers [49,86,89,94,120,131,144,155,156,158,162,163] evaluated the performance of the auxiliary tasks with ROC-AUC, accuracy and other typical classification or segmentation metrics, as shown in Table 6.
sent100: Note that in any of these cases, the task is a previous or intermediary step of the process and is not derived from the report.
sent101: In consequence, even if the classification has great performance, the language component could be performing poorly, and the generated reports still may be inaccurate.
sent102: Accordingly, we believe this type of measure should not be used as the primary report correctness evaluation, unless it can be proven that the report reproduces exactly the classification made (e.g. by a template-filling process).","1. What do the methods in the text quality metrics category measure? sent1
2. What are the most widely used metrics for measuring text quality in the reviewed papers? sent2
3. How are these metrics primarily calculated? sent3
4. What are the characteristics of BLEU, ROUGE-L, METEOR, and CIDEr metrics? sent4
5. Do these metrics have variants and parameters for their calculation? sent5
6. What is SPICE, and what does it evaluate? sent6
7. How does SPICE measure the underlying meaning of sentences? sent7
8. Why is SPICE considered unsuitable for medical reports? sent8
9. What metric did Zhang et al. present, and what is its purpose? sent9
10. Besides standard captioning metrics, what other approaches are identified for measuring text quality? sent10
    10.1. What system did Alsharid et al. use to count grammatical errors? sent11
    10.2. How did Harzig et al. measure sentence variability? sent12
        10.2.1. What argument is made regarding sentence variability? sent13
        10.2.2. Why is a certain level of variability desired in report generation? sent14
11. How did Li et al. perform human evaluation of text quality? sent15
    11.1. What procedure was followed in the human evaluation by Li et al.? sent16
    11.2. What were the results of the human evaluation conducted by Li et al.? sent17
    11.3. How is this evaluation categorized, and why? sent18
12. What is the main purpose of text quality metrics, and what do they fail to capture? sent20
13. Why is an evaluation directly measuring report correctness required? sent21
14. What methods are used to address the issue of capturing medical facts in reports? sent23
    14.1. How do several works detect concepts in reports, and what metrics are used for comparison? sent25
    14.2. What is the main difference between these methods? sent26
    14.3. What are the simplest approaches for concept detection in reports? sent27
    14.4. What are some abnormality-based approaches for classifying abnormalities in reports? sent29
    14.5. What is the Anatomical Relevance Score (ARS), and how does it work? sent30
15. What is the consensus on the usefulness of these methods for measuring medical correctness? sent31
16. What did Zhang et al. present as a more robust approach for assessing medical facts in reports? sent33
    16.1. How does MIRQI work, and what does it aim to capture? sent34
    16.2. How are abnormality graphs built and compared in MIRQI? sent35
    16.3. Why is this approach considered more robust compared to other correctness metrics? sent37
17. What is the current state of expert evaluation for medical correctness, and what are the challenges? sent39
    17.1. How did Alsharid et al. conduct expert evaluation, and what were the results? sent40
    17.2. What were the findings of Gale et al. regarding hip fracture descriptions? sent41
    17.3. How did Tian et al. evaluate reports, and what was the average score? sent42
    17.4. What questions did Spinks and Moens ask experts, and what were the results? sent43
18. What is the current standard for expert evaluation, and what are the recommendations? sent46
19. How is the performance of auxiliary tasks evaluated, and what are the limitations? sent48
    19.1. Why should auxiliary task performance not be used as the primary report correctness evaluation? sent51"
112487687,Overview of the 2013 ALTA Shared Task,https://www.semanticscholar.org/paper/200d6e8c0aae03704c5c3f51823532597e36d325,The Training and Test Sets,8,"We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.

The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus. Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems. The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard. The second test set was a ""private"" test set that was used to determine the final scores. By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day. As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.

To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 . We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words. We lowercased the tokens and removed those that matched our list of punctuation marks.

The Wikipedia training data consisted of 18 files with a total of 306,445 words. The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.

We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.

The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus. Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems. The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard. The second test set was a ""private"" test set that was used to determine the final scores. By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day. As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.

To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 . We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words. We lowercased the tokens and removed those that matched our list of punctuation marks.

The Wikipedia training data consisted of 18 files with a total of 306,445 words. The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.","sent1: We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.
sent2: The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus.
sent3: Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems.
sent4: The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard.
sent5: The second test set was a ""private"" test set that was used to determine the final scores.
sent6: By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day.
sent7: As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.
sent8: To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 .
sent9: We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words.
sent10: We lowercased the tokens and removed those that matched our list of punctuation marks.
sent11: The Wikipedia training data consisted of 18 files with a total of 306,445 words.
sent12: The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.
sent13: We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.
sent14: The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus.
sent15: Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems.
sent16: The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard.
sent17: The second test set was a ""private"" test set that was used to determine the final scores.
sent18: By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day.
sent19: As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.
sent20: To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 .
sent21: We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words.
sent22: We lowercased the tokens and removed those that matched our list of punctuation marks.
sent23: The Wikipedia training data consisted of 18 files with a total of 306,445 words.
sent24: The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.","1. What data sources were used to produce the training and test sets? sent1
    1.1. From which sections of the English Gigaword Corpus are the data by Baldwin and Joseph (2009) derived? sent2
2. How were the two test sets utilized in the task? sent3
    2.1. What was the purpose of the ""public"" test set? sent3
    2.2. How did participants interact with the ""public"" test set? sent4
    2.3. What was the purpose of the ""private"" test set? sent5
    2.4. Why were separate ""public"" and ""private"" test sets used? sent6
3. What training data was used in addition to the data by Baldwin and Joseph (2009)? sent7
    3.1. How was the Wikipedia text processed for training data? sent8, sent9, sent10
    3.2. How many words did the Wikipedia training data consist of? sent11
4. What are the word counts for the files from Baldwin and Joseph (2009)? sent12"
227061,Detection of User Demographics on Social Media: A Review of Methods and Recommendations for Best Practices,https://www.semanticscholar.org/paper/43b55cc08eba13fb309e77a1033da0ad60bd7a5e,Race and ethnicity.,21,"Of the papers that address race or ethnicity, eight used supervised learning approaches [35], [36], [52], [53], [53], [56], [83], [98], two used adjusted data matching [84], [99] and two relied on facial recognition [94], [95]. Some studies focused on user names or posts, or both to predict race or ethnicity [52], [53], [56], [83]. The classification performance among these paperswhile promising -did not vary significantly. Three studies obtained maximum F-scores between approximately 0.70 and 0.76 [36], [53], [54], and two obtained accuracy between 0.78 and 0.84 [35], [36].

Certain profile features could potentially improve predictions of race and ethnicity. First, profile photos appear to be a good indicator of race or ethnicity. Pennacchiotti and Popescu [53] for instance, obtained a higher precision (0.878) using profile photos to evaluate race, compared to a gradient boosted decision tree classifier that incorporated a combination of lexical features from users posts, and user activity measures (0.629). Second, user profile descriptions could improve methods for predicting race and ethnicity. For example, Chen et al. [36] observed that adding user descriptions into classifiers consistently improved accuracy, precision and recall for n-gram and name based models. Third, Chang et al. [86], and Mislove et al. [97] also found user location to improve predictability of surnames, and Mohammady and Culotta [56] used location as a feature for calibrating supervised learning models.

One challenge associated with the prediction of race and ethnicity is the need to create a clear, bounded definition. Racial and ethnic identity is complex and evaluations by others may not match an individuals' self-identification. Most of the selected studies were vague and varied significantly in their definition of race or ethnicity. Pennacchiotti and Popescu [53] claimed to predict ethnicity, but classified users as African American or not. Chang et al. [99] and Chen et al. [36] both used the term ethnicity to refer to a classification system that includes both racial and ethnic identities (black, white, Asian, Hispanic), and Mohammady and Culotta [56] used the same classification system but addressed it as race. Future work should directly address this challenge.","sent1: Of the papers that address race or ethnicity, eight used supervised learning approaches [35], [36], [52], [53], [53], [56], [83], [98], two used adjusted data matching [84], [99] and two relied on facial recognition [94], [95].
sent2: Some studies focused on user names or posts, or both to predict race or ethnicity [52], [53], [56], [83].
sent3: The classification performance among these paperswhile promising -did not vary significantly.
sent4: Three studies obtained maximum F-scores between approximately 0.70 and 0.76 [36], [53], [54], and two obtained accuracy between 0.78 and 0.84 [35], [36].Certain profile features could potentially improve predictions of race and ethnicity.
sent5: First, profile photos appear to be a good indicator of race or ethnicity.
sent6: Pennacchiotti and Popescu [53] for instance, obtained a higher precision (0.878) using profile photos to evaluate race, compared to a gradient boosted decision tree classifier that incorporated a combination of lexical features from users posts, and user activity measures (0.629).
sent7: Second, user profile descriptions could improve methods for predicting race and ethnicity.
sent8: For example, Chen et al. [36] observed that adding user descriptions into classifiers consistently improved accuracy, precision and recall for n-gram and name based models.
sent9: Third, Chang et al. [86], and Mislove et al. [97] also found user location to improve predictability of surnames, and Mohammady and Culotta [56] used location as a feature for calibrating supervised learning models.
sent10: One challenge associated with the prediction of race and ethnicity is the need to create a clear, bounded definition.
sent11: Racial and ethnic identity is complex and evaluations by others may not match an individuals' self-identification.
sent12: Most of the selected studies were vague and varied significantly in their definition of race or ethnicity.
sent13: Pennacchiotti and Popescu [53] claimed to predict ethnicity, but classified users as African American or not.
sent14: Chang et al. [99] and Chen et al. [36] both used the term ethnicity to refer to a classification system that includes both racial and ethnic identities (black, white, Asian, Hispanic), and Mohammady and Culotta [56] used the same classification system but addressed it as race.
sent15: Future work should directly address this challenge.","1. What methods have been used in papers addressing race or ethnicity detection? sent1
    1.1. What features have some studies focused on to predict race or ethnicity? sent2
    1.2. How does the classification performance vary among these papers? sent3
        1.2.1. What are the maximum F-scores and accuracy ranges obtained in some studies? sent4
2. What profile features could potentially improve predictions of race and ethnicity? sent5
    2.1. How did Pennacchiotti and Popescu [53] achieve higher precision using profile photos? sent6
    2.2. How can user profile descriptions improve prediction methods? sent7
        2.2.1. What improvements did Chen et al. [36] observe by adding user descriptions? sent8
    2.3. How does user location contribute to the predictability of race and ethnicity? sent9
3. What is a challenge associated with the prediction of race and ethnicity? sent10
    3.1. Why is racial and ethnic identity complex in evaluations? sent11
    3.2. How do the definitions of race or ethnicity vary in selected studies? sent12
        3.2.1. How did Pennacchiotti and Popescu [53] define ethnicity in their study? sent13
        3.2.2. How do Chang et al. [99] and Chen et al. [36] define ethnicity, and how does it differ from Mohammady and Culotta [56]? sent14
4. What should future work address regarding the prediction of race and ethnicity? sent15"
181562553,A Systematic Literature Review on Image Captioning,https://www.semanticscholar.org/paper/92ccf5a39c63cb5e1639be518e6db2e357acd58e,Datasets,10,"Most of the works are evaluated on Flickr30k [90] and MSCOCO [91] datasets. Both datasets are rich in the number of images and each image has five captions assigned which makes it very suitable to train and test the models. It is of course necessary to continuously compare models with the same datasets in order to check the performance, however, they are very limited in the object classes and scenarios presented. The need of new datasets has always been an open question in image captioning. Ref. [92] proposed a method for gathering large datasets of images from the internet which might be helpful for replacing MS COCO or Flickr datasets which were used in most of the previous researches. There have been several other datasets used for model evaluation, such as Lifelog dataset [10], Visual Genome dataset [20,36], IAPRTC-12 [45], OpenImages and Visual Relationship Detection datasets [36], but they were just single cases.

Recently the popularity in novel image scenarios has grown which has increased the demand of newer datasets even more. In ref. [93] the first rigorous and large-scale data set for novel object captioning, which contains more than 500 novel object classes, was introduced. Another realistic dataset was introduced in ref. [94]. It contains news images and their actual captions, along with their associated news articles, news categories, and keyword labels. Moreover, it is clear, that social networks are highly integrated into people's lifestyle. There are more and more images appearing on the social media, especially from the young generation, so it is important to analyze this data as well-for the most natural background, for the newest trends to be interpreted by machines, and to start learning and improving on those as well. Ref. [95] proposed a novel deep feature learning paradigm based on social collective intelligence, which can be acquired from the inexhaustible social multimedia content on the Web, particularly largely social images and tags, however, it was not further continued, at least to our knowledge.","sent1: Most of the works are evaluated on Flickr30k [90] and MSCOCO [91] datasets.
sent2: Both datasets are rich in the number of images and each image has five captions assigned which makes it very suitable to train and test the models.
sent3: It is of course necessary to continuously compare models with the same datasets in order to check the performance, however, they are very limited in the object classes and scenarios presented.
sent4: The need of new datasets has always been an open question in image captioning.
sent5: Ref. [92] proposed a method for gathering large datasets of images from the internet which might be helpful for replacing MS COCO or Flickr datasets which were used in most of the previous researches.
sent6: There have been several other datasets used for model evaluation, such as Lifelog dataset [10], Visual Genome dataset [20,36], IAPRTC-12 [45], OpenImages and Visual Relationship Detection datasets [36], but they were just single cases.
sent7: Recently the popularity in novel image scenarios has grown which has increased the demand of newer datasets even more.
sent8: In ref. [93] the first rigorous and large-scale data set for novel object captioning, which contains more than 500 novel object classes, was introduced.
sent9: Another realistic dataset was introduced in ref.[94].
sent10: It contains news images and their actual captions, along with their associated news articles, news categories, and keyword labels.
sent11: Moreover, it is clear, that social networks are highly integrated into people's lifestyle.
sent12: There are more and more images appearing on the social media, especially from the young generation, so it is important to analyze this data as well-for the most natural background, for the newest trends to be interpreted by machines, and to start learning and improving on those as well.
sent13: Ref. [95] proposed a novel deep feature learning paradigm based on social collective intelligence, which can be acquired from the inexhaustible social multimedia content on the Web, particularly largely social images and tags, however, it was not further continued, at least to our knowledge.","1. On which datasets are most works in image captioning evaluated? sent1
    1.1. Why are Flickr30k and MSCOCO datasets suitable for training and testing models? sent2
    1.2. What is a limitation of using the same datasets for model comparison? sent3
2. What has been an open question in image captioning regarding datasets? sent4
    2.1. What method was proposed to gather large datasets of images from the internet? sent5
3. What are some other datasets used for model evaluation in image captioning? sent6
4. How has the demand for newer datasets changed recently? sent7
    4.1. What is the first rigorous and large-scale dataset for novel object captioning? sent8
    4.2. What does another realistic dataset introduced in ref. [94] contain? sent9, sent10
5. How are social networks relevant to image captioning datasets? sent11
    5.1. Why is it important to analyze images from social media? sent12
    5.2. What novel paradigm was proposed based on social collective intelligence? sent13"
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,TR Discover,6,"TR Discover (Song et al, 2015) is a system providing an NLI which translates the input question in form of an English sentence (or sentence fragment) into SQL or SPARQL. It is either used for relational databases or ontologies, but does not need an ontology to work for relational databases. During the translation steps, TR Discover uses a First Order Logic (FOL) representation as an intermediate language. Furthermore, it provides auto-suggestions based on the user input. There are two types of suggestions: autocompletion and prediction.

TR Discover helps the users in formulating the question through an auto-suggestion feature. For example, assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1). When the users start typing 'p', TR Discover will not only suggest 'person' but also longer phrases like 'person directing' (autocomplete). After 'person directing' is selected (or typed), TR Discover will again suggest phrases, like 'movies' or even specific movies like 'Inglourious Basterds' (prediction). For input question Q1, the input could be 'person directing Inglourious Basterds'.

The suggestions are based upon the relationships and entities in the dataset and use the linguistic constraints encoded in a feature-based context-free grammar (FCFG). The grammar consists of grammar rules (G1-3) and lexical entries (L1-2). For the sample world (and the input question Q1), the following rules could be defined: The suggestions are computed based on the idea of left-corner parsing: given a query segment, it finds all grammar rules whose left corner on the right side matches the left side of the lexical entry of the query segment. Then, all leaf nodes (lexical entries) in the grammar that can be reached by using the adjacent element are found. For example, while typing 'person' (Q1), the lexical entries L1 and L2 are found and provided to the user.

TR Discover uses three steps to translate the English sentence or fragment of a sentence into a SQL or SPARQL query. The first step parses the input question into a FOL representation. The query parsing uses the FCFG. For the example input, the token 'person' Fig. 12 The parse tree for the FOL representation of the input question 'person directing Inglourious Basterds'. will be parsed by the lexical entry L1 and the token 'directing' will be parsed with the lexical entry L2. This leads to the FOL representation:

How exactly the phrase 'Inglourious Basterds' is matched to the base data and therefore can be used as part of the lexical entry L2 and how it is resolved, is not explained by Song et al (2015). If there are multiple possibilities to parse the input question, the first one is chosen.

The next step is to translate the generated FOL into a parse tree. The FOL parser takes a grammar and the FOL representation from the previous step, and generates a parse tree ( Figure 12) using ANTLER for implementation.

In the third step, an in-order traversal of the parse tree (provided by the previous step) is performed to translate it into an executable SQL or SPARQL query. While traversing the parse tree, the atomic logical conditions and connectors are put on a stack. After traversing, the constraints are popped from the stack to build the correct query constraints. The predicates are mapped to their corresponding attribute names (SQL) or ontology properties (SPARQL).

The strengths of TR Discover are the auto-suggestion and the possibility to translate natural language into different query languages such as SQL and SPARQL, because FOL is used as an intermediate language.

The weaknesses of TR Discover are that quantifiers (e.g., Q3: 'grossed most') cannot be used, synonyms are not properly handled, and negations only work for SPARQL. Song et al (2015) suggest extending TR Discover with a ranking system for multiple parses in the first step and to improve the synonym handling. Furthermore, they pointed out the possibility of applying user query logs to improve auto-suggestions.

TR Discover (Song et al, 2015) is a system providing an NLI which translates the input question in form of an English sentence (or sentence fragment) into SQL or SPARQL. It is either used for relational databases or ontologies, but does not need an ontology to work for relational databases. During the translation steps, TR Discover uses a First Order Logic (FOL) representation as an intermediate language. Furthermore, it provides auto-suggestions based on the user input. There are two types of suggestions: autocompletion and prediction.

TR Discover helps the users in formulating the question through an auto-suggestion feature. For example, assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1). When the users start typing 'p', TR Discover will not only suggest 'person' but also longer phrases like 'person directing' (autocomplete). After 'person directing' is selected (or typed), TR Discover will again suggest phrases, like 'movies' or even specific movies like 'Inglourious Basterds' (prediction). For input question Q1, the input could be 'person directing Inglourious Basterds'.

The suggestions are based upon the relationships and entities in the dataset and use the linguistic constraints encoded in a feature-based context-free grammar (FCFG). The grammar consists of grammar rules (G1-3) and lexical entries (L1-2). For the sample world (and the input question Q1), the following rules could be defined: The suggestions are computed based on the idea of left-corner parsing: given a query segment, it finds all grammar rules whose left corner on the right side matches the left side of the lexical entry of the query segment. Then, all leaf nodes (lexical entries) in the grammar that can be reached by using the adjacent element are found. For example, while typing 'person' (Q1), the lexical entries L1 and L2 are found and provided to the user.

TR Discover uses three steps to translate the English sentence or fragment of a sentence into a SQL or SPARQL query. The first step parses the input question into a FOL representation. The query parsing uses the FCFG. For the example input, the token 'person' Fig. 12 The parse tree for the FOL representation of the input question 'person directing Inglourious Basterds'. will be parsed by the lexical entry L1 and the token 'directing' will be parsed with the lexical entry L2. This leads to the FOL representation:

How exactly the phrase 'Inglourious Basterds' is matched to the base data and therefore can be used as part of the lexical entry L2 and how it is resolved, is not explained by Song et al (2015). If there are multiple possibilities to parse the input question, the first one is chosen.

The next step is to translate the generated FOL into a parse tree. The FOL parser takes a grammar and the FOL representation from the previous step, and generates a parse tree ( Figure 12) using ANTLER for implementation.

In the third step, an in-order traversal of the parse tree (provided by the previous step) is performed to translate it into an executable SQL or SPARQL query. While traversing the parse tree, the atomic logical conditions and connectors are put on a stack. After traversing, the constraints are popped from the stack to build the correct query constraints. The predicates are mapped to their corresponding attribute names (SQL) or ontology properties (SPARQL).

The strengths of TR Discover are the auto-suggestion and the possibility to translate natural language into different query languages such as SQL and SPARQL, because FOL is used as an intermediate language.

The weaknesses of TR Discover are that quantifiers (e.g., Q3: 'grossed most') cannot be used, synonyms are not properly handled, and negations only work for SPARQL. Song et al (2015) suggest extending TR Discover with a ranking system for multiple parses in the first step and to improve the synonym handling. Furthermore, they pointed out the possibility of applying user query logs to improve auto-suggestions.","sent1: TR Discover (Song et al, 2015) is a system providing an NLI which translates the input question in form of an English sentence (or sentence fragment) into SQL or SPARQL.
sent2: It is either used for relational databases or ontologies, but does not need an ontology to work for relational databases.
sent3: During the translation steps, TR Discover uses a First Order Logic (FOL) representation as an intermediate language.
sent4: Furthermore, it provides auto-suggestions based on the user input.
sent5: There are two types of suggestions: autocompletion and prediction.
sent6: TR Discover helps the users in formulating the question through an auto-suggestion feature.
sent7: For example, assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1).
sent8: When the users start typing 'p', TR Discover will not only suggest 'person' but also longer phrases like 'person directing' (autocomplete).
sent9: After 'person directing' is selected (or typed), TR Discover will again suggest phrases, like 'movies' or even specific movies like 'Inglourious Basterds' (prediction).
sent10: For input question Q1, the input could be 'person directing Inglourious Basterds'.
sent11: The suggestions are based upon the relationships and entities in the dataset and use the linguistic constraints encoded in a feature-based context-free grammar (FCFG).
sent12: The grammar consists of grammar rules (G1-3) and lexical entries (L1-2).
sent13: For the sample world (and the input question Q1), the following rules could be defined: The suggestions are computed based on the idea of left-corner parsing: given a query segment, it finds all grammar rules whose left corner on the right side matches the left side of the lexical entry of the query segment.
sent14: Then, all leaf nodes (lexical entries) in the grammar that can be reached by using the adjacent element are found.
sent15: For example, while typing 'person' (Q1), the lexical entries L1 and L2 are found and provided to the user.
sent16: TR Discover uses three steps to translate the English sentence or fragment of a sentence into a SQL or SPARQL query.
sent17: The first step parses the input question into a FOL representation.
sent18: The query parsing uses the FCFG.
sent19: For the example input, the token 'person' Fig. 12 The parse tree for the FOL representation of the input question 'person directing Inglourious Basterds'. will be parsed by the lexical entry L1 and the token 'directing' will be parsed with the lexical entry L2.
sent20: This leads to the FOL representation:How exactly the phrase 'Inglourious Basterds' is matched to the base data and therefore can be used as part of the lexical entry L2 and how it is resolved, is not explained by Song et al (2015).
sent21: If there are multiple possibilities to parse the input question, the first one is chosen.
sent22: The next step is to translate the generated FOL into a parse tree.
sent23: The FOL parser takes a grammar and the FOL representation from the previous step, and generates a parse tree ( Figure 12) using ANTLER for implementation.
sent24: In the third step, an in-order traversal of the parse tree (provided by the previous step) is performed to translate it into an executable SQL or SPARQL query.
sent25: While traversing the parse tree, the atomic logical conditions and connectors are put on a stack.
sent26: After traversing, the constraints are popped from the stack to build the correct query constraints.
sent27: The predicates are mapped to their corresponding attribute names (SQL) or ontology properties (SPARQL).
sent28: The strengths of TR Discover are the auto-suggestion and the possibility to translate natural language into different query languages such as SQL and SPARQL, because FOL is used as an intermediate language.
sent29: The weaknesses of TR Discover are that quantifiers (e.g., Q3: 'grossed most') cannot be used, synonyms are not properly handled, and negations only work for SPARQL.
sent30: Song et al (2015) suggest extending TR Discover with a ranking system for multiple parses in the first step and to improve the synonym handling.
sent31: Furthermore, they pointed out the possibility of applying user query logs to improve auto-suggestions.
sent32: TR Discover (Song et al, 2015) is a system providing an NLI which translates the input question in form of an English sentence (or sentence fragment) into SQL or SPARQL.
sent33: It is either used for relational databases or ontologies, but does not need an ontology to work for relational databases.
sent34: During the translation steps, TR Discover uses a First Order Logic (FOL) representation as an intermediate language.
sent35: Furthermore, it provides auto-suggestions based on the user input.
sent36: There are two types of suggestions: autocompletion and prediction.
sent37: TR Discover helps the users in formulating the question through an auto-suggestion feature.
sent38: For example, assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1).
sent39: When the users start typing 'p', TR Discover will not only suggest 'person' but also longer phrases like 'person directing' (autocomplete).
sent40: After 'person directing' is selected (or typed), TR Discover will again suggest phrases, like 'movies' or even specific movies like 'Inglourious Basterds' (prediction).
sent41: For input question Q1, the input could be 'person directing Inglourious Basterds'.
sent42: The suggestions are based upon the relationships and entities in the dataset and use the linguistic constraints encoded in a feature-based context-free grammar (FCFG).
sent43: The grammar consists of grammar rules (G1-3) and lexical entries (L1-2).
sent44: For the sample world (and the input question Q1), the following rules could be defined: The suggestions are computed based on the idea of left-corner parsing: given a query segment, it finds all grammar rules whose left corner on the right side matches the left side of the lexical entry of the query segment.
sent45: Then, all leaf nodes (lexical entries) in the grammar that can be reached by using the adjacent element are found.
sent46: For example, while typing 'person' (Q1), the lexical entries L1 and L2 are found and provided to the user.
sent47: TR Discover uses three steps to translate the English sentence or fragment of a sentence into a SQL or SPARQL query.
sent48: The first step parses the input question into a FOL representation.
sent49: The query parsing uses the FCFG.
sent50: For the example input, the token 'person' Fig. 12 The parse tree for the FOL representation of the input question 'person directing Inglourious Basterds'. will be parsed by the lexical entry L1 and the token 'directing' will be parsed with the lexical entry L2.
sent51: This leads to the FOL representation:How exactly the phrase 'Inglourious Basterds' is matched to the base data and therefore can be used as part of the lexical entry L2 and how it is resolved, is not explained by Song et al (2015).
sent52: If there are multiple possibilities to parse the input question, the first one is chosen.
sent53: The next step is to translate the generated FOL into a parse tree.
sent54: The FOL parser takes a grammar and the FOL representation from the previous step, and generates a parse tree ( Figure 12) using ANTLER for implementation.
sent55: In the third step, an in-order traversal of the parse tree (provided by the previous step) is performed to translate it into an executable SQL or SPARQL query.
sent56: While traversing the parse tree, the atomic logical conditions and connectors are put on a stack.
sent57: After traversing, the constraints are popped from the stack to build the correct query constraints.
sent58: The predicates are mapped to their corresponding attribute names (SQL) or ontology properties (SPARQL).
sent59: The strengths of TR Discover are the auto-suggestion and the possibility to translate natural language into different query languages such as SQL and SPARQL, because FOL is used as an intermediate language.
sent60: The weaknesses of TR Discover are that quantifiers (e.g., Q3: 'grossed most') cannot be used, synonyms are not properly handled, and negations only work for SPARQL.
sent61: Song et al (2015) suggest extending TR Discover with a ranking system for multiple parses in the first step and to improve the synonym handling.
sent62: Furthermore, they pointed out the possibility of applying user query logs to improve auto-suggestions.","1. What is TR Discover and what does it provide? sent1
    1.1. For what types of databases or ontologies is TR Discover used? sent2
    1.2. What intermediate language does TR Discover use during translation? sent3
    1.3. What feature does TR Discover provide to assist users in formulating questions? sent4
        1.3.1. What are the two types of suggestions provided by TR Discover? sent5
        1.3.2. How does TR Discover help users with auto-suggestions? sent6
            1.3.2.1. Can you provide an example of how TR Discover suggests phrases? sent7, sent8, sent9
            1.3.2.2. What could be an example input question for TR Discover? sent10
    1.4. On what basis are the suggestions in TR Discover made? sent11
        1.4.1. What does the grammar in TR Discover consist of? sent12
        1.4.2. How are suggestions computed in TR Discover? sent13, sent14
            1.4.2.1. Can you provide an example of how lexical entries are found? sent15
2. What are the steps TR Discover uses to translate an English sentence into a SQL or SPARQL query? sent16
    2.1. What is the first step in the translation process? sent17
        2.1.1. What does the query parsing use in the first step? sent18
        2.1.2. How are tokens parsed in the example input? sent19
            2.1.2.1. What is not explained by Song et al. (2015) regarding the phrase 'Inglourious Basterds'? sent20
        2.1.3. What happens if there are multiple possibilities to parse the input question? sent21
    2.2. What is the second step in the translation process? sent22
        2.2.1. How is the parse tree generated in the second step? sent23
    2.3. What is the third step in the translation process? sent24
        2.3.1. What happens during the in-order traversal of the parse tree? sent25
        2.3.2. How are constraints built during the translation process? sent26
        2.3.3. How are predicates mapped in the final query? sent27
3. What are the strengths of TR Discover? sent28
4. What are the weaknesses of TR Discover? sent29
5. What improvements do Song et al. (2015) suggest for TR Discover? sent30
    5.1. What additional possibility did they point out for improving TR Discover? sent31"
237532483,A Survey on Temporal Sentence Grounding in Videos,https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453,Two-stage method,14,"For a two-stage method, the pre-segmenting of proposal candidates is conducted separately with the model computation. It takes the pre-segmented candidates and the sentence query as inputs of a cross-modal matching module for target segment localization. The two-stage methods can be grouped into two categories based on different ways to generate proposals.

2.1.1 sliding window-based. Early methods including MCN [23], CTRL [16], ROLE [38], MCF [63], ACRN [37], SLTA [27] and ACL-K [18], adopt multi-scale sliding window sampling strategy for the generation of candidate proposals. There are two pioneering works MCN [23] and CTRL [16] to define TSGV task and construct benchmark datasets. Firstly, Hendricks et al. [23] propose MCN, which samples all the candidate moments (i.e. segments) via sliding window mechanism, and then projects the video moment representation and query representation into a common embedding space. The ℓ 2 distance between the sentence query and the corresponding target video moment in this space is minimized to supervise the model training (c.f ., Fig. 3b). Specifically, MCN encourages the sentence query to be closer to the target moment than negative moments in a shared embedding space. Since the negative moments either come from other segments within the same video (intra-video) or from different videos (inter-video), MCN devises two similar but different ranking loss functions:

where L ( , ) = max(0, − + ), is a margin. As for training sample , the intra-video ranking loss encourages sentence to be closer to the target moment at the location than the negative moments from other possible locations within the same video, while the inter-video ranking loss encourages sentence to be closer to the target one at location than the negative ones from other videos of the same location . The intra-video ranking loss is able to differentiate between subtle difference within a video while the inter-video ranking loss can differentiate between broad semantic concepts. At the same time, Gao et al. [16] propose CTRL, which is the first one to adapt R-CNN [20] methodology from object detection to the TSGV domain. Particularly, CTRL also leverages sliding window to obtain candidate segments of various lengths, and as shown in Fig. 3a, it exploits a multi-modal processing module to fuse the candidate segment representation with the sentence representation by three operators (i.e., add, multiply, and full-connected layer). Then, CTRL feeds the fused representation into another fully-connected layer to predict the alignment score and location offsets between the candidate segment and the target segment. CTRL designs a multi-task loss function to train the model, including visual-semantic alignment loss and location regression loss:

where is the visual-semantic alignment loss considering both aligned (video segment, query) pairs and misaligned pairs. , measures the alignment score between video segment and sentence . The location regression loss is only accounted for aligned pairs to predict the correct coordinates. is a smooth-L1 function.

Compared to above CTRL that treats the query as a whole, Liu et al. [38] further make some improvements by decomposing the query and adaptively get the important textual components according to the temporal video context.

Since CTRL overlooks the spatial-temporal information inside the moment and the query, Liu et al. [37] further propose an attentive cross-modal retrieval network (ACRN). With a memory attention network guided by the sentence query, ACRN adaptively assigns weights to the contextual moment representations for memorization to augment the moment representation. SLTA [27] also devises a spatial and language-temporal attention model to adaptively identify the relevant objects and interactions based on the query information.

Wu and Han [63] propose a multi-modal circulant fusion (MCF) in contrast to the simple fusion ways employed in CTRL including element-wise product, element-wise sum, or concatenation. MCF extends the visual/textual vector to the circulant matrix, which can fully exploit the interactions of the visual and textual representations. By plugging MCF into CTRL, the grounding accuracy is further improved. Previous works like CTRL, ACRN and MCF directly calculate the visual-semantic correlation without explicitly modelling the activity information within two modalities, and the candidate segments fairly sampled by sliding window may contain various meaningless noisy contents which do not contain any activity. Hence, Ge et al. [18] explicitly mine activity concepts from both visual and textual parts as prior knowledge to provide an actionness score for each candidate segment, reflecting how confident it contains activities, which enhances the localization accuracy.

Despite the simplicity and effectiveness of such two-stage sliding window-based methods, they suffer from inefficient computation since there are too many overlapped areas re-computed due to the densely sampling process with predefined multi-scale sliding windows.","sent1: For a two-stage method, the pre-segmenting of proposal candidates is conducted separately with the model computation.
sent2: It takes the pre-segmented candidates and the sentence query as inputs of a cross-modal matching module for target segment localization.
sent3: The two-stage methods can be grouped into two categories based on different ways to generate proposals.
sent4: 2.1.1 sliding window-based. Early methods including MCN [23], CTRL [16], ROLE [38], MCF [63], ACRN [37], SLTA [27] and ACL-K [18], adopt multi-scale sliding window sampling strategy for the generation of candidate proposals.
sent5: There are two pioneering works MCN [23] and CTRL [16] to define TSGV task and construct benchmark datasets.
sent6: Firstly, Hendricks et al. [23] propose MCN, which samples all the candidate moments (i.e. segments) via sliding window mechanism, and then projects the video moment representation and query representation into a common embedding space.
sent7: The ℓ 2 distance between the sentence query and the corresponding target video moment in this space is minimized to supervise the model training (c.f ., Fig. 3b).
sent8: Specifically, MCN encourages the sentence query to be closer to the target moment than negative moments in a shared embedding space.
sent9: Since the negative moments either come from other segments within the same video (intra-video) or from different videos (inter-video), MCN devises two similar but different ranking loss functions:where L ( , ) = max(0, − + ), is a margin.
sent10: As for training sample , the intra-video ranking loss encourages sentence to be closer to the target moment at the location than the negative moments from other possible locations within the same video, while the inter-video ranking loss encourages sentence to be closer to the target one at location than the negative ones from other videos of the same location .
sent11: The intra-video ranking loss is able to differentiate between subtle difference within a video while the inter-video ranking loss can differentiate between broad semantic concepts.
sent12: At the same time, Gao et al. [16] propose CTRL, which is the first one to adapt R-CNN [20] methodology from object detection to the TSGV domain.
sent13: Particularly, CTRL also leverages sliding window to obtain candidate segments of various lengths, and as shown in Fig. 3a, it exploits a multi-modal processing module to fuse the candidate segment representation with the sentence representation by three operators (i.e., add, multiply, and full-connected layer).
sent14: Then, CTRL feeds the fused representation into another fully-connected layer to predict the alignment score and location offsets between the candidate segment and the target segment.
sent15: CTRL designs a multi-task loss function to train the model, including visual-semantic alignment loss and location regression loss:where is the visual-semantic alignment loss considering both aligned (video segment, query) pairs and misaligned pairs.
sent16: , measures the alignment score between video segment and sentence .
sent17: The location regression loss is only accounted for aligned pairs to predict the correct coordinates.
sent18: is a smooth-L1 function. Compared to above CTRL that treats the query as a whole, Liu et al. [38] further make some improvements by decomposing the query and adaptively get the important textual components according to the temporal video context.
sent19: Since CTRL overlooks the spatial-temporal information inside the moment and the query, Liu et al. [37] further propose an attentive cross-modal retrieval network (ACRN).
sent20: With a memory attention network guided by the sentence query, ACRN adaptively assigns weights to the contextual moment representations for memorization to augment the moment representation.
sent21: SLTA [27] also devises a spatial and language-temporal attention model to adaptively identify the relevant objects and interactions based on the query information.
sent22: Wu and Han [63] propose a multi-modal circulant fusion (MCF) in contrast to the simple fusion ways employed in CTRL including element-wise product, element-wise sum, or concatenation.
sent23: MCF extends the visual/textual vector to the circulant matrix, which can fully exploit the interactions of the visual and textual representations.
sent24: By plugging MCF into CTRL, the grounding accuracy is further improved.
sent25: Previous works like CTRL, ACRN and MCF directly calculate the visual-semantic correlation without explicitly modelling the activity information within two modalities, and the candidate segments fairly sampled by sliding window may contain various meaningless noisy contents which do not contain any activity.
sent26: Hence, Ge et al. [18] explicitly mine activity concepts from both visual and textual parts as prior knowledge to provide an actionness score for each candidate segment, reflecting how confident it contains activities, which enhances the localization accuracy.
sent27: Despite the simplicity and effectiveness of such two-stage sliding window-based methods, they suffer from inefficient computation since there are too many overlapped areas re-computed due to the densely sampling process with predefined multi-scale sliding windows.","1. What is the two-stage method in temporal sentence grounding in videos? sent1
    1.1. What inputs does the cross-modal matching module take in the two-stage method? sent2
    1.2. How are two-stage methods categorized? sent3
        1.2.1. What is the sliding window-based approach in two-stage methods? sent4
            1.2.1.1. What are the pioneering works that define the TSGV task and construct benchmark datasets? sent5
            1.2.1.2. How does MCN utilize the sliding window mechanism? sent6
                1.2.1.2.1. How does MCN supervise model training? sent7
                1.2.1.2.2. What does MCN encourage in the shared embedding space? sent8
                1.2.1.2.3. How does MCN handle negative moments? sent9
                1.2.1.2.4. What are the purposes of intra-video and inter-video ranking losses in MCN? sent10, sent11
            1.2.1.3. How does CTRL adapt R-CNN methodology to the TSGV domain? sent12
                1.2.1.3.1. How does CTRL obtain candidate segments and process them? sent13
                1.2.1.3.2. What does CTRL predict using the fused representation? sent14
                1.2.1.3.3. What is the multi-task loss function in CTRL? sent15
                1.2.1.3.4. How does CTRL measure alignment score and location regression? sent16, sent17, sent18
            1.2.1.4. What improvements does Liu et al. make over CTRL? sent19
            1.2.1.5. How does ACRN enhance moment representation? sent20
            1.2.1.6. What model does SLTA devise? sent21
            1.2.1.7. How does MCF differ from CTRL's fusion methods? sent22, sent23
                1.2.1.7.1. What is the effect of integrating MCF into CTRL? sent24
            1.2.1.8. What limitations do previous works like CTRL, ACRN, and MCF have? sent25
            1.2.1.9. How does Ge et al. enhance localization accuracy? sent26
    1.3. What is a drawback of sliding window-based methods? sent27"
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,Automatic identification of travel blog entries,8,"Travel blogs 1 are defined as travel journals written by bloggers in diary form. Travel blogs are considered useful for obtaining travel information, because many bloggers' travel experiences are written in this form.

There are various portal sites for travel blogs, which we will describe in Section 6. At these sites, travel blogs are manually registered by bloggers themselves, and the blogs are classified according to travel destination. However, there are many more travel blogs in the blogosphere, beyond these portal sites. In an attempt to construct an exhaustive database of travel blogs, Nanba et al. [25] identified travel blog entries written in Japanese in a blog database. 2 Blog entries that contain cue phrases, such as ""travel"", ""sightseeing"", or ""tour"", have a high degree of probability of being travel blogs. However, not every travel blog contains such cue phrases. For example, if a blogger describes his/her journey to Norway in multiple blog entries, the blog might state ""We traveled to Norway"" in the first entry, while only writing ""We ate wild sheep!"" in the second entry. In this case, because the second entry does not contain any expressions related to travel, it is difficult to identify it as a travel blog entry. Therefore, Nanba et al. focused not only on each blog entry but also on the surrounding entries for the identification of travel blog entries. They formulated the identification of travel blog entries as a sequence-labeling problem, and solved it using machine learning. For the machine learning method, they examined the Conditional Random Fields (CRF) method [20]; its empirical success has been reported recently in the field of natural language processing. The CRF-based method identifies the tag 3 of each entry. Features and tags are given in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry (see Figure 1). They used the value of k = 4, which was determined in a pilot study. Here, they used the following features for machine learning: whether an entry contains any of 416 cue phrases, such as ""ÅL (travel)"", ""Ä¢ü (tour)"", and ""úz (departure)"", and the number of location names in each entry.

Using the above method, Nanba et al. identified 17,268 travel blog entries from 1,100,000 blog entries, and constructed a system that plotted travel blog entries on a Google map (see Figure 2). 4 In this figure, travel blog entries are shown as icons. If the user clicks an icon, the corresponding blog entry is shown in a pop-up window.

Travel blogs 1 are defined as travel journals written by bloggers in diary form. Travel blogs are considered useful for obtaining travel information, because many bloggers' travel experiences are written in this form.

There are various portal sites for travel blogs, which we will describe in Section 6. At these sites, travel blogs are manually registered by bloggers themselves, and the blogs are classified according to travel destination. However, there are many more travel blogs in the blogosphere, beyond these portal sites. In an attempt to construct an exhaustive database of travel blogs, Nanba et al. [25] identified travel blog entries written in Japanese in a blog database. 2 Blog entries that contain cue phrases, such as ""travel"", ""sightseeing"", or ""tour"", have a high degree of probability of being travel blogs. However, not every travel blog contains such cue phrases. For example, if a blogger describes his/her journey to Norway in multiple blog entries, the blog might state ""We traveled to Norway"" in the first entry, while only writing ""We ate wild sheep!"" in the second entry. In this case, because the second entry does not contain any expressions related to travel, it is difficult to identify it as a travel blog entry. Therefore, Nanba et al. focused not only on each blog entry but also on the surrounding entries for the identification of travel blog entries. They formulated the identification of travel blog entries as a sequence-labeling problem, and solved it using machine learning. For the machine learning method, they examined the Conditional Random Fields (CRF) method [20]; its empirical success has been reported recently in the field of natural language processing. The CRF-based method identifies the tag 3 of each entry. Features and tags are given in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry (see Figure 1). They used the value of k = 4, which was determined in a pilot study. Here, they used the following features for machine learning: whether an entry contains any of 416 cue phrases, such as ""ÅL (travel)"", ""Ä¢ü (tour)"", and ""úz (departure)"", and the number of location names in each entry.

Using the above method, Nanba et al. identified 17,268 travel blog entries from 1,100,000 blog entries, and constructed a system that plotted travel blog entries on a Google map (see Figure 2). 4 In this figure, travel blog entries are shown as icons. If the user clicks an icon, the corresponding blog entry is shown in a pop-up window.","sent1: Travel blogs 1 are defined as travel journals written by bloggers in diary form.
sent2: Travel blogs are considered useful for obtaining travel information, because many bloggers' travel experiences are written in this form.
sent3: There are various portal sites for travel blogs, which we will describe in Section 6.
sent4: At these sites, travel blogs are manually registered by bloggers themselves, and the blogs are classified according to travel destination.
sent5: However, there are many more travel blogs in the blogosphere, beyond these portal sites.
sent6: In an attempt to construct an exhaustive database of travel blogs, Nanba et al. [25] identified travel blog entries written in Japanese in a blog database.
sent7: 2 Blog entries that contain cue phrases, such as ""travel"", ""sightseeing"", or ""tour"", have a high degree of probability of being travel blogs.
sent8: However, not every travel blog contains such cue phrases.
sent9: For example, if a blogger describes his/her journey to Norway in multiple blog entries, the blog might state ""We traveled to Norway"" in the first entry, while only writing ""We ate wild sheep!""
sent10: in the second entry. In this case, because the second entry does not contain any expressions related to travel, it is difficult to identify it as a travel blog entry.
sent11: Therefore, Nanba et al. focused not only on each blog entry but also on the surrounding entries for the identification of travel blog entries.
sent12: They formulated the identification of travel blog entries as a sequence-labeling problem, and solved it using machine learning.
sent13: For the machine learning method, they examined the Conditional Random Fields (CRF) method [20]; its empirical success has been reported recently in the field of natural language processing.
sent14: The CRF-based method identifies the tag 3 of each entry.
sent15: Features and tags are given in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry (see Figure 1).
sent16: They used the value of k = 4, which was determined in a pilot study.
sent17: Here, they used the following features for machine learning: whether an entry contains any of 416 cue phrases, such as ""ÅL (travel)"", ""Ä¢ü (tour)"", and ""úz (departure)"", and the number of location names in each entry.
sent18: Using the above method, Nanba et al. identified 17,268 travel blog entries from 1,100,000 blog entries, and constructed a system that plotted travel blog entries on a Google map (see Figure 2). 4 In this figure, travel blog entries are shown as icons.
sent19: If the user clicks an icon, the corresponding blog entry is shown in a pop-up window.
sent20: Travel blogs 1 are defined as travel journals written by bloggers in diary form.
sent21: Travel blogs are considered useful for obtaining travel information, because many bloggers' travel experiences are written in this form.
sent22: There are various portal sites for travel blogs, which we will describe in Section 6.
sent23: At these sites, travel blogs are manually registered by bloggers themselves, and the blogs are classified according to travel destination.
sent24: However, there are many more travel blogs in the blogosphere, beyond these portal sites.
sent25: In an attempt to construct an exhaustive database of travel blogs, Nanba et al. [25] identified travel blog entries written in Japanese in a blog database.
sent26: 2 Blog entries that contain cue phrases, such as ""travel"", ""sightseeing"", or ""tour"", have a high degree of probability of being travel blogs.
sent27: However, not every travel blog contains such cue phrases.
sent28: For example, if a blogger describes his/her journey to Norway in multiple blog entries, the blog might state ""We traveled to Norway"" in the first entry, while only writing ""We ate wild sheep!""
sent29: in the second entry. In this case, because the second entry does not contain any expressions related to travel, it is difficult to identify it as a travel blog entry.
sent30: Therefore, Nanba et al. focused not only on each blog entry but also on the surrounding entries for the identification of travel blog entries.
sent31: They formulated the identification of travel blog entries as a sequence-labeling problem, and solved it using machine learning.
sent32: For the machine learning method, they examined the Conditional Random Fields (CRF) method [20]; its empirical success has been reported recently in the field of natural language processing.
sent33: The CRF-based method identifies the tag 3 of each entry.
sent34: Features and tags are given in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry (see Figure 1).
sent35: They used the value of k = 4, which was determined in a pilot study.
sent36: Here, they used the following features for machine learning: whether an entry contains any of 416 cue phrases, such as ""ÅL (travel)"", ""Ä¢ü (tour)"", and ""úz (departure)"", and the number of location names in each entry.
sent37: Using the above method, Nanba et al. identified 17,268 travel blog entries from 1,100,000 blog entries, and constructed a system that plotted travel blog entries on a Google map (see Figure 2). 4 In this figure, travel blog entries are shown as icons.
sent38: If the user clicks an icon, the corresponding blog entry is shown in a pop-up window.","1. How are travel blogs defined? sent1
2. Why are travel blogs considered useful for obtaining travel information? sent2
3. How are travel blogs registered and classified on portal sites? sent3, sent4
4. What challenge exists beyond portal sites for travel blogs? sent5
5. How did Nanba et al. attempt to construct an exhaustive database of travel blogs? sent6
6. What indicates a high probability of a blog entry being a travel blog? sent7
7. Why might some travel blog entries be difficult to identify? sent8, sent9, sent10
8. How did Nanba et al. address the challenge of identifying travel blog entries? sent11
9. How did Nanba et al. formulate the identification of travel blog entries? sent12
10. What machine learning method did Nanba et al. use for identifying travel blog entries? sent13
11. How does the CRF-based method identify tags for each entry? sent14
12. What features and tags are used in the CRF method? sent15
13. What value of k was used in the CRF method, and how was it determined? sent16
14. What features were used for machine learning in identifying travel blog entries? sent17
15. How many travel blog entries did Nanba et al. identify, and what system did they construct? sent18
16. What happens when a user clicks an icon on the system constructed by Nanba et al.? sent19"
257386000,Assessment of the Healthcare Administration of Senior Citizens from Survey Data using Sentiment Analysis,https://www.semanticscholar.org/paper/687f93be9a6c5f9deb053ca8a2fa1875afeb7a5d,A. Healthcare Services for Senior Citizens,4,"Discussions about how seniors use healthcare services are becoming more and more crucial as the senior population increases. A study being conducted in South Korea aims to create an integrated healthcare service system that is centered on elderly citizens, meeting their needs in daily life and promoting well-being, wellness, and well-dying. A natural structure of regular care, professional care, and rehabilitation for senior members of society in line with the responsibilities of the patients, their families, and caregivers are required for the implementation of the integrated medical care system for elderly users presented in this study [13].

The study [14]'s goal was to determine what senior citizens need from ""embedded retirement facilities (ERFs),"" multipurpose, and community-based care facilities for the elderly in mainland China. This study is based on questionnaire data collected in northeast China. The findings show that senior citizens' healthcare services are deemed to be the most significant. Senior citizens use community-based facilities, but decision-makers and facility administrators frequently fail to consider their needs. Seniors in China also tend to be inactive and largely silent in both formal and informal civic involvement because they typically believe that policymakers would take notice of and accommodate their needs.

The purpose of the [15] study is to evaluate older people's well-being to explore whether the data are consistent with previously announced changes in senior treatment in relation to the real resources provided to their patients. The respondents reported being generally satisfied with their lives. The results show that small-town residents felt substantially worse about their quality of life than seniors from large cities. This shows that the healthcare system continues to utterly fail to meet patients' actual demands, particularly in the elderly sector. Being open to a broader discussion about the diverse needs and resources that elderly people in rural and urban areas face is crucial for doing this.

The study of [6] attempts to assess the potential influences on elderly persons' use of healthcare in Davao City, in the Philippines. Various factors were discovered to be significant predictors of healthcare consumption through the use of multiple regression analysis. The findings demonstrated how socioeconomic demographic, personal characteristics and health insurance knowledge affect the way senior citizens use healthcare. By launching health insurance awareness campaigns and creating health-improving initiatives, policymakers and local government organizations may think about enhancing senior citizens' access to healthcare services.","sent1: Discussions about how seniors use healthcare services are becoming more and more crucial as the senior population increases.
sent2: A study being conducted in South Korea aims to create an integrated healthcare service system that is centered on elderly citizens, meeting their needs in daily life and promoting well-being, wellness, and well-dying.
sent3: A natural structure of regular care, professional care, and rehabilitation for senior members of society in line with the responsibilities of the patients, their families, and caregivers are required for the implementation of the integrated medical care system for elderly users presented in this study [13].
sent4: The study [14]'s goal was to determine what senior citizens need from ""embedded retirement facilities (ERFs),"" multipurpose, and community-based care facilities for the elderly in mainland China.
sent5: This study is based on questionnaire data collected in northeast China.
sent6: The findings show that senior citizens' healthcare services are deemed to be the most significant.
sent7: Senior citizens use community-based facilities, but decision-makers and facility administrators frequently fail to consider their needs.
sent8: Seniors in China also tend to be inactive and largely silent in both formal and informal civic involvement because they typically believe that policymakers would take notice of and accommodate their needs.
sent9: The purpose of the [15] study is to evaluate older people's well-being to explore whether the data are consistent with previously announced changes in senior treatment in relation to the real resources provided to their patients.
sent10: The respondents reported being generally satisfied with their lives.
sent11: The results show that small-town residents felt substantially worse about their quality of life than seniors from large cities.
sent12: This shows that the healthcare system continues to utterly fail to meet patients' actual demands, particularly in the elderly sector.
sent13: Being open to a broader discussion about the diverse needs and resources that elderly people in rural and urban areas face is crucial for doing this.
sent14: The study of [6] attempts to assess the potential influences on elderly persons' use of healthcare in Davao City, in the Philippines.
sent15: Various factors were discovered to be significant predictors of healthcare consumption through the use of multiple regression analysis.
sent16: The findings demonstrated how socioeconomic demographic, personal characteristics and health insurance knowledge affect the way senior citizens use healthcare.
sent17: By launching health insurance awareness campaigns and creating health-improving initiatives, policymakers and local government organizations may think about enhancing senior citizens' access to healthcare services.","1. Why are discussions about how seniors use healthcare services becoming more crucial? sent1
2. What is the aim of the study being conducted in South Korea regarding healthcare services for seniors? sent2
3. What is required for the implementation of the integrated medical care system for elderly users in the South Korean study? sent3
4. What was the goal of the study conducted in mainland China regarding senior citizens? sent4
    4.1. On what data is the study in northeast China based? sent5
    4.2. What do the findings of the study in northeast China show about senior citizens' healthcare services? sent6
    4.3. How do decision-makers and facility administrators often fail senior citizens in community-based facilities? sent7
    4.4. Why are seniors in China typically inactive and silent in civic involvement? sent8
5. What is the purpose of the study referenced as [15]? sent9
    5.1. How did the respondents of the study referenced as [15] report their life satisfaction? sent10
    5.2. How do small-town residents feel about their quality of life compared to seniors from large cities? sent11
    5.3. What does the study referenced as [15] indicate about the healthcare system's ability to meet elderly patients' demands? sent12
    5.4. Why is it crucial to have a broader discussion about the needs and resources of elderly people in rural and urban areas? sent13
6. What does the study conducted in Davao City, Philippines, attempt to assess? sent14
    6.1. What factors were found to be significant predictors of healthcare consumption in the Davao City study? sent15
    6.2. How do socioeconomic demographic, personal characteristics, and health insurance knowledge affect senior citizens' use of healthcare? sent16
    6.3. What actions can policymakers and local government organizations consider to enhance senior citizens' access to healthcare services? sent17"
21693765,Graph-based Ontology Summarization: A Survey,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,(9),24,"Radiality (Ra) Pappas et al. [10] also present radiality, which takes the diameter of a graph into account:

where D is the diameter of the graph, namely the greatest distance between any pair of nodes in the graph.

Comments on Closeness Centrality Closeness centrality and its variants (e.g., harmonic centrality, radiality) are similar to betweenness, also involving calculating the shortest paths between all pairs of nodes in a graph. One difference is that, a node with a high closeness value is usually located at the center of the graph (in terms of distance), but such a node may not have a high betweenness value because it may not be a bridging node that resides in many shortest paths connecting other nodes.

Eigenvector Centrality (EC) A widely adopted principle is that a node is important if it is connected with important nodes. For example, in a class graph, a class is important if the classes it connects with are important. This gives rise to eigenvector centrality which iteratively calculates the importance of each node v in a graph:

where N (v) is the set of v's neighbors, and λ is a constant factor for normalization. The eigenvector centrality of a node is the sum of the eigenvector centrality of its neighbors. The computation iterates over all the nodes in the graph, one round after another until convergence. Whereas this basic measure has been used in [3], its improved variants are more popular in the literature. PageRank, a well-known implementation of eigenvector centrality, is used in [20,2]. Different from the above basic measure, PageRank introduces a damping factor which is added to the centrality. Weighted PageRank, weighted HITS, or their variants are used in [24,23,22,21,4], where centrality is defined as a weighted sum. The weight of an edge between v and u indicates the strength of the connection between them; a stronger connection will transport more centrality score from u to v.

Comments on Eigenvector Centrality Eigenvector centrality and its variants (e.g., PageRank, HITS) have shown their effectiveness in many applications. However, they require iterative computation over all the nodes in a graph until convergence, which is time-consuming for large graphs.

Empirical Comparison of Centrality-based Measures It seems that the effectiveness of a centrality-based measure is related to the graph model, and may also depend on the specific ontology to be summarized as the application and the domain of an ontology provide a guideline in order to select a proper set of measures.

Specifically, according to the experiment results presented in [20], the simple degree centrality (DC) appears more effective than PageRank (i.e., EC) on some class graphs. However, Zhang et al. [23] report that weighted PageRank (i.e., EC) outperforms degree (i.e., DC) on several RDF sentence graphs; both of them are considerably better than betweenness (i.e., BC). Pappas et al. [10] find that degree (i.e., DC) and betweenness (i.e., BC, EgC, and BrC) are notably better than closeness (i.e., HC and RA) on a few class graphs.

Unfortunately, we could not draw any reliable conclusions from the current empirical results reported in the literature as they all experiment with a small number of ontologies.

Radiality (Ra) Pappas et al. [10] also present radiality, which takes the diameter of a graph into account:

where D is the diameter of the graph, namely the greatest distance between any pair of nodes in the graph.

Comments on Closeness Centrality Closeness centrality and its variants (e.g., harmonic centrality, radiality) are similar to betweenness, also involving calculating the shortest paths between all pairs of nodes in a graph. One difference is that, a node with a high closeness value is usually located at the center of the graph (in terms of distance), but such a node may not have a high betweenness value because it may not be a bridging node that resides in many shortest paths connecting other nodes.

Eigenvector Centrality (EC) A widely adopted principle is that a node is important if it is connected with important nodes. For example, in a class graph, a class is important if the classes it connects with are important. This gives rise to eigenvector centrality which iteratively calculates the importance of each node v in a graph:

where N (v) is the set of v's neighbors, and λ is a constant factor for normalization. The eigenvector centrality of a node is the sum of the eigenvector centrality of its neighbors. The computation iterates over all the nodes in the graph, one round after another until convergence. Whereas this basic measure has been used in [3], its improved variants are more popular in the literature. PageRank, a well-known implementation of eigenvector centrality, is used in [20,2]. Different from the above basic measure, PageRank introduces a damping factor which is added to the centrality. Weighted PageRank, weighted HITS, or their variants are used in [24,23,22,21,4], where centrality is defined as a weighted sum. The weight of an edge between v and u indicates the strength of the connection between them; a stronger connection will transport more centrality score from u to v.

Comments on Eigenvector Centrality Eigenvector centrality and its variants (e.g., PageRank, HITS) have shown their effectiveness in many applications. However, they require iterative computation over all the nodes in a graph until convergence, which is time-consuming for large graphs.

Empirical Comparison of Centrality-based Measures It seems that the effectiveness of a centrality-based measure is related to the graph model, and may also depend on the specific ontology to be summarized as the application and the domain of an ontology provide a guideline in order to select a proper set of measures.

Specifically, according to the experiment results presented in [20], the simple degree centrality (DC) appears more effective than PageRank (i.e., EC) on some class graphs. However, Zhang et al. [23] report that weighted PageRank (i.e., EC) outperforms degree (i.e., DC) on several RDF sentence graphs; both of them are considerably better than betweenness (i.e., BC). Pappas et al. [10] find that degree (i.e., DC) and betweenness (i.e., BC, EgC, and BrC) are notably better than closeness (i.e., HC and RA) on a few class graphs.

Unfortunately, we could not draw any reliable conclusions from the current empirical results reported in the literature as they all experiment with a small number of ontologies.","sent1: Radiality (Ra) Pappas et al. [10] also present radiality, which takes the diameter of a graph into account:where D is the diameter of the graph, namely the greatest distance between any pair of nodes in the graph.
sent2: Comments on Closeness Centrality Closeness centrality and its variants (e.g., harmonic centrality, radiality) are similar to betweenness, also involving calculating the shortest paths between all pairs of nodes in a graph.
sent3: One difference is that, a node with a high closeness value is usually located at the center of the graph (in terms of distance), but such a node may not have a high betweenness value because it may not be a bridging node that resides in many shortest paths connecting other nodes.
sent4: Eigenvector Centrality (EC) A widely adopted principle is that a node is important if it is connected with important nodes.
sent5: For example, in a class graph, a class is important if the classes it connects with are important.
sent6: This gives rise to eigenvector centrality which iteratively calculates the importance of each node v in a graph:where N (v) is the set of v's neighbors, and λ is a constant factor for normalization.
sent7: The eigenvector centrality of a node is the sum of the eigenvector centrality of its neighbors.
sent8: The computation iterates over all the nodes in the graph, one round after another until convergence.
sent9: Whereas this basic measure has been used in [3], its improved variants are more popular in the literature.
sent10: PageRank, a well-known implementation of eigenvector centrality, is used in [20,2].
sent11: Different from the above basic measure, PageRank introduces a damping factor which is added to the centrality.
sent12: Weighted PageRank, weighted HITS, or their variants are used in [24,23,22,21,4], where centrality is defined as a weighted sum.
sent13: The weight of an edge between v and u indicates the strength of the connection between them; a stronger connection will transport more centrality score from u to v.Comments on Eigenvector Centrality Eigenvector centrality and its variants (e.g., PageRank, HITS) have shown their effectiveness in many applications.
sent14: However, they require iterative computation over all the nodes in a graph until convergence, which is time-consuming for large graphs.
sent15: Empirical Comparison of Centrality-based Measures
sent16: It seems that the effectiveness of a centrality-based measure is related to the graph model, and may also depend on the specific ontology to be summarized as the application and the domain of an ontology provide a guideline in order to select a proper set of measures.
sent17: Specifically, according to the experiment results presented in [20], the simple degree centrality (DC) appears more effective than PageRank (i.e., EC) on some class graphs.
sent18: However, Zhang et al. [23] report that weighted PageRank (i.e., EC) outperforms degree (i.e., DC) on several RDF sentence graphs; both of them are considerably better than betweenness (i.e., BC).
sent19: Pappas et al. [10] find that degree (i.e., DC) and betweenness (i.e., BC, EgC, and BrC) are notably better than closeness (i.e., HC and RA) on a few class graphs.
sent20: Unfortunately, we could not draw any reliable conclusions from the current empirical results reported in the literature as they all experiment with a small number of ontologies.
sent21: Radiality (Ra) Pappas et al. [10] also present radiality, which takes the diameter of a graph into account:where D is the diameter of the graph, namely the greatest distance between any pair of nodes in the graph.
sent22: Comments on Closeness Centrality Closeness centrality and its variants (e.g., harmonic centrality, radiality) are similar to betweenness, also involving calculating the shortest paths between all pairs of nodes in a graph.
sent23: One difference is that, a node with a high closeness value is usually located at the center of the graph (in terms of distance), but such a node may not have a high betweenness value because it may not be a bridging node that resides in many shortest paths connecting other nodes.
sent24: Eigenvector Centrality (EC) A widely adopted principle is that a node is important if it is connected with important nodes.
sent25: For example, in a class graph, a class is important if the classes it connects with are important.
sent26: This gives rise to eigenvector centrality which iteratively calculates the importance of each node v in a graph:where N (v) is the set of v's neighbors, and λ is a constant factor for normalization.
sent27: The eigenvector centrality of a node is the sum of the eigenvector centrality of its neighbors.
sent28: The computation iterates over all the nodes in the graph, one round after another until convergence.
sent29: Whereas this basic measure has been used in [3], its improved variants are more popular in the literature.
sent30: PageRank, a well-known implementation of eigenvector centrality, is used in [20,2].
sent31: Different from the above basic measure, PageRank introduces a damping factor which is added to the centrality.
sent32: Weighted PageRank, weighted HITS, or their variants are used in [24,23,22,21,4], where centrality is defined as a weighted sum.
sent33: The weight of an edge between v and u indicates the strength of the connection between them; a stronger connection will transport more centrality score from u to v.Comments on Eigenvector Centrality Eigenvector centrality and its variants (e.g., PageRank, HITS) have shown their effectiveness in many applications.
sent34: However, they require iterative computation over all the nodes in a graph until convergence, which is time-consuming for large graphs.
sent35: Empirical Comparison of Centrality-based Measures
sent36: It seems that the effectiveness of a centrality-based measure is related to the graph model, and may also depend on the specific ontology to be summarized as the application and the domain of an ontology provide a guideline in order to select a proper set of measures.
sent37: Specifically, according to the experiment results presented in [20], the simple degree centrality (DC) appears more effective than PageRank (i.e., EC) on some class graphs.
sent38: However, Zhang et al. [23] report that weighted PageRank (i.e., EC) outperforms degree (i.e., DC) on several RDF sentence graphs; both of them are considerably better than betweenness (i.e., BC).
sent39: Pappas et al. [10] find that degree (i.e., DC) and betweenness (i.e., BC, EgC, and BrC) are notably better than closeness (i.e., HC and RA) on a few class graphs.
sent40: Unfortunately, we could not draw any reliable conclusions from the current empirical results reported in the literature as they all experiment with a small number of ontologies.","1. What is radiality and how is it related to the diameter of a graph? sent1
2. How are closeness centrality and its variants similar to betweenness? sent2
3. What is a key difference between nodes with high closeness value and high betweenness value? sent3
4. What principle is widely adopted in eigenvector centrality? sent4
    4.1. Can you provide an example of how eigenvector centrality is applied in a class graph? sent5
5. How is eigenvector centrality calculated? sent6
    5.1. What is the role of a node's neighbors in determining its eigenvector centrality? sent7
    5.2. How is the computation of eigenvector centrality performed? sent8
6. What is the popularity of the basic measure of eigenvector centrality compared to its improved variants? sent9
7. What is a well-known implementation of eigenvector centrality? sent10
    7.1. How does PageRank differ from the basic measure of eigenvector centrality? sent11
8. What are some variants of weighted centrality measures used in the literature? sent12
    8.1. How does the weight of an edge affect the centrality score in weighted measures? sent13
9. What is a drawback of eigenvector centrality and its variants? sent14
10. How is the effectiveness of a centrality-based measure related to the graph model and ontology? sent16
11. What do experiment results suggest about the effectiveness of degree centrality compared to PageRank? sent17
12. How does weighted PageRank compare to degree centrality and betweenness on RDF sentence graphs? sent18
13. What findings did Pappas et al. report regarding degree and betweenness centrality? sent19
14. Why is it difficult to draw reliable conclusions from current empirical results on centrality measures? sent20"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Text Summarization,9,"In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature. Researchers and domain experts need to go through a number of biomedical documents. As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186]. Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187]. There are no standard datasets for biomedical text summarization. Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].

Moradi et al. [188] proposed a novel approach to summarize biomedical scientific articles. They embedded sentences, generated clusters, and then extracted the most informative sentences from each of the clusters. They showed that BERT-large outperformed other models including the in-domain BERT models like BioBERT.

In the case of small models, BioBERT outperformed others. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence. The sentences with the highest score are considered as the summary. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.","sent1: In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature.
sent2: Researchers and domain experts need to go through a number of biomedical documents.
sent3: As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186].
sent4: Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187].
sent5: There are no standard datasets for biomedical text summarization.
sent6: Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].Moradi et al. [188] proposed a novel approach to summarize biomedical scientific articles.
sent7: They embedded sentences, generated clusters, and then extracted the most informative sentences from each of the clusters.
sent8: They showed that BERT-large outperformed other models including the in-domain BERT models like BioBERT.
sent9: In the case of small models, BioBERT outperformed others.
sent10: Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text.
sent11: They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences.
sent12: Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text.
sent13: BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence.
sent14: The sentences with the highest score are considered as the summary.
sent15: Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.","1. Why is there a need for automatic biomedical text summarization? sent1, sent2, sent3
2. What are the two broad categories of text summarization? sent4
3. Are there standard datasets for biomedical text summarization? sent5
4. How do researchers typically treat scientific papers and their abstracts in the context of biomedical text summarization? sent6
    4.1. What novel approach did Moradi et al. propose for summarizing biomedical scientific articles? sent7
        4.1.1. How did BERT-large perform compared to other models in Moradi et al.'s study? sent8
        4.1.2. How did small models like BioBERT perform in comparison to others? sent9
    4.2. What approach did Moradi et al. propose based on word embeddings and graph ranking? sent10
        4.2.1. How is the graph generated and ranked in Moradi et al.'s approach? sent11
    4.3. What is the BioBERTSum approach introduced by Du et al.? sent12
        4.3.1. How does BioBERTSum work to summarize biomedical text? sent13, sent14
    4.4. What novel approach did Chen et al. propose for clinical text summarization? sent15"
201070768,Enhancing the Demand for Labour survey by including skills from online job advertisements using model-assisted calibration,https://www.semanticscholar.org/paper/b1826f1b35347fb1c190f9296b3c2fb7139b9b72,Traditional calibration,4,"Calibration was proposed by Deville and Särndal (1992) and is a method of searching for so called calibrated weights by minimizing the distance measure between the sampling weights and the new weights, which satisfy certain calibration constraints. As a consequence, when the new weights are applied to the auxiliary variables in the sample, they reproduce the known population totals of the auxiliary variables exactly. It is also important that the new weights should be as close as possible to sampling weights in the sense of the selected distance measure (Särndal and Lundström, 2005).

Following the notation in Chen et al. (2019), let us define the online (non-probability) sample as s A,t of size n A,t where t = 1, ..., T denotes the wave. For simplicity, we drop subscript t. This sample contains variables of interest Y k , where k = 1, ..., K. Further, let d A n A ×1 be a vector of pseudo-weights that are typically set to N/n A for all units i ∈ s A,t , where N is the size of the target population. In this approach we assume simple random sampling design for sample s A .

Let D A be a diagonal matrix of pseudo-design weights and w n A ×1 be calibrated weights that minimize an expected distance measure with respect to the design of A

under the constraint:

where T X is a row vector of estimated population totals (e.g. from the reference, external probability sample) of sample calibration variables X and g(w i , d A i ) is a differentiable function with respect to w i , strictly convex on an interval containing d A i and g(d A i , d A i ) = 0. The commonly used generalized regression (GREG) estimator uses the

For this distance measure:

The estimate of the population mean of outcome y k assuming that we have k target variables is based on calibrated weights:

The calibrated weights defined do not rely on any outcome variable. Thus the same set of weights can be applied to all variables in the survey.

In the case when only estimates of totals T X are known, Dever and Valliant (2010) introduced estimated control calibration. In this framework, we replace T X in (3) with T X , which results in

and thus the estimated mean is given by

Following Chen et al. (2019) we denote this estimator as ECGREG (Estimated control GREG) to distinguish it from GREG with known population totals.","sent1: Calibration was proposed by Deville and Särndal (1992) and is a method of searching for so called calibrated weights by minimizing the distance measure between the sampling weights and the new weights, which satisfy certain calibration constraints.
sent2: As a consequence, when the new weights are applied to the auxiliary variables in the sample, they reproduce the known population totals of the auxiliary variables exactly.
sent3: It is also important that the new weights should be as close as possible to sampling weights in the sense of the selected distance measure (Särndal and Lundström, 2005).
sent4: Following the notation in Chen et al. (2019), let us define the online (non-probability) sample as s A,t of size n A,t where t = 1, ..., T denotes the wave.
sent5: For simplicity, we drop subscript t.
sent6: This sample contains variables of interest Y k , where k = 1, ..., K. Further, let d A n
sent7: A ×1 be a vector of pseudo-weights that are typically set to N/n A for all units i ∈ s A,t , where N is the size of the target population.
sent8: In this approach we assume simple random sampling design for sample s A .
sent9: Let D A be a diagonal matrix of pseudo-design weights and w n A ×1 be calibrated weights that minimize an expected distance measure with respect to the design of Aunder the constraint:where T X is a row vector of estimated population totals (e.g. from the reference, external probability sample) of sample calibration variables X and g(w i , d A i ) is a differentiable function with respect to w i , strictly convex on an interval containing d A i and g(d A i , d A i ) = 0.
sent10: The commonly used generalized regression (GREG) estimator uses theFor this distance measure:The estimate of the population mean of outcome y k assuming that we have k target variables is based on calibrated weights:The calibrated weights defined do not rely on any outcome variable.
sent11: Thus the same set of weights can be applied to all variables in the survey.
sent12: In the case when only estimates of totals T X are known, Dever and Valliant (2010) introduced estimated control calibration.
sent13: In this framework, we replace T X in (3) with T X , which results inand thus the estimated mean is given byFollowing Chen et al. (2019) we denote this estimator as ECGREG (Estimated control GREG) to distinguish it from GREG with known population totals.","1. What is calibration and who proposed it? sent1
    1.1. What is the consequence of applying new weights to auxiliary variables in the sample? sent2
    1.2. Why should the new weights be close to sampling weights? sent3
2. How is the online (non-probability) sample defined in the context of this paper? sent4
    2.1. What simplification is made regarding the subscript in the sample definition? sent5
    2.2. What does the sample contain and how are pseudo-weights typically set? sent6, sent7
    2.3. What sampling design is assumed for sample s A? sent8
3. How are calibrated weights defined and what is their purpose? sent9
    3.1. What estimator is commonly used for this distance measure? sent10
        3.1.1. How are calibrated weights applied to variables in the survey? sent11
4. What is estimated control calibration and who introduced it? sent12
    4.1. How does estimated control calibration differ from GREG with known population totals? sent13"
7470107,A Review of Unsupervised Approaches of Opinion Target Extraction from Unstructured Reviews,https://www.semanticscholar.org/paper/67a38d9065eebdeb814c1e80e109fc34a237a889,METHODOLOGY,19,"Unsupervised approaches for opinion targets identification: The unsupervised techniques has been popularly used for opinion target identification (Ben-David et al., 2007;Blitzer et al., 2007;Bloom et al., 2007;Carenini et al., 2005;Ferreira et al., 2008;Holzinger et al., 2006;Hu and Liu, 2004;Popescu et al., 2005;Wei et al., 2010;Wong and Lam, 2009;Yi et al., 2003;Zhai et al., 2011).Popescu et al. (2005) used an unsupervised technique to extract product features and opinions from unstructured reviews.This study introduces the OPINE system based on the unsupervised information extraction approach to mine product features from reviews.OPINE uses syntactic patterns for semantic orientation of words for identification of opinion phrases and their polarity.Carenini et al. (2005) developed a model based on user defined knowledge to create taxonomy of product features.This study introduces an improved unsupervised method for feature extraction that uses the taxonomy of the product features.The results of the combined approach are higher than the existing unsupervised technique; however, the pre-knowledge base mechanism makes the approach domain dependent.Holzinger et al. (2006) use domain ontologies based on tabular data from web content to bootstrap a knowledge acquisition process for extraction of product features.This method creates a wrapper for data extraction from Web tables and ontology building.The model uses logical rules and data integration to reason about product specific properties and the higher-order knowledge of product features.Bloom et al. (2007) describe an unsupervised technique for features and appraisal extraction.The authors believe that appraisal expression is a fundamental task in sentiment analysis.The appraisal expression is a textual unit expressing an evaluative attitude towards some target.Their study proposed evaluative expressions to extract opinion targets.The system effectively exploited the adjectival appraisal expressions for target identification.Ben-David et al. (2007) proposed a Structural Correspondence Learning (SCL) algorithm for domain classification.The idea depends on perception to get a prediction of new domain features based on training domain features; in other words, the author describes under what conditions a classifier trained on the source domain can be adapted for use in the target domain?This model is inspired by feature based domain classification.Blitzer et al. (2007) extended the structural SCL algorithm for opinion target identification.Lu and Zhai (2008) proposed automatic integration of opinions expressed in a well-written expert review with opinions scattered in various sources such as blogs and forums.The study proposes a semi-supervised topic model to solve the problem in a principled way.The author performed experiments on integrating opinions about two quite different topics, i.e., a product and political reviews.The focus of this study is to develop a generalized model that should be effective on multiple domains for extraction of opinion targets.Ferreira et al. (2008) describe an extended pattern based feature extraction using a modified Log Likelihood Ratio Test (LRT), which was initially employed by Yi et al. (2003) for target identification.This study also presented an extended annotated scheme for product features, which was initially presented by Hu and Liu (2004) and a comparative analysis between feature extraction through Association Mining and LRT techniques.

The association rule mining for target extraction is initially implemented by Hu and Liu (2004) for target extraction and extended by Wei et al. (2010) using semantic based patterns for frequent feature refinement and identification of infrequent features.

One of the latest works on feature level analysis of opinion is reported by Zhai et al. (2011).This study describes a semi-supervised technique for feature grouping.Feature grouping is an important task for summarization of opinion.Same features can be expressed by different synonyms, words or phrases.To produce a useful summary, these words and phrases are grouped.For feature grouping the process generate an initial list to bootstrap the process using lexical characteristics of terms.This method empirically showed good results.Goujon (2011) presents a text mining approach based on linguistic knowledge to automatically detect opinion targets in relation to topic elements.This study focuses on identification of opinion targets related to the specific topic.This approach exploits linguistic patterns for target identification.

The two most frequently reported unsupervised approaches for target and opinion identification are Association Mining (AM) (Agrawal and Srikant, 1994) and Likelihood Ratio Test (LRT) approach (Dunning, 1993).The following sub sections provide a detail overview these two approaches.","sent1: Unsupervised approaches for opinion targets identification: The unsupervised techniques has been popularly used for opinion target identification (Ben-David et al., 2007;Blitzer et al., 2007;Bloom et al., 2007;Carenini et al., 2005;Ferreira et al., 2008;Holzinger et al., 2006;Hu and Liu, 2004;Popescu et al., 2005;Wei et al., 2010;Wong and Lam, 2009;Yi et al., 2003;Zhai et al., 2011).Popescu et al. (2005) used an unsupervised technique to extract product features and opinions from unstructured reviews.
sent2: This study introduces the OPINE system based on the unsupervised information extraction approach to mine product features from reviews.
sent3: OPINE uses syntactic patterns for semantic orientation of words for identification of opinion phrases and their polarity.
sent4: Carenini et al. (2005) developed a model based on user defined knowledge to create taxonomy of product features.
sent5: This study introduces an improved unsupervised method for feature extraction that uses the taxonomy of the product features.
sent6: The results of the combined approach are higher than the existing unsupervised technique; however, the pre-knowledge base mechanism makes the approach domain dependent.
sent7: Holzinger et al. (2006) use domain ontologies based on tabular data from web content to bootstrap a knowledge acquisition process for extraction of product features.
sent8: This method creates a wrapper for data extraction from Web tables and ontology building.
sent9: The model uses logical rules and data integration to reason about product specific properties and the higher-order knowledge of product features.
sent10: Bloom et al. (2007) describe an unsupervised technique for features and appraisal extraction.
sent11: The authors believe that appraisal expression is a fundamental task in sentiment analysis.
sent12: The appraisal expression is a textual unit expressing an evaluative attitude towards some target.
sent13: Their study proposed evaluative expressions to extract opinion targets.
sent14: The system effectively exploited the adjectival appraisal expressions for target identification.
sent15: Ben-David et al. (2007) proposed a Structural Correspondence Learning (SCL) algorithm for domain classification.
sent16: The idea depends on perception to get a prediction of new domain features based on training domain features; in other words, the author describes under what conditions a classifier trained on the source domain can be adapted for use in the target domain?This model is inspired by feature based domain classification.
sent17: Blitzer et al. (2007) extended the structural SCL algorithm for opinion target identification.
sent18: Lu and Zhai (2008) proposed automatic integration of opinions expressed in a well-written expert review with opinions scattered in various sources such as blogs and forums.
sent19: The study proposes a semi-supervised topic model to solve the problem in a principled way.
sent20: The author performed experiments on integrating opinions about two quite different topics, i.e., a product and political reviews.
sent21: The focus of this study is to develop a generalized model that should be effective on multiple domains for extraction of opinion targets.
sent22: Ferreira et al. (2008) describe an extended pattern based feature extraction using a modified Log Likelihood Ratio Test (LRT), which was initially employed by Yi et al. (2003) for target identification.
sent23: This study also presented an extended annotated scheme for product features, which was initially presented by Hu and Liu (2004) and a comparative analysis between feature extraction through Association Mining and LRT techniques.
sent24: The association rule mining for target extraction is initially implemented by Hu and Liu (2004) for target extraction and extended by Wei et al. (2010) using semantic based patterns for frequent feature refinement and identification of infrequent features.
sent25: One of the latest works on feature level analysis of opinion is reported by Zhai et al.
sent26: (2011).This study describes a semi-supervised technique for feature grouping.
sent27: Feature grouping is an important task for summarization of opinion.
sent28: Same features can be expressed by different synonyms, words or phrases.
sent29: To produce a useful summary, these words and phrases are grouped.
sent30: For feature grouping the process generate an initial list to bootstrap the process using lexical characteristics of terms.
sent31: This method empirically showed good results.
sent32: Goujon (2011) presents a text mining approach based on linguistic knowledge to automatically detect opinion targets in relation to topic elements.
sent33: This study focuses on identification of opinion targets related to the specific topic.
sent34: This approach exploits linguistic patterns for target identification.
sent35: The two most frequently reported unsupervised approaches for target and opinion identification are Association Mining (AM) (Agrawal and Srikant, 1994) and Likelihood Ratio Test (LRT) approach (Dunning, 1993).The following sub sections provide a detail overview these two approaches.","1. What are unsupervised approaches used for in opinion target identification? sent1
    1.1. What system is introduced by Popescu et al. (2005) for extracting product features and opinions? sent2
        1.1.1. How does the OPINE system identify opinion phrases and their polarity? sent3
    1.2. What model did Carenini et al. (2005) develop for product feature taxonomy? sent4
        1.2.1. What does the improved unsupervised method for feature extraction use? sent5
        1.2.2. What is a limitation of the combined approach mentioned in the study? sent6
    1.3. How do Holzinger et al. (2006) use domain ontologies for product feature extraction? sent7
        1.3.1. What does this method create for data extraction and ontology building? sent8
        1.3.2. How does the model reason about product-specific properties? sent9
    1.4. What technique do Bloom et al. (2007) describe for feature and appraisal extraction? sent10
        1.4.1. What is considered a fundamental task in sentiment analysis according to the authors? sent11
        1.4.2. What is an appraisal expression? sent12
        1.4.3. How does their study propose to extract opinion targets? sent13
        1.4.4. How does the system exploit adjectival appraisal expressions? sent14
    1.5. What algorithm did Ben-David et al. (2007) propose for domain classification? sent15
        1.5.1. What is the idea behind the Structural Correspondence Learning (SCL) algorithm? sent16
        1.5.2. How did Blitzer et al. (2007) extend the SCL algorithm? sent17
    1.6. What did Lu and Zhai (2008) propose for integrating opinions from different sources? sent18
        1.6.1. What model does the study propose to solve the problem? sent19
        1.6.2. What experiments were performed in the study? sent20
        1.6.3. What is the focus of this study? sent21
    1.7. What technique did Ferreira et al. (2008) describe for feature extraction? sent22
        1.7.1. What scheme and analysis does the study present? sent23
    1.8. How is association rule mining used for target extraction? sent24
    1.9. What is one of the latest works on feature level analysis of opinion reported by Zhai et al. (2011)? sent25, sent26
        1.9.1. Why is feature grouping important? sent27
        1.9.2. How can the same features be expressed? sent28
        1.9.3. What is the purpose of grouping words and phrases? sent29
        1.9.4. How does the process generate an initial list for feature grouping? sent30
        1.9.5. What were the results of this method? sent31
    1.10. What approach does Goujon (2011) present for opinion target detection? sent32
        1.10.1. What does this study focus on? sent33
        1.10.2. How does this approach identify targets? sent34
2. What are the two most frequently reported unsupervised approaches for target and opinion identification? sent35"
232895648,Generative Adversarial Networks: A Literature Review,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,Inception Score (IS),5,"During the ILSVRC competition in 2014, Google proposed a network called Inception Net [42]. It not only controlled the number of parameters, but also achieved good classification performance. In 2015, Inception net-v3 was proposed and it was exactly the model required for IS calculation in this part [43,44].

As analyzed above, IS focuses on the sharpness and diversity of images, which IS is not high resolution but clear classification [45]. The formula for IS is as follows.

Generate the 1000-dimensional vector y obtained from the input model of picture x, and the probability distribution of x belonging to each category is

is the marginal distribution of all the images obtained after the input model of a large number of images is generated.

is related to variety.

However, the evaluation index of IS was limited to the use of data set, because the Inception V3 model adopted by IS was trained on ImageNet [46], so the image generation of GAN also needs to rely on ImageNet to serve as a real sample. It no longer makes sense to use the classification model and the generation model in different data sets. At the same time, in the case of insufficient samples, the estimation of sample distribution will become very difficult, so we also need other indicators to evaluate the capability of GAN.","sent1: During the ILSVRC competition in 2014, Google proposed a network called Inception Net [42].
sent2: It not only controlled the number of parameters, but also achieved good classification performance.
sent3: In 2015, Inception net-v3 was proposed and it was exactly the model required for IS calculation in this part [43,44].
sent4: As analyzed above, IS focuses on the sharpness and diversity of images, which IS is not high resolution but clear classification [45].
sent5: The formula for IS is as follows.
sent6: Generate the 1000-dimensional vector y obtained from the input model of picture x, and the probability distribution of x belonging to each category isis the marginal distribution of all the images obtained after the input model of a large number of images is generated.
sent7: is related to variety. However, the evaluation index of IS was limited to the use of data set, because the Inception V3 model adopted by IS was trained on ImageNet [46], so the image generation of GAN also needs to rely on ImageNet to serve as a real sample.
sent8: It no longer makes sense to use the classification model and the generation model in different data sets.
sent9: At the same time, in the case of insufficient samples, the estimation of sample distribution will become very difficult, so we also need other indicators to evaluate the capability of GAN.","1. What network did Google propose during the ILSVRC competition in 2014? sent1
    1.1. What were the achievements of Inception Net? sent2
    1.2. What is the significance of Inception net-v3 in relation to IS calculation? sent3
2. What does IS focus on in terms of image evaluation? sent4
3. What is the formula for IS? sent5
4. How is the 1000-dimensional vector y generated for IS calculation? sent6
5. What limitation is associated with the evaluation index of IS? sent7
6. Why does it not make sense to use the classification model and the generation model in different data sets? sent8
7. What challenge arises in the case of insufficient samples for IS evaluation? sent9"
5058880,False Information on Web and Social Media: A Survey,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,Network characteristics.,5,"Rumors and hoaxes can be related to other information in terms of what they say about others and what others say about it. Kumar et al. [50] quantified this for hoaxes on Wikipedia by measuring the connectedness of the different Wikipedia articles referenced in the hoax article. Intuitively, high connectedness indicates interrelated and coherent references. The authors computed the clustering coefficient of the local hyperlink network of the article, i.e., the average clustering coefficient of the subnetwork induced by the articles referenced by the article. They found that hoax information has fewer references and significantly lower clustering coefficient compared to non-hoax articles. This suggests that references in hoaxes are added primarily to appear genuine, instead of adding them by need as legitimate writers do.

Network characteristics of rumors are studied by analyzing the network of users that spread them and by creating co-occurrence networks out of false information tweets-these contain nodes of one or more types, such as URLs, domains, user accounts or hashtags, and use edges to represent the number of times they are mentioned in the same tweet together. Using the user-user network, Subrahmanian et al. [97] found that some bot accounts that spread false information are close to each other and appear as groups in Twitter's follower-followee network, with significant overlap between their followers and followees. Moreover, Bessi et al. [13] conducted a k-core analysis of this follower-followee network and found that the fraction of bots increases steadily in higher cores, suggests that bots become increasingly central in the rebroadcasting network. Using the co-occurrence network, Fig. 10. Cascade of reshares of a Cabela's sporting goods store receipt attributing addition sales tax to ""Obamacare"". The coloring is from early (red) to late (blue). Reprinted with permission from [30].

Starbird [95] found that alternate media (false news) domains form tightly connected clusters, meaning that many users mention these domains together in their false information tweets.","sent1: Rumors and hoaxes can be related to other information in terms of what they say about others and what others say about it.
sent2: Kumar et al. [50] quantified this for hoaxes on Wikipedia by measuring the connectedness of the different Wikipedia articles referenced in the hoax article.
sent3: Intuitively, high connectedness indicates interrelated and coherent references.
sent4: The authors computed the clustering coefficient of the local hyperlink network of the article, i.e., the average clustering coefficient of the subnetwork induced by the articles referenced by the article.
sent5: They found that hoax information has fewer references and significantly lower clustering coefficient compared to non-hoax articles.
sent6: This suggests that references in hoaxes are added primarily to appear genuine, instead of adding them by need as legitimate writers do.
sent7: Network characteristics of rumors are studied by analyzing the network of users that spread them and by creating co-occurrence networks out of false information tweets-these contain nodes of one or more types, such as URLs, domains, user accounts or hashtags, and use edges to represent the number of times they are mentioned in the same tweet together.
sent8: Using the user-user network, Subrahmanian et al. [97] found that some bot accounts that spread false information are close to each other and appear as groups in Twitter's follower-followee network, with significant overlap between their followers and followees.
sent9: Moreover, Bessi et al. [13] conducted a k-core analysis of this follower-followee network and found that the fraction of bots increases steadily in higher cores, suggests that bots become increasingly central in the rebroadcasting network.
sent10: Using the co-occurrence network, Fig. 10.
sent11: Cascade of reshares of a Cabela's sporting goods store receipt attributing addition sales tax to ""Obamacare"".
sent12: The coloring is from early (red) to late (blue).
sent13: Reprinted with permission from [30].
sent14: Starbird [95] found that alternate media (false news) domains form tightly connected clusters, meaning that many users mention these domains together in their false information tweets.","1. How can rumors and hoaxes be related to other information? sent1
2. How did Kumar et al. quantify hoaxes on Wikipedia? sent2
    2.1. What does high connectedness indicate in the context of hoaxes? sent3
    2.2. How did the authors compute the clustering coefficient for hoax articles? sent4
    2.3. What did the authors find about hoax information compared to non-hoax articles? sent5
    2.4. What does the addition of references in hoaxes suggest? sent6
3. How are network characteristics of rumors studied? sent7
    3.1. What did Subrahmanian et al. find using the user-user network? sent8
    3.2. What did Bessi et al. discover through k-core analysis of the follower-followee network? sent9
4. What did Starbird find about alternate media domains using the co-occurrence network? sent14"
244786380,A Review of Factors Affecting the Effectiveness of Phishing,https://www.semanticscholar.org/paper/b83c7598f874602facb4d8b13cc494960dbceb50,Literature Review,7,"Jampen et al. conducted a survey to analyze the effectiveness and sustainability of organizational training programs, aimed at giving anti-phishing awareness to its employees, in reducing their employees"" vulnerability to the phishing attacks. They categorized works using a well-formed methodology that took into consideration various parts of the training programs. They noted important results in the technical literature. They found out that, overall, researchers found a consensus to most research questions that gave regard to the convenience of training programs for anti-phishing. A mixture of findings came from how age affects the likelihood of the accomplishment of anti-phishing programs to train employees. Jampen et al. gave a description based on their comprehensive analysis on the design of a properly structured training program for anti-phishing and a framework with a set of recommended directives for research. (Jampen et al., 2020) Their study reveals how phishing awareness training can reduce the attacker""s chances of success, which will need them to go the extra miles in convincing users who have been trained already. Panum et al. performed an exploratory work on the efficiency of modern and popular and solutions that detect phishing, by analyzing the techniques of detection that they shared in common. They presented sample mechanisms capable of avoiding detection by causing unnoticeable perturbations. They proposed steps and measures to take in the design to improve the evaluation of the robustness of adversaries in the future. They brought to light a terminology, for respective methods, that does not depend on the application or environment to elucidate the conditions for the setting of the adversaries. Three axioms that should be accounted by any solution that detects phishing attacks were presented by them, based on an agreed upon phishing definition. This aided the solutions not to use the wrong inference attributes. They disintegrated the inference methods from the detection solutions into a collection of strategies. They evaluated the capability of absconding capture and cases of perturbations that allowed it. Their findings allow the definition of guidelines to the design of solutions for phishing detection for the community. (Panum et al., 2020) This means that hackers are sure to face impedance as they design common phishing websites, as phishing solutions equipped with such guidelines shall be detected.

Bitaab et al. carried out an extended study of measurements on the attacks related to phishing that occurred during the premature stages of the coronavirus pandemic between January 2020 and May 2020. They used their dataset to perform tracking of the trends and the reason for the growth in the phishing attacks. They analyzed and collected records of Domain Name Systems (DNS), certificates of Transport Layer Security (TLS), the phishing websites"" source codes, web traffic, and Uniform Resource Locators (URLs), emails of phishing attacks, news, and announcements of the government. They found out that the traffic of phishing attacks increased by a fraction of 220% in comparison to the rate before the COVID-19 pandemic, which exceeded previous seasonal trends. The attackers would orchestrate various hacks to manipulate the victims"" uncertainty and fear about the pandemic. The attackers used modern ways to which the existing defense systems could not handle. The analysis of Bitaab et al. displayed the ability for new defenses of ecosystems and upgraded teamwork among parties to promote quicker and efficient plans for ecosystems to tackle the uprising volume of phishing. (Bitaab et al., 2020) Their study provides quicker mutations to phishing defenses, which may be able to tackle unplanned scenarios such as a pandemic. This may reduce the hacker""s ability to take advantage of uprising tragedies to perform their cybercrime.

Steves et al. proposed a rating scale to solve the issue faced by organizations investing in training programs for phishing awareness. Since Chief Information Security Officers (CISOs) are highly dissatisfied if phishing click rates that result from the training exercises are very high. The training budget must be justified to the board officials explaining the necessity of the training as the click rates are not reducing. Their study gave rise to a debate that the level of difficulty of the phishing attack email targeted at an individual should be a factor in measuring the variance of the click rates. Based on previous studies, a phishing email forged to align with the context of a user""s job is much harder for the users to detect malice. This made Steves et al. come up with a Phish Scale to aid CISOs and the implementers of the phishing tests to give a rating of how difficult their attack is. The scale justifies any associated clicks from the tests. Cues of phishing and the context of users from former researches devised the base of their scale. They applied the scale to recent and old published results from the phishing attack exercises performed by enterprises. Their Phish scale had decent results with their selected datasets and revealed large potential as a scaling tool and a catalyst for sharing information regarding the clicks observed during a phishing experiment. (Steves et al., 2020) Their tool provides disambiguation to the top management as phishing training clicks rise in number, which helps them be able to make more effective decisions. This ensures that phishing training may have a stable budget and hackers shall more likely have to face highly trained staff, which again may reduce their chances for success.

Wang et al. attempted to detect phishing using Bidirectional Long Short-Term Memory (BLSTM) and Random Forest classifiers. The results of their experiments were satisfying in regards to the detection of phishing and their study contributed to applying the algorithms that they proposed to the field of information security. The Bidirectional Long Short-Term Memory model produced a rate of recognition of 95.47% in comparison to the Random Forest model that produced 87.53%. The results of Wang et al. reveal that the BLSTM detection method is more reliable in guaranteeing security in the network and uncovering the relevance of the model that they proposed to detect phishing. (Wang et al., 2020) Their study revealed efficient means to tackle phishing, and future solutions may be adapted to fill gaps in weaknesses of solutions of the past.

Mohith Gowda et al. devised a novel mechanism to identify websites used for phishing with the use of a novel architecture for a browser on the client""s side. They applied the extraction framework rule to draw out the websites"" properties using only the URL. A list consisting of 30 various features of a URL is populated. Sharma & Bashir performed a study where they looked into the attackers"" emails that were available to the public in repository databases for phishing attacks. They analyzed the characteristics and contents of those phishing emails. In order for them to understand the language and techniques that attackers used to be able to lure their targets, they considered many variables. Their findings showed that the words of the attackers used in their emails would aim at exploiting emotional triggers in humans such as anticipation and fear. The role played by their findings centered on a human study is a major step directed to improving the programs for training and enhancing the detection of phishing attacks, similarly, human factors may take part in the security of systems. (Sharma & Bashir, 2020) Broadhurst et al. performed a study based on explorations and observations on 138 recruits from the orientation week of a university for several months in 2017. The aim of their study was to find out cybercrime risks. They ran social engineering attacks, observed the responses, and compared how the participants took the risks to cybercrime before and after the phishing campaign. Their quasi-experimental survey exposed the test subjects to fake emails and phishing attacks. The intention of the emails was to steal confidential data from the victims or convince them to navigate to poisoned sites by clicking on the malicious links in the mail. Their techniques varied in terms of individualization. The phishing categories involved targeted or spear, tailored and generic. They classified the subjects based on the awareness of cybercrime in two groups, viz. Hunter and Passive condition. Those in the Hunter class, throughout the experiment, were aware of all forms of swindles and the ones in the Passive class did not get any warning. Broadhurst et al. analyzed the effects of the type of scam, awareness of cybercrime, competence in the field of information technology (IT), gender of the subject, and perception of safety on the internet to how susceptible their email scams were. They found out that spear phishing had a better chance of being engaged to than a generic phishing attempt. Their analysis also pointed out that there was a higher probability that fresh men and international students would face deception from the phishing than the senior and domestic students. For the further exploration of all their variables and the results, they performed a generalized linear model (GLM) analysis. (Broadhurst et al., 2020) Williams & Joinson conducted theoretical based research to investigate the methods in which the present and future phishing interactions may target the users along with the effect they play on the susceptibility of phishing. They developed and validated a survey measure centered on the protection motivation theory constructs across two studies. Such constructs include perceived vulnerability, efficacy to response, severity, and self-efficacy. They assessed the features contributing to the decision that people make on whether they shall stay updated by phishing awareness to protect themselves or not. They analyzed what role each construct played in the intentions of the user to know the latest phishing techniques that will evolve and the capability of phishing discrimination via a phishing quiz assessment. Williams & Joinson observed that larger intentions came from a greater perception of the threats"" response efficacy, severity, and self-efficacy while low intentions to know about the phishing techniques came from a high perception of vulnerability. They did not manage to find a relationship with the ability to discriminate against phishing. With the knowledge on the causes of users"" intentions in maintaining education and pursuing updates about phishing dangers, the assurance is available, that efficient interventions come, and maximum potential effects exist. (Williams & Joinson, 2020) Frauenstein & Flowerday presented a model based on theory to counter the susceptibility of phishing on social network sites (SSNs). They collected data from 215 subjects and observed the contribution of the processing of information to phishing on social networks. They regarded how users are vulnerable to the sites based on their personalities to identify characteristics of users that may be more susceptible to phishing on these social media sites. They performed a Structural Equation Modeling (SEM) analysis, and the results showed that heuristic processing faced a negative impact from the conscious users, and thus were less vulnerable to phishing on SNSs. Their analysis supported and confirmed previous studies that the susceptibility to phishing increases with heuristic processing. The research of Frauenstein & Flowerday contributed to the discipline of information security as being one of the first studies to analyze how the relationship between the Big Five Personality Traits: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Openness, and the systematic heuristic model is affected. (Frauenstein & Flowerday, 2020) ","sent1: Jampen et al. conducted a survey to analyze the effectiveness and sustainability of organizational training programs, aimed at giving anti-phishing awareness to its employees, in reducing their employees"" vulnerability to the phishing attacks.
sent2: They categorized works using a well-formed methodology that took into consideration various parts of the training programs.
sent3: They noted important results in the technical literature.
sent4: They found out that, overall, researchers found a consensus to most research questions that gave regard to the convenience of training programs for anti-phishing.
sent5: A mixture of findings came from how age affects the likelihood of the accomplishment of anti-phishing programs to train employees.
sent6: Jampen et al. gave a description based on their comprehensive analysis on the design of a properly structured training program for anti-phishing and a framework with a set of recommended directives for research.
sent7: (Jampen et al., 2020) Their study reveals how phishing awareness training can reduce the attacker""s chances of success, which will need them to go the extra miles in convincing users who have been trained already.
sent8: Panum et al. performed an exploratory work on the efficiency of modern and popular and solutions that detect phishing, by analyzing the techniques of detection that they shared in common.
sent9: They presented sample mechanisms capable of avoiding detection by causing unnoticeable perturbations.
sent10: They proposed steps and measures to take in the design to improve the evaluation of the robustness of adversaries in the future.
sent11: They brought to light a terminology, for respective methods, that does not depend on the application or environment to elucidate the conditions for the setting of the adversaries.
sent12: Three axioms that should be accounted by any solution that detects phishing attacks were presented by them, based on an agreed upon phishing definition.
sent13: This aided the solutions not to use the wrong inference attributes.
sent14: They disintegrated the inference methods from the detection solutions into a collection of strategies.
sent15: They evaluated the capability of absconding capture and cases of perturbations that allowed it.
sent16: Their findings allow the definition of guidelines to the design of solutions for phishing detection for the community.
sent17: (Panum et al., 2020) This means that hackers are sure to face impedance as they design common phishing websites, as phishing solutions equipped with such guidelines shall be detected.
sent18: Bitaab et al. carried out an extended study of measurements on the attacks related to phishing that occurred during the premature stages of the coronavirus pandemic between January 2020 and May 2020.
sent19: They used their dataset to perform tracking of the trends and the reason for the growth in the phishing attacks.
sent20: They analyzed and collected records of Domain Name Systems (DNS), certificates of Transport Layer Security (TLS), the phishing websites"" source codes, web traffic, and Uniform Resource Locators (URLs), emails of phishing attacks, news, and announcements of the government.
sent21: They found out that the traffic of phishing attacks increased by a fraction of 220% in comparison to the rate before the COVID-19 pandemic, which exceeded previous seasonal trends.
sent22: The attackers would orchestrate various hacks to manipulate the victims"" uncertainty and fear about the pandemic.
sent23: The attackers used modern ways to which the existing defense systems could not handle.
sent24: The analysis of Bitaab et al. displayed the ability for new defenses of ecosystems and upgraded teamwork among parties to promote quicker and efficient plans for ecosystems to tackle the uprising volume of phishing.
sent25: (Bitaab et al., 2020) Their study provides quicker mutations to phishing defenses, which may be able to tackle unplanned scenarios such as a pandemic.
sent26: This may reduce the hacker""s ability to take advantage of uprising tragedies to perform their cybercrime.Steves et al. proposed a rating scale to solve the issue faced by organizations investing in training programs for phishing awareness.
sent27: Since Chief Information Security Officers (CISOs) are highly dissatisfied if phishing click rates that result from the training exercises are very high.
sent28: The training budget must be justified to the board officials explaining the necessity of the training as the click rates are not reducing.
sent29: Their study gave rise to a debate that the level of difficulty of the phishing attack email targeted at an individual should be a factor in measuring the variance of the click rates.
sent30: Based on previous studies, a phishing email forged to align with the context of a user""s job is much harder for the users to detect malice.
sent31: This made Steves et al. come up with a Phish Scale to aid CISOs and the implementers of the phishing tests to give a rating of how difficult their attack is.
sent32: The scale justifies any associated clicks from the tests.
sent33: Cues of phishing and the context of users from former researches devised the base of their scale.
sent34: They applied the scale to recent and old published results from the phishing attack exercises performed by enterprises.
sent35: Their Phish scale had decent results with their selected datasets and revealed large potential as a scaling tool and a catalyst for sharing information regarding the clicks observed during a phishing experiment.
sent36: (Steves et al., 2020) Their tool provides disambiguation to the top management as phishing training clicks rise in number, which helps them be able to make more effective decisions.
sent37: This ensures that phishing training may have a stable budget and hackers shall more likely have to face highly trained staff, which again may reduce their chances for success.
sent38: Wang et al. attempted to detect phishing using Bidirectional Long Short-Term Memory (BLSTM) and Random Forest classifiers.
sent39: The results of their experiments were satisfying in regards to the detection of phishing and their study contributed to applying the algorithms that they proposed to the field of information security.
sent40: The Bidirectional Long Short-Term Memory model produced a rate of recognition of 95.47% in comparison to the Random Forest model that produced 87.53%.
sent41: The results of Wang et al. reveal that the BLSTM detection method is more reliable in guaranteeing security in the network and uncovering the relevance of the model that they proposed to detect phishing.
sent42: (Wang et al., 2020) Their study revealed efficient means to tackle phishing, and future solutions may be adapted to fill gaps in weaknesses of solutions of the past.
sent43: Mohith Gowda et al. devised a novel mechanism to identify websites used for phishing with the use of a novel architecture for a browser on the client""s side.
sent44: They applied the extraction framework rule to draw out the websites"" properties using only the URL.
sent45: A list consisting of 30 various features of a URL is populated.
sent46: Sharma & Bashir performed a study where they looked into the attackers"" emails that were available to the public in repository databases for phishing attacks.
sent47: They analyzed the characteristics and contents of those phishing emails.
sent48: In order for them to understand the language and techniques that attackers used to be able to lure their targets, they considered many variables.
sent49: Their findings showed that the words of the attackers used in their emails would aim at exploiting emotional triggers in humans such as anticipation and fear.
sent50: The role played by their findings centered on a human study is a major step directed to improving the programs for training and enhancing the detection of phishing attacks, similarly, human factors may take part in the security of systems.
sent51: (Sharma & Bashir, 2020) Broadhurst et al. performed a study based on explorations and observations on 138 recruits from the orientation week of a university for several months in 2017.
sent52: The aim of their study was to find out cybercrime risks.
sent53: They ran social engineering attacks, observed the responses, and compared how the participants took the risks to cybercrime before and after the phishing campaign.
sent54: Their quasi-experimental survey exposed the test subjects to fake emails and phishing attacks.
sent55: The intention of the emails was to steal confidential data from the victims or convince them to navigate to poisoned sites by clicking on the malicious links in the mail.
sent56: Their techniques varied in terms of individualization.
sent57: The phishing categories involved targeted or spear, tailored and generic.
sent58: They classified the subjects based on the awareness of cybercrime in two groups, viz.
sent59: Hunter and Passive condition. Those in the Hunter class, throughout the experiment, were aware of all forms of swindles and the ones in the Passive class did not get any warning.
sent60: Broadhurst et al. analyzed the effects of the type of scam, awareness of cybercrime, competence in the field of information technology (IT), gender of the subject, and perception of safety on the internet to how susceptible their email scams were.
sent61: They found out that spear phishing had a better chance of being engaged to than a generic phishing attempt.
sent62: Their analysis also pointed out that there was a higher probability that fresh men and international students would face deception from the phishing than the senior and domestic students.
sent63: For the further exploration of all their variables and the results, they performed a generalized linear model (GLM) analysis.
sent64: (Broadhurst et al., 2020) Williams & Joinson conducted theoretical based research to investigate the methods in which the present and future phishing interactions may target the users along with the effect they play on the susceptibility of phishing.
sent65: They developed and validated a survey measure centered on the protection motivation theory constructs across two studies.
sent66: Such constructs include perceived vulnerability, efficacy to response, severity, and self-efficacy.
sent67: They assessed the features contributing to the decision that people make on whether they shall stay updated by phishing awareness to protect themselves or not.
sent68: They analyzed what role each construct played in the intentions of the user to know the latest phishing techniques that will evolve and the capability of phishing discrimination via a phishing quiz assessment.
sent69: Williams & Joinson observed that larger intentions came from a greater perception of the threats"" response efficacy, severity, and self-efficacy while low intentions to know about the phishing techniques came from a high perception of vulnerability.
sent70: They did not manage to find a relationship with the ability to discriminate against phishing.
sent71: With the knowledge on the causes of users"" intentions in maintaining education and pursuing updates about phishing dangers, the assurance is available, that efficient interventions come, and maximum potential effects exist.
sent72: (Williams & Joinson, 2020) Frauenstein & Flowerday presented a model based on theory to counter the susceptibility of phishing on social network sites (SSNs).
sent73: They collected data from 215 subjects and observed the contribution of the processing of information to phishing on social networks.
sent74: They regarded how users are vulnerable to the sites based on their personalities to identify characteristics of users that may be more susceptible to phishing on these social media sites.
sent75: They performed a Structural Equation Modeling (SEM) analysis, and the results showed that heuristic processing faced a negative impact from the conscious users, and thus were less vulnerable to phishing on SNSs.
sent76: Their analysis supported and confirmed previous studies that the susceptibility to phishing increases with heuristic processing.
sent77: The research of Frauenstein & Flowerday contributed to the discipline of information security as being one of the first studies to analyze how the relationship between the Big Five Personality Traits: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Openness, and the systematic heuristic model is affected.
sent78: (Frauenstein & Flowerday, 2020)","1. What was the focus of the survey conducted by Jampen et al.? sent1
    1.1. How did Jampen et al. categorize works in their survey? sent2
    1.2. What consensus did researchers find regarding anti-phishing training programs? sent4
    1.3. What did Jampen et al. describe in their comprehensive analysis? sent6
    1.4. What does Jampen et al.'s study reveal about phishing awareness training? sent7
2. What did Panum et al. explore in their study? sent8
    2.1. What did Panum et al. propose to improve phishing detection solutions? sent10
    2.2. What terminology did Panum et al. introduce in their study? sent11
    2.3. What guidelines did Panum et al.'s findings allow for phishing detection solutions? sent16
3. What did Bitaab et al. study during the early stages of the coronavirus pandemic? sent18
    3.1. What did Bitaab et al. find about the traffic of phishing attacks during the pandemic? sent21
    3.2. How did attackers exploit the pandemic according to Bitaab et al.? sent22
    3.3. What did Bitaab et al.'s analysis display regarding new defenses? sent24
4. What issue did Steves et al. address with their proposed rating scale? sent26
    4.1. What debate arose from Steves et al.'s study regarding phishing attack emails? sent29
    4.2. What is the purpose of the Phish Scale proposed by Steves et al.? sent31
    4.3. How was the Phish Scale applied and what were the results? sent34, sent35
5. What method did Wang et al. use to detect phishing? sent38
    5.1. What were the results of Wang et al.'s experiments with BLSTM and Random Forest classifiers? sent40
6. What mechanism did Mohith Gowda et al. devise for identifying phishing websites? sent43
    6.1. How did Mohith Gowda et al. extract website properties? sent44
7. What did Sharma & Bashir study regarding phishing emails? sent46
    7.1. What did Sharma & Bashir find about the language used in phishing emails? sent49
8. What was the aim of Broadhurst et al.'s study? sent52
    8.1. What did Broadhurst et al. find about spear phishing compared to generic phishing? sent61
9. What did Williams & Joinson investigate in their research? sent64
    9.1. What constructs did Williams & Joinson's survey measure focus on? sent66
    9.2. What did Williams & Joinson observe about users' intentions regarding phishing awareness? sent69
10. What model did Frauenstein & Flowerday present in their study? sent72
    10.1. What did Frauenstein & Flowerday's analysis show about heuristic processing and phishing vulnerability? sent75"
243886770,A Survey on Sentiment Analysis Approaches in e-Commerce,https://www.semanticscholar.org/paper/51cbfd00189b029802981df08872dded5a34c7c2,D. Evaluation Score,7,"Based on feature extraction and sentiment classification on online reviews is rank the result using statical method [2]. The overall evaluation result is very important to judge subjective online reviews for customers. The result can predict or measure with mean squared error (MSA), confusion matrix, accuracy, precision, recall and F1-score [9] [15] [28] [29].

Equation of precision is presented as true positive (high quality reviews) divide by true positive (high positive reviews) + false positive (low quality reviews) [5] [18] [29].

Recall represented as true positive (high quality reviews) divide by true positive (high quality reviews) + False negative (low quality reviews) [5] [18] [29].","sent1: Based on feature extraction and sentiment classification on online reviews is rank the result using statical method [2].
sent2: The overall evaluation result is very important to judge subjective online reviews for customers.
sent3: The result can predict or measure with mean squared error (MSA), confusion matrix, accuracy, precision, recall and F1-score [9]
sent4: [15] [28] [29]. Equation of precision is presented as true positive (high quality reviews) divide by true positive (high positive reviews) + false positive (low quality reviews) [5] [18] [29].
sent5: Recall represented as true positive (high quality reviews) divide by true positive (high quality reviews) + False negative (low quality reviews) [5] [18] [29].","1. How is the result of feature extraction and sentiment classification on online reviews ranked? sent1
2. Why is the overall evaluation result important for customers? sent2
3. What metrics can be used to predict or measure the evaluation result? sent3
4. How is precision calculated in the context of sentiment analysis on online reviews? sent4
5. How is recall represented in the context of sentiment analysis on online reviews? sent5"
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,Multiple languages,27,"In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","sent1: In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1.
sent2: One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance.
sent3: Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead.
sent4: In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a).
sent5: Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help.
sent6: Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful.
sent7: All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance.
sent8: In contrast, Unrelated is detected almost perfectly by all models (over 99 F1).
sent9: Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general.
sent10: This can be attributed to the use of n-grams, topic models, and lexica.
sent11: Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents.
sent12: Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t.
sent13: the target claim. More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.
sent14: Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help.
sent15: In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree.
sent16: The success of these models is also seen in cross-lingual settings.
sent17: For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT.
sent18: Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets.
sent19: Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).
sent20: Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER .
sent21: To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted.
sent22: The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021).
sent23: Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score.
sent24: Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points.
sent25: Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70.
sent26: Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes.
sent27: Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information.
sent28: Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.
sent29: Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021).
sent30: Such models do not require a retrieval step, as they use the knowledge stored in language models.
sent31: However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"".
sent32: Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type.
sent33: Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).
sent34: Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer.""
sent35: vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club.
sent36: "", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers.""
sent37: (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm.""
sent38: is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.
sent39: Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.
sent40: These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours).
sent41: A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017).
sent42: Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets.
sent43: More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020).
sent44: Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1).
sent45: Li et al. (2020) deviated from this structure and modelled the conversations as a graph.
sent46: Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection.
sent47: Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task.
sent48: Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification .
sent49: Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).
sent50: A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0.
sent51: Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1.
sent52: Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class.
sent53: On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).
sent54: Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.
sent55: Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness.
sent56: However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.
sent57: Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance.
sent58: Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models.
sent59: They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets.
sent60: Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.
sent61: They showed that MDL helps for low-resource and substantively for full-resource scenarios.
sent62: Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance.
sent63: Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1.
sent64: The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc.
sent65: Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","1. What is discussed in this section regarding stance detection for mis-and disinformation detection? sent1
    1.1. What was the evaluation measure used in the Fake News Challenge? sent2
    1.2. What criticism did Hanselowski et al. (2018a) have regarding the FNC score? sent3
    1.3. What features did most teams use in the Fake News Challenge? sent4
        1.3.1. What did Hanselowski et al. (2018a) find about the importance of different feature groups? sent5
        1.3.2. What did Ghanem et al. (2018) find about lexical cues? sent6
    1.4. What challenges do models face in learning the Disagree class? sent7
    1.5. How do models perform on the Unrelated class? sent8
    1.6. What limitations did Hanselowski et al. (2018a) identify in these models? sent9
        1.6.1. What contributes to these limitations? sent10
    1.7. What approach did Mohtarami et al. (2018) investigate to improve stance detection? sent11
        1.7.1. What challenges did their model face? sent12, sent13
    1.8. How has transfer learning with pre-trained Transformers impacted stance detection? sent14
        1.8.1. What improvements were observed using RoBERTa? sent15
        1.8.2. How successful are these models in cross-lingual settings? sent16
            1.8.2.1. What results were achieved for Arabic stance detection? sent17
            1.8.2.2. What approach did Hardalov et al. (2022) use in a cross-lingual setting? sent18
            1.8.2.3. What did Alhindi et al. (2021) find about language-specific pre-training? sent19
    1.9. What additional step is included in some stance detection formulations? sent20
        1.9.1. What is the FEVER score? sent21
        1.9.2. What approaches were used in the FEVER competition? sent22
            1.9.2.1. How did Zhou et al. (2019) improve performance in the FEVER competition? sent23
            1.9.2.2. How does the retrieval model impact performance? sent24
            1.9.2.3. What improvements did Liu et al. (2020b) and Zhong et al. (2020) make? sent25
            1.9.2.4. What did Ye et al. (2020) experiment with? sent26
            1.9.2.5. How did Si et al. (2021) and Ostrowski et al. (2021) improve multi-hop reasoning? sent27
            1.9.2.6. What task did Atanasova et al. (2022) introduce? sent28
    1.10. What is a notable idea for using pre-trained language models as fact-checkers? sent29
        1.10.1. What are the limitations of these models? sent30, sent31, sent32
        1.10.2. How does their performance compare to supervised approaches? sent33
        1.10.3. What challenges are identified in error analysis? sent34, sent35, sent36, sent37, sent38
2. What is the focus of threaded stance detection in conversational threads? sent39
    2.1. How are these approaches typically applied? sent40
    2.2. What is a common pattern in threaded stance detection models? sent41
    2.3. What advancements have been made in threaded stance detection models? sent42, sent43
    2.4. How did Kochkina et al. (2017, 2018) model conversations? sent44
    2.5. How did Li et al. (2020) model conversations differently? sent45
    2.6. What did Tian et al. (2020) show about pre-training on stance data? sent46
    2.7. How did Yang et al. (2019) curate pre-training data? sent47
    2.8. What additional features did Li et al. (2019a, b) incorporate? sent48
    2.9. What challenge is highlighted regarding the stance of a post? sent49
    2.10. What major challenge do rumour detection datasets face? sent50
        2.10.1. How does label semantics impact performance? sent51
        2.10.2. How does thread depth affect performance? sent52
        2.10.3. What positive impact does multitask learning have? sent53
        2.10.4. What role does the temporal dimension play in threaded stance detection? sent54
3. What is Multi-Dataset Learning (MDL) and its benefits? sent55
    3.1. What challenges arise from combining mis-and disinformation identification with stance detection? sent56
    3.2. What did earlier approaches focus on in MDL? sent57
    3.3. What benchmark did Schiller et al. (2021) propose? sent58
    3.4. How did they leverage MT-DNN for improvements? sent59
    3.5. What did Hardalov et al. (2021) achieve in a cross-lingual setting? sent60
    3.6. How does MDL help in different resource scenarios? sent61
    3.7. How can transferring knowledge from English stance datasets boost performance? sent62
4. What does Table 2 show regarding stance detection datasets? sent63
    4.1. How do the datasets vary in their task formulation and composition? sent64
    4.2. How do these factors impact performance and result comparison? sent65"
248530069,Deep Personality Trait Recognition: A Survey,https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc,Static Images,7,"As far as static image-based personality trait recognition is concerned, researchers have found that a facial image presents most of meaningful descriptive cues for personality trait recognition (Willis and Todorov, 2006). Hence, the extracted visual features involve in the analysis of facial features for personality trait prediction. In (Guntuku et al., 2015), the authors proposed to leverage several low-level features of facial images, such as color histograms, local binary patterns (LBP), global descriptor (GIST), and aesthetic features, to train the SVM classifier for detecting mid-level clues (gender, age). Then, they predicted the Big-five personality traits of users in selfportrait images with the lasso regressor. Yan et al. (2016) investigated the connection between facial appearance and personality impression in the manner of trustworthy. They obtained middle-level cues through clustering methods from different low-level features, such as histogram of oriented gradients (HOG), scale-invariant feature transform (SIFT), LBP, and so on. Then, a SVM classifier was used to exploit the connection between facial appearance and personality impression.

In recent years, CNNs were also widely used for facial feature extraction on static image-based personality trait recognition tasks. Zhang et al. (2017) presented an end-to-end CNN structure via fine-tuning a pretrained VGG-face model for feature learning so as to predict personality traits and intelligence jointly. They aimed to explore whether self-reported personality traits and intelligence can be jointly measured from facial images. Segalin et al. (2017) explored the linking the Big-Five personality traits and preferred images in the Flickr social network through image understanding and a deep CNN framework. In particular, they fine-tuned the pretrained AlexNet and VGG-16 modal to capture the aesthetic attributes of the images characterizing the personality traits associated with those images. They changed the last layer of the AlexNet and VGG-16 model to adapt them to a binary classification problem. Experiments results showed that the characterization of each image can be locked within the CNN layers, thereby discovering entangled attributes, such as the aesthetic and semantic information for generalizing the patterns that identify a personality trait. Rodríguez et al. (2020) presented a personality trait analysis in social networks by using a weakly supervised learning method of shared images. They trained a ResNet-50 network to derive personality representations from the posted images in social networks, so as to infer whether the personality scores from the posted images are correlated to those scores obtained from text. For predicting personality traits, the images without manually labeling were used for training the ResNet-50 model. Experiment results indicate that people's personality is not only related to text, but also with the image content. Fu and Zhang (2021) provided a personality trait recognition method by using active shape model (ASM) localization and DBNs. They employed an improved ASM model to extract facial features, followed by a DBN which was used to train and classify the students' four personality traits.","sent1: As far as static image-based personality trait recognition is concerned, researchers have found that a facial image presents most of meaningful descriptive cues for personality trait recognition (Willis and Todorov, 2006).
sent2: Hence, the extracted visual features involve in the analysis of facial features for personality trait prediction.
sent3: In (Guntuku et al., 2015), the authors proposed to leverage several low-level features of facial images, such as color histograms, local binary patterns (LBP), global descriptor (GIST), and aesthetic features, to train the SVM classifier for detecting mid-level clues (gender, age).
sent4: Then, they predicted the Big-five personality traits of users in selfportrait images with the lasso regressor.
sent5: Yan et al. (2016) investigated the connection between facial appearance and personality impression in the manner of trustworthy.
sent6: They obtained middle-level cues through clustering methods from different low-level features, such as histogram of oriented gradients (HOG), scale-invariant feature transform (SIFT), LBP, and so on.
sent7: Then, a SVM classifier was used to exploit the connection between facial appearance and personality impression.
sent8: In recent years, CNNs were also widely used for facial feature extraction on static image-based personality trait recognition tasks.
sent9: Zhang et al. (2017) presented an end-to-end CNN structure via fine-tuning a pretrained VGG-face model for feature learning so as to predict personality traits and intelligence jointly.
sent10: They aimed to explore whether self-reported personality traits and intelligence can be jointly measured from facial images.
sent11: Segalin et al. (2017) explored the linking the Big-Five personality traits and preferred images in the Flickr social network through image understanding and a deep CNN framework.
sent12: In particular, they fine-tuned the pretrained AlexNet and VGG-16 modal to capture the aesthetic attributes of the images characterizing the personality traits associated with those images.
sent13: They changed the last layer of the AlexNet and VGG-16 model to adapt them to a binary classification problem.
sent14: Experiments results showed that the characterization of each image can be locked within the CNN layers, thereby discovering entangled attributes, such as the aesthetic and semantic information for generalizing the patterns that identify a personality trait.
sent15: Rodríguez et al. (2020) presented a personality trait analysis in social networks by using a weakly supervised learning method of shared images.
sent16: They trained a ResNet-50 network to derive personality representations from the posted images in social networks, so as to infer whether the personality scores from the posted images are correlated to those scores obtained from text.
sent17: For predicting personality traits, the images without manually labeling were used for training the ResNet-50 model.
sent18: Experiment results indicate that people's personality is not only related to text, but also with the image content.
sent19: Fu and Zhang (2021) provided a personality trait recognition method by using active shape model (ASM) localization and DBNs.
sent20: They employed an improved ASM model to extract facial features, followed by a DBN which was used to train and classify the students' four personality traits.","1. What have researchers found about static image-based personality trait recognition? sent1
    1.1. What do the extracted visual features involve in the analysis of? sent2
2. What approach did Guntuku et al. (2015) propose for personality trait recognition? sent3
    2.1. How did they predict the Big-five personality traits? sent4
3. What did Yan et al. (2016) investigate regarding facial appearance? sent5
    3.1. How did they obtain middle-level cues? sent6
    3.2. What method was used to exploit the connection between facial appearance and personality impression? sent7
4. What recent advancements have been made in using CNNs for personality trait recognition? sent8
    4.1. What did Zhang et al. (2017) present in their study? sent9
        4.1.1. What was the aim of Zhang et al.'s study? sent10
    4.2. How did Segalin et al. (2017) link personality traits with preferred images on Flickr? sent11
        4.2.1. How did they adapt AlexNet and VGG-16 models for their study? sent12, sent13
        4.2.2. What did the experiment results show in Segalin et al.'s study? sent14
5. What method did Rodríguez et al. (2020) use for personality trait analysis in social networks? sent15
    5.1. How did they train the ResNet-50 network? sent16
    5.2. What did the experiment results indicate about personality prediction? sent17, sent18
6. What method did Fu and Zhang (2021) provide for personality trait recognition? sent19
    6.1. How did they employ the ASM model and DBNs in their study? sent20"
244786380,A Review of Factors Affecting the Effectiveness of Phishing,https://www.semanticscholar.org/paper/b83c7598f874602facb4d8b13cc494960dbceb50,Spear Phishing,6,"Spear phishing is a form of phishing attack that targets a single primary individual. The attacker focuses their attention on a single person and lures them into surrendering their secret data without them having a clue what is going on. The email consists of information concerning the target directly such as their home address, the company they work for, their job title etc. Social media sites such as LinkedIn have become a useful tool for hackers to plot their spear phishing hacks due to the professionalism aimed by users in submitting true information relating to their academic history and professional careers. (FireEye, 2018) (Bullee et al., 2017) Vishing Voice phishing or Vishing is another form of phishing where by the attacker makes a phone call to the victim and then tries to manipulate them over the phone in giving up confidential data. Hackers find vishing convenient as they may avoid detection by bouncing off several cellphone towers making them hard to trace. Reports have revealed that most victims of such attacks do not file reports to the police or responsible authorities. (Maseno, 2017) Whaling Whaling is a form of phishing that targets the high-profile individuals such as chief executive officers (CEOs), presidents, kings or queens etc. The attackers perform intensive profiling for extended periods before initiating an attack. This is a relatively serious type of threat as top executives have access to the most sensitive and critical data. A loss of such information is taken to have grave consequences. (Gupta et al., 2018) Keylogger A keylogger is a form of spyware that can be either hardware or software based. Its sole purpose is to record all the keystrokes that are typed in by the user, without them knowing. It does this behind the scenes so the user cannot suspect anything. The strokes of each key are compiled altogether into a log type of file. The hacker makes it so the logs may be sent to them secretly via email or any other covert mechanism. (Parekh et al., 2020) Backdoor A backdoor is a form of malware that functions to allow hackers to gain access to a machine without any approval, authorization, or authentication. The attackers normally use them to circumvent authentication systems and gain unapproved remote-control sessions to a machine or device. Once in a machine, they can hide themselves in many ways and go on further to steal confidential data or cause a denial of service. (Loi, 2017) ","sent1: Spear phishing is a form of phishing attack that targets a single primary individual.
sent2: The attacker focuses their attention on a single person and lures them into surrendering their secret data without them having a clue what is going on.
sent3: The email consists of information concerning the target directly such as their home address, the company they work for, their job title etc.
sent4: Social media sites such as LinkedIn have become a useful tool for hackers to plot their spear phishing hacks due to the professionalism aimed by users in submitting true information relating to their academic history and professional careers.
sent5: (FireEye, 2018) (Bullee et al., 2017)
sent6: Vishing Voice phishing or Vishing is another form of phishing where by the attacker makes a phone call to the victim and then tries to manipulate them over the phone in giving up confidential data.
sent7: Hackers find vishing convenient as they may avoid detection by bouncing off several cellphone towers making them hard to trace.
sent8: Reports have revealed that most victims of such attacks do not file reports to the police or responsible authorities.
sent9: (Maseno, 2017) Whaling Whaling is a form of phishing that targets the high-profile individuals such as chief executive officers (CEOs), presidents, kings or queens etc.
sent10: The attackers perform intensive profiling for extended periods before initiating an attack.
sent11: This is a relatively serious type of threat as top executives have access to the most sensitive and critical data.
sent12: A loss of such information is taken to have grave consequences.
sent13: (Gupta et al., 2018) Keylogger A keylogger is a form of spyware that can be either hardware or software based.
sent14: Its sole purpose is to record all the keystrokes that are typed in by the user, without them knowing.
sent15: It does this behind the scenes so the user cannot suspect anything.
sent16: The strokes of each key are compiled altogether into a log type of file.
sent17: The hacker makes it so the logs may be sent to them secretly via email or any other covert mechanism.
sent18: (Parekh et al., 2020) Backdoor A backdoor is a form of malware that functions to allow hackers to gain access to a machine without any approval, authorization, or authentication.
sent19: The attackers normally use them to circumvent authentication systems and gain unapproved remote-control sessions to a machine or device.
sent20: Once in a machine, they can hide themselves in many ways and go on further to steal confidential data or cause a denial of service. (Loi, 2017)","1. What is spear phishing? sent1
    1.1. How does an attacker execute a spear phishing attack? sent2
    1.2. What kind of information is typically included in a spear phishing email? sent3
    1.3. Why have social media sites like LinkedIn become useful for spear phishing attacks? sent4
2. What is vishing? sent6
    2.1. Why do hackers find vishing convenient? sent7
    2.2. What is a common issue with reporting vishing attacks? sent8
3. What is whaling? sent9
    3.1. How do attackers prepare for a whaling attack? sent10
    3.2. Why is whaling considered a serious threat? sent11
        3.2.1. What are the potential consequences of a whaling attack? sent12
4. What is a keylogger? sent13
    4.1. How does a keylogger operate? sent14, sent15
    4.2. How are the logs from a keylogger typically sent to the hacker? sent16, sent17
5. What is a backdoor? sent18
    5.1. How do attackers use backdoors? sent19
    5.2. What can attackers do once they have accessed a machine through a backdoor? sent20"
243886770,A Survey on Sentiment Analysis Approaches in e-Commerce,https://www.semanticscholar.org/paper/51cbfd00189b029802981df08872dded5a34c7c2,Recall = (2),4,"F-score is calculated based on recall and precision as) [ 

Mean absolute error (MAE) and Root Mean Square Error (RMSE) measure the closeness between fitted line to the data points [18] [29] [30].

Confusion matrix helps to show data difference between two classes [29]. ","sent1: F-score is calculated based on recall and precision as) [ Mean absolute error (MAE) and Root Mean Square Error (RMSE) measure the closeness between fitted line to the data points [18]
sent2: [29] [30]. Confusion matrix helps to show data difference between two classes [29].","1. How is F-score calculated in the context of sentiment analysis? sent1
    1.1. What do MAE and RMSE measure? sent1
2. What is the purpose of a confusion matrix? sent2"
231925325,Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition,https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23,Attention,48,"Offline ASR Streaming ASR RNN-based [10], [11], [24], [30], [25], [23], [31], [32], [33], [34], [35], [36], [37] [38], [27], [39], [28], [40], [29], [41], [42], [43], [44], [45] Transformerbased [26], [46], [47], [48], [49], [50], [51], [52], [53], [53], [54], [55], [56], [57], [58], [59] [60], [61], [62], [63], [57], [64], [52], [65], [66], [67], [68], [69], [70] An encoder processes X to a high level representation (hidden states) and passes it to the decoder where prediction of Y happens. In most cases, the information required to predict a particular frame y t is confined within a small number of input frames. Therefore, for decoding y t , it is not required to look at each input frames. The Attention model aligns the input frames with y t by assigning match scores to each pair of input frame and y t . The match scores convey how much a particular input frame is relevant to y t and accordingly, the decoder decides the degree of focus on each input frame for predicting y t .

Depending on how the alignments between output and input frames are designed, different types of attention mechanism are presented in the literature. A list of existing attention models along with short descriptions is provided in Table  I. The detailed explanation of different attention models is discussed throughout the paper. In this survey, we have considered the models which are built within RNN or Transformer architecture. Table II provides the list of literature which we have reviewed in the later sections of this paper.","sent1: Offline ASR Streaming ASR RNN-based [10], [11], [24], [30], [25], [23], [31], [32], [33], [34], [35], [36], [37] [38], [27], [39], [28], [40], [29], [41], [42], [43], [44], [45] Transformerbased [26], [46], [47], [48], [49], [50], [51], [52], [53], [53], [54], [55], [56], [57], [58], [59] [60], [61], [62], [63], [57], [64], [52], [65], [66], [67], [68], [69], [70] An encoder processes X to a high level representation (hidden states) and passes it to the decoder where prediction of Y happens.
sent2: In most cases, the information required to predict a particular frame y t is confined within a small number of input frames.
sent3: Therefore, for decoding y t , it is not required to look at each input frames.
sent4: The Attention model aligns the input frames with y t by assigning match scores to each pair of input frame and y t .
sent5: The match scores convey how much a particular input frame is relevant to y t and accordingly, the decoder decides the degree of focus on each input frame for predicting y t .
sent6: Depending on how the alignments between output and input frames are designed, different types of attention mechanism are presented in the literature.
sent7: A list of existing attention models along with short descriptions is provided in Table  I.
sent8: The detailed explanation of different attention models is discussed throughout the paper.
sent9: In this survey, we have considered the models which are built within RNN or Transformer architecture.
sent10: Table II provides the list of literature which we have reviewed in the later sections of this paper.","1. How does an encoder-decoder model process input and make predictions in ASR systems? sent1
    1.1. Is the information required to predict a particular frame confined within a small number of input frames? sent2
    1.2. Is it necessary to look at each input frame for decoding a particular frame? sent3
2. How does the Attention model function in aligning input frames with output frames? sent4
    2.1. What do the match scores in the Attention model convey? sent5
3. How are different types of attention mechanisms presented in the literature? sent6
4. Where can one find a list of existing attention models with descriptions? sent7
5. Where is the detailed explanation of different attention models discussed? sent8
6. What architectures are considered in the survey for attention models? sent9
7. Where can one find the list of literature reviewed in the survey? sent10"
256900869,A Survey on Event-based News Narrative Extraction,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,Connect the Dots Approaches. Linear Representations Shahaf and Guestrin,7,"Expanding upon the Connect the Dots method, Zhu and Oates [135] propose an algorithm to extract story chains from newswire articles that connect two user-defined endpoints based on the following characteristics: relevance (the articles on the chain should be relevant to the endpoints), coherence (the transition between events should be smooth), low Furthermore, the model adds a named entity bias that assigns a higher weight to named entities compared to other terms. This is modeled through a co-occurrence frequency matrix for entity pairs, which is then used to compute a relevance score for each document in the data set based on the named entities. In turn, these elements are used to modify the cluster and document weights in the correlation graph.

Camacho Barranco et al. [18] propose a storyline extraction algorithm that takes a set of user-defined articles as a seed and generates a timeline of articles based on a series of evaluation metrics. First, the authors propose a temporal criterion to filter candidate documents based on a range between the latest publication date of the seed articles and a maximum threshold away from the earliest publication date of the seed articles (i.e., in the interval [ min − ℎ ℎ , max ]).

Next, there is a topical criterion that measures how much a candidate article can deviate from the seed articles based on KLD and LDA topics. Having defined their basic framework, the authors then formalize an optimization problem to extract the storylines by selecting article connections based on different criteria: incoherence, similarity, overlap, and uniformity. Incoherence is based on the average pairwise Soergel distance between documents-measured using TF-IDF information for the entities of the document-with a temporal factor to penalize temporally distant articles.

Similarity is used as a penalty factor to enforce diversity in non-adjacent articles of the storyline, implemented as a negative exponential factor based on the Soergel distance. Both of these metrics are weighted by a relevance factor of the documents and are smoothed using modified Gaussian distributions to measure event overlap. Next, an overall overlap factor for the storyline is computed, assigning a penalty based on the difference between publication dates and a user-defined threshold. The overlap factor ensures that the breakpoints occur at sufficiently distinct dates. The uniformity penalty seeks to avoid the case where the optimal solution selects purely irrelevant events as optimal by penalizing uniform weights. The objective function to minimize consists of the sum of the product between incoherence and similarity, multiplied by the overlap and uniformity penalties.

Graph-based Representations Metro Maps [98,99] are an extension of the Connect the Dots approach that represents more than a single storyline using a directed acyclic graph of events. In particular, the metro maps method is a structured summarization approach that captures the evolution of multiple stories and their interactions. The stories are represented using a metro map metaphor, where each metro line represents a story and stations represent key events.

Metro lines intersect in specific stations, representing how storylines connect with each other. This representation is extracted by solving an optimization problem. In particular, the goal is to maximize connectivity, subject to coverage and coherence constraints. Coverage is computed based on how well specific terms or keywords are represented in the selected events and is defined using a submodular function that encourages diversity (e.g., if a term is already covered, adding a document that covers it provides little extra coverage). These keywords depend on the specific corpus or domain of application. Coherence is defined following Shahaf et al.'s previous work [96,97]. Finally, connectivity is defined as the number of stories that intersect which is used to ensure that the final metro map is connected. The optimization problem is solved in phases. First, a series of coherent candidate metro lines are selected based on a divide-and-conquer approach, which constructs long lines from shorter ones and encodes them in a graph. Then, the method extracts a set of coherent lines that maximize coverage using an approximation algorithm based on the submodularity of the coverage function (otherwise finding these lines is an NP-hard problem). Finally, connectivity is increased using a local search approach that substitutes lines without sacrificing coverage.

Similar to the metro maps metaphor, the Narrative Maps model [51] provides a framework to extract and represent narratives based on a route map metaphor. The narrative and its stories are shown as a series of routes through landmarks, which represent the events. In computational terms, the narrative is modeled through a directed acyclic graph of events. The events are represented through neural embeddings of article headlines. The graph is extracted by solving an optimization problem defined following a linear programming formulation similar to the Connect the Dots approach. The optimization problem is based on maximizing coherence subject to coverage constraints. Coherence measures how much sense it makes to connect two events together and is defined as the geometric mean of the content similarity of events-using cosine or angular similarity-and their topical similarity-based on JS similarity of their topic distributions based on clustering. Coverage is measured by the average percentage of topical clusters covered by the selected events based on their topic distributions. Once the optimal map has been found, the main storyline is extracted by normalizing the coherence values of the edges into probabilities and finding the maximum likelihood path.

Then, a set of representative landmarks (i.e., important events) of each story by finding the maximum antichain, which corresponds to the point of the maximum width of the graph.","sent1: Expanding upon the Connect the Dots method, Zhu and Oates [135] propose an algorithm to extract story chains from newswire articles that connect two user-defined endpoints based on the following characteristics: relevance (the articles on the chain should be relevant to the endpoints), coherence (the transition between events should be smooth), low Furthermore, the model adds a named entity bias that assigns a higher weight to named entities compared to other terms.
sent2: This is modeled through a co-occurrence frequency matrix for entity pairs, which is then used to compute a relevance score for each document in the data set based on the named entities.
sent3: In turn, these elements are used to modify the cluster and document weights in the correlation graph.
sent4: Camacho Barranco et al. [18] propose a storyline extraction algorithm that takes a set of user-defined articles as a seed and generates a timeline of articles based on a series of evaluation metrics.
sent5: First, the authors propose a temporal criterion to filter candidate documents based on a range between the latest publication date of the seed articles and a maximum threshold away from the earliest publication date of the seed articles (i.e., in the interval [ min − ℎ ℎ , max ]).
sent6: Next, there is a topical criterion that measures how much a candidate article can deviate from the seed articles based on KLD and LDA topics.
sent7: Having defined their basic framework, the authors then formalize an optimization problem to extract the storylines by selecting article connections based on different criteria: incoherence, similarity, overlap, and uniformity.
sent8: Incoherence is based on the average pairwise Soergel distance between documents-measured using TF-IDF information for the entities of the document-with a temporal factor to penalize temporally distant articles.
sent9: Similarity is used as a penalty factor to enforce diversity in non-adjacent articles of the storyline, implemented as a negative exponential factor based on the Soergel distance.
sent10: Both of these metrics are weighted by a relevance factor of the documents and are smoothed using modified Gaussian distributions to measure event overlap.
sent11: Next, an overall overlap factor for the storyline is computed, assigning a penalty based on the difference between publication dates and a user-defined threshold.
sent12: The overlap factor ensures that the breakpoints occur at sufficiently distinct dates.
sent13: The uniformity penalty seeks to avoid the case where the optimal solution selects purely irrelevant events as optimal by penalizing uniform weights.
sent14: The objective function to minimize consists of the sum of the product between incoherence and similarity, multiplied by the overlap and uniformity penalties.
sent15: Graph-based Representations Metro Maps [98,99] are an extension of the Connect the Dots approach that represents more than a single storyline using a directed acyclic graph of events.
sent16: In particular, the metro maps method is a structured summarization approach that captures the evolution of multiple stories and their interactions.
sent17: The stories are represented using a metro map metaphor, where each metro line represents a story and stations represent key events.
sent18: Metro lines intersect in specific stations, representing how storylines connect with each other.
sent19: This representation is extracted by solving an optimization problem.
sent20: In particular, the goal is to maximize connectivity, subject to coverage and coherence constraints.
sent21: Coverage is computed based on how well specific terms or keywords are represented in the selected events and is defined using a submodular function that encourages diversity (e.g., if a term is already covered, adding a document that covers it provides little extra coverage).
sent22: These keywords depend on the specific corpus or domain of application.
sent23: Coherence is defined following Shahaf et al.'s previous work [96,97].
sent24: Finally, connectivity is defined as the number of stories that intersect which is used to ensure that the final metro map is connected.
sent25: The optimization problem is solved in phases.
sent26: First, a series of coherent candidate metro lines are selected based on a divide-and-conquer approach, which constructs long lines from shorter ones and encodes them in a graph.
sent27: Then, the method extracts a set of coherent lines that maximize coverage using an approximation algorithm based on the submodularity of the coverage function (otherwise finding these lines is an NP-hard problem).
sent28: Finally, connectivity is increased using a local search approach that substitutes lines without sacrificing coverage.
sent29: Similar to the metro maps metaphor, the Narrative Maps model [51] provides a framework to extract and represent narratives based on a route map metaphor.
sent30: The narrative and its stories are shown as a series of routes through landmarks, which represent the events.
sent31: In computational terms, the narrative is modeled through a directed acyclic graph of events.
sent32: The events are represented through neural embeddings of article headlines.
sent33: The graph is extracted by solving an optimization problem defined following a linear programming formulation similar to the Connect the Dots approach.
sent34: The optimization problem is based on maximizing coherence subject to coverage constraints.
sent35: Coherence measures how much sense it makes to connect two events together and is defined as the geometric mean of the content similarity of events-using cosine or angular similarity-and their topical similarity-based on JS similarity of their topic distributions based on clustering.
sent36: Coverage is measured by the average percentage of topical clusters covered by the selected events based on their topic distributions.
sent37: Once the optimal map has been found, the main storyline is extracted by normalizing the coherence values of the edges into probabilities and finding the maximum likelihood path.
sent38: Then, a set of representative landmarks (i.e., important events) of each story by finding the maximum antichain, which corresponds to the point of the maximum width of the graph.","1. What is the Connect the Dots method proposed by Zhu and Oates? sent1
    1.1. How is relevance modeled in the Connect the Dots method? sent2
    1.2. How are cluster and document weights modified in the correlation graph? sent3
2. What is the storyline extraction algorithm proposed by Camacho Barranco et al.? sent4
    2.1. What is the temporal criterion used in the storyline extraction algorithm? sent5
    2.2. What is the topical criterion used in the storyline extraction algorithm? sent6
    2.3. How is the optimization problem for extracting storylines formalized? sent7
        2.3.1. What is incoherence based on in the optimization problem? sent8
        2.3.2. How is similarity used in the optimization problem? sent9
        2.3.3. How are incoherence and similarity metrics weighted? sent10
        2.3.4. How is the overall overlap factor computed? sent11, sent12
        2.3.5. What is the purpose of the uniformity penalty? sent13
        2.3.6. What does the objective function to minimize consist of? sent14
3. What are Metro Maps in the context of graph-based representations? sent15
    3.1. How does the metro maps method capture the evolution of multiple stories? sent16
    3.2. How are stories represented in the metro maps metaphor? sent17, sent18
    3.3. How is the optimization problem for metro maps defined? sent19, sent20
        3.3.1. How is coverage computed in the metro maps method? sent21, sent22
        3.3.2. How is coherence defined in the metro maps method? sent23
        3.3.3. How is connectivity defined in the metro maps method? sent24
    3.4. How is the optimization problem solved in phases? sent25
        3.4.1. How are coherent candidate metro lines selected? sent26
        3.4.2. How is coverage maximized in the metro maps method? sent27
        3.4.3. How is connectivity increased in the metro maps method? sent28
4. What is the Narrative Maps model? sent29
    4.1. How are narratives and stories represented in the Narrative Maps model? sent30
    4.2. How is the narrative modeled computationally in the Narrative Maps model? sent31
    4.3. How are events represented in the Narrative Maps model? sent32
    4.4. How is the optimization problem for the Narrative Maps model defined? sent33, sent34
        4.4.1. How is coherence measured in the Narrative Maps model? sent35
        4.4.2. How is coverage measured in the Narrative Maps model? sent36
    4.5. How is the main storyline extracted in the Narrative Maps model? sent37
    4.6. How are representative landmarks of each story found in the Narrative Maps model? sent38"
212633493,Deep Neural Networks for Automatic Speech Processing: A Survey from Large Corpora to Limited Data,https://www.semanticscholar.org/paper/7a307b379eb714df1e38fe9a80b7ed94c0fbd64e,5) Graph neural network,6,"The use of Graph Neural Network (GNN) is used by V. Garcia and J. Bruna introduce the use of Graph Neural Network (GNN) in their few-shot framework [33]. This framework is designed to be used with multiple episodes they called tasks. In this framework, one model is used over a complete graph G. G = (V, E) where every node corresponds to an example. GNN for few-shot learning consists in applying Graph Convolutions Layers over the graph G.

Initial vertices construction to guess the ground truth of a queryx i from the query set Q:

, h(y 1 )), . . . , (Enc(x s ), h(y s )), (Enc(x 1 ), u), . . . , (Enc(x r ), u)

where Enc is an embedding extraction function (a Neural Network or any classic feature extraction technique), h the one-hot encoding function and u = K −1 1 K an uniform distribution for examples with unknown labels (the unsupervised ones fromx and/or from the query set Q).

From now the vertices at each layer l (with 0 being the initial vertices) will be denoted:

where n = s + r + 1 and V (l) ∈ R n * d l .

Every layers in GNN are computed as follows:

with A (l) being the adjacency operators constructed from V (l) and Gc being the graph convolution.

a) The adjacency operators construction: The adjacency operator us a set:

withÃ (l) being the adjacency matrix of V (l) .

For every (i, j) ∈ E (recall we have complete graphs), we compute the values of the adjacency matrix such as:

where:

with f being a multi-layer perceptron with its parameter denoted θ f .Ã (l) is then normalized using the softmax function over each line. b) Graph convolution: The graph convolution requires the construction of the adjacency operators set and is computed as follows:

with B being an adjacency operator from A, θ (k) B,l ∈ R d l−1 ,d l learnable parameters and ρ being a point wise linearity (usually leaky ReLU).

c) Training the model: The output of the resulting GNN model is a mapping of the vertices to a K-simplex that give the probability ofx i being in class k. V. Garcia and J. Bruna used the cross-entropy to learn the model other all examples in the query set Q [33]. Hence, the GNN few-shot framework consists in learning θ f and θ 1,l . . . θ card(A),l parameters over all episodes. d) Few-shot GNN on audio: This framework was used by [34] on 5-way audio classification problems. The 5 ways episodes are randomly selected from the initial dataset: Au-dioSet [35] for creating the 5-ways training episodes and [36] data to create the 5-ways test episodes.

S. Zhang et al. compare the use of per class attention (or intra-class) and global attention which gave the best results [34]. They applied it for each layer. Their experiments were done for 1-shot, 5-shots and 10-shots with the respective accuracy of 69.4%±0.66, 78.3%±0.46 and 83.6%±0.98. Such results really motivate us in the path of few-shot learning for speech signals. Nevertheless, this framework does not allow the use of many classes and shots per episode which increase the number of nodes and thus the computations in forward time. Hence, it is not suited for large vocabulary problems.","sent1: The use of Graph Neural Network (GNN) is used by V. Garcia and J. Bruna introduce the use of Graph Neural Network (GNN) in their few-shot framework [33].
sent2: This framework is designed to be used with multiple episodes they called tasks.
sent3: In this framework, one model is used over a complete graph G. G = (V, E) where every node corresponds to an example.
sent4: GNN for few-shot learning consists in applying Graph Convolutions Layers over the graph
sent5: G.Initial vertices construction to guess the ground truth of a queryx i from the query set Q:, h(y 1 )), . . . , (Enc(x s ), h(y s )), (Enc(x 1 ), u), . . . , (Enc(x r ), u)where Enc is an embedding extraction function (a Neural Network or any classic feature extraction technique), h the one-hot encoding function and u = K −1 1 K an uniform distribution for examples with unknown labels (the unsupervised ones fromx and/or from the query set Q).
sent6: From now the vertices at each layer l (with 0 being the initial vertices) will be denoted:where n = s + r + 1 and V (l) ∈ R n * d l .
sent7: Every layers in GNN are computed as follows:with A (l) being the adjacency operators constructed from V (l) and Gc being the graph convolution.
sent8: a) The adjacency operators construction: The adjacency operator us a set:withÃ (l) being the adjacency matrix of V (l) .
sent9: For every (i, j) ∈ E (recall we have complete graphs), we compute the values of the adjacency matrix such as:where:with f being a multi-layer perceptron with its parameter denoted θ f .Ã (l) is then normalized using the softmax function over each line.
sent10: b) Graph convolution: The graph convolution requires the construction of the adjacency operators set and is computed as follows:with B being an adjacency operator from A, θ (k) B,l ∈ R d l−1 ,d l learnable parameters and ρ being a point wise linearity (usually leaky ReLU).c)
sent11: Training the model: The output of the resulting GNN model is a mapping of the vertices to a K-simplex that give the probability ofx i being in class k. V. Garcia and J. Bruna used the cross-entropy to learn the model other all examples in the query set Q [33].
sent12: Hence, the GNN few-shot framework consists in learning θ f and θ
sent13: 1,l . . . θ card(A),l parameters over all episodes.
sent14: d) Few-shot GNN on audio: This framework was used by [34] on 5-way audio classification problems.
sent15: The 5 ways episodes are randomly selected from the initial dataset: Au-dioSet [35] for creating the 5-ways training episodes and [36] data to create the 5-ways test episodes.
sent16: S. Zhang et al. compare the use of per class attention (or intra-class) and global attention which gave the best results [34].
sent17: They applied it for each layer.
sent18: Their experiments were done for 1-shot, 5-shots and 10-shots with the respective accuracy of 69.4%±0.66, 78.3%±0.46 and 83.6%±0.98.
sent19: Such results really motivate us in the path of few-shot learning for speech signals.
sent20: Nevertheless, this framework does not allow the use of many classes and shots per episode which increase the number of nodes and thus the computations in forward time.
sent21: Hence, it is not suited for large vocabulary problems.","1. Who introduced the use of Graph Neural Network (GNN) in their few-shot framework? sent1
2. What is the design of the few-shot framework introduced by V. Garcia and J. Bruna? sent2
3. How is the model used in the few-shot framework over a complete graph? sent3
4. What does GNN for few-shot learning consist of? sent4
5. How are initial vertices constructed in the GNN framework? sent5
6. How are vertices at each layer denoted in the GNN framework? sent6
7. How are layers in GNN computed? sent7
    7.1. How is the adjacency operator constructed? sent8
    7.2. How are the values of the adjacency matrix computed? sent9
    7.3. What is required for graph convolution in GNN? sent10
8. How is the GNN model trained? sent11
9. What does the GNN few-shot framework consist of in terms of learning parameters? sent12, sent13
10. How was the few-shot GNN framework applied to audio classification? sent14
    10.1. How were the 5-way episodes selected for training and testing? sent15
    10.2. What comparison did S. Zhang et al. make in their experiments? sent16
    10.3. What were the results of their experiments for different shots? sent17, sent18
    10.4. What motivates the use of few-shot learning for speech signals? sent19
11. What are the limitations of the few-shot GNN framework? sent20
    11.1. Why is the framework not suited for large vocabulary problems? sent21"
248530069,Deep Personality Trait Recognition: A Survey,https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc,Dynamic Video Sequences,6,"Dynamic video sequences consist of a series of video image frames, thereby providing temporal information and scene dynamics. This brings about certain useful and complementary cues for personality trait analysis .

In , the authors investigated the connection between facial expressions and personality of vloggers in conversation videos (vlogs) from a subset of existing YouTube vlog data set (Biel and Gatica-Perez, 2010). They employed a computer expression recognition toolbox to identify the categories of facial expressions of vloggers. They finally adopted a SVM classifier to predict personality traits in conjunction with facial activity statistics on the basis of frame-by-frame estimation. The results indicate that extraversion has the highest utilization of activity cues. This is consistent with previous findings (Biel et al., 2011;). Aran and Gatica-Perez (2013) adopted the social media contents from conversational videos for analyzing the specific trait of extraversion. To address this issue, they integrated the ridge regression with a SVM classifier on the basis of statistical information derived from the weighted motion energy images. In (Teijeiro-Mosquera et al., 2014), the relations between facial expressions and personality impressions were investigated as an extended version of the used method . To characterize face statistics, they derived four sets of behavioral cues, such as statistic-based cues, Threshold (THR) cues, Hidden Markov Models (HMM) cues, and Winner Takes All (WTA) cues. Their research indicates that when multiple facial expression clues are significantly correlated with a certain number of the Big-Five traits, they could only obviously predict the particular trait of extraversion.

In consideration of the tremendous progress in the areas of deep learning, CNNs and LSTMs are widely for personality trait analysis from dynamic video sequences. Gürpınar et al. (2016) fine-tuned a pretrained VGG-19 network to extract deep facial and scene feature representations, as shown in Figure 3. Then, they were merged and fed into a kernel extreme learning machine (ELM) regressor for first impression estimation. Ventura et al. (2017) adopted an extension of Descriptor Aggregation Networks (DAN) to investigate why CNN models performed well in automatically predicting first impressions. They used class activation maps (CAM) for visualization and provided a possible interpretation on understanding why CNN models succeeded in learning discriminative facial features related to personality traits of users. Figure 4 shows the used CAM to interpret the CNN models in learning facial features. Experimental results indicate that: (1) face presents most of discriminative information for the inference of personality traits, (2) the internal representations of CNNs primarily focus on crucial facial regions including eyes, nose, and mouth, (3) some action units (AUs) provide a partial impact on the inference of facial traits. Beyan et al. (2019) aimed to perceive personality traits by means of using deep visual activity (VA)-based features derived only from key-dynamic images in videos. In order to determine key-dynamic images in videos, they employed three key steps: construction of multiple dynamic images, long-term VA learning with CNN + LSTM, and spatiotemporal saliency detection.","sent1: Dynamic video sequences consist of a series of video image frames, thereby providing temporal information and scene dynamics.
sent2: This brings about certain useful and complementary cues for personality trait analysis .
sent3: In , the authors investigated the connection between facial expressions and personality of vloggers in conversation videos (vlogs) from a subset of existing YouTube vlog data set (Biel and Gatica-Perez, 2010).
sent4: They employed a computer expression recognition toolbox to identify the categories of facial expressions of vloggers.
sent5: They finally adopted a SVM classifier to predict personality traits in conjunction with facial activity statistics on the basis of frame-by-frame estimation.
sent6: The results indicate that extraversion has the highest utilization of activity cues.
sent7: This is consistent with previous findings (Biel et al., 2011;).
sent8: Aran and Gatica-Perez (2013) adopted the social media contents from conversational videos for analyzing the specific trait of extraversion.
sent9: To address this issue, they integrated the ridge regression with a SVM classifier on the basis of statistical information derived from the weighted motion energy images.
sent10: In (Teijeiro-Mosquera et al., 2014), the relations between facial expressions and personality impressions were investigated as an extended version of the used method .
sent11: To characterize face statistics, they derived four sets of behavioral cues, such as statistic-based cues, Threshold (THR) cues, Hidden Markov Models (HMM) cues, and Winner Takes All (WTA) cues.
sent12: Their research indicates that when multiple facial expression clues are significantly correlated with a certain number of the Big-Five traits, they could only obviously predict the particular trait of extraversion.
sent13: In consideration of the tremendous progress in the areas of deep learning, CNNs and LSTMs are widely for personality trait analysis from dynamic video sequences.
sent14: Gürpınar et al. (2016) fine-tuned a pretrained VGG-19 network to extract deep facial and scene feature representations, as shown in Figure 3.
sent15: Then, they were merged and fed into a kernel extreme learning machine (ELM) regressor for first impression estimation.
sent16: Ventura et al. (2017) adopted an extension of Descriptor Aggregation Networks (DAN) to investigate why CNN models performed well in automatically predicting first impressions.
sent17: They used class activation maps (CAM) for visualization and provided a possible interpretation on understanding why CNN models succeeded in learning discriminative facial features related to personality traits of users.
sent18: Figure 4 shows the used CAM to interpret the CNN models in learning facial features.
sent19: Experimental results indicate that: (1) face presents most of discriminative information for the inference of personality traits, (2) the internal representations of CNNs primarily focus on crucial facial regions including eyes, nose, and mouth, (3) some action units (AUs) provide a partial impact on the inference of facial traits.
sent20: Beyan et al. (2019) aimed to perceive personality traits by means of using deep visual activity (VA)-based features derived only from key-dynamic images in videos.
sent21: In order to determine key-dynamic images in videos, they employed three key steps: construction of multiple dynamic images, long-term VA learning with CNN + LSTM, and spatiotemporal saliency detection.","1. What are dynamic video sequences and what information do they provide? sent1
    1.1. What useful cues do dynamic video sequences bring for personality trait analysis? sent2
2. How was the connection between facial expressions and personality traits investigated in vlogs? sent3
    2.1. What tool was used to identify facial expressions in vloggers? sent4
    2.2. How were personality traits predicted in conjunction with facial activity statistics? sent5
    2.3. Which personality trait showed the highest utilization of activity cues? sent6
        2.3.1. How does this finding relate to previous research? sent7
3. How did Aran and Gatica-Perez analyze the trait of extraversion using social media content? sent8
    3.1. What methods were integrated to address the analysis of extraversion? sent9
4. What was investigated in the study by Teijeiro-Mosquera et al. (2014)? sent10
    4.1. What sets of behavioral cues were derived to characterize face statistics? sent11
    4.2. What did their research indicate about predicting personality traits? sent12
5. How are CNNs and LSTMs used in personality trait analysis from dynamic video sequences? sent13
6. How did Gürpınar et al. (2016) utilize a pretrained VGG-19 network for personality trait analysis? sent14
    6.1. What was the next step after extracting deep facial and scene feature representations? sent15
7. What approach did Ventura et al. (2017) use to investigate CNN models' performance in predicting first impressions? sent16
    7.1. How did they use class activation maps (CAM) in their study? sent17
        7.1.1. What does Figure 4 show in relation to their study? sent18
    7.2. What were the experimental results regarding the inference of personality traits? sent19
8. How did Beyan et al. (2019) aim to perceive personality traits using deep visual activity-based features? sent20
    8.1. What key steps were employed to determine key-dynamic images in videos? sent21"
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,Large Language Models,4,"Large Language Models (LLM) refer to huge transformer architecture models (typically above ten billions of parameters) pre-trained with massive corpus (Zhao et al., 2023).With the increment of model size and training corpus, it starts to emerge some new abilities (Wei et al., 2022a).Recent years, LLM have achieved remarkable progress in many NLP fields (Brown et al., 2020;Chowdhery et al., 2022;Chung et al., 2022).","sent1: Large Language Models (LLM) refer to huge transformer architecture models (typically above ten billions of parameters) pre-trained with massive corpus (Zhao et al., 2023).With the increment of model size and training corpus, it starts to emerge some new abilities (Wei et al., 2022a).Recent years, LLM have achieved remarkable progress in many NLP fields (Brown et al., 2020;Chowdhery et al., 2022;Chung et al., 2022).","1. What are Large Language Models (LLM) and how are they characterized? sent1
    1.1. What new abilities emerge with the increase in model size and training corpus? sent1
    1.2. What progress have LLM achieved in recent years? sent1"
257913422,Self-Supervised Multimodal Learning: A Survey,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,Self-supervision in Multimodal Learning,13,"We first delineate the scope of SSML as considered in this survey as this term has been used inconsistently in the literature before. Self-supervision is more straightforward to define in a unimodal context by appealing to the label-free nature of different pretext tasks, e.g., as famously realized by instance discrimination [18] or masked prediction objectives [19]. In contrast, the situation in multimodal learning is more complex as the role of modalities and labels becomes blurred. For example, text is typically considered a label in supervised image captioning [20], but as an input modality in self-supervised multimodal vision and language representation learning [21]. In the multimodal context, the term self-supervision has been used to refer to at least four situations: (1) Labelfree learning from multimodal data that is automatically paired -such as movies with video and audio tracks [22], or images and depth data from RGBD cameras [23]. (2) Learning from multimodal data where one modality has been manually annotated, or two modalities have been manually paired, but this annotation has already been created for a different purpose, and can thus be considered free for the purpose of SSML pretraining. For example, matching image-caption pairs crawled from the web, as used by the seminal CLIP [21], is actually an example of supervised metric-learning [24], [25] where the pairing is the supervision. However, because both the modalities and the pairing are all freely available at scale, it is often described as self-supervised. Such uncurated incidentally created data is often lower-quality and noisier than purposecreated and curated datasets such as COCO [20] and Visual Genome [26]. (3) Learning from high-quality purpose annotated multimodal data (e.g., manually captioned images in the COCO [20]), but with self-supervised style objectives, e.g., Pixel-BERT [27]. (4) Finally, there are 'self-supervised' methods that use a mix of free and manually annotated multimodal data [28], [29].

For the purpose of this survey, we follow the spirit of self-supervision as aiming to scale up by breaking the manual annotation bottleneck. Thus we include methods that fall into the first two and fourth categories above in terms of being able to train on freely available data. We exclude methods only shown to work on manually curated datasets just because they apply typically ""self-supervised"" objectives (e.g., masked prediction) on curated datasets.","sent1: We first delineate the scope of SSML as considered in this survey as this term has been used inconsistently in the literature before.
sent2: Self-supervision is more straightforward to define in a unimodal context by appealing to the label-free nature of different pretext tasks, e.g., as famously realized by instance discrimination [18] or masked prediction objectives [19].
sent3: In contrast, the situation in multimodal learning is more complex as the role of modalities and labels becomes blurred.
sent4: For example, text is typically considered a label in supervised image captioning [20], but as an input modality in self-supervised multimodal vision and language representation learning [21].
sent5: In the multimodal context, the term self-supervision has been used to refer to at least four situations: (1) Labelfree learning from multimodal data that is automatically paired -such as movies with video and audio tracks [22], or images and depth data from RGBD cameras [23].
sent6: (2) Learning from multimodal data where one modality has been manually annotated, or two modalities have been manually paired, but this annotation has already been created for a different purpose, and can thus be considered free for the purpose of SSML pretraining.
sent7: For example, matching image-caption pairs crawled from the web, as used by the seminal CLIP [21], is actually an example of supervised metric-learning [24], [25] where the pairing is the supervision.
sent8: However, because both the modalities and the pairing are all freely available at scale, it is often described as self-supervised.
sent9: Such uncurated incidentally created data is often lower-quality and noisier than purposecreated and curated datasets such as COCO [20] and Visual Genome [26].
sent10: (3) Learning from high-quality purpose annotated multimodal data (e.g., manually captioned images in the COCO [20]), but with self-supervised style objectives, e.g., Pixel-BERT [27].
sent11: (4) Finally, there are 'self-supervised' methods that use a mix of free and manually annotated multimodal data [28], [29].
sent12: For the purpose of this survey, we follow the spirit of self-supervision as aiming to scale up by breaking the manual annotation bottleneck.
sent13: Thus we include methods that fall into the first two and fourth categories above in terms of being able to train on freely available data.
sent14: We exclude methods only shown to work on manually curated datasets just because they apply typically ""self-supervised"" objectives (e.g., masked prediction) on curated datasets.","1. Why is it important to delineate the scope of SSML in this survey? sent1
2. How is self-supervision defined in a unimodal context? sent2
3. Why is the situation more complex in multimodal learning compared to unimodal learning? sent3
4. How is text used differently in supervised image captioning versus self-supervised multimodal learning? sent4
5. What are the four situations in which the term self-supervision is used in the multimodal context? sent5, sent6, sent10, sent11
    5.1. What is an example of label-free learning from automatically paired multimodal data? sent5
    5.2. How is learning from manually annotated or paired multimodal data described in the context of SSML? sent6
        5.2.1. What is an example of supervised metric-learning that is often described as self-supervised? sent7, sent8
        5.2.2. How does the quality of uncurated incidentally created data compare to curated datasets? sent9
    5.3. What is an example of learning from high-quality purpose-annotated multimodal data with self-supervised style objectives? sent10
    5.4. What is the fourth situation involving 'self-supervised' methods? sent11
6. What is the spirit of self-supervision as considered in this survey? sent12
7. Which methods are included in this survey, and why? sent13
8. Which methods are excluded from this survey, and why? sent14"
10137425,Multimodal Machine Learning: A Survey and Taxonomy,https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,Implicit alignment,17,"In contrast to explicit alignment, implicit alignment is used as an intermediate (often latent) step for another task. This allows for better performance in a number of tasks including speech recognition, machine translation, media description, and visual question-answering. Such models do not explicitly align data and do not rely on supervised alignment examples, but learn how to latently align the data during model training. We identify two types of implicit alignment models: earlier work based on graphical models, and more modern neural network methods. Graphical models have seen some early work used to better align words between languages for machine translation [216] and alignment of speech phonemes with their transcriptions [186]. However, they require manual construction of a mapping between the modalities, for example a generative phone model that maps phonemes to acoustic features [186]. Constructing such models requires training data or human expertise to define them manually. Neural networks Translation (Section 4) is an example of a modeling task that can often be improved if alignment is performed as a latent intermediate step. As we mentioned before, neural networks are popular ways to address this translation problem, using either an encoder-decoder model or through cross-modal retrieval. When translation is performed without implicit alignment, it ends up putting a lot of weight on the encoder module to be able to properly summarize the whole image, sentence or a video with a single vectorial representation.

A very popular way to address this is through attention [12], which allows the decoder to focus on sub-components of the source instance. This is in contrast with encoding all source sub-components together, as is performed in a conventional encoder-decoder model. An attention module will tell the decoder to look more at targeted sub-components of the source to be translated -areas of an image [230], words of a sentence [12], segments of an audio sequence [35], [39], frames and regions in a video [236], [241], and even parts of an instruction [140]. For example, in image captioning instead of encoding an entire image using a CNN, an attention mechanism will allow the decoder (typically an RNN) to focus on particular parts of the image when generating each successive word [230]. The attention module which learns what part of the image to focus on is typically a shallow neural network and is trained end-to-end together with a target task (e.g., translation).

Attention models have also been successfully applied to question answering tasks, as they allow for aligning the words in a question with sub-components of an information source such as a piece of text [228], an image [62], or a video sequence [246]. This both allows for better performance in question answering and leads to better model interpretability [4]. In particular, different types of attention models have been proposed to address this problem, including hierarchical [128], stacked [234], and episodic memory attention [228].

Another neural alternative for aligning images with captions for cross-modal retrieval was proposed by Karpathy et al. [98], [99]. Their proposed model aligns sentence fragments to image regions by using a dot product similarity measure between image region and word representations. While it does not use attention, it extracts a latent alignment between modalities through a similarity measure that is learned indirectly by training a retrieval model.","sent1: In contrast to explicit alignment, implicit alignment is used as an intermediate (often latent) step for another task.
sent2: This allows for better performance in a number of tasks including speech recognition, machine translation, media description, and visual question-answering.
sent3: Such models do not explicitly align data and do not rely on supervised alignment examples, but learn how to latently align the data during model training.
sent4: We identify two types of implicit alignment models: earlier work based on graphical models, and more modern neural network methods.
sent5: Graphical models have seen some early work used to better align words between languages for machine translation [216] and alignment of speech phonemes with their transcriptions [186].
sent6: However, they require manual construction of a mapping between the modalities, for example a generative phone model that maps phonemes to acoustic features [186].
sent7: Constructing such models requires training data or human expertise to define them manually.
sent8: Neural networks Translation (Section 4) is an example of a modeling task that can often be improved if alignment is performed as a latent intermediate step.
sent9: As we mentioned before, neural networks are popular ways to address this translation problem, using either an encoder-decoder model or through cross-modal retrieval.
sent10: When translation is performed without implicit alignment, it ends up putting a lot of weight on the encoder module to be able to properly summarize the whole image, sentence or a video with a single vectorial representation.
sent11: A very popular way to address this is through attention [12], which allows the decoder to focus on sub-components of the source instance.
sent12: This is in contrast with encoding all source sub-components together, as is performed in a conventional encoder-decoder model.
sent13: An attention module will tell the decoder to look more at targeted sub-components of the source to be translated -areas of an image [230], words of a sentence [12], segments of an audio sequence [35], [39], frames and regions in a video [236], [241], and even parts of an instruction [140].
sent14: For example, in image captioning instead of encoding an entire image using a CNN, an attention mechanism will allow the decoder (typically an RNN) to focus on particular parts of the image when generating each successive word [230].
sent15: The attention module which learns what part of the image to focus on is typically a shallow neural network and is trained end-to-end together with a target task (e.g., translation).
sent16: Attention models have also been successfully applied to question answering tasks, as they allow for aligning the words in a question with sub-components of an information source such as a piece of text [228], an image [62], or a video sequence [246].
sent17: This both allows for better performance in question answering and leads to better model interpretability [4].
sent18: In particular, different types of attention models have been proposed to address this problem, including hierarchical [128], stacked [234], and episodic memory attention [228].
sent19: Another neural alternative for aligning images with captions for cross-modal retrieval was proposed by Karpathy et al. [98], [99].
sent20: Their proposed model aligns sentence fragments to image regions by using a dot product similarity measure between image region and word representations.
sent21: While it does not use attention, it extracts a latent alignment between modalities through a similarity measure that is learned indirectly by training a retrieval model.","1. What is implicit alignment and how does it differ from explicit alignment? sent1
    1.1. What are the benefits of using implicit alignment in various tasks? sent2
    1.2. How do implicit alignment models function without explicit data alignment? sent3
    1.3. What are the two types of implicit alignment models identified? sent4
        1.3.1. How have graphical models been used in implicit alignment? sent5
            1.3.1.1. What is a requirement for constructing graphical models? sent6
            1.3.1.2. What is needed to manually define graphical models? sent7
        1.3.2. How do neural networks improve translation tasks with implicit alignment? sent8
            1.3.2.1. What are the popular methods for addressing translation problems using neural networks? sent9
            1.3.2.2. What happens when translation is performed without implicit alignment? sent10
            1.3.2.3. How does attention address the challenges of translation without implicit alignment? sent11
                1.3.2.3.1. How does attention differ from conventional encoder-decoder models? sent12
                1.3.2.3.2. What does an attention module do in the context of translation? sent13
                    1.3.2.3.2.1. Can you provide an example of how attention is used in image captioning? sent14
                    1.3.2.3.2.2. How is the attention module trained in image captioning tasks? sent15
                1.3.2.3.3. How has attention been applied to question answering tasks? sent16
                    1.3.2.3.3.1. What are the benefits of using attention in question answering? sent17
                    1.3.2.3.3.2. What types of attention models have been proposed for question answering? sent18
    1.4. What is another neural alternative for aligning images with captions? sent19
        1.4.1. How does the model proposed by Karpathy et al. align sentence fragments to image regions? sent20
        1.4.2. How does this model achieve latent alignment without using attention? sent21"
21693765,Graph-based Ontology Summarization: A Survey,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,Application-specific Measures,10,"The following two methods are not graph-based but are designed for specific applications.

Query Relevance (QR) A special kind of ontology summary is a snippet presented in search results pages of an ontology search engine. In this application, terms [6,2] or RDF sentences [4,11] that are relevant to a user query (e.g., containing query keywords) are prioritized for being presented in a snippet, to show the relevance of an ontology to the user's information needs.

Frequency of Correspondences (FC) Pires et al. [13] consider applications where an ontology to be summarized can be an integrated ontology obtained by merging several local ontologies. In that case, an important term in the integrated ontology is one that has a high frequency of correspondences, namely it finds correspondences to many classes in local ontologies.

The following two methods are not graph-based but are designed for specific applications.

Query Relevance (QR) A special kind of ontology summary is a snippet presented in search results pages of an ontology search engine. In this application, terms [6,2] or RDF sentences [4,11] that are relevant to a user query (e.g., containing query keywords) are prioritized for being presented in a snippet, to show the relevance of an ontology to the user's information needs.

Frequency of Correspondences (FC) Pires et al. [13] consider applications where an ontology to be summarized can be an integrated ontology obtained by merging several local ontologies. In that case, an important term in the integrated ontology is one that has a high frequency of correspondences, namely it finds correspondences to many classes in local ontologies.","sent1: The following two methods are not graph-based but are designed for specific applications.
sent2: Query Relevance (QR) A special kind of ontology summary is a snippet presented in search results pages of an ontology search engine.
sent3: In this application, terms [6,2] or RDF sentences [4,11] that are relevant to a user query (e.g., containing query keywords) are prioritized for being presented in a snippet, to show the relevance of an ontology to the user's information needs.
sent4: Frequency of Correspondences (FC) Pires et al. [13] consider applications where an ontology to be summarized can be an integrated ontology obtained by merging several local ontologies.
sent5: In that case, an important term in the integrated ontology is one that has a high frequency of correspondences, namely it finds correspondences to many classes in local ontologies.
sent6: The following two methods are not graph-based but are designed for specific applications.
sent7: Query Relevance (QR) A special kind of ontology summary is a snippet presented in search results pages of an ontology search engine.
sent8: In this application, terms [6,2] or RDF sentences [4,11] that are relevant to a user query (e.g., containing query keywords) are prioritized for being presented in a snippet, to show the relevance of an ontology to the user's information needs.
sent9: Frequency of Correspondences (FC) Pires et al. [13] consider applications where an ontology to be summarized can be an integrated ontology obtained by merging several local ontologies.
sent10: In that case, an important term in the integrated ontology is one that has a high frequency of correspondences, namely it finds correspondences to many classes in local ontologies.","1. What are the two methods mentioned that are not graph-based but designed for specific applications? sent1
    1.1. What is Query Relevance (QR) in the context of ontology summarization? sent2
        1.1.1. How does Query Relevance (QR) prioritize terms or RDF sentences in ontology search engine snippets? sent3
    1.2. What is Frequency of Correspondences (FC) in the context of ontology summarization? sent4
        1.2.1. How is an important term defined in the context of Frequency of Correspondences (FC)? sent5"
232895648,Generative Adversarial Networks: A Literature Review,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,Training Process,4,"Since it is difficult for the first generated sample to reach the level of real data, we need to continuously train and optimize the GAN. However, the training of GAN is different from the previous single neural network training, and we adopt separate alternating iterative training [39][40][41]. In GAN, we use fixed generator to optimize the discriminator, or fixed discriminator to optimize the generator [21]. The formula for the entire GAN is as follows.

E is the expected value of the distribution function,

is the distribution that generates the sample.","sent1: Since it is difficult for the first generated sample to reach the level of real data, we need to continuously train and optimize the GAN.
sent2: However, the training of GAN is different from the previous single neural network training, and we adopt separate alternating iterative training [39][40][41].
sent3: In GAN, we use fixed generator to optimize the discriminator, or fixed discriminator to optimize the generator [21].
sent4: The formula for the entire GAN is as follows.
sent5: E is the expected value of the distribution function,is the distribution that generates the sample.","1. Why is it necessary to continuously train and optimize GANs? sent1
2. How is the training of GANs different from single neural network training? sent2
3. What approach is used in GANs to optimize the generator and discriminator? sent3
4. What is provided in the section regarding the formula for GANs? sent4
5. What does ""E"" represent in the context of GANs? sent5"
265607982,Overview of PragTag-2023: Low-Resource Multi-Domain Pragmatic Tagging of Peer Reviews,https://www.semanticscholar.org/paper/a6d2fd7c874c0074135ffc09a81f7c409e5b1ae1,+ aux data 4,12,"Figure 1: PragTag-2023 Overview.Given a mixeddomain corpus of peer reviews annotated with pragmatic tags, the participants submit systems trained with varying amounts of training data (1-3) with optional use of unlabeled auxiliary data (4).The systems are evaluated in each of the five domains (Section 3.1), as well as on a previously unseen secret domain (5).

for peer review analysis aims to investigate those issues by analyzing argumentation in peer review reports (e.g.Kang et al., 2018;Cheng et al., 2020;Hua et al., 2019;Kuznetsov et al., 2022;Dycke et al., 2023).The resulting systems have numerous potential applications, incl.facilitating metascientific analysis of reviewing practices, helping authors and program chairs aggregate information from multiple reviews, and supporting junior reviewers in giving thorough, objective and helpful feedback.

Standards and practices of scholarly communication vary across research communities.Yet, to date, NLP for peer review has focused on data from machine learning conferences (Kang et al., 2018;Hua et al., 2019;Cheng et al., 2020;Kennard et al., 2022), and the applications outside of this domain remain under-investigated.This over-focus on one domain can be attributed to data scarcity -while some communities make their reviewing public, peer reviews are generally hard to obtain and legally clear for research use (Dycke et al., The authors address the issue of...This idea reminded me of the work by... Please compare your method to...The discussion is superficial.

The paper is original and sound.2022).In addition, due to the technical nature of peer review texts, they are expensive to annotate.Measuring the effects and mitigating the impact of domain shift and data scarcity are important and under-researched questions in NLP for peer reviews.

The introduction of open multi-domain corpora of peer reviews (Dycke et al., 2023) and domainneutral review analysis tasks (Kuznetsov et al., 2022) makes it possible to investigate these questions empirically.The PragTag-2023 Shared Task1 collaboratively explored multi-domain NLP for peer reviews under data scarcity.As an exemplary task we took pragmatic tagging -a sentence-level argumentation labeling task that classifies peer review statements by their communicative purpose (Section 2).PragTag-2023 has received five diverse submissions that provide new insights into multi-domain low-data pragmatic tagging, and propose a wide spectrum of methods to increase model robustness under four increasingly challenging conditions.This paper describes the shared task setup, summarizes the submissions, and aggregates the main insights from the competition.To support further investigation of multi-domain low-data NLP for peer review, we archive the code and data of the shared task and make them publicly available2 .","sent1: Figure 1: PragTag-2023 Overview.
sent2: Given a mixeddomain corpus of peer reviews annotated with pragmatic tags, the participants submit systems trained with varying amounts of training data (1-3) with optional use of unlabeled auxiliary data (4).The systems are evaluated in each of the five domains (Section 3.1), as well as on a previously unseen secret domain (5).
sent3: for peer review analysis aims to investigate those issues by analyzing argumentation in peer review reports (e.g.Kang et al., 2018;Cheng et al., 2020;Hua et al., 2019;Kuznetsov et al., 2022;Dycke et al., 2023).The resulting systems have numerous potential applications, incl.facilitating metascientific analysis of reviewing practices, helping authors and program chairs aggregate information from multiple reviews, and supporting junior reviewers in giving thorough, objective and helpful feedback.
sent4: Standards and practices of scholarly communication vary across research communities.
sent5: Yet, to date, NLP for peer review has focused on data from machine learning conferences (Kang et al., 2018;Hua et al., 2019;Cheng et al., 2020;Kennard et al., 2022), and the applications outside of this domain remain under-investigated.
sent6: This over-focus on one domain can be attributed to data scarcity -while some communities make their reviewing public, peer reviews are generally hard to obtain and legally clear for research use (Dycke et al., The authors address the issue of...
sent7: This idea reminded me of the work by...
sent8: Please compare your method to...
sent9: The discussion is superficial. The paper is original and sound.2022).In addition, due to the technical nature of peer review texts, they are expensive to annotate.
sent10: Measuring the effects and mitigating the impact of domain shift and data scarcity are important and under-researched questions in NLP for peer reviews.
sent11: The introduction of open multi-domain corpora of peer reviews (Dycke et al., 2023) and domainneutral review analysis tasks (Kuznetsov et al., 2022) makes it possible to investigate these questions empirically.
sent12: The PragTag-2023 Shared Task1 collaboratively explored multi-domain NLP for peer reviews under data scarcity.
sent13: As an exemplary task we took pragmatic tagging -a sentence-level argumentation labeling task that classifies peer review statements by their communicative purpose (Section 2).PragTag-2023 has received five diverse submissions that provide new insights into multi-domain low-data pragmatic tagging, and propose a wide spectrum of methods to increase model robustness under four increasingly challenging conditions.
sent14: This paper describes the shared task setup, summarizes the submissions, and aggregates the main insights from the competition.
sent15: To support further investigation of multi-domain low-data NLP for peer review, we archive the code and data of the shared task and make them publicly available2 .","1. What is the PragTag-2023 overview about? sent1
2. What is the setup for the PragTag-2023 task? sent2
3. What is the aim of peer review analysis in the context of PragTag-2023? sent3
4. How do standards and practices of scholarly communication vary? sent4
5. What has been the focus of NLP for peer review to date, and what remains under-investigated? sent5
6. Why is there an over-focus on one domain in NLP for peer review? sent6
7. What are some challenges associated with peer review texts? sent9
8. What are important and under-researched questions in NLP for peer reviews? sent10
9. How do open multi-domain corpora and domain-neutral review analysis tasks contribute to NLP for peer reviews? sent11
10. What did the PragTag-2023 Shared Task explore? sent12
11. What was the exemplary task in PragTag-2023, and what were the outcomes? sent13
12. What does the paper describe regarding the PragTag-2023 Shared Task? sent14
13. How does the paper support further investigation of multi-domain low-data NLP for peer review? sent15"
243886770,A Survey on Sentiment Analysis Approaches in e-Commerce,https://www.semanticscholar.org/paper/51cbfd00189b029802981df08872dded5a34c7c2,B. Theory of Sentiments Analysis in E-Commerce,5,"This section discussed previous work related sentiment analysis in customers"" reviews e-commerce. Amazon dataset on product""s reviews has been selected by few writers for sentiment analysis. Sentiment analysis of unstructured data in Amazon dataset helps to measure and evaluate information in sentiment in reviews using natural language processing techniques [4] [7] [34]. Sentiment analysis was implemented for analysis of e-commerce product reviews to categorize negative and positive comments and visualize it in charts [4]. The model is developed with unigram and bigram and evaluate with classifier such as linear support vector machine, Multinomial Naïve Bayes, Stochastic Gradient, Random Forest, Logistic regression and Decision tree by product category cellphone, musical and electronics [4] [27]. The result measure using accuracy, precision, recall and F-measure whereby linear support machine shows highest accuracy 93.57 better results compared to other papers. Text mining techniques Apriori and Term Frequency-Inverse Document Frequency (TF-IDF) were applied for identifying text features in proper way [23] [34]. Table I shows implementation of sentiment analysis on review by some researchers based on Amazon dataset. The table presents many research and different sentiment analysis approaches toward resolving problems in e-commerce customers"" reviews. Process of identifying sentiment from unstructured dataset provides different results as different methods are applied. The challenging part of sentiment analysis is to discover what customers like and dislike as written expression [15] [27]. The researchers used sentiment analysis method for identifying sentiment scores in online reviews and overall result as presented in Table I. Different researchers have conducted different approaches for feature extraction and sentiment classification, hence, some future improvement in method application is needed to attain greater accuracy.  Table I, most of the method in sentiment analysis implement feature extraction and sentiment classification in their process flow for get better accuracy results. Many researchers have implemented text mining method for extract most frequent information from dataset like TF-IDF and Apriori algorithm. Machine learning supervised method is used in most of the papers for classification of information. The experimental results from frequent pattern mining and supervised machine learning methods are able to provide more than 90% accuracy result. For future investigation, lexicon method and result analysis needed for compare accuracy result with machine learning method in sentiment analysis of customers"" reviews.","sent1: This section discussed previous work related sentiment analysis in customers"" reviews e-commerce.
sent2: Amazon dataset on product""s reviews has been selected by few writers for sentiment analysis.
sent3: Sentiment analysis of unstructured data in Amazon dataset helps to measure and evaluate information in sentiment in reviews using natural language processing techniques
sent4: [4] [7] [34]. Sentiment analysis was implemented for analysis of e-commerce product reviews to categorize negative and positive comments and visualize it in charts [4].
sent5: The model is developed with unigram and bigram and evaluate with classifier such as linear support vector machine, Multinomial Naïve Bayes, Stochastic Gradient, Random Forest, Logistic regression and Decision tree by product category cellphone, musical and electronics [4] [27].
sent6: The result measure using accuracy, precision, recall and F-measure whereby linear support machine shows highest accuracy 93.57 better results compared to other papers.
sent7: Text mining techniques Apriori and Term Frequency-Inverse Document Frequency (TF-IDF) were applied for identifying text features in proper way [23] [34].
sent8: Table I shows implementation of sentiment analysis on review by some researchers based on Amazon dataset.
sent9: The table presents many research and different sentiment analysis approaches toward resolving problems in e-commerce customers"" reviews.
sent10: Process of identifying sentiment from unstructured dataset provides different results as different methods are applied.
sent11: The challenging part of sentiment analysis is to discover what customers like and dislike as written expression [15] [27].
sent12: The researchers used sentiment analysis method for identifying sentiment scores in online reviews and overall result as presented in Table I.
sent13: Different researchers have conducted different approaches for feature extraction and sentiment classification, hence, some future improvement in method application is needed to attain greater accuracy.
sent14: Table I, most of the method in sentiment analysis implement feature extraction and sentiment classification in their process flow for get better accuracy results.
sent15: Many researchers have implemented text mining method for extract most frequent information from dataset like TF-IDF and Apriori algorithm.
sent16: Machine learning supervised method is used in most of the papers for classification of information.
sent17: The experimental results from frequent pattern mining and supervised machine learning methods are able to provide more than 90% accuracy result.
sent18: For future investigation, lexicon method and result analysis needed for compare accuracy result with machine learning method in sentiment analysis of customers"" reviews.","1. What does this section discuss regarding sentiment analysis in e-commerce? sent1
2. Which dataset has been selected by some writers for sentiment analysis in e-commerce? sent2
3. How does sentiment analysis of the Amazon dataset help in e-commerce? sent3
4. How was sentiment analysis implemented for e-commerce product reviews? sent4
5. What model and classifiers were used for sentiment analysis by product category? sent5
6. Which classifier showed the highest accuracy in sentiment analysis, and what was the accuracy percentage? sent6
7. What text mining techniques were applied for identifying text features? sent7
8. What does Table I show regarding sentiment analysis on reviews? sent8
9. What does the table present about research and sentiment analysis approaches? sent9
10. What is the process of identifying sentiment from unstructured datasets, and what does it provide? sent10
11. What is the challenging part of sentiment analysis in e-commerce? sent11
12. How did researchers use sentiment analysis methods in their studies? sent12
13. What is needed for future improvement in sentiment analysis methods? sent13
14. What do most methods in sentiment analysis implement for better accuracy results? sent14
15. What text mining methods have many researchers implemented for extracting information? sent15
16. What method is used in most papers for classification of information? sent16
17. What do the experimental results from frequent pattern mining and supervised machine learning methods show? sent17
18. What is suggested for future investigation in sentiment analysis of customer reviews? sent18"
65219850,A Survey of Text Mining in Social Media: Facebook and Twitter Perspectives Establish algorithm for to determine an authorized PW error View project Further Investigations on Developing an Arabic Sentiment Lexicon View project Said Salloum British University in Dubai 3 PUBLICATIONS 2 CITATIONS SEE PROFILE,https://www.semanticscholar.org/paper/94d255e0b2a729e918a320ab99e503cfc04e1fc8,Text mining efforts in resolving various NLP issues,4,"A study by [44] stated that text mining is responsible for structuring the irregular data patterns written in the human language. As most of the people interact with each other in the form of text so for those people who are not able to share structured form of data, text mining is the best technique to handle these situations. Among others, NLP is considered as the most amazing research field. The main goal of NLP is to seek information regarding how the computer systems are examining and getting information from the languages of human beings to create applications of high quality [17]. The art of sharing meaningful information with the help of uncommon and meaningless data is truly a good thing. Text mining technique as described by [45] examines the content for extracting the meaningful data which can be used for particular purposes. It looks like text mining that is going to include the overall NLP scheme [46] in its system in order to effectively examine the human language and to structure the unstructured data patterns accordingly. As the technology is advancing day by day, text mining system will get better and better and this is what all people are looking for.","sent1: A study by [44] stated that text mining is responsible for structuring the irregular data patterns written in the human language.
sent2: As most of the people interact with each other in the form of text so for those people who are not able to share structured form of data, text mining is the best technique to handle these situations.
sent3: Among others, NLP is considered as the most amazing research field.
sent4: The main goal of NLP is to seek information regarding how the computer systems are examining and getting information from the languages of human beings to create applications of high quality [17].
sent5: The art of sharing meaningful information with the help of uncommon and meaningless data is truly a good thing.
sent6: Text mining technique as described by [45] examines the content for extracting the meaningful data which can be used for particular purposes.
sent7: It looks like text mining that is going to include the overall NLP scheme [46] in its system in order to effectively examine the human language and to structure the unstructured data patterns accordingly.
sent8: As the technology is advancing day by day, text mining system will get better and better and this is what all people are looking for.","1. What is the responsibility of text mining according to a study by [44]? sent1
2. Why is text mining considered the best technique for handling unstructured data? sent2
3. How is NLP regarded among research fields? sent3
4. What is the main goal of NLP? sent4
5. What is described as a good thing in the context of sharing information? sent5
6. How does text mining examine content according to [45]? sent6
7. How is text mining expected to evolve according to [46]? sent7
8. What is the anticipated future of text mining systems as technology advances? sent8"
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,Deep feature-based methodologies,29,"Deep learning-based methodologies are becoming popular in the past few years due to their efficient feature learning ability. The deep Convolutional Neural Network (DCNN) based approach has gained more attention in the computer vision community. Table 4 shows an overview of the deep feature-based person retrieval methodologies. The performance column shows the highest value reported in the relevant literature in the case of multiple scenario-based analyses. Semantic Retrieval Convolutional Neural Network (SRCNN) developed by Martinho et al. in [124] shows the evaluation of a similar setup of [106]. Binary Cross-Entropy (BCE) and Mean Squared Error (MSE) loss functions quantify binary classification and regression. SRCNN achieves 35.7% and 46.4% at rank-1 accuracy for one-shot and multi-shot identification, respectively. Thus, a deep feature based SRCNN approach demonstrates a rank-1 accuracy improvement of 23.2% and 26.3% over a handcrafted feature-based system of [106].

The baseline method [76] of AVSS 2018 challenge II [5] dataset is implementable using handcrafted features, while all participants [69,79,80] of the challenge have evaluations based on deep features. The following discussions in this section consist of methodologies implemented on AVSS 2018 challenge II [5] dataset as well as some other approaches on large scale datasets like Market-1501 [105], DukeMTMC [107], PA100K [117] and CUHK03 [113].

Galiyawala et al. [69] use height, clothing colour, and gender for person retrieval. Person detection and semantic segmentation using Mask R-CNN [98] help to remove the cluttered background. It results in a clutter-free torso patch for efficient colour classification. Height is available using a camera calibration approach [120]. Torso clothing colour and gender are classified using a fine-tuned AlexNet [125] based individual model. Sequential implementation of height, colour, and gender filter aims to eliminate the detected persons and leave only the target person. This linear filtering-based approach achieves an average IoU of 0.36. Schumann et al. [80] detect the person using the Single-Shot Multibox Detector (SSD) [126]. Early-stage false positives are eliminated by background modelling based on a mixture of Gaussian distribution. The strategy of the ensemble of classifiers is adapted, and predictions are fused by computing mean or weighted mean. The evaluation is based on the Euclidean distance between a query vector and each detected person's attribute probability to produce the final result.  Fig. 16 Sample output frames for the person retrieved using semantic description by approach in [78]: person from Test Sequence 11 and Frame 66 with semantic description height (very short; 140-160 cm), torso type (short sleeve), torso colour-1 (yellow), torso colour-2 (NA) and gender (male). Images from left to right are Mask R-CNN person detection, height filtering, clothing colour filtering and gender filtering.

Yaguchi et al. [79] also use mask R-CNN for person detection and DenseNet-161 for attribute classification. Initially, they estimate all the attributes of the detected persons, and then the matching score is calculated using a Hamming loss. The person with the minimum loss is the target. Galiyawala et al. [78] further improve the linear filtering approach of [69] by introducing adaptive torso patch extraction and bounding box regression. Torso patch extraction is undertaken by deciding the torso region according to clothing type attribute. Thus, it removes noisy pixels from the torso patch and provides better colour classification. An IoU based box regression predicts the bounding box in the frame where soft biometric attribute-based retrieval fails. The approaches in [69] and [78] follow a linear filtering approach that filters out the non-matching person according to attributes and leaves the target in the end. The other two methods in [79] and [80] estimate all the detected attributes of a person in parallel. These methods fuse the characteristics in the end to retrieve the target. The linear filtering approach does not need to estimate all the attributes for all detection persons. However, an error in the first filter will propagate to further attribute filtering and reduce retrieval accuracy.

The AVSS 2018 challenge II [5] dataset is evaluated based on two metrics; an average IoU and percent of frames with an IoU ≥ 0.4. State-of-the-art average IoU of 0.569 is achieved by a linear filtering approach [78]. 75.9% of frames with an IoU ≥ 0.4 is achieved by [80]. Some sample qualitative results of person retrieval using the method in [78] are shown in Fig. 16. It offers a surveillance frame from AVSS 2018 challenge II datasets for test sequence 40 and frame 66 with a semantic description, namely, height (very short; 140-160 cm), torso type (short sleeve), torso colour-1 (yellow), torso colour-2 (NA) and gender (male). The first image shows the person detection output using Mask R-CNN. Height filter output shows that many persons match the given query. Among these persons, the colour filter output produces a couple of matches and this is further refined by using a gender filter. It is to be noted that all deep features-based approaches [59,78,79,80] perform better than the handcrafted feature-based baseline approach [76].

Sun et al. [81] apply part level features because they provide fine-grained information for person description. The authors propose a Part-based Convolutional Baseline (PCB) network for part-based feature extraction. Part-level Table 5 Comparison of handcrafted and deep feature based methods.","sent1: Deep learning-based methodologies are becoming popular in the past few years due to their efficient feature learning ability.
sent2: The deep Convolutional Neural Network (DCNN) based approach has gained more attention in the computer vision community.
sent3: Table 4 shows an overview of the deep feature-based person retrieval methodologies.
sent4: The performance column shows the highest value reported in the relevant literature in the case of multiple scenario-based analyses.
sent5: Semantic Retrieval Convolutional Neural Network (SRCNN) developed by Martinho et al. in [124] shows the evaluation of a similar setup of [106]. Binary Cross-Entropy (BCE) and Mean Squared Error (MSE) loss functions quantify binary classification and regression.
sent6: SRCNN achieves 35.7% and 46.4% at rank-1 accuracy for one-shot and multi-shot identification, respectively.
sent7: Thus, a deep feature based SRCNN approach demonstrates a rank-1 accuracy improvement of 23.2% and 26.3% over a handcrafted feature-based system of [106].
sent8: The baseline method [76] of AVSS 2018 challenge II [5] dataset is implementable using handcrafted features, while all participants [69,79,80] of the challenge have evaluations based on deep features.
sent9: The following discussions in this section consist of methodologies implemented on AVSS 2018 challenge II [5] dataset as well as some other approaches on large scale datasets like Market-1501 [105], DukeMTMC [107], PA100K [117] and CUHK03 [113].
sent10: Galiyawala et al. [69] use height, clothing colour, and gender for person retrieval.
sent11: Person detection and semantic segmentation using Mask R-CNN [98] help to remove the cluttered background.
sent12: It results in a clutter-free torso patch for efficient colour classification.
sent13: Height is available using a camera calibration approach [120].
sent14: Torso clothing colour and gender are classified using a fine-tuned AlexNet [125] based individual model.
sent15: Sequential implementation of height, colour, and gender filter aims to eliminate the detected persons and leave only the target person.
sent16: This linear filtering-based approach achieves an average IoU of 0.36.
sent17: Schumann et al. [80] detect the person using the Single-Shot Multibox Detector (SSD) [126].
sent18: Early-stage false positives are eliminated by background modelling based on a mixture of Gaussian distribution.
sent19: The strategy of the ensemble of classifiers is adapted, and predictions are fused by computing mean or weighted mean.
sent20: The evaluation is based on the Euclidean distance between a query vector and each detected person's attribute probability to produce the final result.
sent21: Fig. 16 Sample output frames for the person retrieved using semantic description by approach in [78]: person from Test Sequence 11 and Frame 66 with semantic description height (very short; 140-160 cm), torso type (short sleeve), torso colour-1
sent22: (yellow), torso colour-2 (NA) and gender (male).
sent23: Images from left to right are Mask R-CNN person detection, height filtering, clothing colour filtering and gender filtering.
sent24: Yaguchi et al. [79] also use mask R-CNN for person detection and DenseNet-161 for attribute classification.
sent25: Initially, they estimate all the attributes of the detected persons, and then the matching score is calculated using a Hamming loss.
sent26: The person with the minimum loss is the target.
sent27: Galiyawala et al. [78] further improve the linear filtering approach of [69] by introducing adaptive torso patch extraction and bounding box regression.
sent28: Torso patch extraction is undertaken by deciding the torso region according to clothing type attribute.
sent29: Thus, it removes noisy pixels from the torso patch and provides better colour classification.
sent30: An IoU based box regression predicts the bounding box in the frame where soft biometric attribute-based retrieval fails.
sent31: The approaches in [69] and [78] follow a linear filtering approach that filters out the non-matching person according to attributes and leaves the target in the end.
sent32: The other two methods in [79] and [80] estimate all the detected attributes of a person in parallel.
sent33: These methods fuse the characteristics in the end to retrieve the target.
sent34: The linear filtering approach does not need to estimate all the attributes for all detection persons.
sent35: However, an error in the first filter will propagate to further attribute filtering and reduce retrieval accuracy.
sent36: The AVSS 2018 challenge II [5] dataset is evaluated based on two metrics; an average IoU and percent of frames with an IoU ≥ 0.4.
sent37: State-of-the-art average IoU of 0.569 is achieved by a linear filtering approach [78].
sent38: 75.9% of frames with an IoU ≥ 0.4 is achieved by [80].
sent39: Some sample qualitative results of person retrieval using the method in [78] are shown in Fig. 16.
sent40: It offers a surveillance frame from AVSS 2018 challenge II datasets for test sequence 40 and frame 66 with a semantic description, namely, height (very short; 140-160 cm), torso type (short sleeve), torso colour-1
sent41: (yellow), torso colour-2 (NA) and gender (male).
sent42: The first image shows the person detection output using Mask R-CNN.
sent43: Height filter output shows that many persons match the given query.
sent44: Among these persons, the colour filter output produces a couple of matches and this is further refined by using a gender filter.
sent45: It is to be noted that all deep features-based approaches [59,78,79,80] perform better than the handcrafted feature-based baseline approach [76].Sun et al. [81] apply part level features because they provide fine-grained information for person description.
sent46: The authors propose a Part-based Convolutional Baseline (PCB) network for part-based feature extraction.
sent47: Part-level Table 5 Comparison of handcrafted and deep feature based methods.","1. Why have deep learning-based methodologies become popular in recent years? sent1
2. Which approach has gained more attention in the computer vision community? sent2
3. What does Table 4 provide an overview of? sent3
4. What does the performance column in Table 4 show? sent4
5. What is the Semantic Retrieval Convolutional Neural Network (SRCNN) and what does it evaluate? sent5
    5.1. What rank-1 accuracy does SRCNN achieve for one-shot and multi-shot identification? sent6
    5.2. How does the SRCNN approach compare to a handcrafted feature-based system? sent7
6. What is the baseline method of the AVSS 2018 challenge II dataset based on? sent8
7. What datasets are discussed in this section? sent9
8. What features do Galiyawala et al. use for person retrieval? sent10
9. How does Mask R-CNN contribute to person detection and semantic segmentation? sent11
    9.1. What is the result of using Mask R-CNN for person detection? sent12
10. How is height information obtained in the approach by Galiyawala et al.? sent13
11. How are torso clothing colour and gender classified in Galiyawala et al.'s approach? sent14
12. What is the aim of the sequential implementation of height, colour, and gender filters? sent15
    12.1. What is the average IoU achieved by this linear filtering-based approach? sent16
13. How do Schumann et al. detect persons and eliminate early-stage false positives? sent17, sent18
    13.1. What strategy is adapted for predictions in Schumann et al.'s approach? sent19
    13.2. How is the evaluation conducted in Schumann et al.'s approach? sent20
14. What does Fig. 16 illustrate in terms of person retrieval? sent21, sent22, sent23
15. What methods do Yaguchi et al. use for person detection and attribute classification? sent24
    15.1. How is the matching score calculated in Yaguchi et al.'s approach? sent25
    15.2. Who is considered the target in Yaguchi et al.'s approach? sent26
16. How do Galiyawala et al. improve the linear filtering approach? sent27
    16.1. How is torso patch extraction undertaken in this improved approach? sent28
    16.2. What does the IoU based box regression predict? sent30
17. How do the approaches in [69] and [78] differ from those in [79] and [80]? sent31, sent32
    17.1. What is the advantage of the linear filtering approach? sent34
    17.2. What is a potential drawback of the linear filtering approach? sent35
18. How is the AVSS 2018 challenge II dataset evaluated? sent36
    18.1. What state-of-the-art average IoU is achieved by a linear filtering approach? sent37
    18.2. What percentage of frames with an IoU ≥ 0.4 is achieved by [80]? sent38
19. What do the sample qualitative results in Fig. 16 show? sent39, sent40, sent41, sent42, sent43, sent44
20. How do deep features-based approaches compare to the handcrafted feature-based baseline approach? sent45
21. What do Sun et al. propose for part-based feature extraction? sent46"
181562553,A Systematic Literature Review on Image Captioning,https://www.semanticscholar.org/paper/92ccf5a39c63cb5e1639be518e6db2e357acd58e,SLR Methodology,4,"The SLR has become a great help in the dynamic, data driven world of today, with massive data volume growth. It is sometimes very difficult to consume all currently existing information before starting to delve into a specific field. In this case, when we talk about image captioning and, as already said, having so much meaning in this task, it was found that there is much literature, which is hard to summarize and thus stay up to date with the newest achievements. There are only a few SLRs that have been conducted for image captioning until now [6][7][8][9], though with such fast progress and increasing popularity in this field we find it necessary that they continue to be undertaken. Moreover, results of image captioning models in previous reviews were not as detailed as they are in this paper. Researchers dedicated time to detailed study of most articles in image captioning-digital libraries, which store most of the articles, were identified, search questions carefully formulated, all articles found were precisely analyzed, and results presented together with important challenges which were captured through the review process. This work follows ref. [6] as a guideline due to the easily understandable structure of their work and the similar ideas.","sent1: The SLR has become a great help in the dynamic, data driven world of today, with massive data volume growth.
sent2: It is sometimes very difficult to consume all currently existing information before starting to delve into a specific field.
sent3: In this case, when we talk about image captioning and, as already said, having so much meaning in this task, it was found that there is much literature, which is hard to summarize and thus stay up to date with the newest achievements.
sent4: There are only a few SLRs that have been conducted for image captioning until now [6][7][8][9], though with such fast progress and increasing popularity in this field we find it necessary that they continue to be undertaken.
sent5: Moreover, results of image captioning models in previous reviews were not as detailed as they are in this paper.
sent6: Researchers dedicated time to detailed study of most articles in image captioning-digital libraries, which store most of the articles, were identified, search questions carefully formulated, all articles found were precisely analyzed, and results presented together with important challenges which were captured through the review process.
sent7: This work follows ref. [6] as a guideline due to the easily understandable structure of their work and the similar ideas.","1. How has the SLR become helpful in today's data-driven world? sent1
2. What challenge is faced when delving into a specific field with massive data volume growth? sent2
3. What difficulty is highlighted in the context of image captioning literature? sent3
4. How many SLRs have been conducted for image captioning, and what is the current necessity in this field? sent4
5. How do the results of image captioning models in previous reviews compare to those in this paper? sent5
6. What steps did researchers take in conducting the SLR for image captioning? sent6
7. Which reference did this work follow as a guideline, and why? sent7"
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,SQUALL (Semantic Query and Update High-Level Language),4,"SQUALL (Ferré, 2014(Ferré, , 2011 is an NLI for searching and updating an RDF store. It uses the style of Montague grammars (context-free generative grammar) as an intermediate language (similar to TR Discover) to split the translation process in two parts: translating the natural language input question into a logical language and translating the logical language into a query language. Because of that, the second part is getting easier: the logical language and the query language share the same semantics and level of detail. The grammar of SQUALL consists of about 120 domain-independent rules.

The translation into the logical form is done in three steps. In the first step, the keywords are recognized (lookup step). The second step is a syntactic analysis based on a descending parser, which is fed with the grammar rules. Afterwards, the next step can generate the logical language based on the definition in the grammar. After the translation into the logical language, the translation in to the chosen formal language can be done.

The strength of SQUALL is that it is able to translate any type of input question, including aggregations, negations and subqueries. The weakness of SQUALL is that the users have to know the RDF vocabulary (e.g., classes and properties). For example, the input question Q1 needs to be formulated as 'Who is the director of Inglourious Basterds? '.

SQUALL (Ferré, 2014(Ferré, , 2011 is an NLI for searching and updating an RDF store. It uses the style of Montague grammars (context-free generative grammar) as an intermediate language (similar to TR Discover) to split the translation process in two parts: translating the natural language input question into a logical language and translating the logical language into a query language. Because of that, the second part is getting easier: the logical language and the query language share the same semantics and level of detail. The grammar of SQUALL consists of about 120 domain-independent rules.

The translation into the logical form is done in three steps. In the first step, the keywords are recognized (lookup step). The second step is a syntactic analysis based on a descending parser, which is fed with the grammar rules. Afterwards, the next step can generate the logical language based on the definition in the grammar. After the translation into the logical language, the translation in to the chosen formal language can be done.

The strength of SQUALL is that it is able to translate any type of input question, including aggregations, negations and subqueries. The weakness of SQUALL is that the users have to know the RDF vocabulary (e.g., classes and properties). For example, the input question Q1 needs to be formulated as 'Who is the director of Inglourious Basterds? '.","sent1: SQUALL (Ferré, 2014(Ferré, , 2011 is an NLI for searching and updating an RDF store. It uses the style of Montague grammars (context-free generative grammar) as an intermediate language (similar to TR Discover) to split the translation process in two parts: translating the natural language input question into a logical language and translating the logical language into a query language.
sent2: Because of that, the second part is getting easier: the logical language and the query language share the same semantics and level of detail.
sent3: The grammar of SQUALL consists of about 120 domain-independent rules.
sent4: The translation into the logical form is done in three steps.
sent5: In the first step, the keywords are recognized (lookup step).
sent6: The second step is a syntactic analysis based on a descending parser, which is fed with the grammar rules.
sent7: Afterwards, the next step can generate the logical language based on the definition in the grammar.
sent8: After the translation into the logical language, the translation in to the chosen formal language can be done.
sent9: The strength of SQUALL is that it is able to translate any type of input question, including aggregations, negations and subqueries.
sent10: The weakness of SQUALL is that the users have to know the RDF vocabulary (e.g., classes and properties).
sent11: For example, the input question Q1 needs to be formulated as 'Who is the director of Inglourious Basterds? '.
sent12: SQUALL (Ferré, 2014(Ferré, , 2011 is an NLI for searching and updating an RDF store. It uses the style of Montague grammars (context-free generative grammar) as an intermediate language (similar to TR Discover) to split the translation process in two parts: translating the natural language input question into a logical language and translating the logical language into a query language.
sent13: Because of that, the second part is getting easier: the logical language and the query language share the same semantics and level of detail.
sent14: The grammar of SQUALL consists of about 120 domain-independent rules.
sent15: The translation into the logical form is done in three steps.
sent16: In the first step, the keywords are recognized (lookup step).
sent17: The second step is a syntactic analysis based on a descending parser, which is fed with the grammar rules.
sent18: Afterwards, the next step can generate the logical language based on the definition in the grammar.
sent19: After the translation into the logical language, the translation in to the chosen formal language can be done.
sent20: The strength of SQUALL is that it is able to translate any type of input question, including aggregations, negations and subqueries.
sent21: The weakness of SQUALL is that the users have to know the RDF vocabulary (e.g., classes and properties).
sent22: For example, the input question Q1 needs to be formulated as 'Who is the director of Inglourious Basterds? '.","1. What is SQUALL and what is its purpose? sent1
    1.1. How does SQUALL split the translation process? sent1
    1.2. Why is the second part of the translation process easier in SQUALL? sent2
2. What does the grammar of SQUALL consist of? sent3
3. How is the translation into the logical form done in SQUALL? sent4
    3.1. What is the first step in the translation process? sent5
    3.2. What is the second step in the translation process? sent6
    3.3. What happens after the syntactic analysis in the translation process? sent7
4. What happens after the translation into the logical language in SQUALL? sent8
5. What is the strength of SQUALL? sent9
6. What is the weakness of SQUALL? sent10
    6.1. Can you provide an example of how an input question needs to be formulated in SQUALL? sent11"
10137425,Multimodal Machine Learning: A Survey and Taxonomy,https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,Non-parallel data,26,"Methods that rely on non-parallel data do not require the modalities to have shared instances, but only shared categories or concepts. Non-parallel co-learning approaches can help when learning representations, allow for better semantic concept understanding and even perform unseen object recognition.  [148] Transfer learning is also possible on non-parallel data and allows to learn better representations through transferring information from a representation built using a data rich or clean modality to a data scarce or noisy modality. This type of trasnfer learning is often achieved by using coordinated multimodal representations (see Section 3.2). For example, Frome et al. [61] used text to improve visual representations for image classification by coordinating CNN visual features with word2vec textual ones [141] trained on separate large datasets. Visual representations trained in such a way result in more meaningful errors -mistaking objects for ones of similar category [61]. Mahasseni and Todorovic [129] demonstrated how to regularize a color video based LSTM using an autoencoder LSTM trained on 3D skeleton data by enforcing similarities between their hidden states. Such an approach is able to improve the original LSTM and lead to state-of-the-art performance in action recognition. Conceptual grounding refers to learning semantic meanings or concepts not purely based on language but also on additional modalities such as vision, sound, or even smell [16]. While the majority of concept learning approaches are purely language-based, representations of meaning in humans are not merely a product of our linguistic exposure, but are also grounded through our sensorimotor experience and perceptual system [17], [126]. Human semantic knowledge relies heavily on perceptual information [126] and many concepts are grounded in the perceptual system and are not purely symbolic [17]. This implies that learning semantic meaning purely from textual information might not be optimal, and motivates the use of visual or acoustic cues to ground our linguistic representations. Starting from work by Feng and Lapata [59], grounding is usually performed by finding a common latent space between the representations [59], [183] (in case of parallel datasets) or by learning unimodal representations separately and then concatenating them to lead to a multimodal one [29], [101], [172], [181] (in case of non-parallel data). Once a multimodal representation is constructed it can be used on purely linguistic tasks. Shutova et al. [181] and Bruni et al. [29] used grounded representations for better classification of metaphors and literal language. Such repre-sentations have also been useful for measuring conceptual similarity and relatedness -identifying how semantically or conceptually related two words are [30], [101], [183] or actions [172]. Furthermore, concepts can be grounded not only using visual signals, but also acoustic ones, leading to better performance especially on words with auditory associations [103], or even olfactory signals [102] for words with smell associations. Finally, there is a lot of overlap between multimodal alignment and conceptual grounding, as aligning visual scenes to their descriptions leads to better textual or visual representations [108], [161], [172], [240].

Conceptual grounding has been found to be an effective way to improve performance on a number of tasks. It also shows that language and vision (or audio) are complementary sources of information and combining them in multimodal models often improves performance. However, one has to be careful as grounding does not always lead to better performance [102], [103], and only makes sense when grounding has relevance for the task -such as grounding using images for visually-related concepts. Zero shot learning (ZSL) refers to recognizing a concept without having explicitly seen any examples of it. For example classifying a cat in an image without ever having seen (labeled) images of cats. This is an important problem to address as in a number of tasks such as visual object classification: it is prohibitively expensive to provide training examples for every imaginable object of interest.

There are two main types of ZSL -unimodal and multimodal. The unimodal ZSL looks at component parts or attributes of the object, such as phonemes to recognize an unheard word or visual attributes such as color, size, and shape to predict an unseen visual class [55]. The multimodal ZSL recognizes the objects in the primary modality through the help of the secondary one -in which the object has been seen. The multimodal version of ZSL is a problem facing non-parallel data by definition as the overlap of seen classes is different between the modalities.

Socher et al. [190] map image features to a conceptual word space and are able to classify between seen and unseen concepts. The unseen concepts can be then assigned to a word that is close to the visual representation -this is enabled by the semantic space being trained on a separate dataset that has seen more concepts. Instead of learning a mapping from visual to concept space Frome et al. [61] learn a coordinated multimodal representation between concepts and images that allows for ZSL. Palatucci et al. [158] perform prediction of words people are thinking of based on functional magnetic resonance images, they show how it is possible to predict unseen words through the use of an intermediate semantic space. Lazaridou et al. [118] present a fast mapping method for ZSL by mapping extracted visual feature vectors to text-based vectors through a neural network.","sent1: Methods that rely on non-parallel data do not require the modalities to have shared instances, but only shared categories or concepts.
sent2: Non-parallel co-learning approaches can help when learning representations, allow for better semantic concept understanding and even perform unseen object recognition.
sent3: [148] Transfer learning is also possible on non-parallel data and allows to learn better representations through transferring information from a representation built using a data rich or clean modality to a data scarce or noisy modality.
sent4: This type of trasnfer learning is often achieved by using coordinated multimodal representations (see Section 3.2).
sent5: For example, Frome et al. [61] used text to improve visual representations for image classification by coordinating CNN visual features with word2vec textual ones [141] trained on separate large datasets.
sent6: Visual representations trained in such a way result in more meaningful errors -mistaking objects for ones of similar category [61].
sent7: Mahasseni and Todorovic [129] demonstrated how to regularize a color video based LSTM using an autoencoder LSTM trained on 3D skeleton data by enforcing similarities between their hidden states.
sent8: Such an approach is able to improve the original LSTM and lead to state-of-the-art performance in action recognition.
sent9: Conceptual grounding refers to learning semantic meanings or concepts not purely based on language but also on additional modalities such as vision, sound, or even smell [16].
sent10: While the majority of concept learning approaches are purely language-based, representations of meaning in humans are not merely a product of our linguistic exposure, but are also grounded through our sensorimotor experience and perceptual system [17], [126].
sent11: Human semantic knowledge relies heavily on perceptual information [126] and many concepts are grounded in the perceptual system and are not purely symbolic [17].
sent12: This implies that learning semantic meaning purely from textual information might not be optimal, and motivates the use of visual or acoustic cues to ground our linguistic representations.
sent13: Starting from work by Feng and Lapata [59], grounding is usually performed by finding a common latent space between the representations [59], [183] (in case of parallel datasets) or by learning unimodal representations separately and then concatenating them to lead to a multimodal one [29], [101], [172], [181] (in case of non-parallel data).
sent14: Once a multimodal representation is constructed it can be used on purely linguistic tasks.
sent15: Shutova et al. [181] and Bruni et al. [29] used grounded representations for better classification of metaphors and literal language.
sent16: Such repre-sentations have also been useful for measuring conceptual similarity and relatedness -identifying how semantically or conceptually related two words are [30], [101], [183] or actions [172].
sent17: Furthermore, concepts can be grounded not only using visual signals, but also acoustic ones, leading to better performance especially on words with auditory associations [103], or even olfactory signals [102] for words with smell associations.
sent18: Finally, there is a lot of overlap between multimodal alignment and conceptual grounding, as aligning visual scenes to their descriptions leads to better textual or visual representations [108], [161], [172], [240].
sent19: Conceptual grounding has been found to be an effective way to improve performance on a number of tasks.
sent20: It also shows that language and vision (or audio) are complementary sources of information and combining them in multimodal models often improves performance.
sent21: However, one has to be careful as grounding does not always lead to better performance [102], [103], and only makes sense when grounding has relevance for the task -such as grounding using images for visually-related concepts.
sent22: Zero shot learning (ZSL) refers to recognizing a concept without having explicitly seen any examples of it.
sent23: For example classifying a cat in an image without ever having seen (labeled) images of cats.
sent24: This is an important problem to address as in a number of tasks such as visual object classification: it is prohibitively expensive to provide training examples for every imaginable object of interest.
sent25: There are two main types of ZSL -unimodal and multimodal.
sent26: The unimodal ZSL looks at component parts or attributes of the object, such as phonemes to recognize an unheard word or visual attributes such as color, size, and shape to predict an unseen visual class [55].
sent27: The multimodal ZSL recognizes the objects in the primary modality through the help of the secondary one -in which the object has been seen.
sent28: The multimodal version of ZSL is a problem facing non-parallel data by definition as the overlap of seen classes is different between the modalities.
sent29: Socher et al. [190] map image features to a conceptual word space and are able to classify between seen and unseen concepts.
sent30: The unseen concepts can be then assigned to a word that is close to the visual representation -this is enabled by the semantic space being trained on a separate dataset that has seen more concepts.
sent31: Instead of learning a mapping from visual to concept space Frome et al. [61] learn a coordinated multimodal representation between concepts and images that allows for ZSL.
sent32: Palatucci et al. [158] perform prediction of words people are thinking of based on functional magnetic resonance images, they show how it is possible to predict unseen words through the use of an intermediate semantic space.
sent33: Lazaridou et al. [118] present a fast mapping method for ZSL by mapping extracted visual feature vectors to text-based vectors through a neural network.","1. What do methods that rely on non-parallel data require? sent1
2. How can non-parallel co-learning approaches be beneficial? sent2
3. Is transfer learning possible on non-parallel data, and how does it work? sent3
    3.1. How is this type of transfer learning often achieved? sent4
    3.2. Can you provide an example of using text to improve visual representations? sent5
    3.3. What is the result of visual representations trained with coordinated multimodal representations? sent6
    3.4. How did Mahasseni and Todorovic demonstrate the use of regularization in LSTM models? sent7
        3.4.1. What was the outcome of this approach? sent8
4. What is conceptual grounding? sent9
    4.1. How do humans typically learn semantic meanings or concepts? sent10
    4.2. Why might learning semantic meaning purely from textual information be suboptimal? sent11, sent12
    4.3. How is grounding usually performed in non-parallel data? sent13
    4.4. What can multimodal representations be used for? sent14
    4.5. How have grounded representations been used in linguistic tasks? sent15
    4.6. What other uses do grounded representations have? sent16
    4.7. Can concepts be grounded using signals other than visual ones? sent17
    4.8. What is the relationship between multimodal alignment and conceptual grounding? sent18
    4.9. What are the benefits of conceptual grounding? sent19, sent20
    4.10. What caution should be taken with grounding? sent21
5. What is zero-shot learning (ZSL)? sent22
    5.1. Can you provide an example of zero-shot learning? sent23
    5.2. Why is zero-shot learning important? sent24
    5.3. What are the two main types of ZSL? sent25
        5.3.1. What does unimodal ZSL focus on? sent26
        5.3.2. How does multimodal ZSL work? sent27
        5.3.3. Why is multimodal ZSL a problem facing non-parallel data? sent28
    5.4. How did Socher et al. approach ZSL? sent29
        5.4.1. How are unseen concepts assigned in this approach? sent30
    5.5. How did Frome et al. contribute to ZSL? sent31
    5.6. What did Palatucci et al. achieve with ZSL? sent32
    5.7. What method did Lazaridou et al. present for ZSL? sent33"
231925325,Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition,https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23,C. Joint attention-CTC with RNN,10,"Two main approaches for end-to-end encoder-decoder ASR are attention-based and CTC [75]-based. In attention-based approach, the decoder network finds an alignment of the encoder hidden states during the prediction of each element of output sequence. The task of speech recognition is mostly monotonic. Therefore, the possibility of right to left dependency is significantly lesser compared to left to right dependency in ASR tasks. However, due to the flexible nature of attention mechanism, non-sequential alignments are also considered. Therefore, noise and irrelevant frames (encoder hidden states) may result in misalignment. This issue becomes worse for longer sequences as the length of input and output sequences vary due to factors, e.g. the rate of speech, accent, and pronunciation. Therefore, the risk of misalignment in longer sequences is higher. In contrast, CTC allows strict monotonic alignment of speech frames using forward-backward algorithm [9], [76] but assumes targets are conditionally independent on each other. Therefore, temporal dependencies are not properly utilised in CTC, unlike in attention mechanism. For effective ASR performance, many researchers have combined the advantages of both attention and CTC in a single model and therefore, the CTC probabilities replaces the incorrect predictions by the attention mechanism.

The discussion on CTC and its application on ASR is beyond the scope of this paper. However, in this section a brief introduction to CTC and how it is jointly used with attention is provided [33], [34]. CTC monotonically maps an input sequence to output sequence. Considering the model outputs Llength letter sequence Y {y l ∈ U |l = 1, · · · , L} with a set of distinct characters U , given the input sequence is X. CTC introduces frame-wise letter sequence with an additional ""blank"" symbol Z = {z t ∈ U ∪ blank|t = 1, · · · , T }. By using conditional independence assumptions, the posterior distribution p(Y |X) is factorized as follows:

CTC has three distribution components by the Bayes theorem similar to the traditional or hybrid ASR. They are frame-wise posterior distribution p(z t |X) -acoustic module, transition probability p(z t |z t−1 , C) -pronunciation module, and letter-based language module p(Y ).

Compared with CTC approaches, the attention-based approach does not make any conditional independence assumptions, and directly estimates the posterior p(Y |X) based on the chain rule:

p ctc (Y |X) and p att (Y |X) are the CTC-based and attentionbased objective functions, respectively. Finally, the logarithmic linear combination of CTC-and attention-based objective functions given by (17) is maximised to leverage the CTC and attention mechanism together in a ASR model.

λ is a tunable parameter in the range [0, 1].

In [33], [34], the CTC objective function was incorporated in the attention-based model during the training only. However, motivated by the effectiveness of this joint approach, in [35], [36], it is used for decoding or inferencing phase as well.

A triggered attention mechanism is proposed in [37]. At each decoder time step, the encoder states which the attention model looks upon are controlled by a trigger model. The encoder states are shared with the trigger model which is a CTC-based network as well as with the attention model. The trigger sequence which is computed based on the CTC generated sequence provides alignment information that controls the attention mechanism. Finally, the objective functions of CTC and attention model are optimised jointly.","sent1: Two main approaches for end-to-end encoder-decoder ASR are attention-based and CTC [75]-based.
sent2: In attention-based approach, the decoder network finds an alignment of the encoder hidden states during the prediction of each element of output sequence.
sent3: The task of speech recognition is mostly monotonic.
sent4: Therefore, the possibility of right to left dependency is significantly lesser compared to left to right dependency in ASR tasks.
sent5: However, due to the flexible nature of attention mechanism, non-sequential alignments are also considered.
sent6: Therefore, noise and irrelevant frames (encoder hidden states) may result in misalignment.
sent7: This issue becomes worse for longer sequences as the length of input and output sequences vary due to factors, e.g.
sent8: the rate of speech, accent, and pronunciation.
sent9: Therefore, the risk of misalignment in longer sequences is higher.
sent10: In contrast, CTC allows strict monotonic alignment of speech frames using forward-backward algorithm [9], [76] but assumes targets are conditionally independent on each other.
sent11: Therefore, temporal dependencies are not properly utilised in CTC, unlike in attention mechanism.
sent12: For effective ASR performance, many researchers have combined the advantages of both attention and CTC in a single model and therefore, the CTC probabilities replaces the incorrect predictions by the attention mechanism.
sent13: The discussion on CTC and its application on ASR is beyond the scope of this paper.
sent14: However, in this section a brief introduction to CTC and how it is jointly used with attention is provided [33], [34].
sent15: CTC monotonically maps an input sequence to output sequence.
sent16: Considering the model outputs Llength letter sequence Y {y l ∈ U |l = 1, · · · , L} with a set of distinct characters U , given the input sequence is X. CTC introduces frame-wise letter sequence with an additional ""blank"" symbol Z = {z t ∈ U ∪ blank|t = 1, · · · , T }.
sent17: By using conditional independence assumptions, the posterior distribution p(Y |X) is factorized as follows:CTC has three distribution components by the Bayes theorem similar to the traditional or hybrid ASR.
sent18: They are frame-wise posterior distribution p(z t |X) -acoustic module, transition probability p(z t |z t−1 , C) -pronunciation module, and letter-based language module p(Y ).
sent19: Compared with CTC approaches, the attention-based approach does not make any conditional independence assumptions, and directly estimates the posterior p(Y |X) based on the chain rule:p ctc (Y |X) and p att (Y |X) are the CTC-based and attentionbased objective functions, respectively.
sent20: Finally, the logarithmic linear combination of CTC-and attention-based objective functions given by (17) is maximised to leverage the CTC and attention mechanism together in a ASR model.
sent21: λ is a tunable parameter in the range [0, 1].
sent22: In [33], [34], the CTC objective function was incorporated in the attention-based model during the training only.
sent23: However, motivated by the effectiveness of this joint approach, in [35], [36], it is used for decoding or inferencing phase as well.
sent24: A triggered attention mechanism is proposed in [37].
sent25: At each decoder time step, the encoder states which the attention model looks upon are controlled by a trigger model.
sent26: The encoder states are shared with the trigger model which is a CTC-based network as well as with the attention model.
sent27: The trigger sequence which is computed based on the CTC generated sequence provides alignment information that controls the attention mechanism.
sent28: Finally, the objective functions of CTC and attention model are optimised jointly.","1. What are the two main approaches for end-to-end encoder-decoder ASR? sent1
    1.1. How does the attention-based approach work in ASR? sent2
    1.2. Why is the task of speech recognition mostly monotonic? sent3
        1.2.1. What is the significance of left to right dependency in ASR tasks? sent4
    1.3. What is a potential issue with the flexible nature of the attention mechanism? sent5
        1.3.1. How can noise and irrelevant frames affect the attention mechanism? sent6
        1.3.2. Why does the risk of misalignment increase for longer sequences? sent7, sent8, sent9
2. How does CTC differ from the attention-based approach in ASR? sent10
    2.1. What is a limitation of CTC regarding temporal dependencies? sent11
3. How have researchers combined attention and CTC for effective ASR performance? sent12
    3.1. What is the scope of the discussion on CTC in this paper? sent13
    3.2. What does this section provide regarding CTC and its joint use with attention? sent14
4. How does CTC map input sequences to output sequences? sent15
    4.1. What additional symbol does CTC introduce in frame-wise letter sequences? sent16
    4.2. How is the posterior distribution p(Y |X) factorized in CTC? sent17
        4.2.1. What are the three distribution components in CTC? sent18
5. How does the attention-based approach differ from CTC in terms of assumptions? sent19
6. How are the objective functions of CTC and attention-based approaches combined? sent20
    6.1. What is the role of the tunable parameter λ? sent21
7. How was the CTC objective function initially incorporated in the attention-based model? sent22
    7.1. How has the joint approach been extended beyond training? sent23
8. What is the triggered attention mechanism proposed in [37]? sent24
    8.1. How does the trigger model control the attention mechanism? sent25
    8.2. How are encoder states shared in the triggered attention mechanism? sent26
    8.3. What role does the trigger sequence play in the attention mechanism? sent27
9. How are the objective functions of CTC and attention model optimized? sent28"
257913422,Self-Supervised Multimodal Learning: A Survey,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,A.1.1 Cross-modal Retrieval,7,"Image-text Retrieval. Image-text retrieval can take two forms: image-to-text and text-to-image retrieval which correspond to using images and text as the query modality respectively. The evaluation metric used to measure performance is Recall@K, with K typically set to 1, 5, or 10. Popular evaluation datasets include COCO [20] and Flickr30K [204]. Video-text Retrieval. Video-text retrieval mainly focuses on text-to-video retrieval, which has two subtasks: (a) retrieving a relevant video based on a text query, or (b) retrieving a video segment within a given video that matches a specific text description. In both cases, the evaluation metric is recall@K. Popular datasets for (a) include MSRVTT [205], YouCook2 [206], MSVD [207], etc., and for (b) include [208], ActivityNet Captions [209], etc.","sent1: Image-text Retrieval. Image-text retrieval can take two forms: image-to-text and text-to-image retrieval which correspond to using images and text as the query modality respectively.
sent2: The evaluation metric used to measure performance is Recall@K, with K typically set to 1, 5, or 10.
sent3: Popular evaluation datasets include COCO [20] and Flickr30K [204].
sent4: Video-text Retrieval. Video-text retrieval mainly focuses on text-to-video retrieval, which has two subtasks: (a) retrieving a relevant video based on a text query, or (b) retrieving a video segment within a given video that matches a specific text description.
sent5: In both cases, the evaluation metric is recall@K. Popular datasets for (a) include MSRVTT [205], YouCook2 [206], MSVD [207], etc., and for (b) include [208], ActivityNet Captions [209], etc.","1. What are the two forms of image-text retrieval? sent1
    1.1. What evaluation metric is used to measure performance in image-text retrieval? sent2
    1.2. What are some popular evaluation datasets for image-text retrieval? sent3
2. What is the main focus of video-text retrieval? sent4
    2.1. What evaluation metric is used in video-text retrieval? sent5
    2.2. What are some popular datasets for text-to-video retrieval and video segment retrieval? sent5"
255024417,Application of named entity recognition method for Indonesian datasets: a review,https://www.semanticscholar.org/paper/941c79b384bc652cca8e8fe5b0e406ae971af713,METHOD 2.1. Systematic literature review,10,"First, this article presents a SLR of the field of NER research. A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research. Then follows a discussion of how NER has been applied to Indonesian texts. SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].

A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1). In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method. At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords. This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.  There follows the RQ that guided the following analysis: RQ. ""What are the trends in the application of NER to extract information from Indonesian online news and social media?"" In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa). According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results. In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process. These processes are sequential processes where each process aims to find the right study to be used in this research. The search and selection process are an elimination process based on the criteria specified in each process.

The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data. Some irrelevant papers were omitted in the first stage of collection based on the title and abstract. The second stage of selection articles is a full-text selection. Figure 2 illustrates the procedure of text-selection. The total number of papers obtained from the four databases was initially 241. Upon completion of the selection procedure, however, only 20 papers remained. The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset. The third stage is reporting the results and analyzing the results of this review. We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research. Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021

Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset 

First, this article presents a SLR of the field of NER research. A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research. Then follows a discussion of how NER has been applied to Indonesian texts. SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].

A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1). In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method. At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords. This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.  There follows the RQ that guided the following analysis: RQ. ""What are the trends in the application of NER to extract information from Indonesian online news and social media?"" In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa). According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results. In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process. These processes are sequential processes where each process aims to find the right study to be used in this research. The search and selection process are an elimination process based on the criteria specified in each process.

The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data. Some irrelevant papers were omitted in the first stage of collection based on the title and abstract. The second stage of selection articles is a full-text selection. Figure 2 illustrates the procedure of text-selection. The total number of papers obtained from the four databases was initially 241. Upon completion of the selection procedure, however, only 20 papers remained. The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset. The third stage is reporting the results and analyzing the results of this review. We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research. Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021

Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset ","sent1: First, this article presents a SLR of the field of NER research.
sent2: A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research.
sent3: Then follows a discussion of how NER has been applied to Indonesian texts.
sent4: SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].
sent5: A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1).
sent6: In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method.
sent7: At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords.
sent8: This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.
sent9: There follows the RQ that guided the following analysis: RQ.
sent10: ""What are the trends in the application of NER to extract information from Indonesian online news and social media?""
sent11: In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa).
sent12: According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results.
sent13: In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process.
sent14: These processes are sequential processes where each process aims to find the right study to be used in this research.
sent15: The search and selection process are an elimination process based on the criteria specified in each process.
sent16: The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data.
sent17: Some irrelevant papers were omitted in the first stage of collection based on the title and abstract.
sent18: The second stage of selection articles is a full-text selection.
sent19: Figure 2 illustrates the procedure of text-selection.
sent20: The total number of papers obtained from the four databases was initially 241.
sent21: Upon completion of the selection procedure, however, only 20 papers remained.
sent22: The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset.
sent23: The third stage is reporting the results and analyzing the results of this review.
sent24: We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research.
sent25: Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria
sent26: The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset First, this article presents a SLR of the field of NER research.
sent27: A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research.
sent28: Then follows a discussion of how NER has been applied to Indonesian texts.
sent29: SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].
sent30: A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1).
sent31: In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method.
sent32: At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords.
sent33: This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.
sent34: There follows the RQ that guided the following analysis: RQ.
sent35: ""What are the trends in the application of NER to extract information from Indonesian online news and social media?""
sent36: In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa).
sent37: According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results.
sent38: In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process.
sent39: These processes are sequential processes where each process aims to find the right study to be used in this research.
sent40: The search and selection process are an elimination process based on the criteria specified in each process.
sent41: The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data.
sent42: Some irrelevant papers were omitted in the first stage of collection based on the title and abstract.
sent43: The second stage of selection articles is a full-text selection.
sent44: Figure 2 illustrates the procedure of text-selection.
sent45: The total number of papers obtained from the four databases was initially 241.
sent46: Upon completion of the selection procedure, however, only 20 papers remained.
sent47: The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset.
sent48: The third stage is reporting the results and analyzing the results of this review.
sent49: We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research.
sent50: Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria
sent51: The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset","1. What does this article present regarding NER research? sent1
    1.1. What is the aim of a systematic literature review (SLR)? sent2
    1.2. What follows the presentation of the SLR in the article? sent3
2. In which research domains has SLR been used? sent4
3. How was the SLR carried out in this study? sent5
    3.1. What is the purpose of the planning stage in the SLR? sent6
        3.1.1. What was developed during the planning stage? sent7
        3.1.2. What strategy was used to determine the research questions (RQ)? sent8
            3.1.2.1. What research question guided the analysis? sent9, sent10
            3.1.2.2. What search string was used in the study? sent11
            3.1.2.3. What criteria were used to define the results according to the research question? sent12
    3.2. What is defined in the second stage of the SLR? sent13
        3.2.1. What is the purpose of the sequential processes in the second stage? sent14
        3.2.2. How is the search and selection process described? sent15
        3.2.3. From which databases were papers collected, and how was the data organized? sent16
        3.2.4. How were irrelevant papers handled in the first stage of collection? sent17
        3.2.5. What is the second stage of article selection? sent18
        3.2.6. How many papers were initially obtained and how many remained after selection? sent20, sent21
        3.2.7. What challenge and opportunity does the low number of papers present? sent22
    3.3. What is the third stage of the SLR? sent23
        3.3.1. What aspects were mapped and examined in the third stage? sent24
4. What are the inclusion and exclusion criteria for the selection studies process? sent25"
112487687,Overview of the 2013 ALTA Shared Task,https://www.semanticscholar.org/paper/200d6e8c0aae03704c5c3f51823532597e36d325,The Training and Test Sets,8,"We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.

The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus. Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems. The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard. The second test set was a ""private"" test set that was used to determine the final scores. By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day. As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.

To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 . We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words. We lowercased the tokens and removed those that matched our list of punctuation marks.

The Wikipedia training data consisted of 18 files with a total of 306,445 words. The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.

We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.

The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus. Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems. The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard. The second test set was a ""private"" test set that was used to determine the final scores. By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day. As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.

To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 . We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words. We lowercased the tokens and removed those that matched our list of punctuation marks.

The Wikipedia training data consisted of 18 files with a total of 306,445 words. The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.","sent1: We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.
sent2: The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus.
sent3: Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems.
sent4: The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard.
sent5: The second test set was a ""private"" test set that was used to determine the final scores.
sent6: By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day.
sent7: As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.
sent8: To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 .
sent9: We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words.
sent10: We lowercased the tokens and removed those that matched our list of punctuation marks.
sent11: The Wikipedia training data consisted of 18 files with a total of 306,445 words.
sent12: The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.
sent13: We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.
sent14: The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus.
sent15: Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems.
sent16: The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard.
sent17: The second test set was a ""private"" test set that was used to determine the final scores.
sent18: By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day.
sent19: As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.
sent20: To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 .
sent21: We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words.
sent22: We lowercased the tokens and removed those that matched our list of punctuation marks.
sent23: The Wikipedia training data consisted of 18 files with a total of 306,445 words.
sent24: The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.","1. What data sources were used to produce the training and test sets? sent1
    1.1. From which sections of the English Gigaword Corpus are the data by Baldwin and Joseph (2009) derived? sent2
2. How were the two test sets utilized in the shared task? sent3
    2.1. What was the purpose of the ""public"" test set? sent3
    2.2. How did participants interact with the ""public"" test set? sent4
    2.3. What was the purpose of the ""private"" test set? sent5
    2.4. Why were separate ""public"" and ""private"" test sets used? sent6
3. What training data was used in addition to the data by Baldwin and Joseph (2009)? sent7
    3.1. How was the Wikipedia text processed for training data? sent8, sent9, sent10
    3.2. How many words did the Wikipedia training data consist of? sent11
4. What are the word counts for the files from Baldwin and Joseph (2009)? sent12"
231603122,Persuasive Natural Language Generation -A Literature Review,https://www.semanticscholar.org/paper/e9ac35f68ef4290d74551bafdfb09a4ea0983940,Trustworthiness,4,"Trust plays an important role in the persuader-persuadee relationship. If established, the persuadee's attitude toward the persuader -as identified in Balance Theory (Heider 1958) -helps a persuadee to reason about the reciprocative nature, honesty or reliability of the counterpart (Kim & Duhachek 2020). An implementation of trustworthiness can, amongst others, be realized through identifying a persuadee's psychological profile (e.g., extroverted individuals respond better to texts that have a positive valence, and are in that case more persuadable, Zarouali et al. 2020, Park et al. 2015 to influence the degree of persuasiveness of a persuasive act. This category collates fifteen determinants pertaining to the increase 8 Determinant","sent1: Trust plays an important role in the persuader-persuadee relationship.
sent2: If established, the persuadee's attitude toward the persuader -as identified in Balance Theory (Heider 1958) -helps a persuadee to reason about the reciprocative nature, honesty or reliability of the counterpart (Kim & Duhachek 2020).
sent3: An implementation of trustworthiness can, amongst others, be realized through identifying a persuadee's psychological profile (e.g., extroverted individuals respond better to texts that have a positive valence, and are in that case more persuadable, Zarouali et al. 2020, Park et al. 2015 to influence the degree of persuasiveness of a persuasive act.
sent4: This category collates fifteen determinants pertaining to the increase 8 Determinant","1. What role does trust play in the persuader-persuadee relationship? sent1
2. How does the persuadee's attitude toward the persuader help in reasoning about the persuader's nature? sent2
3. How can trustworthiness be implemented in persuasive natural language generation? sent3"
61066716,A Corpus-based Survey of Four Electronic Swahili-English Bilingual Dictionaries,https://www.semanticscholar.org/paper/b416759e1941fae9b6474541448dec676636a9fe,3.2,4,"The Freedict Swahili-English Dictionary [Freedict] The Freedict Swahili-English Dictionary is an attempt to unify and homogenize existing bilingual dictionaries (Bański and Wójtowicz 2009).It is based on a previously published electronic dictionary (Dict 2009) and also includes entries from a Freedict dictionary (Freedict 2009), and a Swahili-Esperanto-English dictionary (Ergane 2009).It uses the open-source Freedict architecture for development and dissemination and sources are therefore freely downloadable.

The latest version includes 2 600 entries, associated with an English translation equivalent and a part-of-speech tag. Figure 2 illustrates the typical layout of the entries.While the dictionary itself is very small and the information provided is scarce, the developers seem to have tried as much as possible to provide single-word translation equivalents that bode well in a machine translation environment.","sent1: The Freedict Swahili-English Dictionary [Freedict]
sent2: The Freedict Swahili-English Dictionary is an attempt to unify and homogenize existing bilingual dictionaries (Bański and Wójtowicz 2009).It is based on a previously published electronic dictionary (Dict 2009) and also includes entries from a Freedict dictionary (Freedict 2009), and a Swahili-Esperanto-English dictionary (Ergane 2009).It uses the open-source Freedict architecture for development and dissemination and sources are therefore freely downloadable.
sent3: The latest version includes 2 600 entries, associated with an English translation equivalent and a part-of-speech tag.
sent4: Figure 2 illustrates the typical layout of the entries.
sent5: While the dictionary itself is very small and the information provided is scarce, the developers seem to have tried as much as possible to provide single-word translation equivalents that bode well in a machine translation environment.","1. What is the Freedict Swahili-English Dictionary? sent1
    1.1. What is the purpose and basis of the Freedict Swahili-English Dictionary? sent2
    1.2. How many entries does the latest version of the Freedict Swahili-English Dictionary include, and what information is associated with each entry? sent3
    1.3. What does Figure 2 illustrate? sent4
    1.4. What are the limitations of the Freedict Swahili-English Dictionary, and what effort have the developers made to address these limitations? sent5"
232895648,Generative Adversarial Networks: A Literature Review,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,Variational Autoencoders (VAE),5,"Autoencoder is a data compression algorithm, in which the data compression and decompression are realized by neural network self-learning [34]. The encoder maps the input data to the low-latitude features we need, and then reconstructs the original input data through the decoder. Variational Autoencoder [35] is a method that adds ""Gaussian noise"" to the result of the encoder in Autoencoder to make the result of decoder robust to noise.

In the formula above, X is training data, and Z is the hidden feature that cannot be observed in X data. The characteristic of VAE is that every one-dimensional distribution of Z conforms to a normal distribution, and the learning of characteristics is introduced to make the decoding effect better [36,37]. However, VAE adopted the Variational Inference [38] for approximation. Compared with GAN, the fitting of the real data is not as good as GAN. From the result of the generated picture, the picture clarity of GAN is also better than that of VAE.","sent1: Autoencoder is a data compression algorithm, in which the data compression and decompression are realized by neural network self-learning [34].
sent2: The encoder maps the input data to the low-latitude features we need, and then reconstructs the original input data through the decoder.
sent3: Variational Autoencoder [35] is a method that adds ""Gaussian noise"" to the result of the encoder in Autoencoder to make the result of decoder robust to noise.
sent4: In the formula above, X is training data, and Z is the hidden feature that cannot be observed in X data.
sent5: The characteristic of VAE is that every one-dimensional distribution of Z conforms to a normal distribution, and the learning of characteristics is introduced to make the decoding effect better [36,37].
sent6: However, VAE adopted the Variational Inference [38] for approximation.
sent7: Compared with GAN, the fitting of the real data is not as good as GAN.
sent8: From the result of the generated picture, the picture clarity of GAN is also better than that of VAE.","1. What is an autoencoder and how does it function? sent1
    1.1. How does the encoder and decoder work in an autoencoder? sent2
2. What is a Variational Autoencoder (VAE) and how does it differ from a standard autoencoder? sent3
3. In the context of VAE, what do X and Z represent? sent4
4. What is a key characteristic of VAE regarding the distribution of Z? sent5
5. What method does VAE use for approximation? sent6
6. How does VAE compare to GAN in terms of fitting real data and picture clarity? sent7, sent8"
256461385,Narrative Why-Question Answering: A Review of Challenges and Datasets,https://www.semanticscholar.org/paper/98f4b43487b8386728c14aba24f848b2eadad1e2,Narratives,5,"Narratives are texts in which events are causally or thematically linked and develop within a temporal framework (Brewer, 2017). Narratives are generally agent-oriented and their main scope is centered on characters, their actions, and motivations (Sang et al., 2022). In narrative QA, stories, fairytales, books, and (movie) scripts are commonly utilized as narrative texts. Characteristics of narrative texts, such as causality of events and motivations of agents, make narratives a suitable context for asking why-questions. Additionally, fictional narratives can ensure the test of comprehension because they are self-contained, meaning that all elements needed to understand the narrative, such as events, characters, and settings, are present in the text and QA models need to comprehend the narrative in order to answer questions (Dunietz et al., 2020;Richardson et al., 2013;Kočiský et al., 2018). Implicitness is a key feature of narratives that makes it different from other types of texts. Length is another characteristic dimension of narratives which is also very important for QA systems. In the following subsections, we will review these characteristics in more detail.","sent1: Narratives are texts in which events are causally or thematically linked and develop within a temporal framework (Brewer, 2017).
sent2: Narratives are generally agent-oriented and their main scope is centered on characters, their actions, and motivations (Sang et al., 2022).
sent3: In narrative QA, stories, fairytales, books, and (movie) scripts are commonly utilized as narrative texts.
sent4: Characteristics of narrative texts, such as causality of events and motivations of agents, make narratives a suitable context for asking why-questions.
sent5: Additionally, fictional narratives can ensure the test of comprehension because they are self-contained, meaning that all elements needed to understand the narrative, such as events, characters, and settings, are present in the text and QA models need to comprehend the narrative in order to answer questions (Dunietz et al., 2020;Richardson et al., 2013;Kočiský et al., 2018).
sent6: Implicitness is a key feature of narratives that makes it different from other types of texts.
sent7: Length is another characteristic dimension of narratives which is also very important for QA systems.
sent8: In the following subsections, we will review these characteristics in more detail.","1. What are narratives and how are they structured? sent1
    1.1. What is the main focus of narratives? sent2
    1.2. What types of texts are commonly used in narrative QA? sent3
    1.3. Why are narratives suitable for asking why-questions? sent4
    1.4. How do fictional narratives test comprehension in QA models? sent5
2. What are some key features of narratives that differentiate them from other types of texts? sent6
3. Why is the length of narratives important for QA systems? sent7
4. What will be reviewed in more detail in the following subsections? sent8"
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,Ensemble,9,"Ensemble learning is an effective strategy which combines diverse learners, enhancing the model performance comparing to a single learner (Zhou, 2012).Recent work achieved superior performance when using ensemble strategy on CoT prompting (Wang et al., 2022d;Li et al., 2022b;Wang et al., 2022e), which can help to correct errors made by individual reasoning process and integrate diverse prompts and demonstrations into a single prediction.Wang et al. (2022e) point out ensemble methods can even bring performance increment on tasks where vanilla CoT fails.However, unnecessary ensembles on problems which vanilla CoT can already effectively solve may inject noise to a confident prediction and instead do harm to model performance (Wang et al., 2022d).

To go a step further, what elements should be embraced into ensemble also matters a lot.According to different ensemble materials, we categorize these methods into prompts ensemble method and predictions ensemble method.Prompts ensemble focuses on the ensemble of results generated with various prompts.This method construct diverse CoT prompts by repeating sampling different demonstrations from exemplars set (Li et al., 2022b).Predictions ensemble focuses on integrating output space materials including rationales and answers.This method generates various predictions given a fixed input query by LLM sampling algorithms (Wang et al., 2022e;Fu et al., 2022;Yoran et al., 2023).It is found predictions ensemble may lead to more performance gain comparing to prompts ensemble (Wang et al., 2022d), but mul-tiple decoding for predictions ensemble may lead to higher computation cost.How to choose the ensemble strategy depends on the access of demonstrations number and computing resource.It's also possible to jointly combine two ensemble methods (Wang et al., 2022d).","sent1: Ensemble learning is an effective strategy which combines diverse learners, enhancing the model performance comparing to a single learner (Zhou, 2012).Recent work achieved superior performance when using ensemble strategy on CoT prompting (Wang et al., 2022d;Li et al., 2022b;Wang et al., 2022e), which can help to correct errors made by individual reasoning process and integrate diverse prompts and demonstrations into a single prediction.
sent2: Wang et al. (2022e) point out ensemble methods can even bring performance increment on tasks where vanilla CoT fails.
sent3: However, unnecessary ensembles on problems which vanilla CoT can already effectively solve may inject noise to a confident prediction and instead do harm to model performance (Wang et al., 2022d).
sent4: To go a step further, what elements should be embraced into ensemble also matters a lot.
sent5: According to different ensemble materials, we categorize these methods into prompts ensemble method and predictions ensemble method.
sent6: Prompts ensemble focuses on the ensemble of results generated with various prompts.
sent7: This method construct diverse CoT prompts by repeating sampling different demonstrations from exemplars set (Li et al., 2022b).Predictions ensemble focuses on integrating output space materials including rationales and answers.
sent8: This method generates various predictions given a fixed input query by LLM sampling algorithms (Wang et al., 2022e;Fu et al., 2022;Yoran et al., 2023).It is found predictions ensemble may lead to more performance gain comparing to prompts ensemble (Wang et al., 2022d), but mul-tiple decoding for predictions ensemble may lead to higher computation cost.
sent9: How to choose the ensemble strategy depends on the access of demonstrations number and computing resource.
sent10: It's also possible to jointly combine two ensemble methods (Wang et al., 2022d).","1. What is ensemble learning and how does it enhance model performance? sent1
    1.1. What benefits does the ensemble strategy provide in CoT prompting? sent1
    1.2. Can ensemble methods improve performance on tasks where vanilla CoT fails? sent2
    1.3. What potential downside is there to using unnecessary ensembles? sent3
2. What factors are important when considering what elements to include in an ensemble? sent4
    2.1. How are ensemble methods categorized based on different materials? sent5
        2.1.1. What is the focus of prompts ensemble methods? sent6
            2.1.1.1. How are diverse CoT prompts constructed in prompts ensemble methods? sent7
        2.1.2. What is the focus of predictions ensemble methods? sent7
            2.1.2.1. How are various predictions generated in predictions ensemble methods? sent8
            2.1.2.2. What are the advantages and disadvantages of predictions ensemble compared to prompts ensemble? sent8
3. What factors influence the choice of ensemble strategy? sent9
4. Is it possible to combine both ensemble methods? sent10"
56657817,Analysis Methods in Neural Language Processing: A Survey,https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,Methods,6,"The most common approach for associating neural network components with linguistic properties is to predict such properties from activations of the neural network. Typically, in this approach a neural network model is trained on some task (say, MT) and its weights are frozen. Then, the trained model is used for generating feature representations for another task by running it on a corpus with linguistic annotations and recording the representations (say, hidden state activations). Another classifier is then used for predicting the property of interest (say, part-of-speech [POS] tags). The performance of this classifier is used for evaluating the quality of the generated representations, and by proxy that of the original model. This kind of approach has been used in numerous papers in recent years; see Table SM1 for references. 5 It is referred to by various names, including ''auxiliary prediction tasks'' (Adi et al., 2017b), ''diagnostic classifiers'' (Veldhoen et al., 2016), and ''probing tasks' ' (Conneau et al., 2018).

As an example of this approach, let us walk through an application to analyzing syntax in neural machine translation (NMT) by Shi et al. (2016b). In this work, two NMT models were trained on standard parallel data-English→ French and English→German. The trained models (specifically, the encoders) were run on an annotated corpus and their hidden states were used for training a logistic regression classifier that predicts different syntactic properties. The authors concluded that the NMT encoders learn significant syntactic information at both word level and sentence level. They also compared representations at different encoding layers and found that ''local features are somehow preserved in the lower layer whereas more global, abstract information tends to be stored in the upper layer.'' These results demonstrate the kind of insights that the classification analysis may lead to, especially when comparing different models or model components.

Other methods for finding correspondences between parts of the neural network and certain properties include counting how often attention weights agree with a linguistic property like anaphora resolution (Voita et al., 2018) or directly computing correlations between neural network activations and some property; for example, correlating RNN state activations with depth in a syntactic tree (Qian et al., 2016a) or with Melfrequency cepstral coefficient (MFCC) acoustic features (Wu and King, 2016). Such correspondence may also be computed indirectly. For instance, Alishahi et al. (2017) defined an ABX discrimination task to evaluate how a neural model of speech (grounded in vision) encoded phonology. Given phoneme representations from different layers in their model, and three phonemes, A, B, and X, they compared whether the model representation for X is closer to A or B. This discrimination task enabled them to draw conclusions about which layers encoder phonology better, observing that lower layers generally encode more phonological information.","sent1: The most common approach for associating neural network components with linguistic properties is to predict such properties from activations of the neural network.
sent2: Typically, in this approach a neural network model is trained on some task (say, MT) and its weights are frozen.
sent3: Then, the trained model is used for generating feature representations for another task by running it on a corpus with linguistic annotations and recording the representations (say, hidden state activations).
sent4: Another classifier is then used for predicting the property of interest (say, part-of-speech [POS] tags).
sent5: The performance of this classifier is used for evaluating the quality of the generated representations, and by proxy that of the original model.
sent6: This kind of approach has been used in numerous papers in recent years; see Table SM1 for references.
sent7: 5 It is referred to by various names, including ''auxiliary prediction tasks'' (Adi et al., 2017b), ''diagnostic classifiers'' (Veldhoen et al., 2016), and ''probing tasks' '
sent8: (Conneau et al., 2018).As an example of this approach, let us walk through an application to analyzing syntax in neural machine translation (NMT) by Shi et al. (2016b).
sent9: In this work, two NMT models were trained on standard parallel data-English→ French and English→German.
sent10: The trained models (specifically, the encoders) were run on an annotated corpus and their hidden states were used for training a logistic regression classifier that predicts different syntactic properties.
sent11: The authors concluded that the NMT encoders learn significant syntactic information at both word level and sentence level.
sent12: They also compared representations at different encoding layers and found that ''local features are somehow preserved in the lower layer whereas more global, abstract information tends to be stored in the upper layer.''
sent13: These results demonstrate the kind of insights that the classification analysis may lead to, especially when comparing different models or model components.
sent14: Other methods for finding correspondences between parts of the neural network and certain properties include counting how often attention weights agree with a linguistic property like anaphora resolution (Voita et al., 2018) or directly computing correlations between neural network activations and some property; for example, correlating RNN state activations with depth in a syntactic tree (Qian et al., 2016a) or with Melfrequency cepstral coefficient (MFCC) acoustic features (Wu and King, 2016).
sent15: Such correspondence may also be computed indirectly.
sent16: For instance, Alishahi et al. (2017) defined an ABX discrimination task to evaluate how a neural model of speech (grounded in vision) encoded phonology.
sent17: Given phoneme representations from different layers in their model, and three phonemes, A, B, and X, they compared whether the model representation for X is closer to A or B.
sent18: This discrimination task enabled them to draw conclusions about which layers encoder phonology better, observing that lower layers generally encode more phonological information.","1. What is the most common approach for associating neural network components with linguistic properties? sent1
    1.1. How is a neural network model typically used in this approach? sent2
        1.1.1. How are feature representations generated for another task? sent3
        1.1.2. What is the role of another classifier in this approach? sent4
        1.1.3. How is the performance of the classifier used in this approach? sent5
    1.2. How prevalent is this approach in recent research? sent6
        1.2.1. What are some of the names by which this approach is known? sent7, sent8
    1.3. Can you provide an example of this approach applied to analyzing syntax in neural machine translation? sent9
        1.3.1. How were the NMT models used in this example? sent10
        1.3.2. What conclusions were drawn from this example? sent11
        1.3.3. What additional insights were gained from comparing representations at different encoding layers? sent12
        1.3.4. What do these results demonstrate about the classification analysis? sent13
2. What are other methods for finding correspondences between parts of the neural network and certain properties? sent14
    2.1. How can correspondence be computed indirectly? sent15
        2.1.1. Can you provide an example of an indirect method for evaluating neural models? sent16
            2.1.1.1. How was the ABX discrimination task used to evaluate phonology encoding? sent17
            2.1.1.2. What conclusions were drawn from the ABX discrimination task? sent18"
265928847,Fake account detection in social media using machine learning methods: literature review,https://www.semanticscholar.org/paper/3ae6eeba3711501965bb004760510f1979f4b52f,Reference,4,"Features selected Total Gupta and Kaushal [7] Received likes, likes, received comments, comments, tags, tag user, tags from other users, page tags, tags in comments, page tags in the comments section, tags by other users in the comments section, shared posts, wall posts, like wall posts, comments in wall posts, used applications.17 Elyusufi et al. [11] Status, followers, friends, favorites.

4 Reddy [23] Profile ID, name, status, followers, friends, location, account creation date, shares, gender, language.Wang et al. [31] The average of mentions, emojis, stop words, topics, links, retweets, similar posts, post length, forwarded posts, and punctuations.","sent1: Features selected Total Gupta and Kaushal [7]
sent2: Received likes, likes, received comments, comments, tags, tag user, tags from other users, page tags, tags in comments, page tags in the comments section, tags by other users in the comments section, shared posts, wall posts, like wall posts, comments in wall posts, used applications.17 Elyusufi et al. [11] Status, followers, friends, favorites.
sent3: 4 Reddy [23] Profile ID, name, status, followers, friends, location, account creation date, shares, gender, language.
sent4: Wang et al. [31] The average of mentions, emojis, stop words, topics, links, retweets, similar posts, post length, forwarded posts, and punctuations.","1. What features were selected by Gupta and Kaushal in their study? sent1
2. What features were considered by Elyusufi et al. in their research? sent2
3. What features did Reddy focus on in their study? sent3
4. What features did Wang et al. analyze in their research? sent4"
149117098,Article Respondent Robotics: Simulating Responses to Likert-Scale Survey Items,https://www.semanticscholar.org/paper/30d7c07ec6575bf8b1e7b132dbd2d5ae10c087ee,Semantics and Correlations,24,"Rensis Likert assumed that his scales delivered measures of attitude strength (Likert, 1932). Statistic modeling of such data in classic psychometrics viewed survey responses as basically composed of a true score and an error component. The error component of the score would reflect random influences on the response, and these could be minimized by averaging scores of semantically related questions for each variable (Nunnally & Bernstein, 2010). The error variance is assumed to converge around 0, making average scale scores a better expression of the true attitude strength of the respondents. The relationships among other surveyed variables should however not be determined by the semantics of the items, but instead only covary to the extent that they are empirically related. A frequent way of demonstrating this relative independence has been done by applying factor analytical techniques (Abdi, 2003;Hu & Bentler, 1999). In short, the prevalent psychometric practices have until now been treating the systematic variation among items as expression of attitude strength toward topics in the survey.

The STSR proposes a contrasting view. Here, the relationships among items and among survey variables are first and foremost semantic (Arnulf et al., 2014), a view corroborated by independent researchers (Nimon, Shuck, & Zigarmi, 2016). Every respondent may begin the survey by expressing attitude strength toward the surveyed topic in the form of a score on the Likert-type scale. However, in the succeeding responses, the scores on the coming items may be predominantly determined by the degree to which these items are semantically similar. This was earlier argued and documented by Feldman and Lynch (1988). A slightly different version of this hypothesis was also formulated by Schwarz (1999). However, both these precursors to STSR were speculating that calculation of responses may be exceptional to situations where people hold no real attitudes, or become unduly influenced in their response patterns by recent responses to other items. The formulation of STSR was the first claim that semantic calculation may actually be the fundamental mechanism explaining systematic variance among items.

Another antecedent to STSR is ""unfolding theory"" as described by Coombs (Coombs, 1964;Coombs & Kao, 1960) and later by Michell (1994). We will deal with unfolding theory in some detail as it has direct consequences for creating algorithms to mimic the real responses. A practical example may be a job satisfaction item, such as ""I like working here."" When respondents choose to answer this on a scale from 1 to 5, it may be hard to explain what the number means. To quantify an attitude, one could split the statement in discreet answering categories such as the extremely positive attitude: ""I would prefer working here to any other job or even leisure activity."" A neutral attitude could be a statement such as ""I do not care if I work here or not,"" or the negative statement ""I would take any other job to get away from this one."" The central point in unfolding theory is that any respondent's preferred response would be the point at which item response scale ""folds."" Folding implies that the response alternatives need to be sorted in their mutual distance from the preferred option. If someone picks the option 4 on a scale from 1 to 5, it would mean that the options 3 and 5 are about equally distant from 4, but that 2 and certainly 1 would be further away from the preferred statement. In this way, the scale is said to be ""folding"" around the preferred value 4, which determines the distance of all other responses from the folding point. Michell (1994) showed mathematically and experimentally that the quantitative properties of surveys stem from these semantic distinctions. Just as Coombs claimed, all respondents need to understand the common semantic properties-the meaning-of any survey item to attach numerical values to the questions in the survey. For two respondents to rate an item such as ""I like to work here"" with 1 or 5, they need to agree on the meaning of this response-the one respondent likes his job, the other does not, but both need to understand the meaning of the other response alternatives for one's own response to be quantitatively comparable. Michell showed how any survey scale needs to fold along a ""dominant path"" -the mutual meaning of items and response options used in a scale. This ""dominant path"" will affect the responses to other items if they are semantically related. Take the following simple example measuring job satisfaction and turnover intention, two commonly measured variables in OB research: One item measuring job satisfaction is the item ""I like working here,"" and one item measuring turnover intention is ""I will probably look for a new job in the next weeks."" A person who answers 5 to ""I like working here"" is by semantic implication less likely to look for a new job in the next week than someone who scores 1, and vice versa. Less obvious is the effect of what Michell called the ""dominant path"": If someone has a slightly positive attitude toward the job without giving it full score, this person will be slightly inclined, but maybe not determined, to turn down offers for a new job. The dominant path of such items will make the respondents rank the mutual answering alternatives in an ""unfolding way."" Not only are the extreme points of the Likert-type scales semantically linked but people also appear to rank the response option of all items in mutual order. A third item measuring organizational citizenship behavior (OCB), for example, is ""I frequently attend to problems that really are not part of my job."" The semantic specification of responses to this scale may be negative items such as ""I only do as little as possible so I don't get fired"" or positive items such as ""I feel capable and responsible for correcting any problem that may arise.""

According to unfolding theory, people will respond such that their response pattern is semantically coherent, that is, consistent with an unfolding of the semantic properties of items. The dominant path will prevent most people from choosing answer alternatives that are not semantically coherent.

Any survey will need a semantically invariant structure to attain reliably different but consistent responses from different people. Coombs and Kao showed experimentally that there is a necessary structure in all surveys emanating from how respondents commonly understand the survey (Coombs & Kao, 1960;Habing, Finch, & Roberts, 2005;Roysamb & Strype, 2002).

In STSR, correlations among survey items are primarily explained by the likelihood that they evoke similar meanings. As we will show below, the semantic relationships among survey items contain information isomorphic to the correlations among the same items in a survey. This implies that individual responses are shaped-and thereby principally computable-because the semantics of items are given and possible to estimate a priori to administering the survey.

To the extent that this is possible, current-day analytical techniques risk treating attitude strength as error variance. This is contrary to what is commonly believed, as the tradition of ""construct validation"" in survey research rests on the assumption that attitude strength across samples of respondents is the source of measures informing the empirical research (Bagozzi, 2011;Lamiell, 2013;MacKenzie, Podsakoff, & Podsakoff, 2011;Michell, 2013;Slaney, 2017;Slaney & Racine, 2013a, 2013b.

Other researchers have reported that the survey structure itself may create distinct factors for items that were originally devised as ""reversed"" or negatively phrased items (Roysamb & Strype, 2002;van Schuur & Kiers, 1994). One reason for this is the uncertain relationship between the actual measurements obtained from the survey and the assumed quantifiable nature of the latent construct in question. Kathleen Slaney's (2017) recent review of construct validation procedures shows how ""measurement"" of attitudes may come about by imposing numbers on an unknown structure. As shown by Andrew Maul (2017), acceptable psychometric properties of scales are obtainable even if keywords in the items are replaced by nonsensical words. The psychometric properties were largely retained even if the item texts were replaced by totally meaningless sentences or even by entirely empty items carrying nothing but response alternatives. The survey structure seems to be a powerful source of methods effects, imposing structure on response statistics.

The purpose here is to reconstruct survey responses using semantic information and other a priori known information about the survey structure. Semantic information about the semantic content of items is precisely void of knowledge about attitude strength. If this type of information can be used to create artificial responses with meaningful characteristics akin to the original ones, it will substantiate the claims of STSR. In particular, it will deliver empirical evidence that common psychometric practices may risk treating attitude strength as error variance, leaving mostly semantic relationships in the statistics. This attempt is exploratory in nature, and we will therefore not derive hypotheses but instead seek to explore the research question from various angles. The following exploration is undertaken as two independent studies: Study 1 is an in-depth study of the MLQ, containing the main procedures to investigate and explore. Study 2 is a brief application of the same procedure to a different, shorter scale, and another sample of respondents.","sent1: Rensis Likert assumed that his scales delivered measures of attitude strength (Likert, 1932).
sent2: Statistic modeling of such data in classic psychometrics viewed survey responses as basically composed of a true score and an error component.
sent3: The error component of the score would reflect random influences on the response, and these could be minimized by averaging scores of semantically related questions for each variable (Nunnally & Bernstein, 2010).
sent4: The error variance is assumed to converge around 0, making average scale scores a better expression of the true attitude strength of the respondents.
sent5: The relationships among other surveyed variables should however not be determined by the semantics of the items, but instead only covary to the extent that they are empirically related.
sent6: A frequent way of demonstrating this relative independence has been done by applying factor analytical techniques (Abdi, 2003;Hu & Bentler, 1999).
sent7: In short, the prevalent psychometric practices have until now been treating the systematic variation among items as expression of attitude strength toward topics in the survey.
sent8: The STSR proposes a contrasting view.
sent9: Here, the relationships among items and among survey variables are first and foremost semantic (Arnulf et al., 2014), a view corroborated by independent researchers (Nimon, Shuck, & Zigarmi, 2016).
sent10: Every respondent may begin the survey by expressing attitude strength toward the surveyed topic in the form of a score on the Likert-type scale.
sent11: However, in the succeeding responses, the scores on the coming items may be predominantly determined by the degree to which these items are semantically similar.
sent12: This was earlier argued and documented by Feldman and Lynch (1988).
sent13: A slightly different version of this hypothesis was also formulated by Schwarz (1999).
sent14: However, both these precursors to STSR were speculating that calculation of responses may be exceptional to situations where people hold no real attitudes, or become unduly influenced in their response patterns by recent responses to other items.
sent15: The formulation of STSR was the first claim that semantic calculation may actually be the fundamental mechanism explaining systematic variance among items.
sent16: Another antecedent to STSR is ""unfolding theory"" as described by Coombs (Coombs, 1964;Coombs & Kao, 1960) and later by Michell (1994).
sent17: We will deal with unfolding theory in some detail as it has direct consequences for creating algorithms to mimic the real responses.
sent18: A practical example may be a job satisfaction item, such as ""I like working here.""
sent19: When respondents choose to answer this on a scale from 1 to 5, it may be hard to explain what the number means.
sent20: To quantify an attitude, one could split the statement in discreet answering categories such as the extremely positive attitude: ""I would prefer working here to any other job or even leisure activity.""
sent21: A neutral attitude could be a statement such as ""I do not care if I work here or not,"" or the negative statement ""I would take any other job to get away from this one.""
sent22: The central point in unfolding theory is that any respondent's preferred response would be the point at which item response scale ""folds.""
sent23: Folding implies that the response alternatives need to be sorted in their mutual distance from the preferred option.
sent24: If someone picks the option 4 on a scale from 1 to 5, it would mean that the options 3 and 5 are about equally distant from 4, but that 2 and certainly 1 would be further away from the preferred statement.
sent25: In this way, the scale is said to be ""folding"" around the preferred value 4, which determines the distance of all other responses from the folding point.
sent26: Michell (1994) showed mathematically and experimentally that the quantitative properties of surveys stem from these semantic distinctions.
sent27: Just as Coombs claimed, all respondents need to understand the common semantic properties-the meaning-of any survey item to attach numerical values to the questions in the survey.
sent28: For two respondents to rate an item such as ""I like to work here"" with 1 or 5, they need to agree on the meaning of this response-the one respondent likes his job, the other does not, but both need to understand the meaning of the other response alternatives for one's own response to be quantitatively comparable.
sent29: Michell showed how any survey scale needs to fold along a ""dominant path"" -the mutual meaning of items and response options used in a scale.
sent30: This ""dominant path"" will affect the responses to other items if they are semantically related.
sent31: Take the following simple example measuring job satisfaction and turnover intention, two commonly measured variables in OB research: One item measuring job satisfaction is the item ""I like working here,"" and one item measuring turnover intention is ""I will probably look for a new job in the next weeks.""
sent32: A person who answers 5 to ""I like working here"" is by semantic implication less likely to look for a new job in the next week than someone who scores 1, and vice versa.
sent33: Less obvious is the effect of what Michell called the ""dominant path"": If someone has a slightly positive attitude toward the job without giving it full score, this person will be slightly inclined, but maybe not determined, to turn down offers for a new job.
sent34: The dominant path of such items will make the respondents rank the mutual answering alternatives in an ""unfolding way.""
sent35: Not only are the extreme points of the Likert-type scales semantically linked but people also appear to rank the response option of all items in mutual order.
sent36: A third item measuring organizational citizenship behavior (OCB), for example, is ""I frequently attend to problems that really are not part of my job.""
sent37: The semantic specification of responses to this scale may be negative items such as ""I only do as little as possible so I don't get fired"" or positive items such as ""I feel capable and responsible for correcting any problem that may arise.
sent38: ""According to unfolding theory, people will respond such that their response pattern is semantically coherent, that is, consistent with an unfolding of the semantic properties of items.
sent39: The dominant path will prevent most people from choosing answer alternatives that are not semantically coherent.
sent40: Any survey will need a semantically invariant structure to attain reliably different but consistent responses from different people.
sent41: Coombs and Kao showed experimentally that there is a necessary structure in all surveys emanating from how respondents commonly understand the survey (Coombs & Kao, 1960;Habing, Finch, & Roberts, 2005;Roysamb & Strype, 2002).
sent42: In STSR, correlations among survey items are primarily explained by the likelihood that they evoke similar meanings.
sent43: As we will show below, the semantic relationships among survey items contain information isomorphic to the correlations among the same items in a survey.
sent44: This implies that individual responses are shaped-and thereby principally computable-because the semantics of items are given and possible to estimate a priori to administering the survey.
sent45: To the extent that this is possible, current-day analytical techniques risk treating attitude strength as error variance.
sent46: This is contrary to what is commonly believed, as the tradition of ""construct validation"" in survey research rests on the assumption that attitude strength across samples of respondents is the source of measures informing the empirical research (Bagozzi, 2011;Lamiell, 2013;MacKenzie, Podsakoff, & Podsakoff, 2011;Michell, 2013;Slaney, 2017;Slaney & Racine, 2013a, 2013b.
sent47: Other researchers have reported that the survey structure itself may create distinct factors for items that were originally devised as ""reversed"" or negatively phrased items (Roysamb & Strype, 2002;van Schuur & Kiers, 1994).
sent48: One reason for this is the uncertain relationship between the actual measurements obtained from the survey and the assumed quantifiable nature of the latent construct in question.
sent49: Kathleen Slaney's (2017) recent review of construct validation procedures shows how ""measurement"" of attitudes may come about by imposing numbers on an unknown structure.
sent50: As shown by Andrew Maul (2017), acceptable psychometric properties of scales are obtainable even if keywords in the items are replaced by nonsensical words.
sent51: The psychometric properties were largely retained even if the item texts were replaced by totally meaningless sentences or even by entirely empty items carrying nothing but response alternatives.
sent52: The survey structure seems to be a powerful source of methods effects, imposing structure on response statistics.
sent53: The purpose here is to reconstruct survey responses using semantic information and other a priori known information about the survey structure.
sent54: Semantic information about the semantic content of items is precisely void of knowledge about attitude strength.
sent55: If this type of information can be used to create artificial responses with meaningful characteristics akin to the original ones, it will substantiate the claims of STSR.
sent56: In particular, it will deliver empirical evidence that common psychometric practices may risk treating attitude strength as error variance, leaving mostly semantic relationships in the statistics.
sent57: This attempt is exploratory in nature, and we will therefore not derive hypotheses but instead seek to explore the research question from various angles.
sent58: The following exploration is undertaken as two independent studies: Study 1 is an in-depth study of the MLQ, containing the main procedures to investigate and explore.
sent59: Study 2 is a brief application of the same procedure to a different, shorter scale, and another sample of respondents.","1. What did Rensis Likert assume about his scales? sent1
2. How is survey response data viewed in classic psychometrics? sent2
3. What does the error component of the score reflect, and how can it be minimized? sent3
4. What is assumed about the error variance in survey responses? sent4
5. How should relationships among surveyed variables be determined according to classic psychometrics? sent5
6. What method is frequently used to demonstrate the relative independence of surveyed variables? sent6
7. How have prevalent psychometric practices treated systematic variation among items? sent7
8. What contrasting view does the STSR propose? sent8
    8.1. How does the STSR view the relationships among items and survey variables? sent9
    8.2. How do respondents begin and continue their survey responses according to STSR? sent10, sent11
    8.3. Who earlier argued and documented the semantic influence on survey responses? sent12
    8.4. What did Schwarz (1999) formulate regarding survey responses? sent13
    8.5. What did precursors to STSR speculate about response calculation? sent14
    8.6. What is the fundamental mechanism proposed by STSR for explaining systematic variance among items? sent15
9. What is ""unfolding theory"" and who described it? sent16
    9.1. Why is unfolding theory relevant to creating algorithms for simulating responses? sent17
    9.2. Can you provide a practical example of unfolding theory in action? sent18, sent19, sent20, sent21
    9.3. What is the central point in unfolding theory? sent22
    9.4. How does unfolding theory describe the sorting of response alternatives? sent23, sent24, sent25
    9.5. What did Michell (1994) show about the quantitative properties of surveys? sent26
    9.6. What did Coombs claim about respondents' understanding of survey items? sent27
    9.7. How do respondents need to understand survey items to rate them quantitatively? sent28
    9.8. How does Michell describe the folding of survey scales? sent29
    9.9. How does the ""dominant path"" affect responses to semantically related items? sent30
10. How does unfolding theory apply to job satisfaction and turnover intention? sent31, sent32, sent33
    10.1. How does the dominant path influence respondents' ranking of answering alternatives? sent34, sent35
    10.2. How does unfolding theory apply to organizational citizenship behavior (OCB)? sent36, sent37, sent38
    10.3. What role does the dominant path play in ensuring semantically coherent responses? sent39
11. What is necessary for a survey to attain reliably different but consistent responses? sent40
12. What did Coombs and Kao show about the structure of surveys? sent41
13. How does STSR explain correlations among survey items? sent42
    13.1. What information do semantic relationships among survey items contain? sent43
    13.2. How are individual responses shaped according to STSR? sent44
    13.3. What risk do current-day analytical techniques face according to STSR? sent45
14. What is contrary to common beliefs about attitude strength in survey research? sent46
15. What have other researchers reported about survey structure and item phrasing? sent47
16. What is one reason for the uncertain relationship between survey measurements and latent constructs? sent48
17. What does Kathleen Slaney's review show about the measurement of attitudes? sent49
18. What did Andrew Maul demonstrate about psychometric properties of scales? sent50, sent51
19. How does survey structure affect response statistics? sent52
20. What is the purpose of reconstructing survey responses using semantic information? sent53
    20.1. What is semantic information void of? sent54
    20.2. What will using semantic information to create artificial responses substantiate? sent55
    20.3. What empirical evidence will this deliver regarding psychometric practices? sent56
21. What is the nature of the attempt to explore the research question? sent57
22. How is the exploration of the research question structured? sent58, sent59"
10137425,Multimodal Machine Learning: A Survey and Taxonomy,https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,Hybrid data,5,"In the hybrid data setting two non-parallel modalities are bridged by a shared modality or a dataset (see Figure  3c). The most notable example is the Bridge Correlational Neural Network [167], which uses a pivot modality to learn coordinated multimodal representations in presence of nonparallel data. For example, in the case of multilingual image captioning, the image modality would always be paired with at least one caption in any language. Such methods have also been used to bridge languages that might not have parallel corpora but have access to a shared pivot language, such as for machine translation [148], [167] and document transliteration [100].

Instead of using a separate modality for bridging, some methods rely on existence of large datasets from a similar or related task to lead to better performance in a task that only contains limited annotated data. Socher and Fei-Fei [189] use the existence of large text corpora in order to guide image segmentation. While Hendricks et al. [78] use separately trained visual model and a language model to lead to a better image and video description system, for which only limited data is available.","sent1: In the hybrid data setting two non-parallel modalities are bridged by a shared modality or a dataset (see Figure  3c).
sent2: The most notable example is the Bridge Correlational Neural Network [167], which uses a pivot modality to learn coordinated multimodal representations in presence of nonparallel data.
sent3: For example, in the case of multilingual image captioning, the image modality would always be paired with at least one caption in any language.
sent4: Such methods have also been used to bridge languages that might not have parallel corpora but have access to a shared pivot language, such as for machine translation [148], [167] and document transliteration [100].
sent5: Instead of using a separate modality for bridging, some methods rely on existence of large datasets from a similar or related task to lead to better performance in a task that only contains limited annotated data.
sent6: Socher and Fei-Fei [189] use the existence of large text corpora in order to guide image segmentation.
sent7: While Hendricks et al. [78] use separately trained visual model and a language model to lead to a better image and video description system, for which only limited data is available.","1. What is the hybrid data setting in multimodal machine learning? sent1
    1.1. What is a notable example of a method used in the hybrid data setting? sent2
        1.1.1. Can you provide an example of how the Bridge Correlational Neural Network is used? sent3
    1.2. How are languages without parallel corpora bridged in the hybrid data setting? sent4
    1.3. What alternative approach is used instead of a separate modality for bridging? sent5
        1.3.1. How do Socher and Fei-Fei utilize large datasets in their method? sent6
        1.3.2. How do Hendricks et al. improve image and video description systems with limited data? sent7"
264832783,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825,Pre-training Data,9,"Some studies have suggested that factors related to pre-traning data such as data domain, data term frequency, and data distribution (Chan et al., 2022;Razeghi et al., 2022), are crucial elements influencing the development of emergent abilities.

Data Domain Shin et al. (2022) conducted a study to explore the variations of ICL performance concerning the domain source and the size of the pre-training corpus, focusing primarily on the Korean lexicon.They utilized seven subcorpora from the HyperCLOVA corpus (Kim et al., 2021) to pretrain various language models and evaluated these models on Korean downstream tasks.Interestingly, Shin et al. (2022) found that the size of the pretraining corpus does not always determine the emergence of ICL.Instead, the domain source of the corpus significantly influences ICL performance.For example, language models trained with subcorpora constructed from blog posts exhibited the best ICL capability.This phenomenon may be attributed to the greater token diversity presented in the blog posts corpus compared with other sources like news.Moreover, their experiments highlighted that combining multiple corpora can lead to the emergence of ICL, even if individual corpora did not produce such learning on their own.Surprisingly, Shin et al. (2022) also found that a language model pre-trained with a corpus related to a downstream task did not always guarantee competitive ICL performance.For instance, a model trained on a news-related dataset (Park et al., 2021) showed superior performance in zero-shot news topic classification, but its few-shot performance was not superior.In a similar vein, The authors focused particularly on a crucial type of reasoning in LLMs -numerical reasoning in fewshot settings; and examined the extend to which the frequency of terms from the pre-training data correlates with model performance in these situations.Their analysis focused on the prevalence of numerical reasoning tasks within the training instances and established a connection between frequencies and reasoning performance.This connection is quantified by introducing the ""performance gap"", which is defined as the accuracy of terms appearing more than 90% of the time minus the accuracy of terms appearing less than 10% of the time.They conducted their experiments using GPTbased language models trained on the Pile dataset (Gao et al., 2021), ranging in size from 1.3B to 6B parameters.Evaluation was carried out on 11 datasets spanning three types of mathematical reasoning tasks: Arithmetic, Operation Inference and Time-Unit Conversion.The findings consistently show that models perform better in instances where terms from the pre-training data are more prevalent (Razeghi et al., 2022).In some scenarios, there is a substantial performance gap of over 70% between the most frequently occurring terms and the least frequently occurring terms.The significant performance difference raises questions about the actual generalization capabilities of these models beyond their pre-training data.Razeghi et al. (2022)'s observations suggest that the more prevalent content included in the pre-training data may exert an influ-ence on the emergent abilities, and it is possible that these language models are not actually reasoning to solve arithmetic tasks.In line with this research, Kandpal et al. (2023) (2023) theoretically demonstrated that unseen tasks can be efficiently learned via ICL when the pretraining data distribution comprises a mixture of latent tasks.","sent1: Some studies have suggested that factors related to pre-traning data such as data domain, data term frequency, and data distribution (Chan et al., 2022;Razeghi et al., 2022), are crucial elements influencing the development of emergent abilities.
sent2: Data Domain Shin et al. (2022) conducted a study to explore the variations of ICL performance concerning the domain source and the size of the pre-training corpus, focusing primarily on the Korean lexicon.
sent3: They utilized seven subcorpora from the HyperCLOVA corpus (Kim et al., 2021) to pretrain various language models and evaluated these models on Korean downstream tasks.
sent4: Interestingly, Shin et al. (2022) found that the size of the pretraining corpus does not always determine the emergence of ICL.Instead, the domain source of the corpus significantly influences ICL performance.
sent5: For example, language models trained with subcorpora constructed from blog posts exhibited the best ICL capability.
sent6: This phenomenon may be attributed to the greater token diversity presented in the blog posts corpus compared with other sources like news.
sent7: Moreover, their experiments highlighted that combining multiple corpora can lead to the emergence of ICL, even if individual corpora did not produce such learning on their own.
sent8: Surprisingly, Shin et al. (2022) also found that a language model pre-trained with a corpus related to a downstream task did not always guarantee competitive ICL performance.
sent9: For instance, a model trained on a news-related dataset (Park et al., 2021) showed superior performance in zero-shot news topic classification, but its few-shot performance was not superior.
sent10: In a similar vein, The authors focused particularly on a crucial type of reasoning in LLMs -numerical reasoning in fewshot settings; and examined the extend to which the frequency of terms from the pre-training data correlates with model performance in these situations.
sent11: Their analysis focused on the prevalence of numerical reasoning tasks within the training instances and established a connection between frequencies and reasoning performance.
sent12: This connection is quantified by introducing the ""performance gap"", which is defined as the accuracy of terms appearing more than 90% of the time minus the accuracy of terms appearing less than 10% of the time.
sent13: They conducted their experiments using GPTbased language models trained on the Pile dataset (Gao et al., 2021), ranging in size from 1.3B to 6B parameters.
sent14: Evaluation was carried out on 11 datasets spanning three types of mathematical reasoning tasks: Arithmetic, Operation Inference and Time-Unit Conversion.
sent15: The findings consistently show that models perform better in instances where terms from the pre-training data are more prevalent (Razeghi et al., 2022).In some scenarios, there is a substantial performance gap of over 70% between the most frequently occurring terms and the least frequently occurring terms.
sent16: The significant performance difference raises questions about the actual generalization capabilities of these models beyond their pre-training data.
sent17: Razeghi et al. (2022)'s observations suggest that the more prevalent content included in the pre-training data may exert an influ-ence on the emergent abilities, and it is possible that these language models are not actually reasoning to solve arithmetic tasks.
sent18: In line with this research, Kandpal et al. (2023) (2023) theoretically demonstrated that unseen tasks can be efficiently learned via ICL when the pretraining data distribution comprises a mixture of latent tasks.","1. What factors related to pre-training data influence the development of emergent abilities? sent1
2. What did Shin et al. (2022) study regarding data domain and pre-training corpus size? sent2
    2.1. What corpus did Shin et al. (2022) use for their study, and what was the focus of their evaluation? sent3
    2.2. What was a significant finding regarding the size of the pre-training corpus and ICL performance? sent4
        2.2.1. Can you provide an example of how the domain source of the corpus influences ICL capability? sent5
        2.2.2. What might explain the superior ICL capability of models trained on blog post corpora? sent6
    2.3. What did Shin et al. (2022) find about combining multiple corpora for ICL emergence? sent7
    2.4. What surprising result did Shin et al. (2022) find regarding pre-training with a corpus related to a downstream task? sent8
        2.4.1. Can you provide an example of a model's performance on a news-related dataset? sent9
3. What aspect of reasoning in LLMs did the authors focus on, and what did they examine? sent10
    3.1. What connection did their analysis establish regarding numerical reasoning tasks? sent11
    3.2. How is the ""performance gap"" defined in their study? sent12
    3.3. What dataset and model sizes were used in their experiments? sent13
    3.4. On what types of tasks was the evaluation carried out? sent14
    3.5. What consistent findings were observed regarding term prevalence and model performance? sent15
        3.5.1. What questions are raised by the significant performance difference observed? sent16
    3.6. What do Razeghi et al. (2022)'s observations suggest about the influence of pre-training data on emergent abilities? sent17
4. What did Kandpal et al. (2023) demonstrate regarding unseen tasks and pre-training data distribution? sent18"
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,Natural language description-based person retrieval,66,"Cross-modal retrieval-based applications are drawing attention due to the rapid growth of multimodal data like text, image, video, and audio. Features from different modalities like text and image are not directly comparable as they lie entirely in other spaces. Hence, it is a challenging problem due to the sizeable heterogeneous gap between different text and image modalities. One such issue of person retrieval from surveillance video using natural language description is in this section. Table 6 shows an overview of different methodologies for person retrieval using natural language descriptions.

Zhou et al. [89] develop an attention-based algorithm that localises a person in the surveillance frame using attributes and natural language query. The author annotated the cityscapes dataset [132] surveillance frame with attributes and descriptions because the dataset did not have natural language descriptions. Matrix representations of sentence expressions use the Skip-gram model [143]. Attributes and descriptions are by bidirectional Long-Short Term Memory (BLSTM) [130,131] network. Visual features extraction is done using Faster R-CNN [97] and ResNet152 [127] with the algorithm achieving 74.6% recall@1 i.e., 74.6% of the highest scoring box is correct. The Cityscape dataset contains only street views, i.e., frontal view camera. Hence, it does not cover various view challenges for surveillance. Also, the description annotated dataset is not available publicly and it limits the usability.

Li et al. [85] first published a large-scale person dataset with natural language description, CUHK-PEDES (publicly available), as discussed in Sect. 2.4. The author proposes a Recurrent Neural Network with a Gated Neural Attention mechanism (GNA-RNN) to establish the baseline on CUHK-PEDES. The network consists of a visual sub-network for visual feature extraction from an image and a language sub-network for textual feature extraction from a description. The visual sub-network consists of VGG-16 [133] as a backbone network and generates 512 graphical units. Each optical unit determines the existence of a specific appearance pattern. RNN with Long Short-Term Memory (LSTM) is useful in language sub-network, which takes words and images as input. It outputs unit level attention that decides which visual units should pay more attention to the word. The word-level gate determines the importance of the word, e.g., the word ""white""has more weightage than the word ""the"". The GNA-RNN network is trained in an end-to-end manner and provides a top-1 accuracy of 19.05%. This approach creates the baseline for further research in cross-modal person retrieval based on natural language descriptions. Li et al. [85] 2017 VGG-16 [133] LSTM [130] GNA-RNN [85] CUHK-PEDES [85] Top-1 acc.

(19.05%)

Li et al. [84] 2017 VGG-16 [133] LSTM [130], word2vec [144] IATV [84] CUHK-PEDES [85] Top-1 acc.

(25.94%)

Chen et al. [82] 2018

ResNet50 [127] LSTM [130] GLIA [82] CUHK-PEDES [85] Top-1 acc. Bi-GRU [145,146] MIA [135] CUHK-PEDES [85] Recall@1 (48.00%)

Zhang et al. [136] 2018

MobileNet [128] Bi-LSTM [130,131] CMPC + CMPM [136] CUHK-PEDES [85] Recall@1 (49.37%)

Wang et al. [137] 2019

MobileNet [128] Bi-LSTM [130,131] MCCL [137] CUHK-PEDES [85] Top-1 acc.

(50.58%)

Sarafianos et al. [138] 2019 ResNet-101 [127] BERT [148], LSTM [130] TIMAM [138] CUHK-PEDES [85] Top-1 acc.

(54.51%)

Aggarwal et al. [139] 2020

MobileNet [128] NLTK [149], Bi-LSTM [130,131] CMAAM [139] CUHK-PEDES The approaches discussed below are evaluated on this baseline. Most of the large-scale dataset contains identity level annotations considered by Li et al. [84] to match visual and textual domains. Identity-aware textual-visual matching is done in a two-stage network. Identity-level annotations are effectively utilized by introducing a Cross-Modal Cross-Entropy (CMCE) loss in the stage-1 network. The CMCE loss implicitly maximizes inter-identity feature distances and minimizes intra-identity feature distances. However, the coupling between visual and textual features generated through CMCE loss is loose. Hence, the initial matching of stage-1 is further refined by stage-2 CNN-LSTM with an underlying co-attention mechanism that produces the final textual-visual matching confidence. This two-stage framework achieves 25.94% of top-1 accuracy.

Global and local level image-language association (GLIA) is proposed by Chen et al. [82] to exploit the semantic information available in the description. The GLIA approach gains a significant boost to the top-1 accuracy from the baselines and achieves 43.58% top-1 accuracy. Zheng et al. [134] focus on the limitation of ranking loss. The authors do not explicitly consider the feature distribution in a single modality. They overcome it by considering the problem as instance-level retrieval and propose the instance loss. Each image-text query pair is regarded as an instance and observed as a class during training to learn finer granularity. Instead of considering pre-trained models for feature extraction, the method view adopts end-to-end learning from the data itself. Dual-path CNN-CNN i.e. Dual-Path Convolutional Image-Text Embedding (DPCE) architecture is proposed instead of the CNN-RNN approach for image-text matching. DPCE [134] achieves 44.40% of top-1 rank accuracy.

Multi-granularity Image-text Alignments (MIA) framework [135] adopts a multiple granularities (i.e., global-global, global-local, and local-local alignments) based approach for better similarity evaluations between text and image. The global context of image and description matches global-global granularity. On the other hand, relations between the global context and local components establishes the global-local alignment. Visual human parts fit with noun phrases in the final local-local granularity. The algorithm achieves 48.00% of recall@1. Zhang et al. [136] focus on learning discriminative features by proposing two loss functions, i.e., cross-modal projection matching (CMPM) loss and a cross-modal projection classification (CMPC) loss. Natural language description is first tokenized into words and processed sequentially using Bi-LSTM. Visual features are extractable from the last pooling layer of MobileNet [128]. The association module embeds visual and textual elements into a shared latent space. 49.37% of recall@1 is achieved by the algorithm while considering both CMPM and CMPC losses. Similar to work in [84], Wang et al. [137] also utilize identity-level information and propose Mutually Connected Classification Loss (MCCL). They first create a baseline approach before applying MCCL for better feature embedding. This baseline approach uses MobileNet pre-trained on ImageNet [147] for visual features, Bi-LSTM for textual element, and triplet loss function for cross-modal feature embedding. Triplet loss does not fully exploit feature distribution. The MCCL classifica-tion weight is shared between both modalities. Only the baseline approach with triplet loss achieves 45.55% of recall@1 while MCCL achieves 50.58% of recall@1.

The majority of approaches so far introduce a new loss function for the network to learn better feature representations without the complexity of textual phrases. For example, the word ""t-shirt""is an adjective, but it might be useful as a noun in the description. Such limitations go away by introducing a Text-Image Modality Adversarial Matching (TIMAM) framework [138]. Sarafianos et al. [138] propose adversarial representation learning, which helps bring features from different modalities very close. TIMAM attains top-1 accuracy of 54.51%. Aggarwal et al. [139] use attribute classification as an additional task and identity for bridging the gap between different modalities to improve representation learning. The method uses Deep Coral loss [150] to reduce the modality gap. They achieve state-of-the-art top-1 accuracy of 56.61% on CUHK-PEDES dataset. Thus, most of the work [82,84,85,134,135,136,137,138,139] shows cross-modal person retrieval on the only publicly available CHUHK-PEDES dataset except [89]. CUHK-PEDES contains only the image gallery of cropped persons, limiting practical usage in real-time scenarios to retrieve the person from the input of full surveillance frames.","sent1: Cross-modal retrieval-based applications are drawing attention due to the rapid growth of multimodal data like text, image, video, and audio.
sent2: Features from different modalities like text and image are not directly comparable as they lie entirely in other spaces.
sent3: Hence, it is a challenging problem due to the sizeable heterogeneous gap between different text and image modalities.
sent4: One such issue of person retrieval from surveillance video using natural language description is in this section.
sent5: Table 6 shows an overview of different methodologies for person retrieval using natural language descriptions.
sent6: Zhou et al. [89] develop an attention-based algorithm that localises a person in the surveillance frame using attributes and natural language query.
sent7: The author annotated the cityscapes dataset [132] surveillance frame with attributes and descriptions because the dataset did not have natural language descriptions.
sent8: Matrix representations of sentence expressions use the Skip-gram model [143].
sent9: Attributes and descriptions are by bidirectional Long-Short Term Memory (BLSTM) [130,131] network.
sent10: Visual features extraction is done using Faster R-CNN [97] and ResNet152 [127] with the algorithm achieving 74.6% recall@1 i.e., 74.6% of the highest scoring box is correct.
sent11: The Cityscape dataset contains only street views, i.e., frontal view camera.
sent12: Hence, it does not cover various view challenges for surveillance.
sent13: Also, the description annotated dataset is not available publicly and it limits the usability.
sent14: Li et al. [85] first published a large-scale person dataset with natural language description, CUHK-PEDES (publicly available), as discussed in Sect. 2.4.
sent15: The author proposes a Recurrent Neural Network with a Gated Neural Attention mechanism (GNA-RNN) to establish the baseline on CUHK-PEDES.
sent16: The network consists of a visual sub-network for visual feature extraction from an image and a language sub-network for textual feature extraction from a description.
sent17: The visual sub-network consists of VGG-16 [133] as a backbone network and generates 512 graphical units.
sent18: Each optical unit determines the existence of a specific appearance pattern.
sent19: RNN with Long Short-Term Memory (LSTM) is useful in language sub-network, which takes words and images as input.
sent20: It outputs unit level attention that decides which visual units should pay more attention to the word.
sent21: The word-level gate determines the importance of the word, e.g., the word ""white""has more weightage than the word ""the"".
sent22: The GNA-RNN network is trained in an end-to-end manner and provides a top-1 accuracy of 19.05%.
sent23: This approach creates the baseline for further research in cross-modal person retrieval based on natural language descriptions.
sent24: Li et al. [85] 2017 VGG-16 [133] LSTM [130] GNA-RNN [85] CUHK-PEDES [85]
sent25: Top-1 acc. (19.05%)Li et al. [84] 2017 VGG-16 [133] LSTM [130], word2vec [144] IATV [84] CUHK-PEDES [85]
sent26: Top-1 acc.(25.94%)Chen et al. [82] 2018ResNet50 [127] LSTM [130] GLIA [82] CUHK-PEDES [85]
sent27: Top-1 acc. Bi-GRU [145,146] MIA [135] CUHK-PEDES [85] Recall@1
sent28: (48.00%)Zhang et al. [136] 2018
sent29: MobileNet [128] Bi-LSTM [130,131] CMPC + CMPM [136] CUHK-PEDES [85] Recall@1 (49.37%)Wang et al. [137] 2019MobileNet [128] Bi-LSTM [130,131] MCCL [137] CUHK-PEDES [85]
sent30: Top-1 acc.(50.58%)Sarafianos et al. [138] 2019 ResNet-101 [127] BERT [148], LSTM [130] TIMAM [138] CUHK-PEDES [85]
sent31: Top-1 acc. (54.51%)Aggarwal et al. [139] 2020MobileNet [128] NLTK [149], Bi-LSTM [130,131] CMAAM [139] CUHK-PEDES
sent32: The approaches discussed below are evaluated on this baseline.
sent33: Most of the large-scale dataset contains identity level annotations considered by Li et al. [84] to match visual and textual domains.
sent34: Identity-aware textual-visual matching is done in a two-stage network.
sent35: Identity-level annotations are effectively utilized by introducing a Cross-Modal Cross-Entropy (CMCE) loss in the stage-1 network.
sent36: The CMCE loss implicitly maximizes inter-identity feature distances and minimizes intra-identity feature distances.
sent37: However, the coupling between visual and textual features generated through CMCE loss is loose.
sent38: Hence, the initial matching of stage-1 is further refined by stage-2 CNN-LSTM with an underlying co-attention mechanism that produces the final textual-visual matching confidence.
sent39: This two-stage framework achieves 25.94% of top-1 accuracy.
sent40: Global and local level image-language association (GLIA) is proposed by Chen et al. [82] to exploit the semantic information available in the description.
sent41: The GLIA approach gains a significant boost to the top-1 accuracy from the baselines and achieves 43.58% top-1 accuracy.
sent42: Zheng et al. [134] focus on the limitation of ranking loss.
sent43: The authors do not explicitly consider the feature distribution in a single modality.
sent44: They overcome it by considering the problem as instance-level retrieval and propose the instance loss.
sent45: Each image-text query pair is regarded as an instance and observed as a class during training to learn finer granularity.
sent46: Instead of considering pre-trained models for feature extraction, the method view adopts end-to-end learning from the data itself.
sent47: Dual-path CNN-CNN i.e. Dual-Path Convolutional Image-Text Embedding (DPCE) architecture is proposed instead of the CNN-RNN approach for image-text matching.
sent48: DPCE [134] achieves 44.40% of top-1 rank accuracy.
sent49: Multi-granularity Image-text Alignments (MIA) framework [135] adopts a multiple granularities (i.e., global-global, global-local, and local-local alignments) based approach for better similarity evaluations between text and image.
sent50: The global context of image and description matches global-global granularity.
sent51: On the other hand, relations between the global context and local components establishes the global-local alignment.
sent52: Visual human parts fit with noun phrases in the final local-local granularity.
sent53: The algorithm achieves 48.00% of recall@1.
sent54: Zhang et al. [136] focus on learning discriminative features by proposing two loss functions, i.e., cross-modal projection matching (CMPM) loss and a cross-modal projection classification (CMPC) loss.
sent55: Natural language description is first tokenized into words and processed sequentially using Bi-LSTM.
sent56: Visual features are extractable from the last pooling layer of MobileNet [128].
sent57: The association module embeds visual and textual elements into a shared latent space.
sent58: 49.37% of recall@1 is achieved by the algorithm while considering both CMPM and CMPC losses.
sent59: Similar to work in [84], Wang et al. [137] also utilize identity-level information and propose Mutually Connected Classification Loss (MCCL).
sent60: They first create a baseline approach before applying MCCL for better feature embedding.
sent61: This baseline approach uses MobileNet pre-trained on ImageNet [147] for visual features, Bi-LSTM for textual element, and triplet loss function for cross-modal feature embedding.
sent62: Triplet loss does not fully exploit feature distribution.
sent63: The MCCL classifica-tion weight is shared between both modalities.
sent64: Only the baseline approach with triplet loss achieves 45.55% of recall@1 while MCCL achieves 50.58% of recall@1.
sent65: The majority of approaches so far introduce a new loss function for the network to learn better feature representations without the complexity of textual phrases.
sent66: For example, the word ""t-shirt""is an adjective, but it might be useful as a noun in the description.
sent67: Such limitations go away by introducing a Text-Image Modality Adversarial Matching (TIMAM) framework [138].
sent68: Sarafianos et al. [138] propose adversarial representation learning, which helps bring features from different modalities very close.
sent69: TIMAM attains top-1 accuracy of 54.51%.
sent70: Aggarwal et al. [139] use attribute classification as an additional task and identity for bridging the gap between different modalities to improve representation learning.
sent71: The method uses Deep Coral loss [150] to reduce the modality gap.
sent72: They achieve state-of-the-art top-1 accuracy of 56.61% on CUHK-PEDES dataset.
sent73: Thus, most of the work [82,84,85,134,135,136,137,138,139] shows cross-modal person retrieval on the only publicly available CHUHK-PEDES dataset except [89].
sent74: CUHK-PEDES contains only the image gallery of cropped persons, limiting practical usage in real-time scenarios to retrieve the person from the input of full surveillance frames.","1. Why are cross-modal retrieval-based applications gaining attention? sent1
2. What challenge is associated with features from different modalities like text and image? sent2
3. What is the specific issue discussed in this section regarding person retrieval? sent4
4. What does Table 6 provide an overview of? sent5
5. What algorithm did Zhou et al. develop for person retrieval in surveillance? sent6
    5.1. Why did the author annotate the cityscapes dataset with attributes and descriptions? sent7
    5.2. What model is used for matrix representations of sentence expressions? sent8
    5.3. How are attributes and descriptions processed in the algorithm? sent9
    5.4. What methods are used for visual feature extraction in Zhou et al.'s algorithm? sent10
6. What limitation does the Cityscape dataset have for surveillance? sent11, sent12
7. What is a limitation of the description annotated dataset? sent13
8. What dataset did Li et al. first publish, and what does it include? sent14
    8.1. What baseline does Li et al. propose for CUHK-PEDES? sent15
    8.2. What are the components of the network proposed by Li et al.? sent16
        8.2.1. What does the visual sub-network consist of? sent17
        8.2.2. What is the role of each optical unit in the visual sub-network? sent18
        8.2.3. How is the language sub-network structured? sent19
        8.2.4. What does the unit level attention output determine? sent20
        8.2.5. How does the word-level gate function in the network? sent21
    8.3. What is the top-1 accuracy achieved by the GNA-RNN network? sent22
    8.4. What does this approach establish for future research? sent23
9. What is the focus of the approaches discussed below the baseline? sent32
10. How do most large-scale datasets handle identity level annotations? sent33
    10.1. How is identity-aware textual-visual matching performed? sent34
    10.2. What is the role of CMCE loss in the stage-1 network? sent35
    10.3. What is a limitation of the coupling between visual and textual features in CMCE loss? sent37
    10.4. How is the initial matching refined in the two-stage framework? sent38
    10.5. What top-1 accuracy does the two-stage framework achieve? sent39
11. What does the GLIA approach by Chen et al. aim to exploit? sent40
    11.1. What top-1 accuracy does the GLIA approach achieve? sent41
12. What limitation does Zheng et al. focus on, and how is it addressed? sent42, sent44
    12.1. How is each image-text query pair treated in Zheng et al.'s method? sent45
    12.2. What architecture is proposed instead of CNN-RNN for image-text matching? sent47
    12.3. What top-1 rank accuracy does DPCE achieve? sent48
13. What is the approach of the MIA framework for image-text alignments? sent49
    13.1. How does the MIA framework handle different granularities? sent50, sent51, sent52
    13.2. What recall@1 does the MIA framework achieve? sent53
14. What do Zhang et al. propose to learn discriminative features? sent54
    14.1. How is natural language description processed in Zhang et al.'s method? sent55
    14.2. How are visual features extracted in Zhang et al.'s method? sent56
    14.3. What is the role of the association module in Zhang et al.'s method? sent57
    14.4. What recall@1 is achieved by Zhang et al.'s algorithm? sent58
15. How does Wang et al. utilize identity-level information in their approach? sent59
    15.1. What is the baseline approach used before applying MCCL? sent60
    15.2. What limitation does triplet loss have in Wang et al.'s approach? sent62
    15.3. What recall@1 does MCCL achieve? sent64
16. What is a common strategy among the majority of approaches for learning feature representations? sent65
17. How does the TIMAM framework address limitations in textual phrases? sent67
    17.1. What does Sarafianos et al. propose for adversarial representation learning? sent68
    17.2. What top-1 accuracy does TIMAM achieve? sent69
18. How does Aggarwal et al. improve representation learning? sent70
    18.1. What method is used to reduce the modality gap in Aggarwal et al.'s approach? sent71
    18.2. What top-1 accuracy is achieved by Aggarwal et al.'s method? sent72
19. What is a common dataset used in cross-modal person retrieval research? sent73
20. What limitation does CUHK-PEDES have for real-time scenarios? sent74"
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,Ontology Enriched,5,"T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks. These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text. Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] . Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets. Here, pretraining involves three loss functions namely MLM, NSP, and triple classification. Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model. Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP. The novel multi-label loss function allows the model to connect all the words under the same CUI. CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss. Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge. SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss.  ","sent1: T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks.
sent2: These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text.
sent3: Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] .
sent4: Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets.
sent5: Here, pretraining involves three loss functions namely MLM, NSP, and triple classification.
sent6: Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model.
sent7: Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP.
sent8: The novel multi-label loss function allows the model to connect all the words under the same CUI.
sent9: CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss.
sent10: Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge.
sent11: SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss.","1. What are T-BPLMs and how have they performed in biomedical NLP tasks? sent1
    1.1. How do these models acquire domain-specific knowledge? sent2
    1.2. What limitation is noted about the knowledge acquired by these models? sent3
2. How are Clinical Kb-BERT and Clinical Kb-ALBERT obtained? sent4
    2.1. What does pretraining involve for these models? sent5
    2.2. What is the purpose of triple classification in pretraining? sent6
3. How is Umls-BERT initialized and further pretrained? sent7
    3.1. What is the function of the novel multi-label loss in Umls-BERT? sent8
4. How is CoderBERT initialized and further pretrained? sent9
    4.1. What are the roles of multi-similarity loss and knowledge embedding loss in CoderBERT? sent10
5. How is SapBERT initialized and further pretrained? sent11"
248530069,Deep Personality Trait Recognition: A Survey,https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc,Audio-Based Personality Trait Recognition,5,"The early-used audio features for automatic personality trait recognition are hand-crafted low-level descriptive (LLD) features, such as prosody (intensity, pitch), voice quality (formants), spectral features (Mel Frequency Cepstrum Coefficients, MFCCs), and so on. Specially, Mohammadi and Vinciarelli (2012) utilized the LLD features, such as pitch, formants, energy, and speaking rate to detect personality traits in audio clips with less than 10 s. They adopted Logistic Regression to identify whether an audio clip exceeded the average score for each of the Big-five personality traits. In , 6,373 acoustic-prosodic features like the Interspeech-2013 ComParE feature set (Schuller et al., 2013) were extracted as an input of the SVM classifier for identifying the Big-Five personality traits. In (Carbonneau et al., 2020), the authors learned a discriminating feature dictionary from the extracted patches in the speech spectrograms, followed by the SVM classifier for the classification of the Big-Five personality traits. The recently used audio features for automatic personality trait recognition are deep audio features extracted by deep learning techniques. Su et al. (2017) proposed to employ wavelet-based multiresolution analysis and CNNs for personality trait perception from speech signals. Figure 2 presents the details of the used CNN scheme. The wavelet transform was adopted to decompose the original speech signals at different levels of resolution. Then, based on the extracted prosodic acoustic features, CNNs were leveraged to produce the profiles of the Big-Five Inventory-10 (BFI-10) for a quantitative measure, followed by artificial neural networks (ANNs) for personality trait recognition. Hayat et al. (2019) fine-tuned a pretrained CNN model called AudioSet to learn an audio feature representation for predicting the Big-five personality trait scores of a speaker. They showed the advantages of CNN-based learned features over hand-crafted features.","sent1: The early-used audio features for automatic personality trait recognition are hand-crafted low-level descriptive (LLD) features, such as prosody (intensity, pitch), voice quality (formants), spectral features (Mel Frequency Cepstrum Coefficients, MFCCs), and so on.
sent2: Specially, Mohammadi and Vinciarelli (2012) utilized the LLD features, such as pitch, formants, energy, and speaking rate to detect personality traits in audio clips with less than 10 s.
sent3: They adopted Logistic Regression to identify whether an audio clip exceeded the average score for each of the Big-five personality traits.
sent4: In , 6,373 acoustic-prosodic features like the Interspeech-2013 ComParE feature set (Schuller et al., 2013) were extracted as an input of the SVM classifier for identifying the Big-Five personality traits.
sent5: In (Carbonneau et al., 2020), the authors learned a discriminating feature dictionary from the extracted patches in the speech spectrograms, followed by the SVM classifier for the classification of the Big-Five personality traits.
sent6: The recently used audio features for automatic personality trait recognition are deep audio features extracted by deep learning techniques.
sent7: Su et al. (2017) proposed to employ wavelet-based multiresolution analysis and CNNs for personality trait perception from speech signals.
sent8: Figure 2 presents the details of the used CNN scheme.
sent9: The wavelet transform was adopted to decompose the original speech signals at different levels of resolution.
sent10: Then, based on the extracted prosodic acoustic features, CNNs were leveraged to produce the profiles of the Big-Five Inventory-10 (BFI-10) for a quantitative measure, followed by artificial neural networks (ANNs) for personality trait recognition.
sent11: Hayat et al. (2019) fine-tuned a pretrained CNN model called AudioSet to learn an audio feature representation for predicting the Big-five personality trait scores of a speaker.
sent12: They showed the advantages of CNN-based learned features over hand-crafted features.","1. What were the early-used audio features for automatic personality trait recognition? sent1
    1.1. How did Mohammadi and Vinciarelli utilize LLD features for personality trait detection? sent2
    1.2. What method was used to identify personality traits in audio clips? sent3
    1.3. How were acoustic-prosodic features used in personality trait identification? sent4
    1.4. How did Carbonneau et al. approach the classification of Big-Five personality traits? sent5
2. What are the recently used audio features for automatic personality trait recognition? sent6
    2.1. How did Su et al. propose to use wavelet-based multiresolution analysis and CNNs for personality trait perception? sent7
        2.1.1. What details are presented in Figure 2 regarding the CNN scheme? sent8
        2.1.2. How was the wavelet transform used in Su et al.'s method? sent9
        2.1.3. How were CNNs and ANNs used in Su et al.'s approach? sent10
    2.2. How did Hayat et al. utilize a pretrained CNN model for personality trait prediction? sent11
        2.2.1. What advantages did CNN-based learned features have over hand-crafted features? sent12"
