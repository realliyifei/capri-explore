corpusid,title,domain,url,section_index,section_title,section,filtered_refids,filtered_refids_qualified,num_reference,section_sentence_prefixed
227061,Detection of User Demographics on Social Media: A Review of Methods and Recommendations for Best Practices,"Computer Science, Psychology, Sociology",https://www.semanticscholar.org/paper/43b55cc08eba13fb309e77a1033da0ad60bd7a5e,s7,Race and ethnicity.,"Of the papers that address race or ethnicity, eight used supervised learning approaches [35], [36], [52], [53], [53], [56], [83], [98], two used adjusted data matching [84], [99] and two relied on facial recognition [94], [95]. Some studies focused on user names or posts, or both to predict race or ethnicity [52], [53], [56], [83]. The classification performance among these paperswhile promising -did not vary significantly. Three studies obtained maximum F-scores between approximately 0.70 and 0.76 [36], [53], [54], and two obtained accuracy between 0.78 and 0.84 [35], [36].

Certain profile features could potentially improve predictions of race and ethnicity. First, profile photos appear to be a good indicator of race or ethnicity. Pennacchiotti and Popescu [53] for instance, obtained a higher precision (0.878) using profile photos to evaluate race, compared to a gradient boosted decision tree classifier that incorporated a combination of lexical features from users posts, and user activity measures (0.629). Second, user profile descriptions could improve methods for predicting race and ethnicity. For example, Chen et al. [36] observed that adding user descriptions into classifiers consistently improved accuracy, precision and recall for n-gram and name based models. Third, Chang et al. [86], and Mislove et al. [97] also found user location to improve predictability of surnames, and Mohammady and Culotta [56] used location as a feature for calibrating supervised learning models.

One challenge associated with the prediction of race and ethnicity is the need to create a clear, bounded definition. Racial and ethnic identity is complex and evaluations by others may not match an individuals' self-identification. Most of the selected studies were vague and varied significantly in their definition of race or ethnicity. Pennacchiotti and Popescu [53] claimed to predict ethnicity, but classified users as African American or not. Chang et al. [99] and Chen et al. [36] both used the term ethnicity to refer to a classification system that includes both racial and ethnic identities (black, white, Asian, Hispanic), and Mohammady and Culotta [56] used the same classification system but addressed it as race. Future work should directly address this challenge.","[['b52', 'b82', 'b55', 'b51', 'b97', 'b35', 'b53', 'b34', 'b83', 'b93', 'b98', 'b94'], ['b52', 'b85', 'b55', 'b96', 'b35'], ['b35', 'b52', 'b98', 'b55']]","[['b52', 'b82', 'b55', 'b51', 'b97', 'b35', 'b53', 'b34', 'b83', 'b93', 'b98', 'b94'], ['b52', 'b85', 'b55', 'b96', 'b35'], ['b35', 'b52', 'b98', 'b55']]",21,"sent1: Of the papers that address race or ethnicity, eight used supervised learning approaches [35], [36], [52], [53], [53], [56], [83], [98], two used adjusted data matching [84], [99] and two relied on facial recognition [94], [95].
sent2: Some studies focused on user names or posts, or both to predict race or ethnicity [52], [53], [56], [83].
sent3: The classification performance among these paperswhile promising -did not vary significantly.
sent4: Three studies obtained maximum F-scores between approximately 0.70 and 0.76 [36], [53], [54], and two obtained accuracy between 0.78 and 0.84 [35], [36].Certain profile features could potentially improve predictions of race and ethnicity.
sent5: First, profile photos appear to be a good indicator of race or ethnicity.
sent6: Pennacchiotti and Popescu [53] for instance, obtained a higher precision (0.878) using profile photos to evaluate race, compared to a gradient boosted decision tree classifier that incorporated a combination of lexical features from users posts, and user activity measures (0.629).
sent7: Second, user profile descriptions could improve methods for predicting race and ethnicity.
sent8: For example, Chen et al. [36] observed that adding user descriptions into classifiers consistently improved accuracy, precision and recall for n-gram and name based models.
sent9: Third, Chang et al. [86], and Mislove et al. [97] also found user location to improve predictability of surnames, and Mohammady and Culotta [56] used location as a feature for calibrating supervised learning models.
sent10: One challenge associated with the prediction of race and ethnicity is the need to create a clear, bounded definition.
sent11: Racial and ethnic identity is complex and evaluations by others may not match an individuals' self-identification.
sent12: Most of the selected studies were vague and varied significantly in their definition of race or ethnicity.
sent13: Pennacchiotti and Popescu [53] claimed to predict ethnicity, but classified users as African American or not.
sent14: Chang et al. [99] and Chen et al. [36] both used the term ethnicity to refer to a classification system that includes both racial and ethnic identities (black, white, Asian, Hispanic), and Mohammady and Culotta [56] used the same classification system but addressed it as race.
sent15: Future work should directly address this challenge."
227061,Detection of User Demographics on Social Media: A Review of Methods and Recommendations for Best Practices,"Computer Science, Psychology, Sociology",https://www.semanticscholar.org/paper/43b55cc08eba13fb309e77a1033da0ad60bd7a5e,s1,Social Media and Public Health Research,"Digital traces generated by social media users offer a wealth of data for public health research. For example, studies using these data have focused on attitudes toward tobacco use [2]- [6] and vaccines [7]- [10], the onset of postpartum depression [11], sleep disorders [12], suicide risk [13], patientperceived quality of care in hospitals [14], the dynamics of infectious diseases [15]- [21], neighborhood level trends in health, diet and weight loss [22], [23], mapping of the presence of food deserts [24], monitoring of foodborne illness [25]- [28], the geographic distribution of fitness activity (specifically, cycling) [29], population representation [30], [31] etc.

There are several benefits to using social media data for public health research. First, social media data provide real-time updates on users' thoughts, feelings and experiences, allowing researchers to track users' attitudes and behaviors as they emerge. Both the population and individual-level scale of these data create the opportunity to study behaviors that are difficult to assess through traditional means of data collection [32]. Second, because social media posts are unsolicited, users may report opinions and behaviors with greater fidelity than they would in the context of interviews or surveys. Welldocumented forms of response bias such as, social desirability bias, present in the context of surveys and interviews [33], may be weaker within social media spaces. Factors that influence the emergence of response bias (e.g., a desire to withhold poor behaviors from someone conducting a study) may be absent or lessened within social media spaces. Third, compared to surveys and other traditional means of data collection, these data are low cost and the process of data collection can be automated.

However, there are some limitations regarding the use of these data. For example, similar to traditional data streams, social media data are not always representative of the population of interest. While researchers have proposed methods for minimizing this limitation -such as probabilistically adjusting the data to match the population under study [34] -identifying and quantifying Big Data bias is challenging. This is partly due to a lack of demographic indicators -such as age, race and gender. The ability to accurately and reliably detect users' demographic traits would expand the use of social media as a research tool for social and behavioral sciences and public health. Additionally, linking demographic information to social media data would allow researchers to study disparities and trends in health-related attitudes and behaviors. It should be noted that there are privacy and ethical concerns associated with the use of social media data and the application of methods to infer user demographics, which we discuss in a later section.","[['b27', 'b14', 'b29', 'b10', 'b22', 'b20', 'b28', 'b5', 'b21', 'b1', 'b23', 'b9', 'b30', 'b11', 'b13', 'b12', 'b6', 'b24'], ['b31', 'b32'], ['b33']]","[['b27', 'b14', 'b29', 'b10', 'b22', 'b20', 'b28', 'b5', 'b21', 'b1', 'b23', 'b9', 'b30', 'b11', 'b13', 'b12', 'b6', 'b24'], ['b31', 'b32'], ['b33']]",21,"sent1: Digital traces generated by social media users offer a wealth of data for public health research.
sent2: For example, studies using these data have focused on attitudes toward tobacco use [2]- [6] and vaccines [7]- [10], the onset of postpartum depression [11], sleep disorders [12], suicide risk [13], patientperceived quality of care in hospitals [14], the dynamics of infectious diseases [15]- [21], neighborhood level trends in health, diet and weight loss [22], [23], mapping of the presence of food deserts [24], monitoring of foodborne illness [25]- [28], the geographic distribution of fitness activity (specifically, cycling) [29], population representation [30], [31] etc.There are several benefits to using social media data for public health research.
sent3: First, social media data provide real-time updates on users' thoughts, feelings and experiences, allowing researchers to track users' attitudes and behaviors as they emerge.
sent4: Both the population and individual-level scale of these data create the opportunity to study behaviors that are difficult to assess through traditional means of data collection [32].
sent5: Second, because social media posts are unsolicited, users may report opinions and behaviors with greater fidelity than they would in the context of interviews or surveys.
sent6: Welldocumented forms of response bias such as, social desirability bias, present in the context of surveys and interviews [33], may be weaker within social media spaces.
sent7: Factors that influence the emergence of response bias (e.g., a desire to withhold poor behaviors from someone conducting a study) may be absent or lessened within social media spaces.
sent8: Third, compared to surveys and other traditional means of data collection, these data are low cost and the process of data collection can be automated.
sent9: However, there are some limitations regarding the use of these data.
sent10: For example, similar to traditional data streams, social media data are not always representative of the population of interest.
sent11: While researchers have proposed methods for minimizing this limitation -such as probabilistically adjusting the data to match the population under study [34] -identifying and quantifying Big Data bias is challenging.
sent12: This is partly due to a lack of demographic indicators -such as age, race and gender.
sent13: The ability to accurately and reliably detect users' demographic traits would expand the use of social media as a research tool for social and behavioral sciences and public health.
sent14: Additionally, linking demographic information to social media data would allow researchers to study disparities and trends in health-related attitudes and behaviors.
sent15: It should be noted that there are privacy and ethical concerns associated with the use of social media data and the application of methods to infer user demographics, which we discuss in a later section."
227061,Detection of User Demographics on Social Media: A Review of Methods and Recommendations for Best Practices,"Computer Science, Psychology, Sociology",https://www.semanticscholar.org/paper/43b55cc08eba13fb309e77a1033da0ad60bd7a5e,s3,Data Extraction,"We queried Google Scholar during October of 2016 using combinations of methodological terms: ""prediction,"" ""classification,"" and ""machine learning""; terms related to user traits: ""age,"" ""race,"" ""ethnicity,"" ""gender"", and platform-related terms: ""social media"" and ""Twitter"" and ""blog."" On average, each search returned 121,120 articles. Within the first 20 pages of each search (ordered by relevance), an average of approximately 16 articles appeared relevant based on title and summary text. We then read through the abstracts of the potentially relevant articles and identified 60 studies that proposed methods for inferring or predicting users' demographic characteristics. All identified studies were published between 2006 to 2016. For each study, we extracted the year of publication, a brief description of the methods used, if and how the researchers obtained ground truth data, which metadata fields were used for classification, the precise features extracted from these metadata fields, the intended audience of the paper (i.e. computer science, social science, etc.) and the demographic characteristic detected (i.e. age, race/ethnicity or gender) (See Appendix Table 1). The 60 selected studies focused on different social media platforms; 39 (65%) studies focused on  Twitter, two on Facebook, two on Livejournal, one on Yelp, one on YouTube, and one on Pinterest. The remaining studies focused on other social media sites (e.g., Netlog, Fotolog) and blogs. Additionally, 44 (73%) studies explored supervised or semi-supervised machine learning methods, one (2%) used unsupervised learning, three (5%) used facial evaluation (human or automated), and 11 (18%) used raw or adjusted data matching. The machine learning methods considered include support vector machines [35]- [45], naïve Bayes [43], [46], modified balanced winnow [47], neural networks [48], maximum entropy [49], naïve Bayes decision tree hybrid [50], [51], gradient boosted decision trees [52], [53], as well as regularized linear, logistic or log-linear regression [43], [54]- [59]. Additional details on these methods can be found in the cited articles and in [60].","[['b52', 'b49', 'b50', 'b59', 'b45', 'b47', 'b51', 'b46', 'b48', 'b42', 'b58', 'b53', 'b34', 'b44']]","[['b52', 'b49', 'b50', 'b59', 'b45', 'b47', 'b51', 'b46', 'b48', 'b42', 'b58', 'b53', 'b34', 'b44']]",14,"sent1: We queried Google Scholar during October of 2016 using combinations of methodological terms: ""prediction,"" ""classification,"" and ""machine learning""; terms related to user traits: ""age,"" ""race,"" ""ethnicity,"" ""gender"", and platform-related terms: ""social media"" and ""Twitter"" and ""blog.""
sent2: On average, each search returned 121,120 articles.
sent3: Within the first 20 pages of each search (ordered by relevance), an average of approximately 16 articles appeared relevant based on title and summary text.
sent4: We then read through the abstracts of the potentially relevant articles and identified 60 studies that proposed methods for inferring or predicting users' demographic characteristics.
sent5: All identified studies were published between 2006 to 2016.
sent6: For each study, we extracted the year of publication, a brief description of the methods used, if and how the researchers obtained ground truth data, which metadata fields were used for classification, the precise features extracted from these metadata fields, the intended audience of the paper (i.e. computer science, social science, etc.) and the demographic characteristic detected (i.e. age, race/ethnicity or gender) (See Appendix Table 1).
sent7: The 60 selected studies focused on different social media platforms; 39 (65%) studies focused on  Twitter, two on Facebook, two on Livejournal, one on Yelp, one on YouTube, and one on Pinterest.
sent8: The remaining studies focused on other social media sites (e.g., Netlog, Fotolog) and blogs.
sent9: Additionally, 44 (73%) studies explored supervised or semi-supervised machine learning methods, one (2%) used unsupervised learning, three (5%) used facial evaluation (human or automated), and 11 (18%) used raw or adjusted data matching.
sent10: The machine learning methods considered include support vector machines [35]- [45], naïve Bayes [43], [46], modified balanced winnow [47], neural networks [48], maximum entropy [49], naïve Bayes decision tree hybrid [50], [51], gradient boosted decision trees [52], [53], as well as regularized linear, logistic or log-linear regression [43], [54]- [59].
sent11: Additional details on these methods can be found in the cited articles and in [60]."
227061,Detection of User Demographics on Social Media: A Review of Methods and Recommendations for Best Practices,"Computer Science, Psychology, Sociology",https://www.semanticscholar.org/paper/43b55cc08eba13fb309e77a1033da0ad60bd7a5e,s5,Predicted Traits,"Gender. Of the 60 studies selected, 47 (78%) predicted gender (see Table 1 in the Appendix). Gender in most cases was treated as a binary outcome. Most studies utilized users' posts/tweets, profile colors, names, profile images, social network and preferences (e.g., likes on Facebook) independently or in combination to infer gender. The selection of features used for predicting gender assume that gender is heavily embedded in peoples' identity and may be implicitly expressed through factors such as speech, choices of colors in one's profile [51], or stated interests [55]. Thus, unsupervised learning methods could be adequate for separating features distinct to the two gender classes [61].

Studies that utilized users' posts proposed classification methods aimed at identifying gender differences reflected in linguistic patterns [38], [40], [49], [62]- [83]. In contrast, studies that leveraged users' first names either used these data to link user profiles to ground truth survey data [84]- [86], or to develop and improve upon classifiers that incorporate other features [35], [38], [43], [50], [75], [83], [84], [87], [88]. Although some sites -such as Twitter -do not require users to provide a valid first and last name in their profile, this technique may still capture a large sample of users. Mislove et al. [84], for instance, found that 64.2% of Twitter users elect to include at least a first name in their profile, however about 71.8% were males. The precision, recall and F1-score varied across studies, with some achieving values in the 90s. However, few studies have focused on non-binary gender identity presentations. Bamman et al. [76] and Liu and Ruths [38] address this concept but additional work is needed.","[['b54', 'b50', 'b60'], ['b85', 'b49', 'b82', 'b37', 'b74', 'b75', 'b61', 'b42', 'b83', 'b34', 'b48', 'b39', 'b87', 'b86']]","[['b54', 'b50', 'b60'], ['b85', 'b49', 'b82', 'b37', 'b74', 'b75', 'b61', 'b42', 'b83', 'b34', 'b48', 'b39', 'b87', 'b86']]",17,"sent1: Gender. Of the 60 studies selected, 47 (78%) predicted gender (see Table 1 in the Appendix).
sent2: Gender in most cases was treated as a binary outcome.
sent3: Most studies utilized users' posts/tweets, profile colors, names, profile images, social network and preferences (e.g., likes on Facebook) independently or in combination to infer gender.
sent4: The selection of features used for predicting gender assume that gender is heavily embedded in peoples' identity and may be implicitly expressed through factors such as speech, choices of colors in one's profile [51], or stated interests [55].
sent5: Thus, unsupervised learning methods could be adequate for separating features distinct to the two gender classes [61].Studies that utilized users' posts proposed classification methods aimed at identifying gender differences reflected in linguistic patterns [38], [40], [49], [62]- [83].
sent6: In contrast, studies that leveraged users' first names either used these data to link user profiles to ground truth survey data [84]- [86], or to develop and improve upon classifiers that incorporate other features [35], [38], [43], [50], [75], [83], [84], [87], [88].
sent7: Although some sites -such as Twitter -do not require users to provide a valid first and last name in their profile, this technique may still capture a large sample of users.
sent8: Mislove et al. [84], for instance, found that 64.2% of Twitter users elect to include at least a first name in their profile, however about 71.8% were males.
sent9: The precision, recall and F1-score varied across studies, with some achieving values in the 90s.
sent10: However, few studies have focused on non-binary gender identity presentations.
sent11: Bamman et al. [76] and Liu and Ruths [38] address this concept but additional work is needed."
227061,Detection of User Demographics on Social Media: A Review of Methods and Recommendations for Best Practices,"Computer Science, Psychology, Sociology",https://www.semanticscholar.org/paper/43b55cc08eba13fb309e77a1033da0ad60bd7a5e,s6,Age.,"We identified 29 studies that aimed to predict age (see Table 1 in Appendix). Similar to gender, a variety of features -including profile photos, users' posts and names -have been used to infer age. Several of these studies use text features and supervised learning methods to predict exact numeric age, age category and/or life stage [44], [57], [58], [89]- [93]. Results of these studies indicated that age prediction is more challenging compared to gender. Nguyen et al. [90] suggested that text-dependent classification techniques may have a tendency to conflate numeric age with life stage, and rely on coarse predictions of age such as ""above or below 25"" [39].

Some studies have suggested that human labelers could be highly reliable when assessing a user's age range or category [94], but accuracy of this method based on ground truth data has yet to be determined and the labeling process could be cumbersome and costly. Studies by An and Weber [95] and Zagheni et al. [96] utilized facial recognition software Face ++ (www.faceplusplus.com) -a free facial recognition service that can estimate a user's age within a 10-year span -and found promising results. However, further work is needed to assess the reliability, accuracy and best applications of facial recognition tools.

Additionally, simple approaches -such as mentions of age or birthday within a user's post or matching first names to trends in popularity over time [97] -are promising but not widely employed within existing literature beyond the development of a training dataset [44], [62]. For the former, the available data might be limited to teenagers or milestone birthdays, and the latter could be limited to popular names.","[['b88', 'b43', 'b57', 'b56', 'b89', 'b38', 'b92'], ['b93', 'b95', 'b94'], ['b43', 'b96', 'b61']]","[['b88', 'b43', 'b57', 'b56', 'b89', 'b38', 'b92'], ['b93', 'b95', 'b94'], ['b43', 'b96', 'b61']]",13,"sent1: We identified 29 studies that aimed to predict age (see Table 1 in Appendix).
sent2: Similar to gender, a variety of features -including profile photos, users' posts and names -have been used to infer age.
sent3: Several of these studies use text features and supervised learning methods to predict exact numeric age, age category and/or life stage [44], [57], [58], [89]- [93].
sent4: Results of these studies indicated that age prediction is more challenging compared to gender.
sent5: Nguyen et al. [90] suggested that text-dependent classification techniques may have a tendency to conflate numeric age with life stage, and rely on coarse predictions of age such as ""above or below 25"" [39].
sent6: Some studies have suggested that human labelers could be highly reliable when assessing a user's age range or category [94], but accuracy of this method based on ground truth data has yet to be determined and the labeling process could be cumbersome and costly.
sent7: Studies by An and Weber [95] and Zagheni et al. [96] utilized facial recognition software Face ++ (www.faceplusplus.com) -a free facial recognition service that can estimate a user's age within a 10-year span -and found promising results.
sent8: However, further work is needed to assess the reliability, accuracy and best applications of facial recognition tools.
sent9: Additionally, simple approaches -such as mentions of age or birthday within a user's post or matching first names to trends in popularity over time [97] -are promising but not widely employed within existing literature beyond the development of a training dataset [44], [62].
sent10: For the former, the available data might be limited to teenagers or milestone birthdays, and the latter could be limited to popular names."
227061,Detection of User Demographics on Social Media: A Review of Methods and Recommendations for Best Practices,"Computer Science, Psychology, Sociology",https://www.semanticscholar.org/paper/43b55cc08eba13fb309e77a1033da0ad60bd7a5e,s10,Scalability and Efficiency,"Large quantities of Twitter metadata are required to apply many of the methods described in previous sections. Although having detailed user data -including data from users' posts and friendship networks -can improve the performance of demographic classifiers, the collection, storage, and analyses of such data can be costly. For example, many of the studies reviewed in section two are text-dependent and rely on users' posts -a strategy that Alowibdi et al. [51] described as ""inefficient"" and ""not-scalable"". Additionally, Bergsma et al. [35] stated that while having information on Twitter users' communication networks provide helpful details that amplify the predictive power of other profile indicators, this strategy is taxing on the Twitter API and may be unrealistic in some applications. For smaller studies, (i.e. sample sizes in the hundreds or thousands of users), these techniques might be relatively simple to implement. In contrast, those interested in broad topics such as US county-level exercise and wellbeing [102], may find these methods difficult or extremely time consuming to implement. Furthermore, researchers interested in studying time-sensitive phenomena -such as disease outbreaks [103] -may not have time to collect timeline or network data for all users within their data sample.

Challenges associated with scalability and efficiency may arise at several steps in the research processincluding data collection, processing and analysis. For data collection, these issues may be linked to rate limits associated with the data collection tools. For example, Twitter's public API which provides easy access to user metadata and tweets, includes measures designed to slow the collection of high volumes of data (see Table 1). Most calls to the Twitter API are limited to 180 calls per 15 minutes. This means that only 180 users' metadata can be gathered within 15 minutes. The rate limit for user timelines is higher -900 calls per 15 minutes -but each call may return only a portion of the timeline. To access data on users' ties, one must first pull the IDs of friends and followers, and then link metadata to these IDs -a time-consuming, two-step process. Overall, though it may be appealing to include fine-grained, comprehensive, detailed information about the user, if the dataset contains tens of thousands or millions of users this process could take days, weeks or months. When studying time-sensitive health processes, this delay could be problematic.

In data processing, the challenge of scalability and efficiency could refer to the time and computing power needed to extract and organize information from these data. While the exact number of posts needed to produce quality predictions may vary from one study to another, some report collecting as many as 1000 tweets per user [58], which could lead to the generation of a very large dataset. For example, the study on vaccine sentiments by Salathé and Kandelwal used a sample of 101,853 twitter users. If we collected 1000 tweets per users, this would result in a dataset of over 100 million tweets. Extracting and interpreting features from these data -including breaking these data into unigrams or identifying sociolinguistic indicators -an otherwise simple task -may become cumbersome when processing a dataset of this size.

Furthermore, text-dependent predictive models tend to be high-dimensional. Several of the models discussed in selected studies used unigrams as features, which present unique computational challenges. Burger et al. [30], for instance, developed a classifier that consists of 15.6 million features. These highdimensional models not only require intense computational power to execute, but run the risk of overfitting. ","[['b34', 'b50', 'b101'], [], ['b57'], ['b29']]","[['b34', 'b50', 'b101'], [], ['b57'], ['b29']]",5,"sent1: Large quantities of Twitter metadata are required to apply many of the methods described in previous sections.
sent2: Although having detailed user data -including data from users' posts and friendship networks -can improve the performance of demographic classifiers, the collection, storage, and analyses of such data can be costly.
sent3: For example, many of the studies reviewed in section two are text-dependent and rely on users' posts -a strategy that Alowibdi et al. [51] described as ""inefficient"" and ""not-scalable"".
sent4: Additionally, Bergsma et al. [35] stated that while having information on Twitter users' communication networks provide helpful details that amplify the predictive power of other profile indicators, this strategy is taxing on the Twitter API and may be unrealistic in some applications.
sent5: For smaller studies, (i.e. sample sizes in the hundreds or thousands of users), these techniques might be relatively simple to implement.
sent6: In contrast, those interested in broad topics such as US county-level exercise and wellbeing [102], may find these methods difficult or extremely time consuming to implement.
sent7: Furthermore, researchers interested in studying time-sensitive phenomena -such as disease outbreaks [103] -may not have time to collect timeline or network data for all users within their data sample.
sent8: Challenges associated with scalability and efficiency may arise at several steps in the research processincluding data collection, processing and analysis.
sent9: For data collection, these issues may be linked to rate limits associated with the data collection tools.
sent10: For example, Twitter's public API which provides easy access to user metadata and tweets, includes measures designed to slow the collection of high volumes of data (see Table 1).
sent11: Most calls to the Twitter API are limited to 180 calls per 15 minutes.
sent12: This means that only 180 users' metadata can be gathered within 15 minutes.
sent13: The rate limit for user timelines is higher -900 calls per 15 minutes -but each call may return only a portion of the timeline.
sent14: To access data on users' ties, one must first pull the IDs of friends and followers, and then link metadata to these IDs -a time-consuming, two-step process.
sent15: Overall, though it may be appealing to include fine-grained, comprehensive, detailed information about the user, if the dataset contains tens of thousands or millions of users this process could take days, weeks or months.
sent16: When studying time-sensitive health processes, this delay could be problematic.
sent17: In data processing, the challenge of scalability and efficiency could refer to the time and computing power needed to extract and organize information from these data.
sent18: While the exact number of posts needed to produce quality predictions may vary from one study to another, some report collecting as many as 1000 tweets per user [58], which could lead to the generation of a very large dataset.
sent19: For example, the study on vaccine sentiments by Salathé and Kandelwal used a sample of 101,853 twitter users.
sent20: If we collected 1000 tweets per users, this would result in a dataset of over 100 million tweets.
sent21: Extracting and interpreting features from these data -including breaking these data into unigrams or identifying sociolinguistic indicators -an otherwise simple task -may become cumbersome when processing a dataset of this size.
sent22: Furthermore, text-dependent predictive models tend to be high-dimensional.
sent23: Several of the models discussed in selected studies used unigrams as features, which present unique computational challenges.
sent24: Burger et al. [30], for instance, developed a classifier that consists of 15.6 million features.
sent25: These highdimensional models not only require intense computational power to execute, but run the risk of overfitting."
227061,Detection of User Demographics on Social Media: A Review of Methods and Recommendations for Best Practices,"Computer Science, Psychology, Sociology",https://www.semanticscholar.org/paper/43b55cc08eba13fb309e77a1033da0ad60bd7a5e,s11,Efficient and Scalable Approaches to Demographic Prediction,"A number of studies have attempted to identify user demographics without using computationally intense methods and metadata ( Table 2). Some of these studies rely on detecting explicitly shared information (i.e. stating one's age) or matching metadata to ground truth information. For example, Mislove et al. [84] inferred US-based Twitter users' gender by matching first names to census data. Sloane et al. [104] determined user age by finding mentions or references to age within users' profile descriptions (e.g. searching for phrases such as ""years old""). Longley, Adnan and Lansley [86] proposed a process of parsing user names to extract embedded information on gender and ethnicity. The primary advantages of using a data matching approach is that it depends on simple, often easy to capture user information and allows researchers to categorize users with a high degree of certainty. However, this method excludes users who elect not to share directly identifiable information such as, real name or age. Longley and colleagues, for example, found that about 68% of users in their sample had an identifiable first and/or surname, and that only 47% of users were accurately classified as male or female using this metadata [86]. Though relying on this method may produce incomplete and biased data, researchers suggest that these techniques are effective for capturing a large swath of social media users.

For traits, such as race/ethnicity, the ground truth data matched to simple user metadata -such as names -may be overrepresented by a majority group. Some studies have proposed methods for overcoming this limitation. Chang et al. [99] and Mislove et al. [84] used Bayesian estimation to predict users' ethnicity given: a.) racial/ethnic distribution of the website and b.) distribution of first and last names within each race/ethnicity as indicated by census data. Oktay, Firat and Ertem [97] built upon the work by Chang et al. [99] by expanding their methods to accommodate the detection of age.

There is also a small body of work that relies on text-independent machine learning techniques for classifying users. Using Naïve Bayes Decision Tree classification, Alowibdi, Buy and Yu [50], [51] utilized profile colors to detect user gender with 71% accuracy and phonemes/n-grams within user names to predict gender with 83% accuracy. Bergsma et al. [35] used location and name metadata to predict gender and race. Features in their model consisted of the presence or absence of location and name features, the n-gram length of these features, and a cluster ID value based on users' names and communication. Though the presence of the cluster ID value improved their classifier, they obtained high accuracy for predicting gender (89.5), and race (82.4) without these features as well. Kosinski et al. [55] categorized users on Facebook according to what and who they ""like."" Whereas this may be difficult to replicate in contexts such as Twitter, where ""likes"" may be analogous to ""follows,"" which requires the collection of network information, this strategy may be used within social media contexts structured similarly to Facebook. In contrast to supervised learning approaches, Vicente et al. [43] applied fuzzy c-means clustering to user names to predict user gender with up to 96% accuracy Finally, work using automated facial detection [95], [96] and manual facial classification (e.g., employing workers on Amazon's Mechanical Turk) [94] has provided promising estimates of Twitter users' age, race and gender. While the scalability of crowdsourced methods is contingent upon the budget of the researcher, it nonetheless demonstrates efficiency as it requires only one piece of user metadata to process. Furthermore, additional work is needed to assess the ground truth accuracy of these methods.

There is obviously an imbalance in methods available to predict race/ethnicity and age versus gender. The latent factors that drive gender identity expression are strong and may manifest in a variety of ways, including seemingly peripheral factors such as profile color [51]. Whereas existing work has found a number of efficient and scalable gender-classification methods, the same cannot be said of race/ethnicity and age. As was described in section 2.2, the automated detection of age and race/ethnicity presents unique challenges. Racial identity is complex and may not match perceived race, and numeric age may be conflated with life stage [93]. Further exploration is needed to determine how these challenges can be overcome with less costly user metadata. ","[['b85', 'b83', 'b103'], ['b96', 'b83', 'b98'], ['b50', 'b49', 'b95', 'b42', 'b54', 'b34', 'b93', 'b94'], ['b92', 'b50']]","[['b85', 'b83', 'b103'], ['b96', 'b83', 'b98'], ['b50', 'b49', 'b95', 'b42', 'b54', 'b34', 'b93', 'b94'], ['b92', 'b50']]",16,"sent1: A number of studies have attempted to identify user demographics without using computationally intense methods and metadata ( Table 2).
sent2: Some of these studies rely on detecting explicitly shared information (i.e. stating one's age) or matching metadata to ground truth information.
sent3: For example, Mislove et al. [84] inferred US-based Twitter users' gender by matching first names to census data.
sent4: Sloane et al. [104] determined user age by finding mentions or references to age within users' profile descriptions
sent5: (e.g. searching for phrases such as ""years old"").
sent6: Longley, Adnan and Lansley [86] proposed a process of parsing user names to extract embedded information on gender and ethnicity.
sent7: The primary advantages of using a data matching approach is that it depends on simple, often easy to capture user information and allows researchers to categorize users with a high degree of certainty.
sent8: However, this method excludes users who elect not to share directly identifiable information such as, real name or age.
sent9: Longley and colleagues, for example, found that about 68% of users in their sample had an identifiable first and/or surname, and that only 47% of users were accurately classified as male or female using this metadata [86].
sent10: Though relying on this method may produce incomplete and biased data, researchers suggest that these techniques are effective for capturing a large swath of social media users.
sent11: For traits, such as race/ethnicity, the ground truth data matched to simple user metadata -such as names -may be overrepresented by a majority group.
sent12: Some studies have proposed methods for overcoming this limitation.
sent13: Chang et al. [99] and Mislove et al. [84] used Bayesian estimation to predict users' ethnicity given: a.)
sent14: racial/ethnic distribution of the website and b.) distribution of first and last names within each race/ethnicity as indicated by census data.
sent15: Oktay, Firat and Ertem [97] built upon the work by Chang et al. [99] by expanding their methods to accommodate the detection of age.
sent16: There is also a small body of work that relies on text-independent machine learning techniques for classifying users.
sent17: Using Naïve Bayes Decision Tree classification, Alowibdi, Buy and Yu [50], [51] utilized profile colors to detect user gender with 71% accuracy and phonemes/n-grams within user names to predict gender with 83% accuracy.
sent18: Bergsma et al. [35] used location and name metadata to predict gender and race.
sent19: Features in their model consisted of the presence or absence of location and name features, the n-gram length of these features, and a cluster ID value based on users' names and communication.
sent20: Though the presence of the cluster ID value improved their classifier, they obtained high accuracy for predicting gender (89.5), and race (82.4) without these features as well.
sent21: Kosinski et al. [55] categorized users on Facebook according to what and who they ""like.""
sent22: Whereas this may be difficult to replicate in contexts such as Twitter, where ""likes"" may be analogous to ""follows,"" which requires the collection of network information, this strategy may be used within social media contexts structured similarly to Facebook.
sent23: In contrast to supervised learning approaches, Vicente et al. [43] applied fuzzy c-means clustering to user names to predict user gender with up to 96% accuracy Finally, work using automated facial detection [95], [96] and manual facial classification (e.g., employing workers on Amazon's Mechanical Turk) [94] has provided promising estimates of Twitter users' age, race and gender.
sent24: While the scalability of crowdsourced methods is contingent upon the budget of the researcher, it nonetheless demonstrates efficiency as it requires only one piece of user metadata to process.
sent25: Furthermore, additional work is needed to assess the ground truth accuracy of these methods.
sent26: There is obviously an imbalance in methods available to predict race/ethnicity and age versus gender.
sent27: The latent factors that drive gender identity expression are strong and may manifest in a variety of ways, including seemingly peripheral factors such as profile color [51].
sent28: Whereas existing work has found a number of efficient and scalable gender-classification methods, the same cannot be said of race/ethnicity and age.
sent29: As was described in section 2.2, the automated detection of age and race/ethnicity presents unique challenges.
sent30: Racial identity is complex and may not match perceived race, and numeric age may be conflated with life stage [93].
sent31: Further exploration is needed to determine how these challenges can be overcome with less costly user metadata."
227061,Detection of User Demographics on Social Media: A Review of Methods and Recommendations for Best Practices,"Computer Science, Psychology, Sociology",https://www.semanticscholar.org/paper/43b55cc08eba13fb309e77a1033da0ad60bd7a5e,s14,Privacy and Ethical Concerns,"We acknowledge that there are privacy and ethical concerns associated with the use of social media data for research (noted in several publications [105]- [108]) and the development of tools for inferring user demographics. A major concern is the treatment of human subjects since research subjects no longer 'participate' in studies in a traditional sense [108]. This requires researchers to rethink how to implement standard ethical practices such as informed consent and anticipation of risks and harms. Currently, data ownership and within-site privacy guidelines help determine some research permissions. However, researchers should consider obtaining IRB approval for digital datasets [106], not linking social media data to other online information, and respecting the context -both cultural and situational [108] -in which the data were generated.

Ethical data use should also be considered in the publication and implementation of research results. As Boyd and Crawford [107] suggest, the practice and dissemination of Big Data research raises important questions about truth, power and control. Researchers may address these questions thoughtfully not only within the context of data collection and use, but in regard to study results and the relationship between these results and the perpetuation of inequality. It is important that researchers balance respect for privacy with research transparency and reproducibility, and ensure that findings are correct, reliable and interpretable for the public [105].

Given the breadth of ambiguity regarding the ethical use of social media data we urge researchers who use these data to closely follow the rapidly changing ethical landscape of social media data use, and to exercise beneficence when engaging in data use that does not fall neatly within existing institutional review board guidelines, or for which standard data management and analysis strategies do not exist.","[['b107', 'b104', 'b105'], ['b104', 'b106'], []]","[['b107', 'b104', 'b105'], ['b104', 'b106'], []]",5,"sent1: We acknowledge that there are privacy and ethical concerns associated with the use of social media data for research (noted in several publications [105]- [108]) and the development of tools for inferring user demographics.
sent2: A major concern is the treatment of human subjects since research subjects no longer 'participate' in studies in a traditional sense [108].
sent3: This requires researchers to rethink how to implement standard ethical practices such as informed consent and anticipation of risks and harms.
sent4: Currently, data ownership and within-site privacy guidelines help determine some research permissions.
sent5: However, researchers should consider obtaining IRB approval for digital datasets [106], not linking social media data to other online information, and respecting the context -both cultural and situational [108] -in which the data were generated.
sent6: Ethical data use should also be considered in the publication and implementation of research results.
sent7: As Boyd and Crawford [107] suggest, the practice and dissemination of Big Data research raises important questions about truth, power and control.
sent8: Researchers may address these questions thoughtfully not only within the context of data collection and use, but in regard to study results and the relationship between these results and the perpetuation of inequality.
sent9: It is important that researchers balance respect for privacy with research transparency and reproducibility, and ensure that findings are correct, reliable and interpretable for the public [105].
sent10: Given the breadth of ambiguity regarding the ethical use of social media data we urge researchers who use these data to closely follow the rapidly changing ethical landscape of social media data use, and to exercise beneficence when engaging in data use that does not fall neatly within existing institutional review board guidelines, or for which standard data management and analysis strategies do not exist."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s21,Network characteristics.,"Rumors and hoaxes can be related to other information in terms of what they say about others and what others say about it. Kumar et al. [50] quantified this for hoaxes on Wikipedia by measuring the connectedness of the different Wikipedia articles referenced in the hoax article. Intuitively, high connectedness indicates interrelated and coherent references. The authors computed the clustering coefficient of the local hyperlink network of the article, i.e., the average clustering coefficient of the subnetwork induced by the articles referenced by the article. They found that hoax information has fewer references and significantly lower clustering coefficient compared to non-hoax articles. This suggests that references in hoaxes are added primarily to appear genuine, instead of adding them by need as legitimate writers do.

Network characteristics of rumors are studied by analyzing the network of users that spread them and by creating co-occurrence networks out of false information tweets-these contain nodes of one or more types, such as URLs, domains, user accounts or hashtags, and use edges to represent the number of times they are mentioned in the same tweet together. Using the user-user network, Subrahmanian et al. [97] found that some bot accounts that spread false information are close to each other and appear as groups in Twitter's follower-followee network, with significant overlap between their followers and followees. Moreover, Bessi et al. [13] conducted a k-core analysis of this follower-followee network and found that the fraction of bots increases steadily in higher cores, suggests that bots become increasingly central in the rebroadcasting network. Using the co-occurrence network, Fig. 10. Cascade of reshares of a Cabela's sporting goods store receipt attributing addition sales tax to ""Obamacare"". The coloring is from early (red) to late (blue). Reprinted with permission from [30].

Starbird [95] found that alternate media (false news) domains form tightly connected clusters, meaning that many users mention these domains together in their false information tweets.","[['b59'], ['b39', 'b106', 'b22'], ['b104']]","[['b59'], ['b39', 'b106', 'b22'], ['b104']]",5,"sent1: Rumors and hoaxes can be related to other information in terms of what they say about others and what others say about it.
sent2: Kumar et al. [50] quantified this for hoaxes on Wikipedia by measuring the connectedness of the different Wikipedia articles referenced in the hoax article.
sent3: Intuitively, high connectedness indicates interrelated and coherent references.
sent4: The authors computed the clustering coefficient of the local hyperlink network of the article, i.e., the average clustering coefficient of the subnetwork induced by the articles referenced by the article.
sent5: They found that hoax information has fewer references and significantly lower clustering coefficient compared to non-hoax articles.
sent6: This suggests that references in hoaxes are added primarily to appear genuine, instead of adding them by need as legitimate writers do.
sent7: Network characteristics of rumors are studied by analyzing the network of users that spread them and by creating co-occurrence networks out of false information tweets-these contain nodes of one or more types, such as URLs, domains, user accounts or hashtags, and use edges to represent the number of times they are mentioned in the same tweet together.
sent8: Using the user-user network, Subrahmanian et al. [97] found that some bot accounts that spread false information are close to each other and appear as groups in Twitter's follower-followee network, with significant overlap between their followers and followees.
sent9: Moreover, Bessi et al. [13] conducted a k-core analysis of this follower-followee network and found that the fraction of bots increases steadily in higher cores, suggests that bots become increasingly central in the rebroadcasting network.
sent10: Using the co-occurrence network, Fig. 10.
sent11: Cascade of reshares of a Cabela's sporting goods store receipt attributing addition sales tax to ""Obamacare"".
sent12: The coloring is from early (red) to late (blue).
sent13: Reprinted with permission from [30].
sent14: Starbird [95] found that alternate media (false news) domains form tightly connected clusters, meaning that many users mention these domains together in their false information tweets."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s20,User characteristics.,"Several studies have shown that the characteristics of creators of false information are different from those of true information creators. Kumar et al. [50] found that the creators of hoaxes have typically more recently registered accounts and less editing experience (Figure 9(c)), suggesting the use of ""throw-away"" accounts. Surprisingly, non-hoax articles that are wrongly assumed to be hoaxes were also created by similar editors, meaning that they lack the skills to create well-written articles, which leads to others believing that the article is a hoax.

In cases of rumors, Bessi et al. [13] studied over 20.7 million tweets related to US presidential election, and identified users involved in tweeting as bots or honest users using a classification tool produced by Davis et al. [21]. Their analysis found that about one-fifth of content created and spread was by bots, showing that rumors are spread by automated accounts in short-bursts of time. Shao et al. [87] came to similar conclusions in their experiments.","[['b59'], ['b30', 'b96', 'b22']]","[['b59'], ['b30', 'b96', 'b22']]",4,"sent1: Several studies have shown that the characteristics of creators of false information are different from those of true information creators.
sent2: Kumar et al. [50] found that the creators of hoaxes have typically more recently registered accounts and less editing experience (Figure 9(c)), suggesting the use of ""throw-away"" accounts.
sent3: Surprisingly, non-hoax articles that are wrongly assumed to be hoaxes were also created by similar editors, meaning that they lack the skills to create well-written articles, which leads to others believing that the article is a hoax.
sent4: In cases of rumors, Bessi et al. [13] studied over 20.7 million tweets related to US presidential election, and identified users involved in tweeting as bots or honest users using a classification tool produced by Davis et al. [21].
sent5: Their analysis found that about one-fifth of content created and spread was by bots, showing that rumors are spread by automated accounts in short-bursts of time.
sent6: Shao et al. [87] came to similar conclusions in their experiments."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s19,Textual characteristics.,"There is a vast literature that studies fake news in social media. False information in the form of fake news is created in such a way to invoke interest and/or be believable to consumers. Various strategies may be used to deceive these consumers. Silverman [91] found that about 13% of over 1600 news articles had incoherent headline and content body, for example, by using declarative headlines paired with bodies which are skeptical about the veracity of the information.

In a recent paper, Horne and Adali [37] studied the textual characteristics of fake news using several sources of data: Buzzfeed fake news analysis articles [92], and articles from well known satire and fake news agencies (e.g., The Onion, Ending the Fed, and others). Reputed journalistic websites were used for comparison. The authors find interesting relations by comparing fake, satirical, and real news. Below are two news article titles, one of which is fake. Can you identify the fake one? 4 1. BREAKING BOMBSHELL: NYPD Blows Whistle on New Hillary Emails: Money Laundering, Sex Crimes with Children, Child Exploitation, Pay to Play, Perjury 2. Preexisting Conditions and Republican Plans to Replace Obamacare Fake news tends to pack the main claim of the article into its title. The titles are longer but use fewer stopwords and more proper nouns and verb phrases, meaning that the creators tend to put as much information in the title as possible. The words used in the title are smaller and capitalized more often, to generate emphasis. Not surprisingly, titles of fake news and satire are very similar. In terms of the body content, fake news articles are short, repetitive, and less informative (fewer nouns and analytical words). They contain fewer technical words, more smaller words, and are generally easier to read. This allows the reader to skip reading the entire article, and instead just take information away from the title itself, which may be disparate from the rest of the content of the article. Interestingly, Rubin et al. [80] studied satire news separately from the viewpoint of misleading readers into believing it is true, and also found that satirical articles pack a lot of information in single sentences. Thus, fake news articles are more similar to satirical ones than to real news-the bodies are less wordy and contain fewer nouns, technical and analytical words. In addition, Perez et al. [74] also analyzed the textual properties of fake news using two datasets-one generated by Amazon Mechanical Turk workers and other one scraped on celebrity rumors from gossip websites. They found that fake news contains more social and positive words, is more certain, focuses more on present and future actions by using more verbs and time words.

But do people discuss false information differently from true information? To answer this, Mitra et a. [60] recently analyzed the language of how people discuss true and false information pieces using tweets of 1400 events. These events were part of their CREDBANK dataset, which used crowdsourcing to label ground truth credibility judgments [59]. Using LIWC categories [72], they found that discussions around false information are marked with increased use of more confusion, disbelief, and hedging words which indicates skepticism among readers. Surprisingly, they found while more agreement words signaled high credibility, more positive sentiment words are associated with low credibility events. The latter is because it includes words like 'ha', 'grins', 'joking' are positive sentiments, but instead mean mockery. Their findings show that in addition to the text of the tweet itself, its surrounding discussion give important information to identify false information.

Hoaxes have similar textual properties as rumors. Kumar et al. [50] compared the content of hoax articles and non-hoax articles. They found that hoaxes were surprisingly longer compared to non-hoax articles (Figure 9(a)), but they contained far fewer web and internal Wikipedia references (Figure 9(b)). This indicated that hoaxsters tried to give more information to appear more genuine, though they did not have sufficient sources to substantiate their claims.","[['b100'], ['b101', 'b46', 'b89', 'b83', 'b13'], ['b68', 'b81', 'b69'], ['b59']]","[['b100'], ['b101', 'b46', 'b89', 'b83', 'b13'], ['b68', 'b81', 'b69'], ['b59']]",10,"sent1: There is a vast literature that studies fake news in social media.
sent2: False information in the form of fake news is created in such a way to invoke interest and/or be believable to consumers.
sent3: Various strategies may be used to deceive these consumers.
sent4: Silverman [91] found that about 13% of over 1600 news articles had incoherent headline and content body, for example, by using declarative headlines paired with bodies which are skeptical about the veracity of the information.
sent5: In a recent paper, Horne and Adali [37] studied the textual characteristics of fake news using several sources of data: Buzzfeed fake news analysis articles [92], and articles from well known satire and fake news agencies (e.g., The Onion, Ending the Fed, and others).
sent6: Reputed journalistic websites were used for comparison.
sent7: The authors find interesting relations by comparing fake, satirical, and real news.
sent8: Below are two news article titles, one of which is fake.
sent9: Can you identify the fake one? 4 1.
sent10: BREAKING BOMBSHELL: NYPD Blows Whistle on New Hillary Emails: Money Laundering, Sex Crimes with Children, Child Exploitation, Pay to Play, Perjury 2.
sent11: Preexisting Conditions and Republican Plans to Replace Obamacare Fake news tends to pack the main claim of the article into its title.
sent12: The titles are longer but use fewer stopwords and more proper nouns and verb phrases, meaning that the creators tend to put as much information in the title as possible.
sent13: The words used in the title are smaller and capitalized more often, to generate emphasis.
sent14: Not surprisingly, titles of fake news and satire are very similar.
sent15: In terms of the body content, fake news articles are short, repetitive, and less informative (fewer nouns and analytical words).
sent16: They contain fewer technical words, more smaller words, and are generally easier to read.
sent17: This allows the reader to skip reading the entire article, and instead just take information away from the title itself, which may be disparate from the rest of the content of the article.
sent18: Interestingly, Rubin et al. [80] studied satire news separately from the viewpoint of misleading readers into believing it is true, and also found that satirical articles pack a lot of information in single sentences.
sent19: Thus, fake news articles are more similar to satirical ones than to real news-the bodies are less wordy and contain fewer nouns, technical and analytical words.
sent20: In addition, Perez et al. [74] also analyzed the textual properties of fake news using two datasets-one generated by Amazon Mechanical Turk workers and other one scraped on celebrity rumors from gossip websites.
sent21: They found that fake news contains more social and positive words, is more certain, focuses more on present and future actions by using more verbs and time words.
sent22: But do people discuss false information differently from true information?
sent23: To answer this, Mitra et a. [60] recently analyzed the language of how people discuss true and false information pieces using tweets of 1400 events.
sent24: These events were part of their CREDBANK dataset, which used crowdsourcing to label ground truth credibility judgments [59].
sent25: Using LIWC categories [72], they found that discussions around false information are marked with increased use of more confusion, disbelief, and hedging words which indicates skepticism among readers.
sent26: Surprisingly, they found while more agreement words signaled high credibility, more positive sentiment words are associated with low credibility events.
sent27: The latter is because it includes words like 'ha', 'grins', 'joking' are positive sentiments, but instead mean mockery.
sent28: Their findings show that in addition to the text of the tweet itself, its surrounding discussion give important information to identify false information.
sent29: Hoaxes have similar textual properties as rumors.
sent30: Kumar et al. [50] compared the content of hoax articles and non-hoax articles.
sent31: They found that hoaxes were surprisingly longer compared to non-hoax articles (Figure 9(a)), but they contained far fewer web and internal Wikipedia references (Figure 9(b)).
sent32: This indicated that hoaxsters tried to give more information to appear more genuine, though they did not have sufficient sources to substantiate their claims."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s17,5.1.4,"Graph-based characteristics. Several works show that dense subgraphs produced by coordinated or ""lock-step"" behavior in the underlying connections of the social (in this case, review) graph are associated with fraudulent behavior [16,71,83]. Figure 8 demonstrates this pattern in page-likes on Facebook [16]. Alternatively, other works look at the local network structure of the users instead of global structure. For example, Lin et al. [55] showed that for review platforms where multiple ratings/reviews can be given to the same product, review fraudsters often repeatedly post to the same product instead of diversifying like a real reviewer.

Studying group structure, Mukherjee et al. [61] showed that ratios for review group (defined as a set of reviewers who have reviewed at least k common products) size to the total number of reviewers for an associated product tend to be significantly higher in fraudster groups than real users. This is because many products (especially bad ones) have ratings/reviews almost entirely given by fraudsters, whereas this case is uncommon for real reviewers. Furthermore, fraudster groups tend to have larger group size and higher support count (in that they share a large number of target products)-these features essentially reflect the size and density of the group's subgraph.

Incorporating time component, Beutel et al. [16] extended the group definition beyond graph connections by incorporating temporal closeness, and shows that group fraud (e.g., bots coordinating to post fake reviews) is temporally coherent as well and forms bipartite cores in a rearranged user-page bipartite network (Figure 8). The existence of large temporally-coherent bipartite cores is highly suggestive of fraud.

Overall, opinion-based false information tends to be shorter, more exaggerated, and has more extreme ratings (1-stars and 5-stars). The fraudsters that create this false information give several ratings in a short time period ('bursty') and operate in a coordinated fashion ('lockstep').","[['b25', 'b80', 'b64', 'b92'], ['b70'], ['b25'], []]","[['b25', 'b80', 'b64', 'b92'], ['b70'], ['b25'], []]",6,"sent1: Graph-based characteristics. Several works show that dense subgraphs produced by coordinated or ""lock-step"" behavior in the underlying connections of the social (in this case, review) graph are associated with fraudulent behavior [16,71,83].
sent2: Figure 8 demonstrates this pattern in page-likes on Facebook [16].
sent3: Alternatively, other works look at the local network structure of the users instead of global structure.
sent4: For example, Lin et al. [55] showed that for review platforms where multiple ratings/reviews can be given to the same product, review fraudsters often repeatedly post to the same product instead of diversifying like a real reviewer.
sent5: Studying group structure, Mukherjee et al. [61] showed that ratios for review group (defined as a set of reviewers who have reviewed at least k common products) size to the total number of reviewers for an associated product tend to be significantly higher in fraudster groups than real users.
sent6: This is because many products (especially bad ones) have ratings/reviews almost entirely given by fraudsters, whereas this case is uncommon for real reviewers.
sent7: Furthermore, fraudster groups tend to have larger group size and higher support count (in that they share a large number of target products)-these features essentially reflect the size and density of the group's subgraph.
sent8: Incorporating time component, Beutel et al. [16] extended the group definition beyond graph connections by incorporating temporal closeness, and shows that group fraud (e.g., bots coordinating to post fake reviews) is temporally coherent as well and forms bipartite cores in a rearranged user-page bipartite network (Figure 8).
sent9: The existence of large temporally-coherent bipartite cores is highly suggestive of fraud.
sent10: Overall, opinion-based false information tends to be shorter, more exaggerated, and has more extreme ratings (1-stars and 5-stars).
sent11: The fraudsters that create this false information give several ratings in a short time period ('bursty') and operate in a coordinated fashion ('lockstep')."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s16,Temporal characteristics.,"Fraudulent review writers typically give reviews in ""lockstep,"" or at the same/similar times. The rationale is similar to that for dense subgraph connectivity-the review writer's accounts are often controlled by scripts, and are thus temporally synchronized in short windows. A number of papers have leveraged the distribution of interarrival times (IATs) between each user's successive ratings/reviews to detect review spammers. Shah et al. [84], Hooi et al. [36], and Ye et al. [115] showed that in e-commerce websites, spammers are often characteristic of very short IATs (on the order of seconds or minutes) between subsequent ratings, unlike typical users who would rate sporadically and likely only upon making and receiving a purchase. Xie et al. [111] substantiated these findings, with particular emphasis on singleton review spammer attacks. Further, Li et al. [52] and Minnich et al. [58] showed that many fraudulent check-ins/reviews in such networks occur with short, but moreover ""infeasible"" IATs between check-ins. Since users must be physically present at a Fig. 8. Fraudulent reviewers often operate in coordinated or ""lock-step"" manner, which can be represented as temporally coherent dense blocks in the underlying graph adjacency matrix. Reprinted with permission from [14].

location to be allowed to check-in and leave a review for a location, reviews at far-away places in very short IATs are a notable distinguishing characteristic of fraud.

In addition, more recent research by Li et al. [53] surprisingly found that the posting rates of both fraudulent and non-fraudulent users is bimodal-some reviews are written in a short time bursts, while some others with more time between consecutive reviews. Fraudulent users are still more bursty than non-fraudulent users, as the latter have the tendency to be more active after a period of inaction to summarize their recent experiences.","[['b67', 'b124', 'b45', 'b61', 'b120', 'b23', 'b93'], [], ['b62']]","[['b67', 'b124', 'b45', 'b61', 'b120', 'b23', 'b93'], [], ['b62']]",8,"sent1: Fraudulent review writers typically give reviews in ""lockstep,"" or at the same/similar times.
sent2: The rationale is similar to that for dense subgraph connectivity-the review writer's accounts are often controlled by scripts, and are thus temporally synchronized in short windows.
sent3: A number of papers have leveraged the distribution of interarrival times (IATs) between each user's successive ratings/reviews to detect review spammers.
sent4: Shah et al. [84], Hooi et al. [36], and Ye et al. [115] showed that in e-commerce websites, spammers are often characteristic of very short IATs (on the order of seconds or minutes) between subsequent ratings, unlike typical users who would rate sporadically and likely only upon making and receiving a purchase.
sent5: Xie et al. [111] substantiated these findings, with particular emphasis on singleton review spammer attacks.
sent6: Further, Li et al. [52] and Minnich et al. [58] showed that many fraudulent check-ins/reviews in such networks occur with short, but moreover ""infeasible"" IATs between check-ins.
sent7: Since users must be physically present at a Fig. 8.
sent8: Fraudulent reviewers often operate in coordinated or ""lock-step"" manner, which can be represented as temporally coherent dense blocks in the underlying graph adjacency matrix.
sent9: Reprinted with permission from [14].location to be allowed to check-in and leave a review for a location, reviews at far-away places in very short IATs are a notable distinguishing characteristic of fraud.
sent10: In addition, more recent research by Li et al. [53] surprisingly found that the posting rates of both fraudulent and non-fraudulent users is bimodal-some reviews are written in a short time bursts, while some others with more time between consecutive reviews.
sent11: Fraudulent users are still more bursty than non-fraudulent users, as the latter have the tendency to be more active after a period of inaction to summarize their recent experiences."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s9,3.2.3,"Other reasons of successful deception. Publishers of false information succeed at deceiving and spreading it by playing upon naivetÃľ and biases of consumers. Flynn et al. [28] showed that prior belief in false information is rooted in the biased reasoning of the presented information. Two major factors that make consumers vulnerable or susceptible to believing false information are confirmation bias and naive realism [90]. Naive realism suggests consumers believe that they have the ""true"" perception of reality whereas disagreements or nonalignment of views is construed as the others' lack of rationality or cognizance [108]. Moreover, Nickerson et al. [65] characterized confirmation bias, or the tendency of consumers to seek or interpret evidence which confirms their pre-existing notions or beliefs. These biases lead consumers to look for and find meaning in pieces of information (no matter the veracity) which substantiate their own claims. For example, political liberals are prone to having more affinity towards posts promoting liberal viewpoints and condemning conservative ones, and vice versa. Furthermore, social normative theory suggests that sharing content aligned with the beliefs of their peers is attractive [10], in order to gain the acceptance or favor of their peers, regardless of its veracity. Alarmingly, Nyhan et al. [68] showed that even the presentation of corrections to false information by means of facts can actually further polarize idealogical groups and increase their misperceptions.

Researchers have explored the psychological and demographics of information consumers and their propensity to believe in it. Pennycook et al. [73] investigated the psychological profiles to show a positive correlation between propensity for analytic thinking and the ability to discern false from real information, suggesting that false information often spreads due to poor analytical skills on the part of the consumer spreader. Additionally, inability to discern the publisher's original (possibly genuine) intentions can lead to consumer's misunderstanding of original information and lead to creation of misinformation [4]. Recent analysis of demographics by Allcott and Gentzkow [9] concluded that people who spend more time consuming media, people with higher education, and older people have a more accurate perception of information.

Overall, false information spread is orchestrated on large-scale by using fake social media accounts, such as bots and sockpuppets. Once their false message spreads and deceives readers, the readers themselves echo and spread the message. Several reasons lead to deception by false information. First, humans are unable to distinguish false information from true ones when they come across them, and this is difficult even when they are trained to do so. Second, echo chambers are formed in social platforms, such that true and false information is spread among different groups of users, which do not interact with one another. And finally, human biases lead to increase in susceptibility, with some demographics (less educated and low consumers of media) being more likely to fall for false information.","[['b37', 'b74', 'b99', 'b77', 'b117', 'b19'], ['b18', 'b13', 'b82'], []]","[['b37', 'b74', 'b99', 'b77', 'b117', 'b19'], ['b18', 'b13', 'b82'], []]",9,"sent1: Other reasons of successful deception.
sent2: Publishers of false information succeed at deceiving and spreading it by playing upon naivetÃľ and biases of consumers.
sent3: Flynn et al. [28] showed that prior belief in false information is rooted in the biased reasoning of the presented information.
sent4: Two major factors that make consumers vulnerable or susceptible to believing false information are confirmation bias and naive realism [90].
sent5: Naive realism suggests consumers believe that they have the ""true"" perception of reality whereas disagreements or nonalignment of views is construed as the others' lack of rationality or cognizance [108].
sent6: Moreover, Nickerson et al. [65] characterized confirmation bias, or the tendency of consumers to seek or interpret evidence which confirms their pre-existing notions or beliefs.
sent7: These biases lead consumers to look for and find meaning in pieces of information
sent8: (no matter the veracity) which substantiate their own claims.
sent9: For example, political liberals are prone to having more affinity towards posts promoting liberal viewpoints and condemning conservative ones, and vice versa.
sent10: Furthermore, social normative theory suggests that sharing content aligned with the beliefs of their peers is attractive [10], in order to gain the acceptance or favor of their peers, regardless of its veracity.
sent11: Alarmingly, Nyhan et al. [68] showed that even the presentation of corrections to false information by means of facts can actually further polarize idealogical groups and increase their misperceptions.
sent12: Researchers have explored the psychological and demographics of information consumers and their propensity to believe in it.
sent13: Pennycook et al. [73] investigated the psychological profiles to show a positive correlation between propensity for analytic thinking and the ability to discern false from real information, suggesting that false information often spreads due to poor analytical skills on the part of the consumer spreader.
sent14: Additionally, inability to discern the publisher's original (possibly genuine) intentions can lead to consumer's misunderstanding of original information and lead to creation of misinformation [4].
sent15: Recent analysis of demographics by Allcott and Gentzkow [9] concluded that people who spend more time consuming media, people with higher education, and older people have a more accurate perception of information.
sent16: Overall, false information spread is orchestrated on large-scale by using fake social media accounts, such as bots and sockpuppets.
sent17: Once their false message spreads and deceives readers, the readers themselves echo and spread the message.
sent18: Several reasons lead to deception by false information.
sent19: First, humans are unable to distinguish false information from true ones when they come across them, and this is difficult even when they are trained to do so.
sent20: Second, echo chambers are formed in social platforms, such that true and false information is spread among different groups of users, which do not interact with one another.
sent21: And finally, human biases lead to increase in susceptibility, with some demographics (less educated and low consumers of media) being more likely to fall for false information."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s14,Textual characteristics.,"Since most reviews include textual content, researchers have extensively studied textual and linguistic features for discerning review fraud. Several works have posited that review fraudsters minimize effort by repeating the same reviews. Jindal et al. [43] provided the first well-known characterizations of review fraud, in which the authors characterized duplicate reviews (according to Jaccard similarity) across Amazon data as cases of fraud. The authors showed that many of these fraudulent duplicate reviews were from the same user on different products, rather than different users on the same product or different products. Figure 6 shows the distribution of maximum similarity between two reviewers' reviews. At the higher similarity end, 6% of the reviewers with more than one review have a maximum similarity score of 1, which is a sudden jump indicating that many reviewers copy reviews. Furthermore, Sandulescu et al. [82] showed that many review fraudsters adjust their reviews slightly so as not to post near or exactly similar reviews and be easily caught-instead, these sophisticated fraudsters tend to post semantically similar text (i.e. instead of duplicating ""the hotel room had an excellent view, "" the fraudster might post ""the hotel room had a superb view"" instead).

Researchers have also focused more on the linguistic features of deceptive reviews, such as using stylistic analysis (number of words, characters, etc.), lexical analysis (number of verbs, nouns, etc.), psycholinguistic analysis (LIWC [72]), and sentiment analysis (emotion, sentiment, etc.). Mukherjee et al. [62] showed that fake reviews were shorter than real reviews, and Ott et al. [70] found that imaginative ""faked"" writing is typically more exaggerated and consists of more verbs, adverbs, pronouns and pre-determiners. Furthermore, deceptive text tends to have an increased focus on aspects external to the venue being reviewed (more emphasis on family, vacation, business, etc.) [70]. Looking at negative reviews, Ott el at. [69] found that fake negative review writers exaggerate too negatively, including words which communicated negative emotion far more than normal reviews  (a) Aggregate e-commerce rating behavior typically follows the J -shaped curve in blue, whereas review spammers commonly have strongly positively or negatively-biased rating distributions like those in green and red [84]. (b) Fraudulent and non-fraudulent users have bimodal rating distribution [53]. Figures reprinted with permission from [84] and [53].

(terrible, disappointed, etc.). Furthermore, fake reviews eschew the use of pronouns such as ""I, "" perhaps in order to distance themselves from the negative sentiments. Similar observations were made by Li et al. [54] on fake reviews generated by domain experts. Finally, Harris [33] demonstrated that deceptive opinion spam tends to be on average less readable than truthful reviews (measured by Average Readability Index), and is also more polarized and sentimental than those reviews, supporting previous results.","[['b52', 'b91'], ['b71', 'b62', 'b81', 'b79', 'b78', 'b93'], ['b63', 'b42']]","[['b52', 'b91'], ['b71', 'b62', 'b81', 'b79', 'b78', 'b93'], ['b63', 'b42']]",10,"sent1: Since most reviews include textual content, researchers have extensively studied textual and linguistic features for discerning review fraud.
sent2: Several works have posited that review fraudsters minimize effort by repeating the same reviews.
sent3: Jindal et al. [43] provided the first well-known characterizations of review fraud, in which the authors characterized duplicate reviews (according to Jaccard similarity) across Amazon data as cases of fraud.
sent4: The authors showed that many of these fraudulent duplicate reviews were from the same user on different products, rather than different users on the same product or different products.
sent5: Figure 6 shows the distribution of maximum similarity between two reviewers' reviews.
sent6: At the higher similarity end, 6% of the reviewers with more than one review have a maximum similarity score of 1, which is a sudden jump indicating that many reviewers copy reviews.
sent7: Furthermore, Sandulescu et al. [82] showed that many review fraudsters adjust their reviews slightly so as not to post near or exactly similar reviews and be easily caught-instead, these sophisticated fraudsters tend to post semantically similar text (i.e. instead of duplicating ""the hotel room had an excellent view, "" the fraudster might post ""the hotel room had a superb view"" instead).
sent8: Researchers have also focused more on the linguistic features of deceptive reviews, such as using stylistic analysis (number of words, characters, etc.), lexical analysis (number of verbs, nouns, etc.), psycholinguistic analysis (LIWC [72]), and sentiment analysis (emotion, sentiment, etc.).
sent9: Mukherjee et al. [62] showed that fake reviews were shorter than real reviews, and Ott et al. [70] found that imaginative ""faked"" writing is typically more exaggerated and consists of more verbs, adverbs, pronouns and pre-determiners.
sent10: Furthermore, deceptive text tends to have an increased focus on aspects external to the venue being reviewed (more emphasis on family, vacation, business, etc.) [70].
sent11: Looking at negative reviews, Ott el at.
sent12: [69] found that fake negative review writers exaggerate too negatively, including words which communicated negative emotion far more than normal reviews  (a) Aggregate e-commerce rating behavior typically follows the J -shaped curve in blue, whereas review spammers commonly have strongly positively or negatively-biased rating distributions like those in green and red [84].
sent13: (b) Fraudulent and non-fraudulent users have bimodal rating distribution [53].
sent14: Figures reprinted with permission from [84] and [53].(terrible, disappointed, etc.).
sent15: Furthermore, fake reviews eschew the use of pronouns such as ""I, "" perhaps in order to distance themselves from the negative sentiments.
sent16: Similar observations were made by Li et al. [54] on fake reviews generated by domain experts.
sent17: Finally, Harris [33] demonstrated that deceptive opinion spam tends to be on average less readable than truthful reviews (measured by Average Readability Index), and is also more polarized and sentimental than those reviews, supporting previous results."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s7,Rationale of successful deception by false information,"In the previous section, we discussed the multifaceted motives and spreading mechanisms used by those who publish false information. But what about the susceptibility of its consumers: do the readers tend to believe it, and if so, why?

3.2.1 Human inability to discern false information. False information would not have any influence if readers were able to tell that it is false. However, several research studies have conducted experiments to measure the ability of humans to detect false information including hoaxes, fake reviews, and fake news, and have shown that humans are not particularly good at discerning false from true information [50,70,74,114]. We describe these studies in detail below.

To understand reader susceptibility, Kumar et al. [50] conducted an experiment with hoax articles created by hoaxsters on Wikipedia. They hired Amazon Mechanical Turk workers and showed them one hoax and one non-hoax article side-by-side, with the task to identify which one of the two articles was a hoax article without searching for information elsewhere. A total of 320 pairs of hoax and non-hoax articles were created and each pair was shown to 5 different workers. Humans correctly identified the hoax a mere 66% of times, only marginally higher than the random guessing baseline of 50%. They further studied the reasons for mistakes that workers made, shown in Figure 3, which compares several statistical properties of easily-identifiable and hard-to-identify hoaxes. They found that workers frequently misjudged long and well referenced hoax articles to be true, and short but true articles that lacked references to be hoaxes. In fact, even trained and trusted Wikipedia volunteers, called ""patrollers, "" make the similar mistakes by approving long and well-referenced hoax articles for publication on Wikipedia instead of rejecting and deleting them. So, if false information is purposefully created to look genuine, both trained and casual readers are deceived. This indicates that humans give a lot of emphasis on the appearance of false information when judging its veracity.

In the domain of fake reviews, several of research studies have come to similar conclusions. Ott et al. [70] demonstrated that humans are not very good at discerning deceptive opinion spam from real reviews. As a compelling example, below are two TripAdvisor reviews (one real and one fake). Can you identify which one is fake? 3 ""I have stayed at many hotels traveling for both business and pleasure and I can honestly stay that The James is tops. The service at the hotel is first class. The rooms are modern and very comfortable. The location is perfect within walking distance to all of the great sights and restaurants. Highly recommend to both business travelers and couples. ""

""My husband and I stayed at the James Chicago Hotel for our anniversary. This place is fantastic! We knew as soon as we arrived we made the right choice! The rooms are BEAUTIFUL and the staff very attentive and wonderful!! The area of the hotel is great, since I love to shop I couldn't ask for more!! We will definatly [sic] be back to Chicago and we will for sure be back to the James Chicago. ""

The fake reviews were generated by Amazon Mechanical Turkers. Three humans were given a total of 160 reviews which contained both real and fake reviews, and workers had an accuracy between 53.1% and 61.9% in identifying the fake reviews, again showing that humans are poor judges of deception, and perform close to random.

For fake news, a similar recent study was conducted by Perez et al. [74]. They created a dataset of crowdsourced and crawled celebrity-oriented real and fake news, and gave 680 pieces of news (50% fake) to two humans to identify fake ones from them. They achieved an average accuracy of 70.5% in detecting made-up crowdsourced news, and 78.5% in detecting celebrity news.

More recently, with the advancement in deep learning, false information can be generated automatically. When fine tuned, this false information can be as deceptive as those created by humans. Yao et al. [114] created a deep neural network model that generates fake reviews for restaurants, by training on Yelp review data. Mechanical Turk workers were shown a set of 20 reviews for each restaurant, which contained between 0 and 5 machine generated reviews. The task of the workers was to identify which of the reviews were fake. A total of 600 sets of reviews were labeled, and the workers achieved a very low precision of 40.6% precision and 16.2% recall. Humans were able to identify fake reviews if they contained repetitive errors, but not when they had minor spelling or grammar mistakes.

Altogether, these four studies show that humans can easily be deceived into believing that false information is true when it is created intelligently to appear like true information, both manually or by machines.","[[], ['b123', 'b79', 'b83', 'b59'], ['b59'], ['b79', 'b12'], [], [], ['b83'], ['b123'], []]","[[], ['b123', 'b79', 'b83', 'b59'], ['b59'], ['b79', 'b12'], [], [], ['b83'], ['b123'], []]",9,"sent1: In the previous section, we discussed the multifaceted motives and spreading mechanisms used by those who publish false information.
sent2: But what about the susceptibility of its consumers: do the readers tend to believe it, and if so, why?3.2.1 Human inability to discern false information.
sent3: False information would not have any influence if readers were able to tell that it is false.
sent4: However, several research studies have conducted experiments to measure the ability of humans to detect false information including hoaxes, fake reviews, and fake news, and have shown that humans are not particularly good at discerning false from true information [50,70,74,114].
sent5: We describe these studies in detail below.
sent6: To understand reader susceptibility, Kumar et al. [50] conducted an experiment with hoax articles created by hoaxsters on Wikipedia.
sent7: They hired Amazon Mechanical Turk workers and showed them one hoax and one non-hoax article side-by-side, with the task to identify which one of the two articles was a hoax article without searching for information elsewhere.
sent8: A total of 320 pairs of hoax and non-hoax articles were created and each pair was shown to 5 different workers.
sent9: Humans correctly identified the hoax a mere 66% of times, only marginally higher than the random guessing baseline of 50%.
sent10: They further studied the reasons for mistakes that workers made, shown in Figure 3, which compares several statistical properties of easily-identifiable and hard-to-identify hoaxes.
sent11: They found that workers frequently misjudged long and well referenced hoax articles to be true, and short but true articles that lacked references to be hoaxes.
sent12: In fact, even trained and trusted Wikipedia volunteers, called ""patrollers, "" make the similar mistakes by approving long and well-referenced hoax articles for publication on Wikipedia instead of rejecting and deleting them.
sent13: So, if false information is purposefully created to look genuine, both trained and casual readers are deceived.
sent14: This indicates that humans give a lot of emphasis on the appearance of false information when judging its veracity.
sent15: In the domain of fake reviews, several of research studies have come to similar conclusions.
sent16: Ott et al. [70] demonstrated that humans are not very good at discerning deceptive opinion spam from real reviews.
sent17: As a compelling example, below are two TripAdvisor reviews (one real and one fake).
sent18: Can you identify which one is fake?
sent19: 3 ""I have stayed at many hotels traveling for both business and pleasure and I can honestly stay that The James is tops.
sent20: The service at the hotel is first class.
sent21: The rooms are modern and very comfortable.
sent22: The location is perfect within walking distance to all of the great sights and restaurants.
sent23: Highly recommend to both business travelers and couples.
sent24: """"My husband and I stayed at the James Chicago Hotel for our anniversary.
sent25: This place is fantastic! We knew as soon as we arrived we made the right choice!
sent26: The rooms are BEAUTIFUL and the staff very attentive and wonderful!!
sent27: The area of the hotel is great, since I love to shop I couldn't ask for more!!
sent28: We will definatly [sic] be back to Chicago and we will for sure be back to the James Chicago.
sent29: ""The fake reviews were generated by Amazon Mechanical Turkers.
sent30: Three humans were given a total of 160 reviews which contained both real and fake reviews, and workers had an accuracy between 53.1% and 61.9% in identifying the fake reviews, again showing that humans are poor judges of deception, and perform close to random.
sent31: For fake news, a similar recent study was conducted by Perez et al. [74].
sent32: They created a dataset of crowdsourced and crawled celebrity-oriented real and fake news, and gave 680 pieces of news (50% fake) to two humans to identify fake ones from them.
sent33: They achieved an average accuracy of 70.5% in detecting made-up crowdsourced news, and 78.5% in detecting celebrity news.
sent34: More recently, with the advancement in deep learning, false information can be generated automatically.
sent35: When fine tuned, this false information can be as deceptive as those created by humans.
sent36: Yao et al. [114] created a deep neural network model that generates fake reviews for restaurants, by training on Yelp review data.
sent37: Mechanical Turk workers were shown a set of 20 reviews for each restaurant, which contained between 0 and 5 machine generated reviews.
sent38: The task of the workers was to identify which of the reviews were fake.
sent39: A total of 600 sets of reviews were labeled, and the workers achieved a very low precision of 40.6% precision and 16.2% recall.
sent40: Humans were able to identify fake reviews if they contained repetitive errors, but not when they had minor spelling or grammar mistakes.
sent41: Altogether, these four studies show that humans can easily be deceived into believing that false information is true when it is created intelligently to appear like true information, both manually or by machines."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s6,Bad actors: bots and sockpuppets,"Humans are susceptible to false information and spread false information [105]. However, the creation and spread of false information is complex, and fueled by the use of nefarious actors which act independently or on a large-scale using a network of social media bots. Both deceive readers by creating an illusion of consensus towards the false piece of information, for instance by echoing it multiple times or expressing direct support for it. These accounts aim to artificially engineer the virality of their content (e.g., by 'upvoting'/promoting content in its early phase [109]) in order to spread posts with false information even faster and deeper than true information [11,19,30].

Lone-wolves operate by creating a handful of fake ""sockpuppet"" or ""sybil"" accounts and using them in coordination to reflect the same point of view, by writing similar reviews on e-commerce platforms or making similar comments on public forums. Lone-wolf operations using multiple accounts can be especially convincing as readers are typically not aware that a whole discussion is fabricated and actually originates from a single source. For instance, in online conversations, Kumar et al. [47] characterize this behavior by studying 61 million comments made by 2.1 million users across several discussion platforms. They found that while sockpuppets can be used with benign intention, sockpuppets with deceptive intentions are twice as common. Deceptive sockpuppets reply to each other with agreement and support, and are negative towards accounts that disagree. Moreover, these accounts hold central locations in the communication network, and are therefore in key spots to spread false content. Similarly, sybil accounts in communication and social networks are created to integrate themselves well into the network and prevent detection in order to increase their influence over others [113].

On a larger scale, social media botnets are used to spread false information. Bots, which are fake or compromised accounts controlled by a single individual or a program, are used to serve two main purposes: to send the same information to a large audience quickly, and to inflate the ""social status"" of certain users, both of which make false information to appear credible and legitimate [26,85,97]. Figure 10 visualizes an online Twitter conversation on a controversial topic (hashtag #SB277) showing overwhelming presence of bots (red nodes) engaging with humans (blue nodes) [2]. Bessi et al. [13] and Shao et al. [87] studied the use of bots in political campaigns and found that bot accounts are responsible for almost one-fifth of all Twitter political chatter, and that false information is more likely to be spread by bots than real users. Similarly, Nied et al. [66] found that 25% of false information tweets were generated by bots. A common strategy employed by bots is to target information towards more influential real users, who may sometimes get influenced and reshare the false message forward to a broader audience [13,87]. In efforts to increase ""social status"", botnet operators offer services that provide fake followers by using their bots to follow paying customer accounts. Shah et al. [85] studied these services and found that they operate on ""freemium"" and ""premium"" models, where the former is made of compromised or real user accounts and the latter is comprised of fake or bot accounts. These two models operate quite distinctly-freemium fraud accounts create high-density cliques of opted-in accounts who trade follows amongst themselves, while premium fraud accounts create dense bipartite cores, i.e., one set of accounts follows the paying customers. This increases the apparent trustworthiness of the users, who can then be used to spread false information further.

In a recent study, Vosoughi et al. [105] analyzed over 126,000 false information cascades on Twitter over a period of 11 years and showed that humans were responsible for spread of false information on Twitter, not bots. Using the BotOrNot Twitter bot-detection tool developed by Davis et al [21], they identified the bot and non-bot accounts that engaged in false information spread. They found that on Twitter, humans, not bots, were responsible for spread of false information, as the bots were responsible for accelerating the spread of both true and false information roughly equally. Even after removing bot activity, false information was observed to spread farther, deeper, faster, and broader than true information. Further, they found that the non-bot accounts on Twitter that were responsible for spreading false information were newer, had fewer followers and followees, and were less active. While this is the case on Twitter, other platforms may behave differently, and the proliferation of nefarious actors in the creation and spread of false information is common.

Thus, using sockpuppets and botnets are used to engineer the spread of false information to massive numbers of real users on social media. These accounts operate using fake and computerized accounts to increase the visibility of false information and social status of accounts that spread it. ","[['b28', 'b20', 'b114', 'b118', 'b39'], ['b122', 'b56'], ['b22', 'b96', 'b35', 'b106', 'b75', 'b11', 'b94'], ['b30', 'b114'], []]","[['b28', 'b20', 'b114', 'b118', 'b39'], ['b122', 'b56'], ['b22', 'b96', 'b35', 'b106', 'b75', 'b11', 'b94'], ['b30', 'b114'], []]",16,"sent1: Humans are susceptible to false information and spread false information [105].
sent2: However, the creation and spread of false information is complex, and fueled by the use of nefarious actors which act independently or on a large-scale using a network of social media bots.
sent3: Both deceive readers by creating an illusion of consensus towards the false piece of information, for instance by echoing it multiple times or expressing direct support for it.
sent4: These accounts aim to artificially engineer the virality of their content (e.g., by 'upvoting'/promoting content in its early phase [109]) in order to spread posts with false information even faster and deeper than true information [11,19,30].
sent5: Lone-wolves operate by creating a handful of fake ""sockpuppet"" or ""sybil"" accounts and using them in coordination to reflect the same point of view, by writing similar reviews on e-commerce platforms or making similar comments on public forums.
sent6: Lone-wolf operations using multiple accounts can be especially convincing as readers are typically not aware that a whole discussion is fabricated and actually originates from a single source.
sent7: For instance, in online conversations, Kumar et al. [47] characterize this behavior by studying 61 million comments made by 2.1 million users across several discussion platforms.
sent8: They found that while sockpuppets can be used with benign intention, sockpuppets with deceptive intentions are twice as common.
sent9: Deceptive sockpuppets reply to each other with agreement and support, and are negative towards accounts that disagree.
sent10: Moreover, these accounts hold central locations in the communication network, and are therefore in key spots to spread false content.
sent11: Similarly, sybil accounts in communication and social networks are created to integrate themselves well into the network and prevent detection in order to increase their influence over others [113].
sent12: On a larger scale, social media botnets are used to spread false information.
sent13: Bots, which are fake or compromised accounts controlled by a single individual or a program, are used to serve two main purposes: to send the same information to a large audience quickly, and to inflate the ""social status"" of certain users, both of which make false information to appear credible and legitimate
sent14: [26,85,97]. Figure 10 visualizes an online Twitter conversation on a controversial topic (hashtag #SB277) showing overwhelming presence of bots (red nodes) engaging with humans (blue nodes)[2].
sent15: Bessi et al. [13] and Shao et al. [87] studied the use of bots in political campaigns and found that bot accounts are responsible for almost one-fifth of all Twitter political chatter, and that false information is more likely to be spread by bots than real users.
sent16: Similarly, Nied et al. [66] found that 25% of false information tweets were generated by bots.
sent17: A common strategy employed by bots is to target information towards more influential real users, who may sometimes get influenced and reshare the false message forward to a broader audience [13,87].
sent18: In efforts to increase ""social status"", botnet operators offer services that provide fake followers by using their bots to follow paying customer accounts.
sent19: Shah et al. [85] studied these services and found that they operate on ""freemium"" and ""premium"" models, where the former is made of compromised or real user accounts and the latter is comprised of fake or bot accounts.
sent20: These two models operate quite distinctly-freemium fraud accounts create high-density cliques of opted-in accounts who trade follows amongst themselves, while premium fraud accounts create dense bipartite cores, i.e., one set of accounts follows the paying customers.
sent21: This increases the apparent trustworthiness of the users, who can then be used to spread false information further.
sent22: In a recent study, Vosoughi et al. [105] analyzed over 126,000 false information cascades on Twitter over a period of 11 years and showed that humans were responsible for spread of false information on Twitter, not bots.
sent23: Using the BotOrNot Twitter bot-detection tool developed by Davis et al [21], they identified the bot and non-bot accounts that engaged in false information spread.
sent24: They found that on Twitter, humans, not bots, were responsible for spread of false information, as the bots were responsible for accelerating the spread of both true and false information roughly equally.
sent25: Even after removing bot activity, false information was observed to spread farther, deeper, faster, and broader than true information.
sent26: Further, they found that the non-bot accounts on Twitter that were responsible for spreading false information were newer, had fewer followers and followees, and were less active.
sent27: While this is the case on Twitter, other platforms may behave differently, and the proliferation of nefarious actors in the creation and spread of false information is common.
sent28: Thus, using sockpuppets and botnets are used to engineer the spread of false information to massive numbers of real users on social media.
sent29: These accounts operate using fake and computerized accounts to increase the visibility of false information and social status of accounts that spread it."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s15,Ratings characteristics,". Many e-commerce sites disallow users from giving feedback without giving an associated numerical rating. The rating is typically a 5-star system (1 representing the worst possible rating, and 5 representing the best), and is employed by numerous major online marketplaces including Amazon, eBay, Flipkart, and more. Prior work in review fraud has shown that those who engage in spreading fake e-commerce reviews also typically have skewed rating distributions [15,36,79,84] which are not typical of real users who share non-uniform opinions over many products. Figure 7 shows an example from Shah et al. [84], comparing aggregate (dataset-wide) rating habits from the Flipkart platform with two common, naive fraudster rating habits depicting very positive and negative raters. Some fraudulent reviewers give only positive ratings as they are created in order to inflate ratings for customer products, whereas other such reviewers give only negative ratings as they intend to slander competitors' products. Further, Kumar et al. [48] recently showed that fraudulent review writers are typically unfair, in that they give ""unreliable"" rating scores that differ largely from the product's average score. Furthermore, these fraudulent writers often give high ratings to products that otherwise receive highly negative ratings from fair users.","[['b88', 'b57', 'b45', 'b93', 'b24']]","[['b88', 'b57', 'b45', 'b93', 'b24']]",5,"sent1: . Many e-commerce sites disallow users from giving feedback without giving an associated numerical rating.
sent2: The rating is typically a 5-star system (1 representing the worst possible rating, and 5 representing the best), and is employed by numerous major online marketplaces including Amazon, eBay, Flipkart, and more.
sent3: Prior work in review fraud has shown that those who engage in spreading fake e-commerce reviews also typically have skewed rating distributions [15,36,79,84] which are not typical of real users who share non-uniform opinions over many products.
sent4: Figure 7 shows an example from Shah et al. [84], comparing aggregate (dataset-wide) rating habits from the Flipkart platform with two common, naive fraudster rating habits depicting very positive and negative raters.
sent5: Some fraudulent reviewers give only positive ratings as they are created in order to inflate ratings for customer products, whereas other such reviewers give only negative ratings as they intend to slander competitors' products.
sent6: Further, Kumar et al. [48] recently showed that fraudulent review writers are typically unfair, in that they give ""unreliable"" rating scores that differ largely from the product's average score.
sent7: Furthermore, these fraudulent writers often give high ratings to products that otherwise receive highly negative ratings from fair users."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s1,Social Platform Research papers,"Twitter Bessi et al. [13], Ferrara et al. [26], Gupta et al. [32], Howard et al. [38], Jin et al. [41,42], Kim et al. [44], Mendoza et al. [57], Mitra et al. [59,60], Nied et al. [66], Qazvinian et al. [77], Ruchansky et al. [81], Shah et al. [85], Shao et al. [86,87], Starbird et al. [95,96], Subrahmanian et al. [97], Tripathy et al. [103], Vosoughi et al. [105], Zeng et al. [118], Zubiaga et al. [121] Facebook Beutel et al. [16], Del et al. [22], Friggeri et al. [30], Nguyen et al. [64], Silverman et al. [91,92], Tacchini et al. [100] Review platforms

Akoglu et al. [5], Beutel et al. [15], Harris et al. [33], Hooi et al. [36], Jindal et al. [43], Kumar et al. [48], Li et al. [51][52][53][54], Lin et al. [55], Luca et al. [56], Minnich et al. [58], Mukherjee et al. [61,62], Ott et al. [69,70], Rayana et al. [79], Sandulescu et al. [82], Shah et al. [84], Wang et al. [106], Xie et al. [111], Yao et al. [114], Ye et al. [115] Sina Weibo Jiang et al. [40], Kim et al. [44], Ruchansky et al. [81], Wu et al. [110], Yang et al. [112] Multi-platform Reddit+Twitter+4chan: Zannettou et al. [117] Other Fake news articles: Horne et al. [37], Silverman et al. [92], Rubin et al. [80], Perez et al. [74], Wikipedia: Kumar et al. [50], False information websites: Albright et al. [7,8], Fact checking website: Shu et al. [89] and Wang et al. [107], Crowdsourcing and tabloid websites: Perez et al. [74]  longer, more exaggerated, and more opinionated compared to real reviews. Temporally, fake reviews are created in short bursts, i.e., several fake reviews are usually written by the same account or group of accounts in a short time period. The users who write these fake reviews and hoaxes are typically relatively new accounts with fewer reviews, and their local networks are often highly dense or overlapping. Additionally, majority of fake news is spread by a very small number of users and it spreads rapidly during its initial release, before it is even debunked. The characteristics of false information are discussed extensively in Section 5. Finally, several algorithms have been created for effective detection of false information from its true counterparts. These algorithms can broadly be categorized into three categories: feature-based, graph-based, and propagation-modeling based. Feature-based algorithms leverage the unique characteristics for detection by using them as features in a machine learning model or rule-based framework [32,37,43,50,70,77,82]. Graph-based algorithms are developed to identify dense-blocks or dense subgraphs of users and information in the network [6,16,40,48,79,106]. Propagation-modeling algorithms create information spread models for true information and use these models to identify false information [3,18,41,64,103,110,112]. Naturally, the accuracy of these algorithms depends on the task and datasets used. However, several reach the high eighties and nineties, showing their effectiveness on large-scale real-world datasets of fake reviews, fake news, and hoaxes. Detection algorithms are discussed in depth in Section 6.

Overall, this survey gives a comprehensive overview of the state of false information on the web and social media. Table 1 categorizes the research papers according to the platforms they study. The remainder of the survey is organized as follows: Section 2 explains the two broad categories of false information, Section 3.2 discusses the mechanisms and rationale for the success of false information. Then, in Section 4, we describe the impact of false information. Section 5 elaborates on various characteristics of false information, and is followed by Section 6 which describes several algorithms for its detection.","[['b41', 'b95', 'b35', 'b53', 'b130', 'b94', 'b69', 'b104', 'b22', 'b66', 'b100', 'b114', 'b106', 'b73', 'b86', 'b50', 'b101', 'b25', 'b51', 'b96', 'b112', 'b75', 'b90', 'b39', 'b68', 'b105', 'b47', None, 'b31', 'b109', 'b127'], ['b67', 'b41', 'b116', 'b63', 'b61', 'b91', 'b53', 'b98', 'b14', 'b57', 'b45', 'b120', 'b121', 'b93', 'b73', 'b86', 'b64', 'b115', 'b59', 'b124', 'b60', 'b101', 'b16', 'b79', 'b46', 'b25', 'b50', 'b42', 'b89', 'b112', 'b65', 'b90', 'b15', 'b12', 'b88', 'b27', 'b52', 'b70', 'b62', 'b71', 'b49', 'b119', 'b126', 'b17', 'b123', 'b78', None, 'b83', 'b24'], []]","[['b41', 'b95', 'b35', 'b53', 'b130', 'b94', 'b69', 'b104', 'b22', 'b66', 'b100', 'b114', 'b106', 'b73', 'b86', 'b50', 'b101', 'b25', 'b51', 'b96', 'b112', 'b75', 'b90', 'b39', 'b68', 'b105', 'b47', None, 'b31', 'b109', 'b127'], ['b67', 'b41', 'b116', 'b63', 'b61', 'b91', 'b53', 'b98', 'b14', 'b57', 'b45', 'b120', 'b121', 'b93', 'b73', 'b86', 'b64', 'b115', 'b59', 'b124', 'b60', 'b101', 'b16', 'b79', 'b46', 'b25', 'b50', 'b42', 'b89', 'b112', 'b65', 'b90', 'b15', 'b12', 'b88', 'b27', 'b52', 'b70', 'b62', 'b71', 'b49', 'b119', 'b126', 'b17', 'b123', 'b78', None, 'b83', 'b24'], []]",80,"sent1: Twitter Bessi et al. [13], Ferrara et al. [26], Gupta et al. [32], Howard et al. [38], Jin et al. [41,42], Kim et al. [44], Mendoza et al. [57], Mitra et al. [59,60], Nied et al. [66], Qazvinian et al. [77], Ruchansky et al. [81], Shah et al. [85], Shao et al. [86,87], Starbird et al. [95,96], Subrahmanian et al. [97], Tripathy et al. [103], Vosoughi et al. [105], Zeng et al. [118], Zubiaga et al. [121] Facebook Beutel et al. [16], Del et al. [22], Friggeri et al. [30], Nguyen et al. [64], Silverman et al. [91,92], Tacchini et al. [100] Review platformsAkoglu et al. [5], Beutel et al. [15], Harris et al. [33], Hooi et al. [36], Jindal et al. [43], Kumar et al. [48], Li et al. [51][52][53][54], Lin et al. [55], Luca et al. [56], Minnich et al. [58], Mukherjee et al. [61,62], Ott et al. [69,70], Rayana et al. [79], Sandulescu et al. [82], Shah et al. [84], Wang et al. [106], Xie et al. [111], Yao et al. [114], Ye et al. [115] Sina Weibo Jiang et al. [40], Kim et al. [44], Ruchansky et al. [81], Wu et al. [110], Yang et al. [112] Multi-platform Reddit+Twitter+4chan: Zannettou et al. [117] Other Fake news articles: Horne et al. [37], Silverman et al. [92], Rubin et al. [80], Perez et al. [74], Wikipedia: Kumar et al. [50], False information websites: Albright et al. [7,8], Fact checking website: Shu et al. [89] and Wang et al. [107], Crowdsourcing and tabloid websites: Perez et al. [74]  longer, more exaggerated, and more opinionated compared to real reviews.
sent2: Temporally, fake reviews are created in short bursts, i.e., several fake reviews are usually written by the same account or group of accounts in a short time period.
sent3: The users who write these fake reviews and hoaxes are typically relatively new accounts with fewer reviews, and their local networks are often highly dense or overlapping.
sent4: Additionally, majority of fake news is spread by a very small number of users and it spreads rapidly during its initial release, before it is even debunked.
sent5: The characteristics of false information are discussed extensively in Section 5.
sent6: Finally, several algorithms have been created for effective detection of false information from its true counterparts.
sent7: These algorithms can broadly be categorized into three categories: feature-based, graph-based, and propagation-modeling based.
sent8: Feature-based algorithms leverage the unique characteristics for detection by using them as features in a machine learning model or rule-based framework [32,37,43,50,70,77,82].
sent9: Graph-based algorithms are developed to identify dense-blocks or dense subgraphs of users and information in the network [6,16,40,48,79,106].
sent10: Propagation-modeling algorithms create information spread models for true information and use these models to identify false information [3,18,41,64,103,110,112].
sent11: Naturally, the accuracy of these algorithms depends on the task and datasets used.
sent12: However, several reach the high eighties and nineties, showing their effectiveness on large-scale real-world datasets of fake reviews, fake news, and hoaxes.
sent13: Detection algorithms are discussed in depth in Section 6.
sent14: Overall, this survey gives a comprehensive overview of the state of false information on the web and social media.
sent15: Table 1 categorizes the research papers according to the platforms they study.
sent16: The remainder of the survey is organized as follows: Section 2 explains the two broad categories of false information, Section 3.2 discusses the mechanisms and rationale for the success of false information.
sent17: Then, in Section 4, we describe the impact of false information.
sent18: Section 5 elaborates on various characteristics of false information, and is followed by Section 6 which describes several algorithms for its detection."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s8,3.2.2,"Formation of echo-chambers. Given the advent of improved recommendation algorithms which promote personalized content for easy user access and exposure, social media platforms are often party to an ""echo-chamber"" effect [22]. This effect primarily refers to the self-selective polarizing effect of content where people immerse themselves in social circles in such a way that they are primarily exposed to content that agree with their beliefs. For example, a political liberal might friend more liberals on Facebook, thumbs-up liberal-minded content, and thus constantly be exposed to posts and news which aligns with his worldview. Figure 4 visualizes this echo-chamber effect on Twitter on a controversial topic of #beefban, where red and blue nodes represent users with opposing beliefs and edges represent who-retweets-whom, as shown by Garimella et al. [31]. Notice that both groups are mostly disconnected with few messages between nodes of different types. The echo-chamber effect in social networks is substantiated by Nikolov et al. [67] by demonstrating that the diversity of sources (links) clicked by users is significantly lower on social media platforms than in general search engines. Several studies have studied the effects and causes of echo-chambers. Quattrociocchi et al. [78] demonstrated that such resulting echo-chambers can serve to polarize the user's viewpoints by means of confirmation bias and lead to less diverse exposure and discussion between unaligned users. The resulting echo-chambers can contribute to the spread of false information by lowering the bar for critical fact-checking. Moreover, Trilling et al. [102] and Zajonc [116] posited that the perceived accuracy of false information increases linearly with the frequency of exposure of a participant to the same false information. This suggests that familiarity with repeatedly shared content (highly common and expected in echo-chambers) increases the perceived accuracy of the content, irrespective of its credibility. This calls for research on how to create effective techniques to break echo-chambers and slow down false information spread.","[['b111', 'b125', 'b76', 'b31', 'b40', 'b87']]","[['b111', 'b125', 'b76', 'b31', 'b40', 'b87']]",6,"sent1: Formation of echo-chambers. Given the advent of improved recommendation algorithms which promote personalized content for easy user access and exposure, social media platforms are often party to an ""echo-chamber"" effect [22].
sent2: This effect primarily refers to the self-selective polarizing effect of content where people immerse themselves in social circles in such a way that they are primarily exposed to content that agree with their beliefs.
sent3: For example, a political liberal might friend more liberals on Facebook, thumbs-up liberal-minded content, and thus constantly be exposed to posts and news which aligns with his worldview.
sent4: Figure 4 visualizes this echo-chamber effect on Twitter on a controversial topic of #beefban, where red and blue nodes represent users with opposing beliefs and edges represent who-retweets-whom, as shown by Garimella et al. [31].
sent5: Notice that both groups are mostly disconnected with few messages between nodes of different types.
sent6: The echo-chamber effect in social networks is substantiated by Nikolov et al. [67] by demonstrating that the diversity of sources (links) clicked by users is significantly lower on social media platforms than in general search engines.
sent7: Several studies have studied the effects and causes of echo-chambers.
sent8: Quattrociocchi et al. [78] demonstrated that such resulting echo-chambers can serve to polarize the user's viewpoints by means of confirmation bias and lead to less diverse exposure and discussion between unaligned users.
sent9: The resulting echo-chambers can contribute to the spread of false information by lowering the bar for critical fact-checking.
sent10: Moreover, Trilling et al. [102] and Zajonc [116] posited that the perceived accuracy of false information increases linearly with the frequency of exposure of a participant to the same false information.
sent11: This suggests that familiarity with repeatedly shared content (highly common and expected in echo-chambers) increases the perceived accuracy of the content, irrespective of its credibility.
sent12: This calls for research on how to create effective techniques to break echo-chambers and slow down false information spread."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s29,Feature-based detection. Text-based features:,"Text-analysis is core to identifying misinformation as the information being conveyed is primarily textual content. Similarly to opinion-based textual detection methods, research papers in this category are predominantly feature-based, where features can broadly be categorized as either stylometric (e.g., number of characters in a word), psycholinguistic (e.g., LIWC [72]), or complexity-oriented (e.g., readability indices).

One of the first studies on identifying rumors on Twitter was done by Qazninian et al. [77]. They collected manual annotations for over 10,000 tweets, and developed three categories of features to identify the false tweets-primarily based on content (unigram, bigrams, and part-of-speech), but also used user information (whether user has previously posted false information), and Twitter-specific information (hashtags and URLs). These features were converted into their log-likelihood ratio of being from the true or false class based on their distribution in the training data, and the combined score was used for classification. This model achieved a mean average precision score of 95%, indicating near-perfect classification. Individually, content features performed the best, followed by network features, and lastly hashtag and URL based Twitter features. Content based features were also the best performing features in Gupta et al. [32], which focused on fake tweet detection as well.

A recent study on false news detection by Perez-Rosas et al. [74] shows the changing effectiveness of text-based features. They collected a large dataset of false information generated by Amazon Mechanical Turk (AMT) workers and another dataset of celebrity fake news from gossip websites. They used a huge set of text based features for classification, consisting of n-grams, punctuations, LIWC, readability, and syntax features. Since the experiments were conducted on a balanced dataset, the baseline accuracy is 50%, and the combined set of features achieves an average accuracy of 74%. Unsurprisingly, cross-domain analysis, i.e., training model on one dataset and testing on another, dropped the accuracy to 61%. Within the same dataset, but training and testing on different domains of fake news (e.g., technology, education, etc.) the performance lies between 75% and 85%, indicating that knowledge can be transferred from one domain to another with reasonable performance.

Similarly, in their study to identify fake from real news from text, Horne and Adali [37] achieved accuracies between 71%-78%. Identifying fake news from satire was more difficult than identifying fake news from real news, indicating that the former two are written quite similarly. In their experiments, they observe that the news body text is less informative than the news title text in distinguishing real from fake information. This is an important finding and opens avenues of future research to quantify the dissonance between the title and body of an information piece, and using it as an indicator of false information.

The above studies show an alarming trend. In the earlier research papers, content-based detection performed well, but more recent research has shown that content alone is not good enough. This suggests a trend that malicious users and content creators are adapting and becoming more aware of how to create more genuinelooking false information which fools automated classifiers.

User, network, and metadata based detection: These detection models derive features from several different aspects of data which we describe below. We start by looking at the features developed to identify hoaxes in Wikipedia. Kumar et al. [50] developed four different categories of features to identify hoaxes: (i) appearance features, that measures the length of the article, number of references, ratio of text to Wikipedia markup and so on, (ii) network features, that measures the relation between the references of the article in the Wikipedia hyperlink network, (iii) support features, quantifying the number of previous occurrences of the article title in other articles, and the time since its first occurrence to the time the article is created, and (iv) creator features, i.e., properties of article creator in terms of its previous edit  count, time since registration, and so on. Figure 14 shows the performance of the features using a random forest classifier. Individually, creator and network features perform equally well (AUC = 0.90) Further, the performance can be improved significantly when other sets of features are incrementally added. The combination of all four categories of features gives an AUC of 0.98, indicating a near perfect classifier. This shows that in order to identify false information, one needs to look beyond its appearance and dig deeper into who created the piece of false information and how it relates to existing information-simply looking at its appearance is not as effective.

In the domain of fake Twitter images during disasters, Gupta et al. [32] used user features (number of friends, account age, etc.), tweet features (number of words, sentiment, POS, LIWC, etc.), and metadata features (number of hashtags, mentions, URLs, retweets). The dataset had 11,534 tweets, half of which were fake and other half were real, and decision trees were used for classification. User features alone had close to random accuracy of about 53%, while tweet features alone got near perfect accuracy of 97.7%, indicating that in the propagation of false information, what the tweet says is more important than the user who tweets it.

Thus, feature engineering has proven successful in identifying fake from true rumors and hoaxes, primarily using features derived from the text, user, network, and other metadata. These algorithms typically have performance numbers in high 80s and 90s, showing that they are effective and practically useful.

6.2.2 Detection using propagation modeling. The spread of information across social media adds another dimension to use for identification of true information from false information. A common way of using propagation information is to create models that broadly serve two purposes: to closely model the spread of (false) information, and to find ways to prevent its spread.

Creating true information propagation models: Acemoglu et al. [3] presented one of the first models to simulate propagation of information in social media. They considered information spread as exchange of belief about information (e.g., supporting a political candidate), and theoretically find the cases in which false information survives in the network. Nodes in the network are considered to be either normal or forceful. When two normal nodes interact, each of them adopts an average of their existing beliefs. But interactions with forceful nodes only change the belief of the normal node, while forceful node only slightly updates its beliefs. With this interaction model, simulations showed that belief about false information dies out when the social network is well connected and normal nodes interact with each other a lot. On the other hand, echo chambers are formed and false information prevails when there are several forceful nodes who update their own belief from the beliefs of nodes they previously influenced. This model suggests increasing the number of normal nodes and increasing their connectivity with each other may be a way to reduce false information propagation.

More recently, Jin et al. [41] created a false information propagation model which they called SEIZ. Similar to the standard SIS model, each node in SEIZ model lies in one of four states-susceptible (S), exposed (E), infected (I), and skeptical (Z), as shown in Figure 15(a). Initially, nodes are susceptible (state S). When they receive information, they can transition to states I or Z with probabilities β and b, respectively. These transitions may only be successful with some probabilities p and b, respectively, otherwise the nodes transition to state E. The following four equations explain the transitions according to this SEIZ model:

The model parameters are learned by training on real true and false information propagation data, and these are used for prediction of false information. A metric measuring the rate of users entering state E to leaving it is predictive of false information-a high value indicates true information while a low score indicates a rumor, as shown by their case study of eight rumors. Incorporating propagation information in machine learning models: Apart from model creation, propagation information and propagation structure can both augment existing machine learning frameworks. For example, Yang et al. [112] added propagation features such as number of replies and number of retweets to the standard set of features used for their classification tasks, such as content, client (device type), user, and location, to identify false information spread on the Sina Weibo network. Using this feature set with an SVM classifier gave an average accuracy of 78%.

A more structured way of using propagation information was created by Wu et al. [110], who also studied 11,466 rumors on Sina Weibo by using propagation information. Each thread of information was represented as a tree, with the original message as the root and its replies as children, and so on. This is shown in Figure 15 (b) and (c) for false and true information, respectively. Popular nodes, i.e., ones with at least 1,000 followers, are denoted as p and others as n, to understand if popular nodes boost false information. The authors observe that false information is usually started by a normal user, then reported and supported by some opinion leaders, and then finally reshared by a large number of normal users (Figure 15(b)). On the other hand, true information is posted by opinion leaders and then reposted directly by normal users (Figure 15(c)). With features representing the propagation structure, user features, average sentiment, doubt, surprise and emotion, an SVM classifier achieved 91% accuracy. Further, early detection of false information achieved 72% accuracy even without any propagation information, and 88% with propagation information of its first 24 hours. Mitigation by modeling true and false information: Previously we described propagation models spreading false information. Here, we consider models that model the spread of both true and false information simultaneously, where success is measured as the number of users saved from accepting false information. The aim of these models is to create mitigation strategies to reduce and prevent the spread of false information. Several such models have been developed. Tripathy et al. [103] created two models in which true information (anti-rumor) is spread after false information(rumor) starts to spread, based on real data. In one model, truth is spread n days after falsehood to simulate real-world observed time-lag, and in the second model, truth is spread by some special nodes (e.g., official accounts) as soon as they receive false information. They conducted experiments with Twitter and simulated networks, and find that there is a super-linear relationship between the lifetime of a rumor and delay of its detection. When the special nodes detect and spread anti-rumors, it reduces the average lifetime of rumors by over 60%.

Similarly, Budak et al. [18] presented an independent cascade model called Multi-Campaign Independence Cascade Model (MCICM). The model contains a rumor campaign and a true information 'limiting' campaign spreading through the network. Each node, whenever infected with true or false information, spreads its belief to its neighbors, which accept the information with some probability. Their algorithm learns the model parameters, even with missing data. Their simulations show that a 30% increase in delay in starting the true information spread reduces its reach by 80% of the population. More recently, Nguyen et al. [64] created another model with both linear threshold and independent cascade, and found that when true information can only be spread by a few nodes, it is most effective to do it via highly influential nodes in large communities. When more nodes can be selected, influential nodes in smaller communities are more effective in preventing the spread of false information. This method is 16-41% better than other methods. Related models have also been developed [119].

Thus, several propagation models have been developed that capture the spread of true and false information on social media. These models are used independently or in conjunction with other machine learning algorithms to identify false information. These algorithms are effective in detecting spread of rumors, and their simulations suggest rumor mitigation strategies.

Overall, several categories of false information detection algorithms have been developed for opinion-based and fact-based false information. The common category of algorithm is feature-based, which converts observations into feature vectors, derived from text, user, network, and metadata. Graph-based algorithms have primarily been developed for opinion-based false information (e.g., fake reviews), and identify dense block of users or information, potentially also occurring in bursty short time period. Temporal modeling algorithms use time-series analysis, and co-clustering on one or more of time evolving properties of information (e.g., number of ratings per day) to identify opinion-based false information as well. Fact-based information that spreads (e.g., rumors) is also detected by creating true and false information propagation models. All these types of algorithms perform well in their respective datasets to identify false information, and usually achieve a high accuracy, precision, or AUC score in the 80s or 90s.","[['b81'], ['b41', 'b86'], ['b83'], ['b46'], [], ['b59'], ['b41'], [], [], ['b12'], ['b50'], ['b121'], ['b119', 'b112'], ['b73', 'b27', 'b128'], [], []]","[['b81'], ['b41', 'b86'], ['b83'], ['b46'], [], ['b59'], ['b41'], [], [], ['b12'], ['b50'], ['b121'], ['b119', 'b112'], ['b73', 'b27', 'b128'], [], []]",15,"sent1: Text-analysis is core to identifying misinformation as the information being conveyed is primarily textual content.
sent2: Similarly to opinion-based textual detection methods, research papers in this category are predominantly feature-based, where features can broadly be categorized as either stylometric (e.g., number of characters in a word), psycholinguistic (e.g., LIWC [72]), or complexity-oriented (e.g., readability indices).One of the first studies on identifying rumors on Twitter was done by Qazninian et al. [77].
sent3: They collected manual annotations for over 10,000 tweets, and developed three categories of features to identify the false tweets-primarily based on content (unigram, bigrams, and part-of-speech), but also used user information (whether user has previously posted false information), and Twitter-specific information (hashtags and URLs).
sent4: These features were converted into their log-likelihood ratio of being from the true or false class based on their distribution in the training data, and the combined score was used for classification.
sent5: This model achieved a mean average precision score of 95%, indicating near-perfect classification.
sent6: Individually, content features performed the best, followed by network features, and lastly hashtag and URL based Twitter features.
sent7: Content based features were also the best performing features in Gupta et al. [32], which focused on fake tweet detection as well.
sent8: A recent study on false news detection by Perez-Rosas et al. [74] shows the changing effectiveness of text-based features.
sent9: They collected a large dataset of false information generated by Amazon Mechanical Turk (AMT) workers and another dataset of celebrity fake news from gossip websites.
sent10: They used a huge set of text based features for classification, consisting of n-grams, punctuations, LIWC, readability, and syntax features.
sent11: Since the experiments were conducted on a balanced dataset, the baseline accuracy is 50%, and the combined set of features achieves an average accuracy of 74%.
sent12: Unsurprisingly, cross-domain analysis, i.e., training model on one dataset and testing on another, dropped the accuracy to 61%.
sent13: Within the same dataset, but training and testing on different domains of fake news (e.g., technology, education, etc.) the performance lies between 75% and 85%, indicating that knowledge can be transferred from one domain to another with reasonable performance.
sent14: Similarly, in their study to identify fake from real news from text, Horne and Adali [37] achieved accuracies between 71%-78%.
sent15: Identifying fake news from satire was more difficult than identifying fake news from real news, indicating that the former two are written quite similarly.
sent16: In their experiments, they observe that the news body text is less informative than the news title text in distinguishing real from fake information.
sent17: This is an important finding and opens avenues of future research to quantify the dissonance between the title and body of an information piece, and using it as an indicator of false information.
sent18: The above studies show an alarming trend.
sent19: In the earlier research papers, content-based detection performed well, but more recent research has shown that content alone is not good enough.
sent20: This suggests a trend that malicious users and content creators are adapting and becoming more aware of how to create more genuinelooking false information which fools automated classifiers.
sent21: User, network, and metadata based detection: These detection models derive features from several different aspects of data which we describe below.
sent22: We start by looking at the features developed to identify hoaxes in Wikipedia.
sent23: Kumar et al. [50] developed four different categories of features to identify hoaxes: (i) appearance features, that measures the length of the article, number of references, ratio of text to Wikipedia markup and so on, (ii) network features, that measures the relation between the references of the article in the Wikipedia hyperlink network, (iii) support features, quantifying the number of previous occurrences of the article title in other articles, and the time since its first occurrence to the time the article is created, and (iv) creator features, i.e., properties of article creator in terms of its previous edit  count, time since registration, and so on.
sent24: Figure 14 shows the performance of the features using a random forest classifier.
sent25: Individually, creator and network features perform equally well (AUC = 0.90)
sent26: Further, the performance can be improved significantly when other sets of features are incrementally added.
sent27: The combination of all four categories of features gives an AUC of 0.98, indicating a near perfect classifier.
sent28: This shows that in order to identify false information, one needs to look beyond its appearance and dig deeper into who created the piece of false information and how it relates to existing information-simply looking at its appearance is not as effective.
sent29: In the domain of fake Twitter images during disasters, Gupta et al. [32] used user features (number of friends, account age, etc.), tweet features (number of words, sentiment, POS, LIWC, etc.), and metadata features (number of hashtags, mentions, URLs, retweets).
sent30: The dataset had 11,534 tweets, half of which were fake and other half were real, and decision trees were used for classification.
sent31: User features alone had close to random accuracy of about 53%, while tweet features alone got near perfect accuracy of 97.7%, indicating that in the propagation of false information, what the tweet says is more important than the user who tweets it.
sent32: Thus, feature engineering has proven successful in identifying fake from true rumors and hoaxes, primarily using features derived from the text, user, network, and other metadata.
sent33: These algorithms typically have performance numbers in high 80s and 90s, showing that they are effective and practically useful.
sent34: 6.2.2 Detection using propagation modeling.
sent35: The spread of information across social media adds another dimension to use for identification of true information from false information.
sent36: A common way of using propagation information is to create models that broadly serve two purposes: to closely model the spread of (false) information, and to find ways to prevent its spread.
sent37: Creating true information propagation models: Acemoglu et al. [3] presented one of the first models to simulate propagation of information in social media.
sent38: They considered information spread as exchange of belief about information (e.g., supporting a political candidate), and theoretically find the cases in which false information survives in the network.
sent39: Nodes in the network are considered to be either normal or forceful.
sent40: When two normal nodes interact, each of them adopts an average of their existing beliefs.
sent41: But interactions with forceful nodes only change the belief of the normal node, while forceful node only slightly updates its beliefs.
sent42: With this interaction model, simulations showed that belief about false information dies out when the social network is well connected and normal
sent43: nodes interact with each other a lot.
sent44: On the other hand, echo chambers are formed and false information prevails when there are several forceful nodes who update their own belief from the beliefs of nodes they previously influenced.
sent45: This model suggests increasing the number of normal nodes and increasing their connectivity with each other may be a way to reduce false information propagation.
sent46: More recently, Jin et al. [41] created a false information propagation model which they called SEIZ.
sent47: Similar to the standard SIS model, each node in SEIZ model lies in one of four states-susceptible (S), exposed (E), infected (I), and skeptical (Z), as shown in Figure 15(a).
sent48: Initially, nodes are susceptible (state S).
sent49: When they receive information, they can transition to states I or Z with probabilities β and b, respectively.
sent50: These transitions may only be successful with some probabilities p and b, respectively, otherwise the nodes transition to state E.
sent51: The following four equations explain the transitions according to this SEIZ model:
sent52: The model parameters are learned by training on real true and false information propagation data, and these are used for prediction of false information.
sent53: A metric measuring the rate of users entering state E to leaving it is predictive of false information-a high value indicates true information while a low score indicates a rumor, as shown by their case study of eight rumors.
sent54: Incorporating propagation information in machine learning models: Apart from model creation, propagation information and propagation structure can both augment existing machine learning frameworks.
sent55: For example, Yang et al. [112] added propagation features such as number of replies and number of retweets to the standard set of features used for their classification tasks, such as content, client (device type), user, and location, to identify false information spread on the Sina Weibo network.
sent56: Using this feature set with an SVM classifier gave an average accuracy of 78%.
sent57: A more structured way of using propagation information was created by Wu et al. [110], who also studied 11,466 rumors on Sina Weibo by using propagation information.
sent58: Each thread of information was represented as a tree, with the original message as the root and its replies as children, and so on.
sent59: This is shown in Figure 15 (b) and (c) for false and true information, respectively.
sent60: Popular nodes, i.e., ones with at least 1,000 followers, are denoted as p and others as n, to understand if popular nodes boost false information.
sent61: The authors observe that false information is usually started by a normal user, then reported and supported by some opinion leaders, and then finally reshared by a large number of normal users (Figure 15(b)).
sent62: On the other hand, true information is posted by opinion leaders and then reposted directly by normal users (Figure 15(c)).
sent63: With features representing the propagation structure, user features, average sentiment, doubt, surprise and emotion, an SVM classifier achieved 91% accuracy.
sent64: Further, early detection of false information achieved 72% accuracy even without any propagation information, and 88% with propagation information of its first 24 hours.
sent65: Mitigation by modeling true and false information: Previously we described propagation models spreading false information.
sent66: Here, we consider models that model the spread of both true and false information simultaneously, where success is measured as the number of users saved from accepting false information.
sent67: The aim of these models is to create mitigation strategies to reduce and prevent the spread of false information.
sent68: Several such models have been developed.
sent69: Tripathy et al. [103] created two models in which true information (anti-rumor) is spread after false information(rumor) starts to spread, based on real data.
sent70: In one model, truth is spread n days after falsehood to simulate real-world observed time-lag, and in the second model, truth is spread by some special nodes (e.g., official accounts) as soon as they receive false information.
sent71: They conducted experiments with Twitter and simulated networks, and find that there is a super-linear relationship between the lifetime of a rumor and delay of its detection.
sent72: When the special nodes detect and spread anti-rumors, it reduces the average lifetime of rumors by over 60%.
sent73: Similarly, Budak et al. [18] presented an independent cascade model called Multi-Campaign Independence Cascade Model (MCICM).
sent74: The model contains a rumor campaign and a true information 'limiting' campaign spreading through the network.
sent75: Each node, whenever infected with true or false information, spreads its belief to its neighbors, which accept the information with some probability.
sent76: Their algorithm learns the model parameters, even with missing data.
sent77: Their simulations show that a 30% increase in delay in starting the true information spread reduces its reach by 80% of the population.
sent78: More recently, Nguyen et al. [64] created another model with both linear threshold and independent cascade, and found that when true information can only be spread by a few nodes, it is most effective to do it via highly influential nodes in large communities.
sent79: When more nodes can be selected, influential nodes in smaller communities are more effective in preventing the spread of false information.
sent80: This method is 16-41% better than other methods.
sent81: Related models have also been developed [119].
sent82: Thus, several propagation models have been developed that capture the spread of true and false information on social media.
sent83: These models are used independently or in conjunction with other machine learning algorithms to identify false information.
sent84: These algorithms are effective in detecting spread of rumors, and their simulations suggest rumor mitigation strategies.
sent85: Overall, several categories of false information detection algorithms have been developed for opinion-based and fact-based false information.
sent86: The common category of algorithm is feature-based, which converts observations into feature vectors, derived from text, user, network, and metadata.
sent87: Graph-based algorithms have primarily been developed for opinion-based false information (e.g., fake reviews), and identify dense block of users or information, potentially also occurring in bursty short time period.
sent88: Temporal modeling algorithms use time-series analysis, and co-clustering on one or more of time evolving properties of information (e.g., number of ratings per day) to identify opinion-based false information as well.
sent89: Fact-based information that spreads (e.g., rumors) is also detected by creating true and false information propagation models.
sent90: All these types of algorithms perform well in their respective datasets to identify false information, and usually achieve a high accuracy, precision, or AUC score in the 80s or 90s."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s3,Categorization based on intent,"False information can be classified based on the intent of the author, as misinformation and disinformation [25,35]. By definition, misinformation is spread without the intent to deceive. Thus, common causes of misinformation include misrepresentation or distortion of an original piece of true information by an actor, due to lack of understanding, attention or even cognitive biases [24,93]. These actors can then spread misinformation unwittingly to others via blogs, articles, comments, tweets, and so on. Note that readers can often simply have different interpretations and perception of the same piece of true information, leading to differences in how they communicate their understandings and in turn inform others' perception of the facts [4,45].

Conversely, disinformation is spread with the intent to deceive [76]. Thus, understanding the motives for disinformation are much akin to understanding the motives for deception [25,93]. Deception on the web occurs for many purposes, and for similar (though less interpersonal) reasons as in human interactions. The large potential audience leads most web disinformation campaigns to focus on swaying public opinion in one way or another, or driving online traffic to target websites to earn money by advertisements. One recent example is political disinformation spread during 2016 USA presidential elections [29,38,87], which even led to public shootings [27]. In this survey, we focus primarily on the technological aspects of disinformation, as the majority of research focuses around it.","[['b102', 'b54', 'b34', 'b44', 'b13', 'b33'], ['b85', 'b102', 'b47', 'b96', 'b38', 'b36', 'b34']]","[['b102', 'b54', 'b34', 'b44', 'b13', 'b33'], ['b85', 'b102', 'b47', 'b96', 'b38', 'b36', 'b34']]",13,"sent1: False information can be classified based on the intent of the author, as misinformation and disinformation [25,35].
sent2: By definition, misinformation is spread without the intent to deceive.
sent3: Thus, common causes of misinformation include misrepresentation or distortion of an original piece of true information by an actor, due to lack of understanding, attention or even cognitive biases [24,93].
sent4: These actors can then spread misinformation unwittingly to others via blogs, articles, comments, tweets, and so on.
sent5: Note that readers can often simply have different interpretations and perception of the same piece of true information, leading to differences in how they communicate their understandings and in turn inform others' perception of the facts [4,45].
sent6: Conversely, disinformation is spread with the intent to deceive [76].
sent7: Thus, understanding the motives for disinformation are much akin to understanding the motives for deception [25,93].
sent8: Deception on the web occurs for many purposes, and for similar (though less interpersonal) reasons as in human interactions.
sent9: The large potential audience leads most web disinformation campaigns to focus on swaying public opinion in one way or another, or driving online traffic to target websites to earn money by advertisements.
sent10: One recent example is political disinformation spread during 2016 USA presidential elections [29,38,87], which even led to public shootings [27].
sent11: In this survey, we focus primarily on the technological aspects of disinformation, as the majority of research focuses around it."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s26,Detection of opinion-based false information,"Here we look at the algorithms that have been developed in literature to identify opinion-based false information. Specifically, we look at the research on identifying fake reviews in online platforms using text, time, and graph algorithms. Text-based algorithms primarily convert the textual information into a huge feature vector and feed that vector into supervised learning models to identify duplicate and fake reviews. Graph-based algorithms leverage the user-review-product graph to propagate beliefs and to jointly model 'trustworthiness' of users, reviews, and products. Time-based algorithms employ time-series modeling and co-clustering along with feature engineering. We will elaborate on these algorithms in the next few subsections.

6.1.1 Feature-based detection. As text is the primary source to convey (false) information on web platforms, it is one of the most widely studied component for fake review detection. Algorithms in this domain are based on feature engineering, detecting duplicate reviews, or a combination of the two. We primarily focus on text-based detection, as other features, such as user, graph, score, and time, are usually used in conjunction with text features in this task.

Several algorithms have been developed to efficiently identify duplicate reviews, with the notion that fraudsters give identical or near-identical reviews while genuine reviewers give more unique reviews. Jindal et al. [43] studied three major types of duplicates: different users reviewing the same product, same user reviewing different products, and different users on different products. They built a logistic regression model to detect fraudulent reviews incorporating rating and textual features such as review title and body length, sentiment, cosine similarity between review and product texts, and others, and achieved an AUC of 78%. Similarly, Mukherjee et al. [61] leveraged cosine similarity across a user's given reviews and across a product's received reviews in addition to rating and temporal features in an unsupervised generative Bayesian model to automatically discern separating features of truthful and fraudulent reviewers (AUC = 0.86). Going beyond syntax, Sandulescu et al. [82] studied the problem of detecting singleton review spammers by comparing both review syntax and semantic similarity in pairwise reviews per business, and marked reviews with high similarity as fraudulent. Syntactic similarity was measured using part-of-speech tags and semantic similarity using word-to-word distances in the WordNet synonyms database. This approach achieved F1-score between 0.5 and 0.7 on Yelp and Trustpilot customer reviews data, and suggests that intelligent fraudsters often duplicate semantically similar messages by replacing some words between their fake reviews with synonymous or similar words in order to avoid generating blatant duplicates and be caught.

However, more complex review fraud exists, where fraudsters put considerably more effort than just duplicating review text in order to write sophisticated, deceptive reviews. To get ground truth deceptive reviews, Amazon Mechanical Turk (AMT) workers are frequently employed. The associated detection algorithms largely rely on text-processing and feature engineering for detecting such reviews. Ott et al. [70] collected 400 truthful and 400 positive-sentiment deceptive AMT-sourced reviews and trained Support Vector Machine (SVM) classifiers using a variety of feature sets, such as n-grams and LIWC features [72]. This achieved high 0.9 F1-score compared to human judges, who at best achieved a 0.7 F1-score. In a followup work [69], negative sentiment deceptive reviews were studied, with additional 400 negative reviews from AMT. Experiments showed that an SVM classifier trained on bigrams was again able to achieve strong performance in detecting such reviews, with a 0.86 F1-score. The authors additionally studied classifier performance when training on positive sentiment reviews and testing on negative sentiment reviews, and vice versa-results showed that such heterogeneity between training and testing data produced considerably worse F1-score (roughly a 0.1-0.2 reduction) than the homogeneous case, indicating different statistical patterns in positive and negative sentiment deceptive reviews versus truthful reviews.

While AMT generated reviews are common to use, they lack domain expertise. To address that, Li et al. [54] collected additional deceptive reviews from domain experts such as employees at target venues. The authors used n-gram features as in previous works and employ both Sparse Additive Generative Model (SAGE) and SVM classifiers to evaluate pairwise and three-class classification (truthful customer vs deceptive Turker vs deceptive employee) performance ( 65% accuracy). Their results showed that distinguishing truthful customers from deceptive employees is somewhat more difficult than from deceptive Turkers. Further, Li et al. [51] added sentiment, subjectivity, and pronoun usage features to the set. They then created a semi-supervised co-training algorithm that iteratively learns classifiers from review and reviewer features separately, and augments the training set in each successive iteration with the most agreed-upon and confidently scored reviews. This model achieves an F1-score of 0.63 on an Epinions review dataset.

A major aim of these systems is to aid humans in identifying fraud. Harris [33] focused on the usefulness of these models to human judges. Equipping human judges with these simple summary statistics of reviews improved their manual classification accuracy by up to 20% over the alternative (without), showing the effectiveness of augmented detection for humans at a cheaper computational cost.

6.1.2 Graph-based detection. These algorithms leverage the rating graph for identifying fake edges. Nodes in the graph represent users and products, and edge from u to p represents a review by user u to product p. Some algorithms also use features which may be available on nodes and/or edges. Fig. 12. Graph-based fake review detection algorithms are usually based on homophily, where good (green) users give positive ""thumbs up"" to other good products, while bad (red) users give negative ""thumbs down"" to them. The opposite is true for ratings given to bad products. Figure reprinted with permission from [5].

Belief propagation on the rating graph is one common way to identify fraud. Rayana et al. [79] used loopy belief propagation on the review graph network for identifying fake reviews, extending the idea of FraudEagle from Akoglu et al. [5]. The basic idea is presented in Figure 12. These algorithms take a signed network, i.e. a network where the edges are converted to be positive (thumbs up) and negative (thumbs down), and employ the notion of homophily which suggests that most honest users give genuine positive ratings to good products and negative ratings to bad products, and vice-versa for bad fraudulent users. This is expressed as a Markov Random Field, where the joint probability P(y) of inferred labels Y i is a product of entity i's prior beliefs ϕ i (y i ) and its compatibility with labels of its neighbors j represented as γ s i j (y i , y j ), with the compatibility matrix s. Mathematically,

s)∈E ±γ s i j (y i , y j ) This is solved using loopy belief propagation, with prior-based initialization and transfer of beliefs across the network till convergence. Based on this idea, Rayana et al. [79] combines belief propagation with feature values of nodes and edges as well. This SpEagle algorithm is highly accurate in identifying fake reviews (and users) in three Yelp fake review datasets, with area under the ROC curve scores around 0.78 on average. Several algorithms have been developed for jointly modeling user, review, and product information, with applications to fake review detection. Wang et al. [106] uses the review network to measure trustiness of users T (u), honesty of reviews H (r ), and reliability of stores R(s), all of which lie between -1 and +1. For calculating H (r ) for a review r given to product p, the trustiness of users S + r and S − r who gave similar and different scores, respectively, to product p close in time to r . This is calculated as agreement score

. Logistic functions are used to bound the scores in (-1, +1). The honest score H (r ) can then be used to identify fake reviews. The formulation is mutually interdependent as follows: The authors tested the algorithm to identify fake reviewers, which is a closely related problem, and get a precision of 49% on the 100 users with the smallest trustiness scores.

Closely related to the previous algorithm is Rev2 by Kumar et al. [48], which is also an iterative algorithm which calculates reviewer fairness, rating reliability and product goodness scores. The algorithm is based on the intuition that fraudulent review writers are typically unfair, in that they give unreliable rating scores to products that differ largely from the product's average score. Reliable reviewers give ratings that are close to the scores of other reliable reviewers. This algorithm incorporates user and product features by merging scores from a prior algorithm called Birdnest [36], and uses Bayesian priors for addressing cold start problems, i.e., judging the quality of users and products that only have a few ratings. This formulation is also interdependent, and the rating reliability is used to identify fake reviews. This algorithm achieves an AUC of over 0.85 for identifying fraudulent reviewers.

Several graph based algorithms have been developed to identify fraudulent nodes in review networks, using edge distributions [36,84], dense block detection [16,40], co-clustering [15], and more. This problem is closely related to identifying fake reviews, as the intuition is that by identifying fraudulent users, one can identify remove all their reviews and eliminate fake reviews. However, while these techniques may work for identifying bad users, these may not work well as-is in fake review detection because of two reasons: first, not all reviews by fraudulent users are necessarily fake [48] (for example, the user might aim to camouflage themselves by giving a few genuine reviews) and second, not all fake reviews are given by fraudulent users, which would hinder recall for fake review detection. Thus we do not focus on these algorithms in detail.","[[], [], ['b52', 'b91', 'b70'], ['b78', 'b79', 'b81'], ['b63', 'b60'], ['b42'], ['b14'], ['b88', 'b14'], ['b88', 'b115'], [], ['b45', 'b57'], ['b49', 'b57', 'b45', 'b25', 'b93', 'b24']]","[[], [], ['b52', 'b91', 'b70'], ['b78', 'b79', 'b81'], ['b63', 'b60'], ['b42'], ['b14'], ['b88', 'b14'], ['b88', 'b115'], [], ['b45', 'b57'], ['b49', 'b57', 'b45', 'b25', 'b93', 'b24']]",22,"sent1: Here we look at the algorithms that have been developed in literature to identify opinion-based false information.
sent2: Specifically, we look at the research on identifying fake reviews in online platforms using text, time, and graph algorithms.
sent3: Text-based algorithms primarily convert the textual information into a huge feature vector and feed that vector into supervised learning models to identify duplicate and fake reviews.
sent4: Graph-based algorithms leverage the user-review-product graph to propagate beliefs and to jointly model 'trustworthiness' of users, reviews, and products.
sent5: Time-based algorithms employ time-series modeling and co-clustering along with feature engineering.
sent6: We will elaborate on these algorithms in the next few subsections.
sent7: 6.1.1 Feature-based detection. As text is the primary source to convey (false) information on web platforms, it is one of the most widely studied component for fake review detection.
sent8: Algorithms in this domain are based on feature engineering, detecting duplicate reviews, or a combination of the two.
sent9: We primarily focus on text-based detection, as other features, such as user, graph, score, and time, are usually used in conjunction with text features in this task.
sent10: Several algorithms have been developed to efficiently identify duplicate reviews, with the notion that fraudsters give identical or near-identical reviews while genuine reviewers give more unique reviews.
sent11: Jindal et al. [43] studied three major types of duplicates: different users reviewing the same product, same user reviewing different products, and different users on different products.
sent12: They built a logistic regression model to detect fraudulent reviews incorporating rating and textual features such as review title and body length, sentiment, cosine similarity between review and product texts, and others, and achieved an AUC of 78%.
sent13: Similarly, Mukherjee et al. [61] leveraged cosine similarity across a user's given reviews and across a product's received reviews in addition to rating and temporal features in an unsupervised generative Bayesian model to automatically discern separating features of truthful and fraudulent reviewers (AUC = 0.86).
sent14: Going beyond syntax, Sandulescu et al. [82] studied the problem of detecting singleton review spammers by comparing both review syntax and semantic similarity in pairwise reviews per business, and marked reviews with high similarity as fraudulent.
sent15: Syntactic similarity was measured using part-of-speech tags and semantic similarity using word-to-word distances in the WordNet synonyms database.
sent16: This approach achieved F1-score between 0.5 and 0.7 on Yelp and Trustpilot customer reviews data, and suggests that intelligent fraudsters often duplicate semantically similar messages by replacing some words between their fake reviews with synonymous or similar words in order to avoid generating blatant duplicates and be caught.
sent17: However, more complex review fraud exists, where fraudsters put considerably more effort than just duplicating review text in order to write sophisticated, deceptive reviews.
sent18: To get ground truth deceptive reviews, Amazon Mechanical Turk (AMT) workers are frequently employed.
sent19: The associated detection algorithms largely rely on text-processing and feature engineering for detecting such reviews.
sent20: Ott et al. [70] collected 400 truthful and 400 positive-sentiment deceptive AMT-sourced reviews and trained Support Vector Machine (SVM) classifiers using a variety of feature sets, such as n-grams and LIWC features [72].
sent21: This achieved high 0.9 F1-score compared to human judges, who at best achieved a 0.7 F1-score.
sent22: In a followup work [69], negative sentiment deceptive reviews were studied, with additional 400 negative reviews from AMT.
sent23: Experiments showed that an SVM classifier trained on bigrams was again able to achieve strong performance in detecting such reviews, with a 0.86 F1-score.
sent24: The authors additionally studied classifier performance when training on positive sentiment reviews and testing on negative sentiment reviews, and vice versa-results showed that such heterogeneity between training and testing data produced considerably worse F1-score (roughly a 0.1-0.2 reduction) than the homogeneous case, indicating different statistical patterns in positive and negative sentiment deceptive reviews versus truthful reviews.
sent25: While AMT generated reviews are common to use, they lack domain expertise.
sent26: To address that, Li et al. [54] collected additional deceptive reviews from domain experts such as employees at target venues.
sent27: The authors used n-gram features as in previous works and employ both Sparse Additive Generative Model (SAGE) and SVM classifiers to evaluate pairwise and three-class classification (truthful customer vs deceptive Turker vs deceptive employee) performance ( 65% accuracy).
sent28: Their results showed that distinguishing truthful customers from deceptive employees is somewhat more difficult than from deceptive Turkers.
sent29: Further, Li et al. [51] added sentiment, subjectivity, and pronoun usage features to the set.
sent30: They then created a semi-supervised co-training algorithm that iteratively learns classifiers from review and reviewer features separately, and augments the training set in each successive iteration with the most agreed-upon and confidently scored reviews.
sent31: This model achieves an F1-score of 0.63 on an Epinions review dataset.
sent32: A major aim of these systems is to aid humans in identifying fraud.
sent33: Harris [33] focused on the usefulness of these models to human judges.
sent34: Equipping human judges with these simple summary statistics of reviews improved their manual classification accuracy by up to 20% over the alternative (without), showing the effectiveness of augmented detection for humans at a cheaper computational cost.
sent35: 6.1.2 Graph-based detection. These algorithms leverage the rating graph for identifying fake edges.
sent36: Nodes in the graph represent users and products, and edge from u to p represents a review by user u to product p.
sent37: Some algorithms also use features which may be available on nodes and/or edges.
sent38: Fig. 12. Graph-based fake review detection algorithms are usually based on homophily, where good (green) users give positive ""thumbs up"" to other good products, while bad (red) users give negative ""thumbs down"" to them.
sent39: The opposite is true for ratings given to bad products.
sent40: Figure reprinted with permission from [5].
sent41: Belief propagation on the rating graph is one common way to identify fraud.
sent42: Rayana et al. [79] used loopy belief propagation on the review graph network for identifying fake reviews, extending the idea of FraudEagle from Akoglu et al. [5].
sent43: The basic idea is presented in Figure 12.
sent44: These algorithms take a signed network, i.e. a network where the edges are converted to be positive (thumbs up) and negative (thumbs down), and employ the notion of homophily which suggests that most honest users give genuine positive ratings to good products and negative ratings to bad products, and vice-versa for bad fraudulent users.
sent45: This is expressed as a Markov Random Field, where the joint probability P(y) of inferred labels Y i is a product of entity i's prior beliefs
sent46: ϕ i (y i ) and its compatibility with labels of its neighbors j represented as γ s i j (y i , y j ), with the compatibility matrix s. Mathematically,s)∈E ±γ s
sent47: i j (y i , y j ) This is solved using loopy belief propagation, with prior-based initialization and transfer of beliefs across the network till convergence.
sent48: Based on this idea, Rayana et al. [79] combines belief propagation with feature values of nodes and edges as well.
sent49: This SpEagle algorithm is highly accurate in identifying fake reviews (and users) in three Yelp fake review datasets, with area under the ROC curve scores around 0.78 on average.
sent50: Several algorithms have been developed for jointly modeling user, review, and product information, with applications to fake review detection.
sent51: Wang et al. [106] uses the review network to measure trustiness of users T (u), honesty of reviews H (r ), and reliability of stores R(s), all of which lie between -1 and +1.
sent52: For calculating H (r ) for a review r given to product p, the trustiness of users S + r and S − r who gave similar and different scores, respectively, to product p close in time to r .
sent53: This is calculated as agreement score.
sent54: Logistic functions are used to bound the scores in (-1, +1).
sent55: The honest score H (r ) can then be used to identify fake reviews.
sent56: The formulation is mutually interdependent as follows: The authors tested the algorithm to identify fake reviewers, which is a closely related problem, and get a precision of 49% on the 100 users with the smallest trustiness scores.
sent57: Closely related to the previous algorithm is Rev2 by Kumar et al. [48], which is also an iterative algorithm which calculates reviewer fairness, rating reliability and product goodness scores.
sent58: The algorithm is based on the intuition that fraudulent review writers are typically unfair, in that they give unreliable rating scores to products that differ largely from the product's average score.
sent59: Reliable reviewers give ratings that are close to the scores of other reliable reviewers.
sent60: This algorithm incorporates user and product features by merging scores from a prior algorithm called Birdnest [36], and uses Bayesian priors for addressing cold start problems, i.e., judging the quality of users and products that only have a few ratings.
sent61: This formulation is also interdependent, and the rating reliability is used to identify fake reviews.
sent62: This algorithm achieves an AUC of over 0.85 for identifying fraudulent reviewers.
sent63: Several graph based algorithms have been developed to identify fraudulent nodes in review networks, using edge distributions [36,84], dense block detection [16,40], co-clustering [15], and more.
sent64: This problem is closely related to identifying fake reviews, as the intuition is that by identifying fraudulent users, one can identify remove all their reviews and eliminate fake reviews.
sent65: However, while these techniques may work for identifying bad users, these may not work well as-is in fake review detection because of two reasons: first, not all reviews by fraudulent users are necessarily fake [48] (for example, the user might aim to camouflage themselves by giving a few genuine reviews) and second, not all fake reviews are given by fraudulent users, which would hinder recall for fake review detection.
sent66: Thus we do not focus on these algorithms in detail."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s23,5.2.5,"Debunking characteristics. Once false information spreads, attempts are made to debunk it and limit its spread. Recent research has shown that there is a significant time delay between the spread and its debunking. Zubiaga et al. [121] found that true information tends to be resolved faster than false information, which tends to take about 14 hours to be debunked. Shao et al. [86] came to a similar conclusion-they found a delay of 10-20 hours between the start of a rumor and sharing of its fact-checking contents.

But once debunking information reaches the rumor spreaders, do they stop spreading it or does it 'back-fire', as observed in in-lab settings [68] where corrections led to an increase in misperception? Several empirical studies on web-based false information suggest that debunking rumors is in fact effective, and people start deleting and questioning the rumor when presented with corrective information. Frigerri et al. [30] studied the spread of thousands of rumor reshare cascades on Facebook, and found that false information is more likely to be linked to debunking articles than true information. Moreover, once it is linked, it leads to a 4.4 times increase in deletion probability of false information than when it is not, and the probability is even higher if the link is made shortly after the post is created. Moreover, Zubiaga et al. [121] found that there are more tweets denying a rumor than supporting it after it is debunked, while prior to debunking, more tweets support the rumor. Furthermore, Vosoughi et al. [105] showed that there is a striking difference between replies on tweet containing false information than those containing true information-while people express fear, disgust, and surprise in replies, true information generates anticipation, sadness, joy, and trust. These differences can potentially be used to create early detection and debunking tools.

Overall, research on characterization of fact-based false information has shown that it tends to be longer, generates more disbelief and confusion during discussions, is created by newer and less experienced accounts that are tightly connected to each other, spreads deeper and faster in one and across multiple platforms, and gets deleted when debunking information spreads.","[['b95', 'b130'], ['b39', 'b77', 'b114', 'b130'], []]","[['b95', 'b130'], ['b39', 'b77', 'b114', 'b130'], []]",6,"sent1: Debunking characteristics. Once false information spreads, attempts are made to debunk it and limit its spread.
sent2: Recent research has shown that there is a significant time delay between the spread and its debunking.
sent3: Zubiaga et al. [121] found that true information tends to be resolved faster than false information, which tends to take about 14 hours to be debunked.
sent4: Shao et al. [86] came to a similar conclusion-they found a delay of 10-20 hours between the start of a rumor and sharing of its fact-checking contents.
sent5: But once debunking information reaches the rumor spreaders, do they stop spreading it or does it 'back-fire', as observed in in-lab settings [68] where corrections led to an increase in misperception?
sent6: Several empirical studies on web-based false information suggest that debunking rumors is in fact effective, and people start deleting and questioning the rumor when presented with corrective information.
sent7: Frigerri et al. [30] studied the spread of thousands of rumor reshare cascades on Facebook, and found that false information is more likely to be linked to debunking articles than true information.
sent8: Moreover, once it is linked, it leads to a 4.4 times increase in deletion probability of false information than when it is not, and the probability is even higher if the link is made shortly after the post is created.
sent9: Moreover, Zubiaga et al. [121] found that there are more tweets denying a rumor than supporting it after it is debunked, while prior to debunking, more tweets support the rumor.
sent10: Furthermore, Vosoughi et al. [105] showed that there is a striking difference between replies on tweet containing false information than those containing true information-while people express fear, disgust, and surprise in replies, true information generates anticipation, sadness, joy, and trust.
sent11: These differences can potentially be used to create early detection and debunking tools.
sent12: Overall, research on characterization of fact-based false information has shown that it tends to be longer, generates more disbelief and confusion during discussions, is created by newer and less experienced accounts that are tightly connected to each other, spreads deeper and faster in one and across multiple platforms, and gets deleted when debunking information spreads."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s22,Propagation characteristics.,"The spread of false information in social media makes it highly impactful. Several research studies have shown that only a small fraction of users are responsible for most of the spread, instead of being akin to a grass-roots movement. Gupta et al. [32] found that the top 30 users contributed towards 90% of the retweets of fake images during hurricane Sandy on Twitter. Shao et al. [86] came to a similar conclusion in their study of about 1.3 million rumor tweets as well. Their analysis suggested that fake news spread was mostly dominated by a handful of very active users, whereas fact-checking of rumors was a more grass-roots activity with more conversation, and therefore slower. This suggests that repetition and perseverance play an important role in the spread of false information. Since people tend to spread unverified claims [91,121], making false information believable may not be as important as persistently spreading it.

When false information spreads in social platforms, it spreads deeper compared to real news. In their study of rumor reshares on Facebook, Frigerri et al. [30] concluded that false information reshare cascades spread much deeper compared to that of true reference cascades. In other words, they are more likely to be reshared at greater depth and thus reach more people. One such reshare cascade is shown in Figure 10, with cascades colored by time. Additionally, Zeng et al. [118] showed that information related to rumors, both supportive and denying, spread faster than non-rumors. Simulations conducted by Doerr et al. [23] on realistic spread of simple rumors, on several graphs having the structure of existing large social networks, showed that even a rumor started by a random node on average reaches 45.6 million of the total of 51.2 million members within only eight rounds of communication. This is corroborated by the bursty behavior of rumors shown in several other research studies [92,121].

Several researchers have shown that false information spreads quickly, especially during its early stage. Zubiaga et al. [121] studied the entire lifecycle of true and false information as it spreads through social media, both before and after its veracity is checked. They collected 330 rumor threads with 4,842 tweets of nine popular cases, such as Charlie Hebdo shooting, and Michael Essien contracting Ebola. Journalists then annotated the discussion threads of these rumors to quantify support expressed in tweets, i.e., their level of certainty (level of confidence indicated by the tweet) and evidence (whether the tweet substantiates the rumor). They found that the spread of false information occurs largely before it is even debunked. Tweets that supported unverified claims generated the most retweets, sparked by sudden bursts of retweets even during the first few minutes, with interest in the rumor decreasing substantially after its veracity is checked. During the initial spread of information, all users including normal users as well as reputed ones affiliated with news organizations, tend to tweet with a bias towards supporting unverified claims as opposed to denying them, irrespective of whether the information is later confirmed or denied. Silverman [91] corroborates this finding. The level of certainty of tweets tends to remain the same before and after information is fact-checked, but users give more evidence before the rumor is fact-checked and less later on. These findings together further evidence the virality of popular false information during its initial phase.Further, Vosoughi et al. [105] also showed that tweets about false information spread significantly farther, deeper, faster, and broader than those about true information. This was observed for all categories of false information, such as politics, urban legend, science, business, and more.

While the above studies focus on spread of (false) information on a single platform, recent studies by Zannettou et al. [117] and Albright [7,8] mapped the false information ecosystem across social media platforms. Zannettou et al. [117] studied the temporal relation between same information piece appearing on Twitter, Reddit, and 4chan platforms. They found that false information pieces are more likely to spread across platforms (18% appear on multiple platforms) compared to true information (11%). Moreover, false information appears faster across platforms than legitimate ones, and seems to 'flow' from one to another, with Reddit to Twitter to 4chan being the most common route. This spread across platforms is dangerous-Albright [8] studied the logs seven false information websites, and found that a whooping 60% of incoming traffic was from Facebook and Twitter, and rest were from emails, search engines, messaging, or direct visits. To study how these platforms connect to one another, Albright [7] crawled 117 false information websites and created a hyperlink network of domains that these websites refer to. He found that right-wing news websites link highly to other similar websites, thus supporting each other. Very surprisingly, YouTube was the most linked website overall, suggesting high use of multimedia content in conveying false information messages.

Thus, these studies have found that false information tends to propagate deeper and faster than true information, especially during the early stages of the false information. This happens on a single as well as across multiple platforms, and a handful of users are primarily responsible for this spread.","[['b95', 'b41', 'b130', 'b100'], ['b101', 'b39', 'b130', 'b32', 'b127'], ['b114', 'b130', 'b100'], ['b17', 'b16', 'b126'], []]","[['b95', 'b41', 'b130', 'b100'], ['b101', 'b39', 'b130', 'b32', 'b127'], ['b114', 'b130', 'b100'], ['b17', 'b16', 'b126'], []]",15,"sent1: The spread of false information in social media makes it highly impactful.
sent2: Several research studies have shown that only a small fraction of users are responsible for most of the spread, instead of being akin to a grass-roots movement.
sent3: Gupta et al. [32] found that the top 30 users contributed towards 90% of the retweets of fake images during hurricane Sandy on Twitter.
sent4: Shao et al. [86] came to a similar conclusion in their study of about 1.3 million rumor tweets as well.
sent5: Their analysis suggested that fake news spread was mostly dominated by a handful of very active users, whereas fact-checking of rumors was a more grass-roots activity with more conversation, and therefore slower.
sent6: This suggests that repetition and perseverance play an important role in the spread of false information.
sent7: Since people tend to spread unverified claims [91,121], making false information believable may not be as important as persistently spreading it.
sent8: When false information spreads in social platforms, it spreads deeper compared to real news.
sent9: In their study of rumor reshares on Facebook, Frigerri et al. [30] concluded that false information reshare cascades spread much deeper compared to that of true reference cascades.
sent10: In other words, they are more likely to be reshared at greater depth and thus reach more people.
sent11: One such reshare cascade is shown in Figure 10, with cascades colored by time.
sent12: Additionally, Zeng et al. [118] showed that information related to rumors, both supportive and denying, spread faster than non-rumors.
sent13: Simulations conducted by Doerr et al. [23] on realistic spread of simple rumors, on several graphs having the structure of existing large social networks, showed that even a rumor started by a random node on average reaches 45.6 million of the total of 51.2 million members within only eight rounds of communication.
sent14: This is corroborated by the bursty behavior of rumors shown in several other research studies [92,121].
sent15: Several researchers have shown that false information spreads quickly, especially during its early stage.
sent16: Zubiaga et al. [121] studied the entire lifecycle of true and false information as it spreads through social media, both before and after its veracity is checked.
sent17: They collected 330 rumor threads with 4,842 tweets of nine popular cases, such as Charlie Hebdo shooting, and Michael Essien contracting Ebola.
sent18: Journalists then annotated the discussion threads of these rumors to quantify support expressed in tweets, i.e., their level of certainty (level of confidence indicated by the tweet) and evidence (whether the tweet substantiates the rumor).
sent19: They found that the spread of false information occurs largely before it is even debunked.
sent20: Tweets that supported unverified claims generated the most retweets, sparked by sudden bursts of retweets even during the first few minutes, with interest in the rumor decreasing substantially after its veracity is checked.
sent21: During the initial spread of information, all users including normal users as well as reputed ones affiliated with news organizations, tend to tweet with a bias towards supporting unverified claims as opposed to denying them, irrespective of whether the information is later confirmed or denied.
sent22: Silverman [91] corroborates this finding.
sent23: The level of certainty of tweets tends to remain the same before and after information is fact-checked, but users give more evidence before the rumor is fact-checked and less later on.
sent24: These findings together further evidence the virality of popular false information during its initial phase.
sent25: Further, Vosoughi et al. [105] also showed that tweets about false information spread significantly farther, deeper, faster, and broader than those about true information.
sent26: This was observed for all categories of false information, such as politics, urban legend, science, business, and more.
sent27: While the above studies focus on spread of (false) information on a single platform, recent studies by Zannettou et al. [117] and Albright [7,8] mapped the false information ecosystem across social media platforms.
sent28: Zannettou et al. [117] studied the temporal relation between same information piece appearing on Twitter, Reddit, and 4chan platforms.
sent29: They found that false information pieces are more likely to spread across platforms (18% appear on multiple platforms) compared to true information (11%).
sent30: Moreover, false information appears faster across platforms than legitimate ones, and seems to 'flow' from one to another, with Reddit to Twitter to 4chan being the most common route.
sent31: This spread across platforms is dangerous-Albright [8] studied the logs seven false information websites, and found that a whooping 60% of incoming traffic was from Facebook and Twitter, and rest were from emails, search engines, messaging, or direct visits.
sent32: To study how these platforms connect to one another, Albright [7] crawled 117 false information websites and created a hyperlink network of domains that these websites refer to.
sent33: He found that right-wing news websites link highly to other similar websites, thus supporting each other.
sent34: Very surprisingly, YouTube was the most linked website overall, suggesting high use of multimedia content in conveying false information messages.
sent35: Thus, these studies have found that false information tends to propagate deeper and faster than true information, especially during the early stages of the false information.
sent36: This happens on a single as well as across multiple platforms, and a handful of users are primarily responsible for this spread."
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s10,IMPACT OF FALSE INFORMATION,"Given that there are several factors that lead to deception by false information (Section 3.1), what is the impact of false information on its readers on web and social media? In the real world, false information has been shown to have significant impact on the stock market [17], hampering response during natural disasters [32], and terroristic activity [27,96]. On web and social media, the impact is measured as the engagement it produces via its readers, using statistics such as number of reads, number of days it survived without being removed, or number of people it reached via reshares. Several research studies have been conducted to measure the impact of hoaxes, fake reviews, and fake news. Frigerri et al. [30] studied the spread of rumors on Facebook, Kumar et al. [50] measured the impact of hoax articles on Wikipedia, and Silverman [92] analyzed the engagement of fake election news articles on Facebook. We discuss these studies to measure the impact of false information on web platforms.

False information spreads far and wide on social media because there is an average delay of 12 hours between start of false information spread and that of its debunking information [86,121]. False information spreads rapidly during its starting phase-an unverified and not yet debunked rumor has high potential of becoming viral [121]. As a result, rumors with the possibility of being true start to spread, sometimes even by reputed news organizations [91].

On Wikipedia, Kumar et al. [50] measured impact of hoaxes in terms of their viewcount, number of days they survived before they are deleted, and their spread across the web. Figure 5 shows the distributions for the first two statistics. Figure 5(a) shows the distribution of the time it takes from when the article is created and approved ('patrolled') till the time it is identified as a hoax ('flagged'). It shows that while 90% of hoax articles are identified immediately within an hour of being approved, about 1% of hoaxes that are well-written hoaxes survive for over one year without being detected. However, survival is not enough for a hoax article to be successful; it must be viewed as well. Figure 5(b) plots the counter-cumulative distribution of average view count of hoaxes that survive for at least a week and their equivalent non-hoaxes. On average, hoaxes are viewed less frequently than non-hoaxes (median 3 views per day vs 3.5 views per day), but a non-negligible 1% of hoaxes are viewed at least 100 times a day. Finally, the impact of hoaxes is measured in terms of spread over the web, by counting the links that were clicked by readers to reach the hoax article. For this, 5 months of Wikipedia server logs were used. The results were alarming-at least 5 distinct links were clicked from across the web for 7% of hoaxes, and on average, each hoax had 1.1 such links. This traffic was observed from search engines, social networks such as Facebook and Twitter, and from within Wikipedia itself. Overall, this analysis shows that while most hoax articles are ineffective, a small fraction of hoax articles on Wikipedia is highly impactful.

Buzzfeed news analyzed highly impactful fake political news on the web. They analyzed both true and false election-related stories with the highest engagement on Facebook during the 2016 US Presidential election [92]. Engagement was measured as the total number of shares, reactions, and comments on the Facebook story. They analyzed the 20 top-performing false election stories generated by fake websites and blogs, and compared them to the 20 top-performing true election stories from major news websites, like New York Times, Washington Post, and others. The fake news stories got a total of 8,711,000 engagements, significantly higher than the 7,367,000 engagements of the real news stories. As this analysis was restricted to top stories, a complete analysis of all news stories may reveal a different picture. Prior to this study, Gupta et al. [32] studied the spread of eight fake images on Twitter during Hurricane Sandy, and found that that fake images were shared almost twice as much as real images.

On a larger scale, Frigerri et al. [30] conducted a comprehensive study of the spread of false and real information on Facebook. They collected 4,761 rumors from snopes.com, which is a website that catalogues popular stories on social media and checks their veracity. In their dataset, 45% of stories were fake, 26% were ""true"" (i.e., not a fake story), and the rest had intermediate truth values. They analyzed the rumor cascades propagating as photos on Facebook during July and August 2013. Each cascade was identified as a tree of reshares starting from the original post of the photo, whenever a link to a valid snopes article was posted as a comment to the original photo or one of its reshares. A total of 16,672 such cascades were identified, with 62,497,651 shares, showing the large visibility false rumors can have. Surprisingly, they found that false information cascades were deeper, as there were more reshares at greater depths than the reference cascades. At lower depth, i.e., closer to the original photo post, the reference cascades have more reshares-about 20% reference cascades have depth of at least two, compared to 10% of false information cascades. But the reference cascades die very soon, while false information cascades run deeper. About 3% of false cascades have depth of at least 10 reshares, while less than 1% of reference cascades have the same property. The difference increases in magnitude as the depth of the cascade increases. This study shows the large reach of false information on social media, fueled by its highly contagious nature.

Recently, the largest study of spread of over 126,000 rumors on Twitter over a period of 11 years was conducted by Vosoughi et al. [105]. The authors took the set of false information cascade identified by various independent fact-checking agencies and traced their spread from their very beginning. This was done by identifying cascades that contained a link to any of the agencies. For comparison, they also considered cascades of verified true information linking to these agencies. Compared to true information, tweets containing false information spread significantly farther (more number of users retweeted), faster (more number of retweets in a shorter time), deeper (more number of retweet hops), and more broadly (more number of users at some retweet depth). This was observed in all categories of false information, such as politics, urban legend, science, business, and others, with politics as the biggest category of false information. In fact, they found that the top 1% of false tweets reached over 1,000 users, which true information tweets rarely did. False information reached more number of people than truth at every cascade depth, which was aided by its virality, showing that it was spread by multiple people in a peer-to-peer manner, instead of a few accounts simply broadcasting it. Moreover, false information was six times faster in reaching the same number of people as true information did. Thus, this study showed the widespread reach and impact of false information in Twittersphere.

Overall, impact of false information on the web is measured using engagement statistics such as view count, share count, and more. Research has shown that while most false information is not effective, a small fraction","[['b41', 'b105', 'b59', 'b101', 'b26', 'b36', 'b39'], ['b95', 'b130', 'b100'], ['b59'], ['b41', 'b101'], ['b39'], ['b114'], []]","[['b41', 'b105', 'b59', 'b101', 'b26', 'b36', 'b39'], ['b95', 'b130', 'b100'], ['b59'], ['b41', 'b101'], ['b39'], ['b114'], []]",15,"sent1: Given that there are several factors that lead to deception by false information (Section 3.1), what is the impact of false information on its readers on web and social media?
sent2: In the real world, false information has been shown to have significant impact on the stock market [17], hampering response during natural disasters [32], and terroristic activity [27,96].
sent3: On web and social media, the impact is measured as the engagement it produces via its readers, using statistics such as number of reads, number of days it survived without being removed, or number of people it reached via reshares.
sent4: Several research studies have been conducted to measure the impact of hoaxes, fake reviews, and fake news.
sent5: Frigerri et al. [30] studied the spread of rumors on Facebook, Kumar et al. [50] measured the impact of hoax articles on Wikipedia, and Silverman [92] analyzed the engagement of fake election news articles on Facebook.
sent6: We discuss these studies to measure the impact of false information on web platforms.
sent7: False information spreads far and wide on social media because there is an average delay of 12 hours between start of false information spread and that of its debunking information [86,121].
sent8: False information spreads rapidly during its starting phase-an unverified and not yet debunked rumor has high potential of becoming viral [121].
sent9: As a result, rumors with the possibility of being true start to spread, sometimes even by reputed news organizations [91].
sent10: On Wikipedia, Kumar et al. [50] measured impact of hoaxes in terms of their viewcount, number of days they survived before they are deleted, and their spread across the web.
sent11: Figure 5 shows the distributions for the first two statistics.
sent12: Figure 5(a) shows the distribution of the time it takes from when the article is created and approved ('patrolled') till the time it is identified as a hoax ('flagged').
sent13: It shows that while 90% of hoax articles are identified immediately within an hour of being approved, about 1% of hoaxes that are well-written hoaxes survive for over one year without being detected.
sent14: However, survival is not enough for a hoax article to be successful; it must be viewed as well.
sent15: Figure 5(b) plots the counter-cumulative distribution of average view count of hoaxes that survive for at least a week and their equivalent non-hoaxes.
sent16: On average, hoaxes are viewed less frequently than non-hoaxes (median 3 views per day vs 3.5 views per day), but a non-negligible 1% of hoaxes are viewed at least 100 times a day.
sent17: Finally, the impact of hoaxes is measured in terms of spread over the web, by counting the links that were clicked by readers to reach the hoax article.
sent18: For this, 5 months of Wikipedia server logs were used.
sent19: The results were alarming-at least 5 distinct links were clicked from across the web for 7% of hoaxes, and on average, each hoax had 1.1 such links.
sent20: This traffic was observed from search engines, social networks such as Facebook and Twitter, and from within Wikipedia itself.
sent21: Overall, this analysis shows that while most hoax articles are ineffective, a small fraction of hoax articles on Wikipedia is highly impactful.
sent22: Buzzfeed news analyzed highly impactful fake political news on the web.
sent23: They analyzed both true and false election-related stories with the highest engagement on Facebook during the 2016 US Presidential election [92].
sent24: Engagement was measured as the total number of shares, reactions, and comments on the Facebook story.
sent25: They analyzed the 20 top-performing false election stories generated by fake websites and blogs, and compared them to the 20 top-performing true election stories from major news websites, like New York Times, Washington Post, and others.
sent26: The fake news stories got a total of 8,711,000 engagements, significantly higher than the 7,367,000 engagements of the real news stories.
sent27: As this analysis was restricted to top stories, a complete analysis of all news stories may reveal a different picture.
sent28: Prior to this study, Gupta et al. [32] studied the spread of eight fake images on Twitter during Hurricane Sandy, and found that that fake images were shared almost twice as much as real images.
sent29: On a larger scale, Frigerri et al. [30] conducted a comprehensive study of the spread of false and real information on Facebook.
sent30: They collected 4,761 rumors from snopes.com, which is a website that catalogues popular stories on social media and checks their veracity.
sent31: In their dataset, 45% of stories were fake, 26% were ""true"" (i.e., not a fake story), and the rest had intermediate truth values.
sent32: They analyzed the rumor cascades propagating as photos on Facebook during July and August 2013.
sent33: Each cascade was identified as a tree of reshares starting from the original post of the photo, whenever a link to a valid snopes article was posted as a comment to the original photo or one of its reshares.
sent34: A total of 16,672 such cascades were identified, with 62,497,651 shares, showing the large visibility false rumors can have.
sent35: Surprisingly, they found that false information cascades were deeper, as there were more reshares at greater depths than the reference cascades.
sent36: At lower depth, i.e., closer to the original photo post, the reference cascades have more reshares-about 20% reference cascades have depth of at least two, compared to 10% of false information cascades.
sent37: But the reference cascades die very soon, while false information cascades run deeper.
sent38: About 3% of false cascades have depth of at least 10 reshares, while less than 1% of reference cascades have the same property.
sent39: The difference increases in magnitude as the depth of the cascade increases.
sent40: This study shows the large reach of false information on social media, fueled by its highly contagious nature.
sent41: Recently, the largest study of spread of over 126,000 rumors on Twitter over a period of 11 years was conducted by Vosoughi et al. [105].
sent42: The authors took the set of false information cascade identified by various independent fact-checking agencies and traced their spread from their very beginning.
sent43: This was done by identifying cascades that contained a link to any of the agencies.
sent44: For comparison, they also considered cascades of verified true information linking to these agencies.
sent45: Compared to true information, tweets containing false information spread significantly farther (more number of users retweeted), faster (more number of retweets in a shorter time), deeper (more number of retweet hops), and more broadly (more number of users at some retweet depth).
sent46: This was observed in all categories of false information, such as politics, urban legend, science, business, and others, with politics as the biggest category of false information.
sent47: In fact, they found that the top 1% of false tweets reached over 1,000 users, which true information tweets rarely did.
sent48: False information reached more number of people than truth at every cascade depth, which was aided by its virality, showing that it was spread by multiple people in a peer-to-peer manner, instead of a few accounts simply broadcasting it.
sent49: Moreover, false information was six times faster in reaching the same number of people as true information did.
sent50: Thus, this study showed the widespread reach and impact of false information in Twittersphere.
sent51: Overall, impact of false information on the web is measured using engagement statistics such as view count, share count, and more.
sent52: Research has shown that while most false information is not effective, a small fraction"
5058880,False Information on Web and Social Media: A Survey,Computer Science,https://www.semanticscholar.org/paper/1bb23fd4252a76b55ff77ed982fc78c3e06d1452,s28,Detection of fact-based false information,"In this part, we will look at the algorithms to detect hoaxes, fake news, and rumors in social media. These algorithms can be categorized into two major categories: feature engineering based and propagation based. Similar to opinion-based feature engineering methods, here features are created from their textual properties, their relation to other existing information, the properties of the users interacting with this information (e.g., the creator), and propagation dependent features (e.g., number of users that reshare a tweet). Feature-based algorithms have been used to identify various different types of malicious users and activities, such as identifying bots [97], trolls [20], vandals [49], sockpuppets [47], and many more.

Fact-based false information propagates through social networks, as opposed to opinion-based false information. Thus, propagation based algorithms model how true information propagates in these networks and anomalies of these models are predicted as false information. Some algorithms also create separate models for true and false information propagation. Alternatively, propagation structures and information can be fed into machine learning models for prediction as well.

We will first discuss feature based algorithms (Section 6.2.1) and then explain propagation based models in Section 6.2.2.","[['b58', 'b29', 'b106', 'b56'], [], []]","[['b58', 'b29', 'b106', 'b56'], [], []]",4,"sent1: In this part, we will look at the algorithms to detect hoaxes, fake news, and rumors in social media.
sent2: These algorithms can be categorized into two major categories: feature engineering based and propagation based.
sent3: Similar to opinion-based feature engineering methods, here features are created from their textual properties, their relation to other existing information, the properties of the users interacting with this information (e.g., the creator), and propagation dependent features (e.g., number of users that reshare a tweet).
sent4: Feature-based algorithms have been used to identify various different types of malicious users and activities, such as identifying bots [97], trolls [20], vandals [49], sockpuppets [47], and many more.
sent5: Fact-based false information propagates through social networks, as opposed to opinion-based false information.
sent6: Thus, propagation based algorithms model how true information propagates in these networks and anomalies of these models are predicted as false information.
sent7: Some algorithms also create separate models for true and false information propagation.
sent8: Alternatively, propagation structures and information can be fed into machine learning models for prediction as well.
sent9: We will first discuss feature based algorithms (Section 6.2.1) and then explain propagation based models in Section 6.2.2."
7470107,A Review of Unsupervised Approaches of Opinion Target Extraction from Unstructured Reviews,"Engineering, Computer Science",https://www.semanticscholar.org/paper/67a38d9065eebdeb814c1e80e109fc34a237a889,s1,METHODOLOGY,"Unsupervised approaches for opinion targets identification: The unsupervised techniques has been popularly used for opinion target identification (Ben-David et al., 2007;Blitzer et al., 2007;Bloom et al., 2007;Carenini et al., 2005;Ferreira et al., 2008;Holzinger et al., 2006;Hu and Liu, 2004;Popescu et al., 2005;Wei et al., 2010;Wong and Lam, 2009;Yi et al., 2003;Zhai et al., 2011).Popescu et al. (2005) used an unsupervised technique to extract product features and opinions from unstructured reviews.This study introduces the OPINE system based on the unsupervised information extraction approach to mine product features from reviews.OPINE uses syntactic patterns for semantic orientation of words for identification of opinion phrases and their polarity.Carenini et al. (2005) developed a model based on user defined knowledge to create taxonomy of product features.This study introduces an improved unsupervised method for feature extraction that uses the taxonomy of the product features.The results of the combined approach are higher than the existing unsupervised technique; however, the pre-knowledge base mechanism makes the approach domain dependent.Holzinger et al. (2006) use domain ontologies based on tabular data from web content to bootstrap a knowledge acquisition process for extraction of product features.This method creates a wrapper for data extraction from Web tables and ontology building.The model uses logical rules and data integration to reason about product specific properties and the higher-order knowledge of product features.Bloom et al. (2007) describe an unsupervised technique for features and appraisal extraction.The authors believe that appraisal expression is a fundamental task in sentiment analysis.The appraisal expression is a textual unit expressing an evaluative attitude towards some target.Their study proposed evaluative expressions to extract opinion targets.The system effectively exploited the adjectival appraisal expressions for target identification.Ben-David et al. (2007) proposed a Structural Correspondence Learning (SCL) algorithm for domain classification.The idea depends on perception to get a prediction of new domain features based on training domain features; in other words, the author describes under what conditions a classifier trained on the source domain can be adapted for use in the target domain?This model is inspired by feature based domain classification.Blitzer et al. (2007) extended the structural SCL algorithm for opinion target identification.Lu and Zhai (2008) proposed automatic integration of opinions expressed in a well-written expert review with opinions scattered in various sources such as blogs and forums.The study proposes a semi-supervised topic model to solve the problem in a principled way.The author performed experiments on integrating opinions about two quite different topics, i.e., a product and political reviews.The focus of this study is to develop a generalized model that should be effective on multiple domains for extraction of opinion targets.Ferreira et al. (2008) describe an extended pattern based feature extraction using a modified Log Likelihood Ratio Test (LRT), which was initially employed by Yi et al. (2003) for target identification.This study also presented an extended annotated scheme for product features, which was initially presented by Hu and Liu (2004) and a comparative analysis between feature extraction through Association Mining and LRT techniques.

The association rule mining for target extraction is initially implemented by Hu and Liu (2004) for target extraction and extended by Wei et al. (2010) using semantic based patterns for frequent feature refinement and identification of infrequent features.

One of the latest works on feature level analysis of opinion is reported by Zhai et al. (2011).This study describes a semi-supervised technique for feature grouping.Feature grouping is an important task for summarization of opinion.Same features can be expressed by different synonyms, words or phrases.To produce a useful summary, these words and phrases are grouped.For feature grouping the process generate an initial list to bootstrap the process using lexical characteristics of terms.This method empirically showed good results.Goujon (2011) presents a text mining approach based on linguistic knowledge to automatically detect opinion targets in relation to topic elements.This study focuses on identification of opinion targets related to the specific topic.This approach exploits linguistic patterns for target identification.

The two most frequently reported unsupervised approaches for target and opinion identification are Association Mining (AM) (Agrawal and Srikant, 1994) and Likelihood Ratio Test (LRT) approach (Dunning, 1993).The following sub sections provide a detail overview these two approaches.","[['b8', 'b29', 'b10', 'b16', 'b17', 'b28', 'b22', 'b7', 'b9', 'b31', 'b30', 'b13', 'b24'], ['b17', 'b28'], ['b14', 'b31'], ['b12', 'b0']]","[['b8', 'b29', 'b10', 'b16', 'b17', 'b28', 'b22', 'b7', 'b9', 'b31', 'b30', 'b13', 'b24'], ['b17', 'b28'], ['b14', 'b31'], ['b12', 'b0']]",19,"sent1: Unsupervised approaches for opinion targets identification: The unsupervised techniques has been popularly used for opinion target identification (Ben-David et al., 2007;Blitzer et al., 2007;Bloom et al., 2007;Carenini et al., 2005;Ferreira et al., 2008;Holzinger et al., 2006;Hu and Liu, 2004;Popescu et al., 2005;Wei et al., 2010;Wong and Lam, 2009;Yi et al., 2003;Zhai et al., 2011).Popescu et al. (2005) used an unsupervised technique to extract product features and opinions from unstructured reviews.
sent2: This study introduces the OPINE system based on the unsupervised information extraction approach to mine product features from reviews.
sent3: OPINE uses syntactic patterns for semantic orientation of words for identification of opinion phrases and their polarity.
sent4: Carenini et al. (2005) developed a model based on user defined knowledge to create taxonomy of product features.
sent5: This study introduces an improved unsupervised method for feature extraction that uses the taxonomy of the product features.
sent6: The results of the combined approach are higher than the existing unsupervised technique; however, the pre-knowledge base mechanism makes the approach domain dependent.
sent7: Holzinger et al. (2006) use domain ontologies based on tabular data from web content to bootstrap a knowledge acquisition process for extraction of product features.
sent8: This method creates a wrapper for data extraction from Web tables and ontology building.
sent9: The model uses logical rules and data integration to reason about product specific properties and the higher-order knowledge of product features.
sent10: Bloom et al. (2007) describe an unsupervised technique for features and appraisal extraction.
sent11: The authors believe that appraisal expression is a fundamental task in sentiment analysis.
sent12: The appraisal expression is a textual unit expressing an evaluative attitude towards some target.
sent13: Their study proposed evaluative expressions to extract opinion targets.
sent14: The system effectively exploited the adjectival appraisal expressions for target identification.
sent15: Ben-David et al. (2007) proposed a Structural Correspondence Learning (SCL) algorithm for domain classification.
sent16: The idea depends on perception to get a prediction of new domain features based on training domain features; in other words, the author describes under what conditions a classifier trained on the source domain can be adapted for use in the target domain?This model is inspired by feature based domain classification.
sent17: Blitzer et al. (2007) extended the structural SCL algorithm for opinion target identification.
sent18: Lu and Zhai (2008) proposed automatic integration of opinions expressed in a well-written expert review with opinions scattered in various sources such as blogs and forums.
sent19: The study proposes a semi-supervised topic model to solve the problem in a principled way.
sent20: The author performed experiments on integrating opinions about two quite different topics, i.e., a product and political reviews.
sent21: The focus of this study is to develop a generalized model that should be effective on multiple domains for extraction of opinion targets.
sent22: Ferreira et al. (2008) describe an extended pattern based feature extraction using a modified Log Likelihood Ratio Test (LRT), which was initially employed by Yi et al. (2003) for target identification.
sent23: This study also presented an extended annotated scheme for product features, which was initially presented by Hu and Liu (2004) and a comparative analysis between feature extraction through Association Mining and LRT techniques.
sent24: The association rule mining for target extraction is initially implemented by Hu and Liu (2004) for target extraction and extended by Wei et al. (2010) using semantic based patterns for frequent feature refinement and identification of infrequent features.
sent25: One of the latest works on feature level analysis of opinion is reported by Zhai et al.
sent26: (2011).This study describes a semi-supervised technique for feature grouping.
sent27: Feature grouping is an important task for summarization of opinion.
sent28: Same features can be expressed by different synonyms, words or phrases.
sent29: To produce a useful summary, these words and phrases are grouped.
sent30: For feature grouping the process generate an initial list to bootstrap the process using lexical characteristics of terms.
sent31: This method empirically showed good results.
sent32: Goujon (2011) presents a text mining approach based on linguistic knowledge to automatically detect opinion targets in relation to topic elements.
sent33: This study focuses on identification of opinion targets related to the specific topic.
sent34: This approach exploits linguistic patterns for target identification.
sent35: The two most frequently reported unsupervised approaches for target and opinion identification are Association Mining (AM) (Agrawal and Srikant, 1994) and Likelihood Ratio Test (LRT) approach (Dunning, 1993).The following sub sections provide a detail overview these two approaches."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s15,Implicit alignment,"In contrast to explicit alignment, implicit alignment is used as an intermediate (often latent) step for another task. This allows for better performance in a number of tasks including speech recognition, machine translation, media description, and visual question-answering. Such models do not explicitly align data and do not rely on supervised alignment examples, but learn how to latently align the data during model training. We identify two types of implicit alignment models: earlier work based on graphical models, and more modern neural network methods. Graphical models have seen some early work used to better align words between languages for machine translation [216] and alignment of speech phonemes with their transcriptions [186]. However, they require manual construction of a mapping between the modalities, for example a generative phone model that maps phonemes to acoustic features [186]. Constructing such models requires training data or human expertise to define them manually. Neural networks Translation (Section 4) is an example of a modeling task that can often be improved if alignment is performed as a latent intermediate step. As we mentioned before, neural networks are popular ways to address this translation problem, using either an encoder-decoder model or through cross-modal retrieval. When translation is performed without implicit alignment, it ends up putting a lot of weight on the encoder module to be able to properly summarize the whole image, sentence or a video with a single vectorial representation.

A very popular way to address this is through attention [12], which allows the decoder to focus on sub-components of the source instance. This is in contrast with encoding all source sub-components together, as is performed in a conventional encoder-decoder model. An attention module will tell the decoder to look more at targeted sub-components of the source to be translated -areas of an image [230], words of a sentence [12], segments of an audio sequence [35], [39], frames and regions in a video [236], [241], and even parts of an instruction [140]. For example, in image captioning instead of encoding an entire image using a CNN, an attention mechanism will allow the decoder (typically an RNN) to focus on particular parts of the image when generating each successive word [230]. The attention module which learns what part of the image to focus on is typically a shallow neural network and is trained end-to-end together with a target task (e.g., translation).

Attention models have also been successfully applied to question answering tasks, as they allow for aligning the words in a question with sub-components of an information source such as a piece of text [228], an image [62], or a video sequence [246]. This both allows for better performance in question answering and leads to better model interpretability [4]. In particular, different types of attention models have been proposed to address this problem, including hierarchical [128], stacked [234], and episodic memory attention [228].

Another neural alternative for aligning images with captions for cross-modal retrieval was proposed by Karpathy et al. [98], [99]. Their proposed model aligns sentence fragments to image regions by using a dot product similarity measure between image region and word representations. While it does not use attention, it extracts a latent alignment between modalities through a similarity measure that is learned indirectly by training a retrieval model.","[['b216', 'b186'], ['b236', 'b35', 'b230', 'b241', 'b140', 'b39', 'b12'], ['b4', 'b228', 'b234', 'b62', 'b128', 'b246'], ['b99', 'b98']]","[['b216', 'b186'], ['b236', 'b35', 'b230', 'b241', 'b140', 'b39', 'b12'], ['b4', 'b228', 'b234', 'b62', 'b128', 'b246'], ['b99', 'b98']]",17,"sent1: In contrast to explicit alignment, implicit alignment is used as an intermediate (often latent) step for another task.
sent2: This allows for better performance in a number of tasks including speech recognition, machine translation, media description, and visual question-answering.
sent3: Such models do not explicitly align data and do not rely on supervised alignment examples, but learn how to latently align the data during model training.
sent4: We identify two types of implicit alignment models: earlier work based on graphical models, and more modern neural network methods.
sent5: Graphical models have seen some early work used to better align words between languages for machine translation [216] and alignment of speech phonemes with their transcriptions [186].
sent6: However, they require manual construction of a mapping between the modalities, for example a generative phone model that maps phonemes to acoustic features [186].
sent7: Constructing such models requires training data or human expertise to define them manually.
sent8: Neural networks Translation (Section 4) is an example of a modeling task that can often be improved if alignment is performed as a latent intermediate step.
sent9: As we mentioned before, neural networks are popular ways to address this translation problem, using either an encoder-decoder model or through cross-modal retrieval.
sent10: When translation is performed without implicit alignment, it ends up putting a lot of weight on the encoder module to be able to properly summarize the whole image, sentence or a video with a single vectorial representation.
sent11: A very popular way to address this is through attention [12], which allows the decoder to focus on sub-components of the source instance.
sent12: This is in contrast with encoding all source sub-components together, as is performed in a conventional encoder-decoder model.
sent13: An attention module will tell the decoder to look more at targeted sub-components of the source to be translated -areas of an image [230], words of a sentence [12], segments of an audio sequence [35], [39], frames and regions in a video [236], [241], and even parts of an instruction [140].
sent14: For example, in image captioning instead of encoding an entire image using a CNN, an attention mechanism will allow the decoder (typically an RNN) to focus on particular parts of the image when generating each successive word [230].
sent15: The attention module which learns what part of the image to focus on is typically a shallow neural network and is trained end-to-end together with a target task (e.g., translation).
sent16: Attention models have also been successfully applied to question answering tasks, as they allow for aligning the words in a question with sub-components of an information source such as a piece of text [228], an image [62], or a video sequence [246].
sent17: This both allows for better performance in question answering and leads to better model interpretability [4].
sent18: In particular, different types of attention models have been proposed to address this problem, including hierarchical [128], stacked [234], and episodic memory attention [228].
sent19: Another neural alternative for aligning images with captions for cross-modal retrieval was proposed by Karpathy et al. [98], [99].
sent20: Their proposed model aligns sentence fragments to image regions by using a dot product similarity measure between image region and word representations.
sent21: While it does not use attention, it extracts a latent alignment between modalities through a similarity measure that is learned indirectly by training a retrieval model."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s24,Hybrid data,"In the hybrid data setting two non-parallel modalities are bridged by a shared modality or a dataset (see Figure  3c). The most notable example is the Bridge Correlational Neural Network [167], which uses a pivot modality to learn coordinated multimodal representations in presence of nonparallel data. For example, in the case of multilingual image captioning, the image modality would always be paired with at least one caption in any language. Such methods have also been used to bridge languages that might not have parallel corpora but have access to a shared pivot language, such as for machine translation [148], [167] and document transliteration [100].

Instead of using a separate modality for bridging, some methods rely on existence of large datasets from a similar or related task to lead to better performance in a task that only contains limited annotated data. Socher and Fei-Fei [189] use the existence of large text corpora in order to guide image segmentation. While Hendricks et al. [78] use separately trained visual model and a language model to lead to a better image and video description system, for which only limited data is available.","[['b167', 'b148', 'b100'], ['b78', 'b189']]","[['b167', 'b148', 'b100'], ['b78', 'b189']]",5,"sent1: In the hybrid data setting two non-parallel modalities are bridged by a shared modality or a dataset (see Figure  3c).
sent2: The most notable example is the Bridge Correlational Neural Network [167], which uses a pivot modality to learn coordinated multimodal representations in presence of nonparallel data.
sent3: For example, in the case of multilingual image captioning, the image modality would always be paired with at least one caption in any language.
sent4: Such methods have also been used to bridge languages that might not have parallel corpora but have access to a shared pivot language, such as for machine translation [148], [167] and document transliteration [100].
sent5: Instead of using a separate modality for bridging, some methods rely on existence of large datasets from a similar or related task to lead to better performance in a task that only contains limited annotated data.
sent6: Socher and Fei-Fei [189] use the existence of large text corpora in order to guide image segmentation.
sent7: While Hendricks et al. [78] use separately trained visual model and a language model to lead to a better image and video description system, for which only limited data is available."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s23,Non-parallel data,"Methods that rely on non-parallel data do not require the modalities to have shared instances, but only shared categories or concepts. Non-parallel co-learning approaches can help when learning representations, allow for better semantic concept understanding and even perform unseen object recognition.  [148] Transfer learning is also possible on non-parallel data and allows to learn better representations through transferring information from a representation built using a data rich or clean modality to a data scarce or noisy modality. This type of trasnfer learning is often achieved by using coordinated multimodal representations (see Section 3.2). For example, Frome et al. [61] used text to improve visual representations for image classification by coordinating CNN visual features with word2vec textual ones [141] trained on separate large datasets. Visual representations trained in such a way result in more meaningful errors -mistaking objects for ones of similar category [61]. Mahasseni and Todorovic [129] demonstrated how to regularize a color video based LSTM using an autoencoder LSTM trained on 3D skeleton data by enforcing similarities between their hidden states. Such an approach is able to improve the original LSTM and lead to state-of-the-art performance in action recognition. Conceptual grounding refers to learning semantic meanings or concepts not purely based on language but also on additional modalities such as vision, sound, or even smell [16]. While the majority of concept learning approaches are purely language-based, representations of meaning in humans are not merely a product of our linguistic exposure, but are also grounded through our sensorimotor experience and perceptual system [17], [126]. Human semantic knowledge relies heavily on perceptual information [126] and many concepts are grounded in the perceptual system and are not purely symbolic [17]. This implies that learning semantic meaning purely from textual information might not be optimal, and motivates the use of visual or acoustic cues to ground our linguistic representations. Starting from work by Feng and Lapata [59], grounding is usually performed by finding a common latent space between the representations [59], [183] (in case of parallel datasets) or by learning unimodal representations separately and then concatenating them to lead to a multimodal one [29], [101], [172], [181] (in case of non-parallel data). Once a multimodal representation is constructed it can be used on purely linguistic tasks. Shutova et al. [181] and Bruni et al. [29] used grounded representations for better classification of metaphors and literal language. Such repre-sentations have also been useful for measuring conceptual similarity and relatedness -identifying how semantically or conceptually related two words are [30], [101], [183] or actions [172]. Furthermore, concepts can be grounded not only using visual signals, but also acoustic ones, leading to better performance especially on words with auditory associations [103], or even olfactory signals [102] for words with smell associations. Finally, there is a lot of overlap between multimodal alignment and conceptual grounding, as aligning visual scenes to their descriptions leads to better textual or visual representations [108], [161], [172], [240].

Conceptual grounding has been found to be an effective way to improve performance on a number of tasks. It also shows that language and vision (or audio) are complementary sources of information and combining them in multimodal models often improves performance. However, one has to be careful as grounding does not always lead to better performance [102], [103], and only makes sense when grounding has relevance for the task -such as grounding using images for visually-related concepts. Zero shot learning (ZSL) refers to recognizing a concept without having explicitly seen any examples of it. For example classifying a cat in an image without ever having seen (labeled) images of cats. This is an important problem to address as in a number of tasks such as visual object classification: it is prohibitively expensive to provide training examples for every imaginable object of interest.

There are two main types of ZSL -unimodal and multimodal. The unimodal ZSL looks at component parts or attributes of the object, such as phonemes to recognize an unheard word or visual attributes such as color, size, and shape to predict an unseen visual class [55]. The multimodal ZSL recognizes the objects in the primary modality through the help of the secondary one -in which the object has been seen. The multimodal version of ZSL is a problem facing non-parallel data by definition as the overlap of seen classes is different between the modalities.

Socher et al. [190] map image features to a conceptual word space and are able to classify between seen and unseen concepts. The unseen concepts can be then assigned to a word that is close to the visual representation -this is enabled by the semantic space being trained on a separate dataset that has seen more concepts. Instead of learning a mapping from visual to concept space Frome et al. [61] learn a coordinated multimodal representation between concepts and images that allows for ZSL. Palatucci et al. [158] perform prediction of words people are thinking of based on functional magnetic resonance images, they show how it is possible to predict unseen words through the use of an intermediate semantic space. Lazaridou et al. [118] present a fast mapping method for ZSL by mapping extracted visual feature vectors to text-based vectors through a neural network.","[['b161', 'b129', 'b102', 'b141', 'b181', 'b61', 'b148', 'b183', 'b30', 'b29', 'b172', 'b16', 'b59', 'b101', 'b103', 'b240', 'b108', 'b17', 'b126'], ['b102', 'b103'], ['b55'], ['b158', 'b190', 'b118', 'b61']]","[['b161', 'b129', 'b102', 'b141', 'b181', 'b61', 'b148', 'b183', 'b30', 'b29', 'b172', 'b16', 'b59', 'b101', 'b103', 'b240', 'b108', 'b17', 'b126'], ['b102', 'b103'], ['b55'], ['b158', 'b190', 'b118', 'b61']]",26,"sent1: Methods that rely on non-parallel data do not require the modalities to have shared instances, but only shared categories or concepts.
sent2: Non-parallel co-learning approaches can help when learning representations, allow for better semantic concept understanding and even perform unseen object recognition.
sent3: [148] Transfer learning is also possible on non-parallel data and allows to learn better representations through transferring information from a representation built using a data rich or clean modality to a data scarce or noisy modality.
sent4: This type of trasnfer learning is often achieved by using coordinated multimodal representations (see Section 3.2).
sent5: For example, Frome et al. [61] used text to improve visual representations for image classification by coordinating CNN visual features with word2vec textual ones [141] trained on separate large datasets.
sent6: Visual representations trained in such a way result in more meaningful errors -mistaking objects for ones of similar category [61].
sent7: Mahasseni and Todorovic [129] demonstrated how to regularize a color video based LSTM using an autoencoder LSTM trained on 3D skeleton data by enforcing similarities between their hidden states.
sent8: Such an approach is able to improve the original LSTM and lead to state-of-the-art performance in action recognition.
sent9: Conceptual grounding refers to learning semantic meanings or concepts not purely based on language but also on additional modalities such as vision, sound, or even smell [16].
sent10: While the majority of concept learning approaches are purely language-based, representations of meaning in humans are not merely a product of our linguistic exposure, but are also grounded through our sensorimotor experience and perceptual system [17], [126].
sent11: Human semantic knowledge relies heavily on perceptual information [126] and many concepts are grounded in the perceptual system and are not purely symbolic [17].
sent12: This implies that learning semantic meaning purely from textual information might not be optimal, and motivates the use of visual or acoustic cues to ground our linguistic representations.
sent13: Starting from work by Feng and Lapata [59], grounding is usually performed by finding a common latent space between the representations [59], [183] (in case of parallel datasets) or by learning unimodal representations separately and then concatenating them to lead to a multimodal one [29], [101], [172], [181] (in case of non-parallel data).
sent14: Once a multimodal representation is constructed it can be used on purely linguistic tasks.
sent15: Shutova et al. [181] and Bruni et al. [29] used grounded representations for better classification of metaphors and literal language.
sent16: Such repre-sentations have also been useful for measuring conceptual similarity and relatedness -identifying how semantically or conceptually related two words are [30], [101], [183] or actions [172].
sent17: Furthermore, concepts can be grounded not only using visual signals, but also acoustic ones, leading to better performance especially on words with auditory associations [103], or even olfactory signals [102] for words with smell associations.
sent18: Finally, there is a lot of overlap between multimodal alignment and conceptual grounding, as aligning visual scenes to their descriptions leads to better textual or visual representations [108], [161], [172], [240].
sent19: Conceptual grounding has been found to be an effective way to improve performance on a number of tasks.
sent20: It also shows that language and vision (or audio) are complementary sources of information and combining them in multimodal models often improves performance.
sent21: However, one has to be careful as grounding does not always lead to better performance [102], [103], and only makes sense when grounding has relevance for the task -such as grounding using images for visually-related concepts.
sent22: Zero shot learning (ZSL) refers to recognizing a concept without having explicitly seen any examples of it.
sent23: For example classifying a cat in an image without ever having seen (labeled) images of cats.
sent24: This is an important problem to address as in a number of tasks such as visual object classification: it is prohibitively expensive to provide training examples for every imaginable object of interest.
sent25: There are two main types of ZSL -unimodal and multimodal.
sent26: The unimodal ZSL looks at component parts or attributes of the object, such as phonemes to recognize an unheard word or visual attributes such as color, size, and shape to predict an unseen visual class [55].
sent27: The multimodal ZSL recognizes the objects in the primary modality through the help of the secondary one -in which the object has been seen.
sent28: The multimodal version of ZSL is a problem facing non-parallel data by definition as the overlap of seen classes is different between the modalities.
sent29: Socher et al. [190] map image features to a conceptual word space and are able to classify between seen and unseen concepts.
sent30: The unseen concepts can be then assigned to a word that is close to the visual representation -this is enabled by the semantic space being trained on a separate dataset that has seen more concepts.
sent31: Instead of learning a mapping from visual to concept space Frome et al. [61] learn a coordinated multimodal representation between concepts and images that allows for ZSL.
sent32: Palatucci et al. [158] perform prediction of words people are thinking of based on functional magnetic resonance images, they show how it is possible to predict unseen words through the use of an intermediate semantic space.
sent33: Lazaridou et al. [118] present a fast mapping method for ZSL by mapping extracted visual feature vectors to text-based vectors through a neural network."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s22,Parallel data,"In parallel data co-learning both modalities share a set of instances -audio recordings with the corresponding videos, images and their sentence descriptions. This allows for two types of algorithms to exploit that data to better model the modalities: co-training and representation learning. parallel -modalities are from the same dataset and there is a direct correspondence between instances; non-parallel -modalities are from different datasets and do not have overlapping instances, but overlap in general categories or concepts; hybrid -the instances or concepts are bridged by a third modality or a dataset.

Co-training is the process of creating more labeled training samples when we have few labeled samples in a multimodal problem [21]. The basic algorithm builds weak classifiers in each modality to bootstrap each other with labels for the unlabeled data. It has been shown to discover more training samples for web-page classification based on the web-page itself and hyper-links leading in the seminal work of Blum and Mitchell [21]. By definition this task requires parallel data as it relies on the overlap of multimodal samples. Co-training has been used for statistical parsing [178] to build better visual detectors [120] and for audio-visual speech recognition [40]. It has also been extended to deal with disagreement between modalities, by filtering out unreliable samples [41]. While co-training is a powerful method for generating more labeled data, it can also lead to biased training samples resulting in overfitting. Transfer learning is another way to exploit co-learning with parallel data. Multimodal representation learning (Section 3.1) approaches such as multimodal deep Boltzmann machines [198] and multimodal autoencoders [151] transfer information from representation of one modality to that of another. This not only leads to multimodal representations, but also to better unimodal ones, with only one modality being used during test time [151] .

Moon et al. [143] show how to transfer information from a speech recognition neural network (based on audio) to a lip-reading one (based on images), leading to a better visual representation, and a model that can be used for lip-reading without need for audio information during test time. Similarly, Arora and Livescu [10] build better acoustic features using CCA on acoustic and articulatory (location of lips, tongue and jaw) data. They use articulatory data only during CCA construction and use only the resulting acoustic (unimodal) representation during test time.","[[], ['b178', 'b41', 'b151', 'b21', 'b120', 'b198', 'b40'], ['b143', 'b10']]","[[], ['b178', 'b41', 'b151', 'b21', 'b120', 'b198', 'b40'], ['b143', 'b10']]",9,"sent1: In parallel data co-learning both modalities share a set of instances -audio recordings with the corresponding videos, images and their sentence descriptions.
sent2: This allows for two types of algorithms to exploit that data to better model the modalities: co-training and representation learning.
sent3: parallel -modalities are from the same dataset and there is a direct correspondence between instances; non-parallel -modalities are from different datasets and do not have overlapping instances, but overlap in general categories or concepts; hybrid -the instances or concepts are bridged by a third modality or a dataset.
sent4: Co-training is the process of creating more labeled training samples when we have few labeled samples in a multimodal problem [21].
sent5: The basic algorithm builds weak classifiers in each modality to bootstrap each other with labels for the unlabeled data.
sent6: It has been shown to discover more training samples for web-page classification based on the web-page itself and hyper-links leading in the seminal work of Blum and Mitchell [21].
sent7: By definition this task requires parallel data as it relies on the overlap of multimodal samples.
sent8: Co-training has been used for statistical parsing [178] to build better visual detectors [120] and for audio-visual speech recognition [40].
sent9: It has also been extended to deal with disagreement between modalities, by filtering out unreliable samples [41].
sent10: While co-training is a powerful method for generating more labeled data, it can also lead to biased training samples resulting in overfitting.
sent11: Transfer learning is another way to exploit co-learning with parallel data.
sent12: Multimodal representation learning (Section 3.1) approaches such as multimodal deep Boltzmann machines [198] and multimodal autoencoders [151] transfer information from representation of one modality to that of another.
sent13: This not only leads to multimodal representations, but also to better unimodal ones, with only one modality being used during test time [151] .
sent14: Moon et al. [143] show how to transfer information from a speech recognition neural network (based on audio) to a lip-reading one (based on images), leading to a better visual representation, and a model that can be used for lip-reading without need for audio information during test time.
sent15: Similarly, Arora and Livescu [10] build better acoustic features using CCA on acoustic and articulatory (location of lips, tongue and jaw) data.
sent16: They use articulatory data only during CCA construction and use only the resulting acoustic (unimodal) representation during test time."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s19,Model-based approaches,"While model-agnostic approaches are easy to implement using unimodal machine learning methods, they end up using techniques that are not designed to cope with multimodal data. In this section we describe three categories of approaches that are designed to perform multimodal fusion: kernel-based methods, graphical models, and neural networks. Multiple kernel learning (MKL) methods are an extension to kernel support vector machines (SVM) that allow for the use of different kernels for different modalities/views of the data [70]. As kernels can be seen as similarity functions between data points, modality-specific kernels in MKL allows for better fusion of heterogeneous data.

MKL approaches have been an especially popular method for fusing visual descriptors for object detection [31], [66] and only recently have been overtaken by deep learning methods for the task [109]. They have also seen use for multimodal affect recognition [36], [90], [182], multimodal sentiment analysis [162], and multimedia event detection (MED) [237]. Furthermore, McFee and Lanckriet [137] proposed to use MKL to perform musical artist similarity ranking from acoustic, semantic and social view data. Finally, Liu et al. [125] used MKL for multimodal fusion in Alzheimer's disease classification. Their broad applicability demonstrates the strength of such approaches in various domains and across different modalities.

Besides flexibility in kernel selection, an advantage of MKL is the fact that the loss function is convex, allowing for model training using standard optimization packages and global optimum solutions [70]. Furthermore, MKL can be used to both perform regression and classification. One of the main disadvantages of MKL is the reliance on training data (support vectors) during test time, leading to slow inference and a large memory footprint. Graphical models are another family of popular methods for multimodal fusion. In this section we overview work done on multimodal fusion using shallow graphical models. A description of deep graphical models such as deep belief networks can be found in Section 3.1.

Majority of graphical models can be classified into two main categories: generative -modeling joint probability; or discriminative -modeling conditional probability [200]. Some of the earliest approaches to use graphical models for multimodal fusion include generative models such as coupled [149] and factorial hidden Markov models [67] alongside dynamic Bayesian networks [64]. A more recentlyproposed multi-stream HMM method proposes dynamic weighting of modalities for AVSR [75].

Arguably, generative models lost popularity to discriminative ones such as conditional random fields (CRF) [115] which sacrifice the modeling of joint probability for predictive power. A CRF model was used to better segment images by combining visual and textual information of image description [60]. CRF models have been extended to model latent states using hidden conditional random fields [165] and have been applied to multimodal meeting segmentation [173]. Other multimodal uses of latent variable discriminative graphical models include multi-view hidden CRF [194] and latent variable models [193]. More recently Jiang et al. [93] have shown the benefits of multimodal hidden conditional random fields for the task of multimedia classification. While most graphical models are aimed at classification, CRF models have been extended to a continuous version for regression [164] and applied in multimodal settings [13] for audio visual emotion recognition.

The benefit of graphical models is their ability to easily exploit spatial and temporal structure of the data, making them especially popular for temporal modeling tasks, such as AVSR and multimodal affect recognition. They also allow to build in human expert knowledge into the models. and often lead to interpretable models. Neural Networks have been used extensively for the task of multimodal fusion [151]. The earliest examples of using neural networks for multi-modal fusion come from work on AVSR [163]. Nowadays they are being used to fuse information for visual and media question answering [63], [130], [229], gesture recognition [150], affect analysis [96], [153], and video description generation [94]. While the modalities used, architectures, and optimization techniques might differ, the general idea of fusing information in joint hidden layer of a neural network remains the same.

Neural networks have also been used for fusing temporal multimodal information through the use of RNNs and LSTMs. One of the earlier such applications used a bidirectional LSTM was used to perform audio-visual emotion classification [224]. More recently, Wöllmer et al. [223] used LSTM models for continuous multimodal emotion recognition, demonstrating its advantage over graphical models and SVMs. Similarly, Nicolaou et al. [152] used LSTMs for continuous emotion prediction. Their proposed method used an LSTM to fuse the results from a modality specific (audio and facial expression) LSTMs.

Approaching modality fusion through recurrent neural networks has been used in various image captioning tasks, example models include: neural image captioning [214] where a CNN image representation is decoded using an LSTM language model, gLSTM [91] which incorporates the image data together with sentence decoding at every time step fusing the visual and sentence data in a joint representation. A more recent example is the multi-view LSTM (MV-LSTM) model proposed by Rajagopalan et al. [166]. MV-LSTM model allows for flexible fusion of modalities in the LSTM framework by explicitly modeling the modalityspecific and cross-modality interactions over time.

A big advantage of deep neural network approaches in data fusion is their capacity to learn from large amount of data. Secondly, recent neural architectures allow for end-toend training of both the multimodal representation component and the fusion component. Finally, they show good performance when compared to non neural network based system and are able to learn complex decision boundaries that other approaches struggle with.

The major disadvantage of neural network approaches is their lack of interpretability. It is difficult to tell what the prediction relies on, and which modalities or features play an important role. Furthermore, neural networks require large training datasets to be successful.","[['b70'], ['b125', 'b66', 'b137', 'b182', 'b162', 'b36', 'b31', 'b109', 'b90', 'b237'], ['b70'], ['b200', 'b67', 'b64', 'b75', 'b149'], ['b115', 'b173', 'b165', 'b60', 'b193', 'b93', 'b194', 'b13', 'b164'], ['b153', 'b151', 'b163', 'b229', 'b96', 'b63', 'b150', 'b130', 'b94'], ['b224', 'b223', 'b152'], ['b166', 'b214', 'b91'], [], []]","[['b70'], ['b125', 'b66', 'b137', 'b182', 'b162', 'b36', 'b31', 'b109', 'b90', 'b237'], ['b70'], ['b200', 'b67', 'b64', 'b75', 'b149'], ['b115', 'b173', 'b165', 'b60', 'b193', 'b93', 'b194', 'b13', 'b164'], ['b153', 'b151', 'b163', 'b229', 'b96', 'b63', 'b150', 'b130', 'b94'], ['b224', 'b223', 'b152'], ['b166', 'b214', 'b91'], [], []]",41,"sent1: While model-agnostic approaches are easy to implement using unimodal machine learning methods, they end up using techniques that are not designed to cope with multimodal data.
sent2: In this section we describe three categories of approaches that are designed to perform multimodal fusion: kernel-based methods, graphical models, and neural networks.
sent3: Multiple kernel learning (MKL) methods are an extension to kernel support vector machines (SVM) that allow for the use of different kernels for different modalities/views of the data [70].
sent4: As kernels can be seen as similarity functions between data points, modality-specific kernels in MKL allows for better fusion of heterogeneous data.
sent5: MKL approaches have been an especially popular method for fusing visual descriptors for object detection [31], [66] and only recently have been overtaken by deep learning methods for the task [109].
sent6: They have also seen use for multimodal affect recognition [36], [90], [182], multimodal sentiment analysis [162], and multimedia event detection (MED) [237].
sent7: Furthermore, McFee and Lanckriet [137] proposed to use MKL to perform musical artist similarity ranking from acoustic, semantic and social view data.
sent8: Finally, Liu et al. [125] used MKL for multimodal fusion in Alzheimer's disease classification.
sent9: Their broad applicability demonstrates the strength of such approaches in various domains and across different modalities.
sent10: Besides flexibility in kernel selection, an advantage of MKL is the fact that the loss function is convex, allowing for model training using standard optimization packages and global optimum solutions [70].
sent11: Furthermore, MKL can be used to both perform regression and classification.
sent12: One of the main disadvantages of MKL is the reliance on training data (support vectors) during test time, leading to slow inference and a large memory footprint.
sent13: Graphical models are another family of popular methods for multimodal fusion.
sent14: In this section we overview work done on multimodal fusion using shallow graphical models.
sent15: A description of deep graphical models such as deep belief networks can be found in Section 3.1.
sent16: Majority of graphical models can be classified into two main categories: generative -modeling joint probability; or discriminative -modeling conditional probability [200].
sent17: Some of the earliest approaches to use graphical models for multimodal fusion include generative models such as coupled [149] and factorial hidden Markov models [67] alongside dynamic Bayesian networks [64].
sent18: A more recentlyproposed multi-stream HMM method proposes dynamic weighting of modalities for AVSR [75].
sent19: Arguably, generative models lost popularity to discriminative ones such as conditional random fields (CRF) [115] which sacrifice the modeling of joint probability for predictive power.
sent20: A CRF model was used to better segment images by combining visual and textual information of image description [60].
sent21: CRF models have been extended to model latent states using hidden conditional random fields [165] and have been applied to multimodal meeting segmentation [173].
sent22: Other multimodal uses of latent variable discriminative graphical models include multi-view hidden CRF [194] and latent variable models [193].
sent23: More recently Jiang et al. [93] have shown the benefits of multimodal hidden conditional random fields for the task of multimedia classification.
sent24: While most graphical models are aimed at classification, CRF models have been extended to a continuous version for regression [164] and applied in multimodal settings [13] for audio visual emotion recognition.
sent25: The benefit of graphical models is their ability to easily exploit spatial and temporal structure of the data, making them especially popular for temporal modeling tasks, such as AVSR and multimodal affect recognition.
sent26: They also allow to build in human expert knowledge into the models.
sent27: and often lead to interpretable models.
sent28: Neural Networks have been used extensively for the task of multimodal fusion [151].
sent29: The earliest examples of using neural networks for multi-modal fusion come from work on AVSR [163].
sent30: Nowadays they are being used to fuse information for visual and media question answering [63], [130], [229], gesture recognition [150], affect analysis [96], [153], and video description generation [94].
sent31: While the modalities used, architectures, and optimization techniques might differ, the general idea of fusing information in joint hidden layer of a neural network remains the same.
sent32: Neural networks have also been used for fusing temporal multimodal information through the use of RNNs and LSTMs.
sent33: One of the earlier such applications used a bidirectional LSTM was used to perform audio-visual emotion classification [224].
sent34: More recently, Wöllmer et al. [223] used LSTM models for continuous multimodal emotion recognition, demonstrating its advantage over graphical models and SVMs.
sent35: Similarly, Nicolaou et al. [152] used LSTMs for continuous emotion prediction.
sent36: Their proposed method used an LSTM to fuse the results from a modality specific (audio and facial expression) LSTMs.
sent37: Approaching modality fusion through recurrent neural networks has been used in various image captioning tasks, example models include: neural image captioning [214] where a CNN image representation is decoded using an LSTM language model, gLSTM [91] which incorporates the image data together with sentence decoding at every time step fusing the visual and sentence data in a joint representation.
sent38: A more recent example is the multi-view LSTM (MV-LSTM) model proposed by Rajagopalan et al. [166].
sent39: MV-LSTM model allows for flexible fusion of modalities in the LSTM framework by explicitly modeling the modalityspecific and cross-modality interactions over time.
sent40: A big advantage of deep neural network approaches in data fusion is their capacity to learn from large amount of data.
sent41: Secondly, recent neural architectures allow for end-toend training of both the multimodal representation component and the fusion component.
sent42: Finally, they show good performance when compared to non neural network based system and are able to learn complex decision boundaries that other approaches struggle with.
sent43: The major disadvantage of neural network approaches is their lack of interpretability.
sent44: It is difficult to tell what the prediction relies on, and which modalities or features play an important role.
sent45: Furthermore, neural networks require large training datasets to be successful."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s18,Model-agnostic approaches,"Historically, the vast majority of multimodal fusion has been done using model-agnostic approaches [50]. Such approaches can be split into early (i.e., feature-based), late (i.e., decision-based) and hybrid fusion [11]. Early fusion integrates features immediately after they are extracted (often by simply concatenating their representations). Late fusion on the other hand performs integration after each of the modalities has made a decision (e.g., classification or regression). Finally, hybrid fusion combines outputs from early fusion and individual unimodal predictors. An advantage of model agnostic approaches is that they can be implemented using almost any unimodal classifiers or regressors.

Early fusion could be seen as an initial attempt by multimodal researchers to perform multimodal representation learning -as it can learn to exploit the correlation and interactions between low level features of each modality. Furthermore it only requires the training of a single model, making the training pipeline easier compared to late and hybrid fusion.

In contrast, late fusion uses unimodal decision values and fuses them using a fusion mechanism such as averaging [181], voting schemes [144], weighting based on channel noise [163] and signal variance [53], or a learned model [68], [168]. It allows for the use of different models for each modality as different predictors can model each individual modality better, allowing for more flexibility. Furthermore, it makes it easier to make predictions when one or more of the modalities is missing and even allows for training when no parallel data is available. However, late fusion ignores the low level interaction between the modalities.

Hybrid fusion attempts to exploit the advantages of both of the above described methods in a common framework. It has been used successfully for multimodal speaker identification [226] and multimedia event detection (MED) [117].","[['b11', 'b50'], [], ['b68', 'b163', 'b181', 'b53', 'b168', 'b144'], ['b226', 'b117']]","[['b11', 'b50'], [], ['b68', 'b163', 'b181', 'b53', 'b168', 'b144'], ['b226', 'b117']]",10,"sent1: Historically, the vast majority of multimodal fusion has been done using model-agnostic approaches [50].
sent2: Such approaches can be split into early (i.e., feature-based), late (i.e., decision-based) and hybrid fusion [11].
sent3: Early fusion integrates features immediately after they are extracted (often by simply concatenating their representations).
sent4: Late fusion on the other hand performs integration after each of the modalities has made a decision (e.g., classification or regression).
sent5: Finally, hybrid fusion combines outputs from early fusion and individual unimodal predictors.
sent6: An advantage of model agnostic approaches is that they can be implemented using almost any unimodal classifiers or regressors.
sent7: Early fusion could be seen as an initial attempt by multimodal researchers to perform multimodal representation learning -as it can learn to exploit the correlation and interactions between low level features of each modality.
sent8: Furthermore it only requires the training of a single model, making the training pipeline easier compared to late and hybrid fusion.
sent9: In contrast, late fusion uses unimodal decision values and fuses them using a fusion mechanism such as averaging [181], voting schemes [144], weighting based on channel noise [163] and signal variance [53], or a learned model [68], [168].
sent10: It allows for the use of different models for each modality as different predictors can model each individual modality better, allowing for more flexibility.
sent11: Furthermore, it makes it easier to make predictions when one or more of the modalities is missing and even allows for training when no parallel data is available.
sent12: However, late fusion ignores the low level interaction between the modalities.
sent13: Hybrid fusion attempts to exploit the advantages of both of the above described methods in a common framework.
sent14: It has been used successfully for multimodal speaker identification [226] and multimedia event detection (MED) [117]."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s17,FUSION,"Multimodal fusion is one of the original topics in multimodal machine learning, with previous surveys emphasizing early, late and hybrid fusion approaches [50], [247]. In technical terms, multimodal fusion is the concept of integrating information from multiple modalities with the goal of predicting an outcome measure: a class (e.g., happy vs. sad) through classification, or a continuous value (e.g., positivity of sentiment) through regression. It is one of the most researched aspects of multimodal machine learning with work dating to 25 years ago [243].

The interest in multimodal fusion arises from three main benefits it can provide. First, having access to multiple modalities that observe the same phenomenon may allow for more robust predictions. This has been especially explored and exploited by the AVSR community [163]. Second, having access to multiple modalities might allow us to capture complementary information -something that is not visible in individual modalities on their own. Third, a multimodal system can still operate when one of the modalities is missing, for example recognizing emotions from the visual signal when the person is not speaking [50].

Multimodal fusion has a very broad range of applications, including audio-visual speech recognition (AVSR) [163], multimodal emotion recognition [192], medical image analysis [89], and multimedia event detection [117]. There are a number of reviews on the subject [11], [163], [188], [247]. Most of them concentrate on multimodal fusion for a particular task, such as multimedia analysis, information retrieval or emotion recognition. In contrast, we concentrate on the machine learning approaches themselves and the technical challenges associated with these approaches.

While some prior work used the term multimodal fusion to include all multimodal algorithms, in this survey paper we classify approaches as fusion category when the multimodal integration is performed at the later prediction Table 5: A summary of our taxonomy of multimodal fusion approaches. OUT -output type (class -classification or reg -regression), TEMP -is temporal modeling possible. stages, with the goal of predicting outcome measures. In recent work, the line between multimodal representation and fusion has been blurred for models such as deep neural networks where representation learning is interlaced with classification or regression objectives. As we will describe in this section, this line is clearer for other approaches such as graphical models and kernel-based methods. We classify multimodal fusion into two main categories: model-agnostic approaches (Section 6.1) that are not directly dependent on a specific machine learning method; and model-based (Section 6.2) approaches that explicitly address fusion in their construction -such as kernel-based approaches, graphical models, and neural networks. An overview of such approaches can be seen in Table 5.","[['b243', 'b50', 'b247'], ['b163', 'b50'], ['b192', 'b163', 'b247', 'b89', 'b11', 'b117', 'b188'], []]","[['b243', 'b50', 'b247'], ['b163', 'b50'], ['b192', 'b163', 'b247', 'b89', 'b11', 'b117', 'b188'], []]",12,"sent1: Multimodal fusion is one of the original topics in multimodal machine learning, with previous surveys emphasizing early, late and hybrid fusion approaches [50], [247].
sent2: In technical terms, multimodal fusion is the concept of integrating information from multiple modalities with the goal of predicting an outcome measure: a class (e.g., happy vs. sad) through classification, or a continuous value (e.g., positivity of sentiment) through regression.
sent3: It is one of the most researched aspects of multimodal machine learning with work dating to 25 years ago [243].
sent4: The interest in multimodal fusion arises from three main benefits it can provide.
sent5: First, having access to multiple modalities that observe the same phenomenon may allow for more robust predictions.
sent6: This has been especially explored and exploited by the AVSR community [163].
sent7: Second, having access to multiple modalities might allow us to capture complementary information -something that is not visible in individual modalities on their own.
sent8: Third, a multimodal system can still operate when one of the modalities is missing, for example recognizing emotions from the visual signal when the person is not speaking [50].
sent9: Multimodal fusion has a very broad range of applications, including audio-visual speech recognition (AVSR) [163], multimodal emotion recognition [192], medical image analysis [89], and multimedia event detection [117].
sent10: There are a number of reviews on the subject [11], [163], [188], [247].
sent11: Most of them concentrate on multimodal fusion for a particular task, such as multimedia analysis, information retrieval or emotion recognition.
sent12: In contrast, we concentrate on the machine learning approaches themselves and the technical challenges associated with these approaches.
sent13: While some prior work used the term multimodal fusion to include all multimodal algorithms, in this survey paper we classify approaches as fusion category when the multimodal integration is performed at the later prediction Table 5: A summary of our taxonomy of multimodal fusion approaches.
sent14: OUT -output type (class -classification or reg -regression), TEMP -is temporal modeling possible.
sent15: stages, with the goal of predicting outcome measures.
sent16: In recent work, the line between multimodal representation and fusion has been blurred for models such as deep neural networks where representation learning is interlaced with classification or regression objectives.
sent17: As we will describe in this section, this line is clearer for other approaches such as graphical models and kernel-based methods.
sent18: We classify multimodal fusion into two main categories: model-agnostic approaches (Section 6.1) that are not directly dependent on a specific machine learning method; and model-based (Section 6.2) approaches that explicitly address fusion in their construction -such as kernel-based approaches, graphical models, and neural networks.
sent19: An overview of such approaches can be seen in Table 5."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s14,Explicit alignment,"We categorize papers as performing explicit alignment if their main modeling objective is alignment between subcomponents of instances from two or more modalities. A very important part of explicit alignment is the similarity metric. Most approaches rely on measuring similarity between sub-components in different modalities as a basic building block. These similarities can be defined manually or learned from data. We identify two types of algorithms that tackle explicit alignment -unsupervised and (weakly) supervised. The first type operates with no direct alignment labels (i.e., labeled correspondences) between instances from the different modalities. The second type has access to such (sometimes weak) labels. Unsupervised multimodal alignment tackles modality alignment without requiring any direct alignment labels. Most of the approaches are inspired from early work on alignment for statistical machine translation [28] and genome sequences [3], [111]. To make the task easier the approaches assume certain constrains on alignment, such as temporal ordering of sequence or an existence of a similarity metric between the modalities.

Dynamic time warping (DTW) [3], [111] is a dynamic programming approach that has been extensively used to align multi-view time series. DTW measures the similarity between two sequences and finds an optimal match between them by time warping (inserting frames). It requires the timesteps in the two sequences to be comparable and requires a similarity measure between them. DTW can be used directly for multimodal alignment by hand-crafting similarity metrics between modalities; for example Anguera et al. [8] use a manually defined similarity between graphemes and phonemes; and Tapaswi et al. [201] define a similarity between visual scenes and sentences based on appearance of same characters [201] to align TV shows and plot synopses. DTW-like dynamic programming approaches have also been used for multimodal alignment of text to speech [77] and video [202].

As the original DTW formulation requires a pre-defined similarity metric between modalities, it was extended using canonical correlation analysis (CCA) to map the modalities to a coordinated space. This allows for both aligning (through DTW) and learning the mapping (through CCA) between different modality streams jointly and in an unsupervised manner [180], [250], [251]. While CCA based DTW models are able to find multimodal data alignment under a linear transformation, they are not able to model nonlinear relationships. This has been addressed by the deep canonical time warping approach [206], which can be seen as a generalization of deep CCA and DTW.

Various graphical models have also been popular for multimodal sequence alignment in an unsupervised manner. Early work by Yu and Ballard [239] used a generative graphical model to align visual objects in images with spoken words. A similar approach was taken by Cour et al. [44] to align movie shots and scenes to the corresponding screenplay. Malmaud et al. [131] used a factored HMM to align recipes to cooking videos, while Noulas et al. [154] used a dynamic Bayesian network to align speakers to videos. Naim et al. [147] matched sentences with corresponding video frames using a hierarchical HMM model to align sentences with frames and a modified IBM [28] algorithm for word and object alignment [15]. This model was then extended to use latent conditional random fields for alignments [146] and to incorporate verb alignment to actions in addition to nouns and objects [195].

Both DTW and graphical model approaches for alignment allow for restrictions on alignment, e.g. temporal consistency, no large jumps in time, and monotonicity. While DTW extensions allow for learning both the similarity metric and alignment jointly, graphical model based approaches require expert knowledge for construction [44], [239]. Supervised alignment methods rely on labeled aligned instances. They are used to train similarity measures that are used for aligning modalities.

A number of supervised sequence alignment techniques take inspiration from unsupervised ones. Bojanowski et al. [22], [23] proposed a method similar to canonical time warping, but have also extended it to take advantage of existing (weak) supervisory alignment data for model training. Plummer et al. [161] used CCA to find a coordinated space between image regions and phrases for alignment. Gebru et al. [65] trained a Gaussian mixture model and performed semi-supervised clustering together with an unsupervised latent-variable graphical model to align speakers in an audio channel with their locations in a video. Kong et al. [108] trained a Markov random field to align objects in 3D scenes to nouns and pronouns in text descriptions.

Deep learning based approaches are becoming popular for explicit alignment (specifically for measuring similarity) due to very recent availability of aligned datasets in the language and vision communities [133], [161]. Zhu et al. [252] aligned books with their corresponding movies/scripts by training a CNN to measure similarities between scenes and text. Mao et al. [133] used an LSTM language model and a CNN visual one to evaluate the quality of a match between a referring expression and an object in an image. Yu et al. [242] extended this model to include relative appearance and context information that allows to better disambiguate between objects of the same type. Finally, Hu et al. [85] used an LSTM based scoring function to find similarities between image regions and their descriptions.","[['b111', 'b2', 'b28'], ['b8', 'b111', 'b77', 'b202', 'b201', 'b2'], ['b251', 'b206', 'b250', 'b180'], ['b15', 'b131', 'b28', 'b147', 'b44', 'b239', 'b195', 'b154', 'b146'], ['b239', 'b44'], ['b161', 'b22', 'b23', 'b65', 'b108'], ['b85', 'b161', 'b133', 'b242', 'b252']]","[['b111', 'b2', 'b28'], ['b8', 'b111', 'b77', 'b202', 'b201', 'b2'], ['b251', 'b206', 'b250', 'b180'], ['b15', 'b131', 'b28', 'b147', 'b44', 'b239', 'b195', 'b154', 'b146'], ['b239', 'b44'], ['b161', 'b22', 'b23', 'b65', 'b108'], ['b85', 'b161', 'b133', 'b242', 'b252']]",34,"sent1: We categorize papers as performing explicit alignment if their main modeling objective is alignment between subcomponents of instances from two or more modalities.
sent2: A very important part of explicit alignment is the similarity metric.
sent3: Most approaches rely on measuring similarity between sub-components in different modalities as a basic building block.
sent4: These similarities can be defined manually or learned from data.
sent5: We identify two types of algorithms that tackle explicit alignment -unsupervised and (weakly) supervised.
sent6: The first type operates with no direct alignment labels (i.e., labeled correspondences) between instances from the different modalities.
sent7: The second type has access to such (sometimes weak) labels.
sent8: Unsupervised multimodal alignment tackles modality alignment without requiring any direct alignment labels.
sent9: Most of the approaches are inspired from early work on alignment for statistical machine translation [28] and genome sequences [3], [111].
sent10: To make the task easier the approaches assume certain constrains on alignment, such as temporal ordering of sequence or an existence of a similarity metric between the modalities.
sent11: Dynamic time warping (DTW) [3], [111] is a dynamic programming approach that has been extensively used to align multi-view time series.
sent12: DTW measures the similarity between two sequences and finds an optimal match between them by time warping (inserting frames).
sent13: It requires the timesteps in the two sequences to be comparable and requires a similarity measure between them.
sent14: DTW can be used directly for multimodal alignment by hand-crafting similarity metrics between modalities; for example Anguera et al. [8] use a manually defined similarity between graphemes and phonemes; and Tapaswi et al. [201] define a similarity between visual scenes and sentences based on appearance of same characters [201] to align TV shows and plot synopses.
sent15: DTW-like dynamic programming approaches have also been used for multimodal alignment of text to speech [77] and video [202].
sent16: As the original DTW formulation requires a pre-defined similarity metric between modalities, it was extended using canonical correlation analysis (CCA) to map the modalities to a coordinated space.
sent17: This allows for both aligning (through DTW) and learning the mapping (through CCA) between different modality streams jointly and in an unsupervised manner [180], [250], [251].
sent18: While CCA based DTW models are able to find multimodal data alignment under a linear transformation, they are not able to model nonlinear relationships.
sent19: This has been addressed by the deep canonical time warping approach [206], which can be seen as a generalization of deep CCA and DTW.
sent20: Various graphical models have also been popular for multimodal sequence alignment in an unsupervised manner.
sent21: Early work by Yu and Ballard [239] used a generative graphical model to align visual objects in images with spoken words.
sent22: A similar approach was taken by Cour et al. [44] to align movie shots and scenes to the corresponding screenplay.
sent23: Malmaud et al. [131] used a factored HMM to align recipes to cooking videos, while Noulas et al. [154] used a dynamic Bayesian network to align speakers to videos.
sent24: Naim et al. [147] matched sentences with corresponding video frames using a hierarchical HMM model to align sentences with frames and a modified IBM [28] algorithm for word and object alignment [15].
sent25: This model was then extended to use latent conditional random fields for alignments [146] and to incorporate verb alignment to actions in addition to nouns and objects [195].Both DTW and graphical model approaches for alignment allow for restrictions on alignment, e.g. temporal consistency, no large jumps in time, and monotonicity.
sent26: While DTW extensions allow for learning both the similarity metric and alignment jointly, graphical model based approaches require expert knowledge for construction [44], [239].
sent27: Supervised alignment methods rely on labeled aligned instances.
sent28: They are used to train similarity measures that are used for aligning modalities.
sent29: A number of supervised sequence alignment techniques take inspiration from unsupervised ones.
sent30: Bojanowski et al. [22], [23] proposed a method similar to canonical time warping, but have also extended it to take advantage of existing (weak) supervisory alignment data for model training.
sent31: Plummer et al. [161] used CCA to find a coordinated space between image regions and phrases for alignment.
sent32: Gebru et al. [65] trained a Gaussian mixture model and performed semi-supervised clustering together with an unsupervised latent-variable graphical model to align speakers in an audio channel with their locations in a video.
sent33: Kong et al. [108] trained a Markov random field to align objects in 3D scenes to nouns and pronouns in text descriptions.
sent34: Deep learning based approaches are becoming popular for explicit alignment (specifically for measuring similarity) due to very recent availability of aligned datasets in the language and vision communities [133], [161].
sent35: Zhu et al. [252] aligned books with their corresponding movies/scripts by training a CNN to measure similarities between scenes and text.
sent36: Mao et al. [133] used an LSTM language model and a CNN visual one to evaluate the quality of a match between a referring expression and an object in an image.
sent37: Yu et al. [242] extended this model to include relative appearance and context information that allows to better disambiguate between objects of the same type.
sent38: Finally, Hu et al. [85] used an LSTM based scoring function to find similarities between image regions and their descriptions."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s5,Coordinated Representations,"An alternative to a joint multimodal representation is a coordinated representation. Instead of projecting the modalities together into a joint space, we learn separate representations for each modality but coordinate them through a constraint. We start our discussion with coordinated representations that enforce similarity between representations, moving on to coordinated representations that enforce more structure on the resulting space (representative works of different coordinated representations can be seen in Table 2).

Similarity models minimize the distance between modalities in the coordinated space. For example such models encourage the representation of the word dog and an image of a dog to have a smaller distance between them than distance between the word dog and an image of a car [61]. One of the earliest examples of such a representation comes from the work by Weston et al. [221], [222] on the WSABIE (web scale annotation by image embedding) model, where a coordinated space was constructed for images and their annotations. WSABIE constructs a simple linear map from image and textual features such that corresponding annotation and image representation would have a higher inner product (smaller cosine distance) between them than noncorresponding ones.

More recently, neural networks have become a popular way to construct coordinated representations, due to their ability to learn representations. Their advantage lies in the fact that they can jointly learn coordinated representations in an end-to-end manner. An example of such coordinated representation is DeViSE -a deep visual-semantic embedding [61]. DeViSE uses a similar inner product and ranking loss function to WSABIE but uses more complex image and word embeddings. Kiros et al. [105] extended this to sentence and image coordinated representation by using an LSTM model and a pairwise ranking loss to coordinate the feature space. Socher et al. [191] tackle the same task, but extend the language model to a dependency tree RNN to incorporate compositional semantics. A similar model was also proposed by Pan et al. [159], but using videos instead of images. Xu et al. [231] also constructed a coordinated space between videos and sentences using a subject, verb, object compositional language model and a deep video model. This representation was then used for the task of cross-modal retrieval and video description.

While the above models enforced similarity between representations, structured coordinated space models go beyond that and enforce additional constraints between the modality representations. The type of structure enforced is often based on the application, with different constraints for hashing, cross-modal retrieval, and image captioning.

Structured coordinated spaces are commonly used in cross-modal hashing -compression of high dimensional data into compact binary codes with similar binary codes for similar objects [218]. The idea of cross-modal hashing is to create such codes for cross-modal retrieval [27], [93], [113]. Hashing enforces certain constraints on the resulting multimodal space: 1) it has to be an N -dimensional Hamming space -a binary representation with controllable number of bits; 2) the same object from different modalities has to have a similar hash code; 3) the space has to be similarity-preserving. Learning how to represent the data as a hash function attempts to enforce all of these three requirements [27], [113]. For example, Jiang and Li [92] introduced a method to learn such common binary space between sentence descriptions and corresponding images using end-to-end trainable deep learning techniques. While Cao et al. [32] extended the approach with a more complex LSTM sentence representation and introduced an outlier insensitive bit-wise margin loss and a relevance feedback based semantic similarity constraint. Similarly, Wang et al. [219] constructed a coordinated space in which images (and sentences) with similar meanings are closer to each other.

Another example of a structured coordinated representation comes from order-embeddings of images and language [212], [249]. The model proposed by Vendrov et al. [212] enforces a dissimilarity metric that is asymmetric and implements the notion of partial order in the multimodal space. The idea is to capture a partial order of the language and image representations -enforcing a hierarchy on the space; for example image of ""a woman walking her dog"" → text ""woman walking her dog"" → text ""woman walking"". A similar model using denotation graphs was also proposed by Young et al. [238] where denotation graphs are used to induce a partial ordering. Lastly, Zhang et al. present how exploiting structured representations of text and images can create concept taxonomies in an unsupervised manner [249].

A special case of a structured coordinated space is one based on canonical correlation analysis (CCA) [84]. CCA computes a linear projection which maximizes the correlation between two random variables (in our case modalities) and enforces orthogonality of the new space. CCA models have been used extensively for cross-modal retrieval [76], [106], [169] and audiovisual signal analysis [177], [187]. Extensions to CCA attempt to construct a correlation maximizing nonlinear projection [7], [116]. Kernel canonical correlation analysis (KCCA) [116] uses reproducing kernel Hilbert spaces for projection. However, as the approach is nonparametric it scales poorly with the size of the training set and has issues with very large real-world datasets. Deep canonical correlation analysis (DCCA) [7] was introduced as an alternative to KCCA and addresses the scalability issue, it was also shown to lead to better correlated representation space. Similar correspondence autoencoder [58] and deep correspondence RBMs [57] have also been proposed for cross-modal retrieval.

CCA, KCCA, and DCCA are unsupervised techniques and only optimize the correlation over the representations, thus mostly capturing what is shared across the modalities. Deep canonically correlated autoencoders [220] also include an autoencoder based data reconstruction term. This encourages the representation to also capture modality specific information. Semantic correlation maximization method [248] also encourages semantic relevance, while retaining correlation maximization and orthogonality of the resulting space -this leads to a combination of CCA and cross-modal hashing techniques.","[[], ['b221', 'b222', 'b61'], ['b191', 'b105', 'b159', 'b231', 'b61'], [], ['b27', 'b113', 'b218', 'b219', 'b93', 'b92', 'b32'], ['b212', 'b238', 'b249'], ['b169', 'b57', 'b84', 'b116', 'b76', 'b7', 'b58', 'b177', 'b106', 'b187'], ['b248', 'b220']]","[[], ['b221', 'b222', 'b61'], ['b191', 'b105', 'b159', 'b231', 'b61'], [], ['b27', 'b113', 'b218', 'b219', 'b93', 'b92', 'b32'], ['b212', 'b238', 'b249'], ['b169', 'b57', 'b84', 'b116', 'b76', 'b7', 'b58', 'b177', 'b106', 'b187'], ['b248', 'b220']]",30,"sent1: An alternative to a joint multimodal representation is a coordinated representation.
sent2: Instead of projecting the modalities together into a joint space, we learn separate representations for each modality but coordinate them through a constraint.
sent3: We start our discussion with coordinated representations that enforce similarity between representations, moving on to coordinated representations that enforce more structure on the resulting space (representative works of different coordinated representations can be seen in Table 2).
sent4: Similarity models minimize the distance between modalities in the coordinated space.
sent5: For example such models encourage the representation of the word dog and an image of a dog to have a smaller distance between them than distance between the word dog and an image of a car [61].
sent6: One of the earliest examples of such a representation comes from the work by Weston et al. [221], [222] on the WSABIE (web scale annotation by image embedding) model, where a coordinated space was constructed for images and their annotations.
sent7: WSABIE constructs a simple linear map from image and textual features such that corresponding annotation and image representation would have a higher inner product (smaller cosine distance) between them than noncorresponding ones.
sent8: More recently, neural networks have become a popular way to construct coordinated representations, due to their ability to learn representations.
sent9: Their advantage lies in the fact that they can jointly learn coordinated representations in an end-to-end manner.
sent10: An example of such coordinated representation is DeViSE -a deep visual-semantic embedding [61]. DeViSE uses a similar inner product and ranking loss function to WSABIE but uses more complex image and word embeddings.
sent11: Kiros et al. [105] extended this to sentence and image coordinated representation by using an LSTM model and a pairwise ranking loss to coordinate the feature space.
sent12: Socher et al. [191] tackle the same task, but extend the language model to a dependency tree RNN to incorporate compositional semantics.
sent13: A similar model was also proposed by Pan et al. [159], but using videos instead of images.
sent14: Xu et al. [231] also constructed a coordinated space between videos and sentences using a subject, verb, object compositional language model and a deep video model.
sent15: This representation was then used for the task of cross-modal retrieval and video description.
sent16: While the above models enforced similarity between representations, structured coordinated space models go beyond that and enforce additional constraints between the modality representations.
sent17: The type of structure enforced is often based on the application, with different constraints for hashing, cross-modal retrieval, and image captioning.
sent18: Structured coordinated spaces are commonly used in cross-modal hashing -compression of high dimensional data into compact binary codes with similar binary codes for similar objects [218].
sent19: The idea of cross-modal hashing is to create such codes for cross-modal retrieval [27], [93], [113].
sent20: Hashing enforces certain constraints on the resulting multimodal space: 1) it has to be an N -dimensional Hamming space -a binary representation with controllable number of bits; 2) the same object from different modalities has to have a similar hash code; 3) the space has to be similarity-preserving.
sent21: Learning how to represent the data as a hash function attempts to enforce all of these three requirements [27], [113].
sent22: For example, Jiang and Li [92] introduced a method to learn such common binary space between sentence descriptions and corresponding images using end-to-end trainable deep learning techniques.
sent23: While Cao et al. [32] extended the approach with a more complex LSTM sentence representation and introduced an outlier insensitive bit-wise margin loss and a relevance feedback based semantic similarity constraint.
sent24: Similarly, Wang et al. [219] constructed a coordinated space in which images (and sentences) with similar meanings are closer to each other.
sent25: Another example of a structured coordinated representation comes from order-embeddings of images and language [212], [249].
sent26: The model proposed by Vendrov et al. [212] enforces a dissimilarity metric that is asymmetric and implements the notion of partial order in the multimodal space.
sent27: The idea is to capture a partial order of the language and image representations -enforcing a hierarchy on the space; for example image of ""a woman walking her dog"" → text ""woman walking her dog"" → text ""woman walking"".
sent28: A similar model using denotation graphs was also proposed by Young et al. [238] where denotation graphs are used to induce a partial ordering.
sent29: Lastly, Zhang et al. present how exploiting structured representations of text and images can create concept taxonomies in an unsupervised manner [249].
sent30: A special case of a structured coordinated space is one based on canonical correlation analysis (CCA) [84].
sent31: CCA computes a linear projection which maximizes the correlation between two random variables (in our case modalities) and enforces orthogonality of the new space.
sent32: CCA models have been used extensively for cross-modal retrieval [76], [106], [169] and audiovisual signal analysis [177], [187].
sent33: Extensions to CCA attempt to construct a correlation maximizing nonlinear projection [7], [116].
sent34: Kernel canonical correlation analysis (KCCA) [116] uses reproducing kernel Hilbert spaces for projection.
sent35: However, as the approach is nonparametric it scales poorly with the size of the training set and has issues with very large real-world datasets.
sent36: Deep canonical correlation analysis (DCCA) [7] was introduced as an alternative to KCCA and addresses the scalability issue, it was also shown to lead to better correlated representation space.
sent37: Similar correspondence autoencoder [58] and deep correspondence RBMs [57] have also been proposed for cross-modal retrieval.
sent38: CCA, KCCA, and DCCA are unsupervised techniques and only optimize the correlation over the representations, thus mostly capturing what is shared across the modalities.
sent39: Deep canonically correlated autoencoders [220] also include an autoencoder based data reconstruction term.
sent40: This encourages the representation to also capture modality specific information.
sent41: Semantic correlation maximization method [248] also encourages semantic relevance, while retaining correlation maximization and orthogonality of the resulting space -this leads to a combination of CCA and cross-modal hashing techniques."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s12,MODALITIES REFERENCE Explicit,"Unsupervised Video + Text [131], [201], [202] Video + Audio [154], [206], [251] Supervised Video + Text [23], [252] Image + Text [108], [133], [161] ","[['b108', 'b131', 'b133', 'b161', 'b206', 'b252', 'b23', 'b202', 'b251', 'b201', 'b154']]","[['b108', 'b131', 'b133', 'b161', 'b206', 'b252', 'b23', 'b202', 'b251', 'b201', 'b154']]",11,"sent1: Unsupervised Video + Text [131], [201], [202] Video + Audio [154], [206], [251] Supervised Video + Text [23], [252] Image + Text [108], [133], [161]"
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s1,APPLICATIONS: A HISTORICAL PERSPECTIVE,"Multimodal machine learning enables a wide range of applications: from audio-visual speech recognition to image captioning. In this section we present a brief history of multimodal applications, from its beginnings in audiovisual speech recognition to a recently renewed interest in language and vision applications.

One of the earliest examples of multimodal research is audio-visual speech recognition (AVSR) [243]. It was motivated by the McGurk effect [138] -an interaction between hearing and vision during speech perception. When human subjects heard the syllable /ba-ba/ while watching the lips of a person saying /ga-ga/, they perceived a third sound: /da-da/. These results motivated many researchers from the speech community to extend their approaches with visual information. Given the prominence of hidden Markov models (HMMs) in the speech community at the time [95], it is without surprise that many of the early models for AVSR were based on various HMM extensions [24], [25]. While research into AVSR is not as common these days, it has seen renewed interest from the deep learning community [151].

While the original vision of AVSR was to improve speech recognition performance (e.g., word error rate) in all contexts, the experimental results showed that the main advantage of visual information was when the speech signal was noisy (i.e., low signal-to-noise ratio) [75], [151], [243]. In other words, the captured interactions between modalities were supplementary rather than complementary. The same information was captured in both, improving the robustness of the multimodal models but not improving the speech recognition performance in noiseless scenarios.

A second important category of multimodal applications comes from the field of multimedia content indexing and retrieval [11], [188]. With the advance of personal computers and the internet, the quantity of digitized multimedia content has increased dramatically [2]. While earlier approaches for indexing and searching these multimedia videos were keyword-based [188], new research problems emerged when trying to search the visual and multimodal content directly. This led to new research topics in multimedia content analysis such as automatic shot-boundary detection [123] and video summarization [53]. These research projects were supported by the TrecVid initiative from the National Institute of Standards and Technologies which introduced many high-quality datasets, including the multimedia event detection (MED) tasks started in 2011 [1].

A third category of applications was established in the early 2000s around the emerging field of multimodal interaction with the goal of understanding human multimodal behaviors during social interactions. One of the first landmark datasets collected in this field is the AMI Meeting Corpus which contains more than 100 hours of video recordings of meetings, all fully transcribed and annotated [33]. Another important dataset is the SEMAINE corpus which allowed to study interpersonal dynamics between speakers and listeners [139]. This dataset formed the basis of the first audio-visual emotion challenge (AVEC) organized in 2011 [179]. The fields of emotion recognition and affective computing bloomed in the early 2010s thanks to strong technical advances in automatic face detection, facial landmark detection, and facial expression recognition [46]. The AVEC challenge continued annually afterward with the later instantiation including healthcare applications such as automatic assessment of depression and anxiety [208]. A great summary of recent progress in multimodal affect recognition was published by D'Mello et al. [50]. Their metaanalysis revealed that a majority of recent work on mul-timodal affect recognition show improvement when using more than one modality, but this improvement is reduced when recognizing naturally-occurring emotions.

Most recently, a new category of multimodal applications emerged with an emphasis on language and vision: media description. One of the most representative applications is image captioning where the task is to generate a text description of the input image [83]. This is motivated by the ability of such systems to help the visually impaired in their daily tasks [20]. The main challenges media description is evaluation: how to evaluate the quality of the predicted descriptions. The task of visual question-answering (VQA) was recently proposed to address some of the evaluation challenges [9], where the goal is to answer a specific question about the image.

In order to bring some of the mentioned applications to the real world we need to address a number of technical challenges facing multimodal machine learning. We summarize the relevant technical challenges for the above mentioned application areas in Table 1. One of the most important challenges is multimodal representation, the focus of our next section.","[[], ['b151', 'b25', 'b95', 'b243', 'b138', 'b24'], ['b243', 'b151', 'b75'], ['b123', 'b1', 'b53', 'b11', 'b188', 'b0'], ['b50', 'b208', 'b46', 'b179', 'b33', 'b139'], ['b20', 'b9', 'b83'], []]","[[], ['b151', 'b25', 'b95', 'b243', 'b138', 'b24'], ['b243', 'b151', 'b75'], ['b123', 'b1', 'b53', 'b11', 'b188', 'b0'], ['b50', 'b208', 'b46', 'b179', 'b33', 'b139'], ['b20', 'b9', 'b83'], []]",24,"sent1: Multimodal machine learning enables a wide range of applications: from audio-visual speech recognition to image captioning.
sent2: In this section we present a brief history of multimodal applications, from its beginnings in audiovisual speech recognition to a recently renewed interest in language and vision applications.
sent3: One of the earliest examples of multimodal research is audio-visual speech recognition (AVSR) [243].
sent4: It was motivated by the McGurk effect [138] -an interaction between hearing and vision during speech perception.
sent5: When human subjects heard the syllable /ba-ba/ while watching the lips of a person saying /ga-ga/, they perceived a third sound: /da-da/.
sent6: These results motivated many researchers from the speech community to extend their approaches with visual information.
sent7: Given the prominence of hidden Markov models (HMMs) in the speech community at the time [95], it is without surprise that many of the early models for AVSR were based on various HMM extensions [24], [25].
sent8: While research into AVSR is not as common these days, it has seen renewed interest from the deep learning community [151].
sent9: While the original vision of AVSR was to improve speech recognition performance (e.g., word error rate) in all contexts, the experimental results showed that the main advantage of visual information was when the speech signal was noisy (i.e., low signal-to-noise ratio) [75], [151], [243].
sent10: In other words, the captured interactions between modalities were supplementary rather than complementary.
sent11: The same information was captured in both, improving the robustness of the multimodal models but not improving the speech recognition performance in noiseless scenarios.
sent12: A second important category of multimodal applications comes from the field of multimedia content indexing and retrieval [11], [188].
sent13: With the advance of personal computers and the internet, the quantity of digitized multimedia content has increased dramatically [2].
sent14: While earlier approaches for indexing and searching these multimedia videos were keyword-based [188], new research problems emerged when trying to search the visual and multimodal content directly.
sent15: This led to new research topics in multimedia content analysis such as automatic shot-boundary detection [123] and video summarization [53].
sent16: These research projects were supported by the TrecVid initiative from the National Institute of Standards and Technologies which introduced many high-quality datasets, including the multimedia event detection (MED) tasks started in 2011 [1].
sent17: A third category of applications was established in the early 2000s around the emerging field of multimodal interaction with the goal of understanding human multimodal behaviors during social interactions.
sent18: One of the first landmark datasets collected in this field is the AMI Meeting Corpus which contains more than 100 hours of video recordings of meetings, all fully transcribed and annotated [33].
sent19: Another important dataset is the SEMAINE corpus which allowed to study interpersonal dynamics between speakers and listeners [139].
sent20: This dataset formed the basis of the first audio-visual emotion challenge (AVEC) organized in 2011 [179].
sent21: The fields of emotion recognition and affective computing bloomed in the early 2010s thanks to strong technical advances in automatic face detection, facial landmark detection, and facial expression recognition [46].
sent22: The AVEC challenge continued annually afterward with the later instantiation including healthcare applications such as automatic assessment of depression and anxiety [208].
sent23: A great summary of recent progress in multimodal affect recognition was published by D'Mello et al. [50].
sent24: Their metaanalysis revealed that a majority of recent work on mul-timodal affect recognition show improvement when using more than one modality, but this improvement is reduced when recognizing naturally-occurring emotions.
sent25: Most recently, a new category of multimodal applications emerged with an emphasis on language and vision: media description.
sent26: One of the most representative applications is image captioning where the task is to generate a text description of the input image [83].
sent27: This is motivated by the ability of such systems to help the visually impaired in their daily tasks [20].
sent28: The main challenges media description is evaluation: how to evaluate the quality of the predicted descriptions.
sent29: The task of visual question-answering (VQA) was recently proposed to address some of the evaluation challenges [9], where the goal is to answer a specific question about the image.
sent30: In order to bring some of the mentioned applications to the real world we need to address a number of technical challenges facing multimodal machine learning.
sent31: We summarize the relevant technical challenges for the above mentioned application areas in Table 1.
sent32: One of the most important challenges is multimodal representation, the focus of our next section."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s13,Implicit,"Graphical models Audio/Text + Text [186], [216] Neural networks Image + Text [98], [228], [230] Video + Text [236], [241] based on text description can include an alignment step between words and image regions [99]. An overview of such approaches can be seen in Table 4 and is presented in more detail in the following sections.","[['b228', 'b186', 'b236', 'b99', 'b230', 'b241', 'b216', 'b98']]","[['b228', 'b186', 'b236', 'b99', 'b230', 'b241', 'b216', 'b98']]",8,"sent1: Graphical models Audio/Text + Text [186], [216]
sent2: Neural networks Image + Text [98], [228], [230] Video + Text [236], [241] based on text description can include an alignment step between words and image regions [99].
sent3: An overview of such approaches can be seen in Table 4 and is presented in more detail in the following sections."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s3,Joint Representations,"We start our discussion with joint representations that project unimodal representations together into a multimodal space (Equation 1). Joint representations are mostly (but not exclusively) used in tasks where multimodal data is present both during training and inference steps. The simplest example of a joint representation is a concatenation of individual modality features (also referred to as early fusion [50]). In this section we discuss more advanced methods for creating joint representations starting with neural networks, followed by graphical models and recurrent neural networks (representative works can be seen in Table 2). Neural networks have become a very popular method for unimodal data representation [18]. They are used to represent visual, acoustic, and textual data, and are increasingly used in the multimodal domain [151], [156], [217]. In this section we describe how neural networks can be used to construct a joint multimodal representation, how to train them, and what advantages they offer. In general, neural networks are made up of successive building blocks of inner products followed by non-linear activation functions. In order to use a neural network as Coordinated representations, on the other hand, exist in their own space, but are coordinated through a similarity (e.g. Euclidean distance) or structure constraint (e.g. partial order).

a way to represent data, it is first trained to perform a specific task (e.g., recognizing objects in images). Due to the multilayer nature of deep neural networks each successive layer is hypothesized to represent the data in a more abstract way [18], hence it is common to use the final or penultimate neural layers as a form of data representation. To construct a multimodal representation using neural networks each modality starts with several individual neural layers followed by a hidden layer that projects the modalities into a joint space [9], [145], [156], [227]. The joint multimodal representation is then be passed through multiple hidden layers itself or used directly for prediction. Such models can be trained end-to-end -learning both to represent the data and to perform a particular task. This results in a close relationship between multimodal representation learning and multimodal fusion when using neural networks. As neural networks require a lot of labeled training data, it is common to pre-train such representations using an autoencoder on unsupervised data [80]. The model proposed by Ngiam et al. [151] extended the idea of using autoencoders to the multimodal domain. They used stacked denoising autoencoders to represent each modality individually and then fused them into a multimodal representation using another autoencoder layer. Similarly, Silberer and Lapata [184] proposed to use a multimodal autoencoder for the task of semantic concept grounding (see Section 7.2). In addition to using a reconstruction loss to train the representation they introduce a term into the loss function that uses the representation to predict object labels. It is also common to fine-tune the resulting representation on a particular task at hand as the representation constructed using an autoencoder is generic and not necessarily optimal for a specific task [217].

The major advantage of neural network based joint representations comes from their often superior performance and the ability to pre-train the representations in an unsupervised manner. The performance gain is, however, dependent on the amount of data available for training. One of the disadvantages comes from the model not being able to handle missing data naturally -although there are ways to alleviate this issue [151], [217]. Finally, deep networks are often difficult to train [69], but the field is making progress in better training techniques [196]. Probabilistic graphical models are another popular way to construct representations through the use of latent random variables [18]. In this section we describe how probabilistic graphical models are used to represent unimodal and multimodal data.

The most popular approaches for graphical-model based representation are deep Boltzmann machines (DBM) [176], that stack restricted Boltzmann machines (RBM) [81] as building blocks. Similar to neural networks, each successive layer of a DBM is expected to represent the data at a higher level of abstraction. The appeal of DBMs comes from the fact that they do not need supervised data for training [176]. As they are graphical models the representation of data is probabilistic, however it is possible to convert them to a deterministic neural network -but this loses the generative aspect of the model [176].

Work by Srivastava and Salakhutdinov [197] introduced multimodal deep belief networks as a multimodal representation. Kim et al. [104] used a deep belief network for each modality and then combined them into joint representation for audiovisual emotion recognition. Huang and Kingsbury [86] used a similar model for AVSR, and Wu et al. [225] for audio and skeleton joint based gesture recognition.

Multimodal deep belief networks have been extended to multimodal DBMs by Srivastava and Salakhutdinov [198]. Multimodal DBMs are capable of learning joint representations from multiple modalities by merging two or more undirected graphs using a binary layer of hidden units on top of them. They allow for the low level representations of each modality to influence each other after the joint training due to the undirected nature of the model.

Ouyang et al. [156] explore the use of multimodal DBMs for the task of human pose estimation from multi-view data. They demonstrate that integrating the data at a later stageafter unimodal data underwent nonlinear transformations -was beneficial for the model. Similarly, Suk et al. [199] use multimodal DBM representation to perform Alzheimer's disease classification from positron emission tomography and magnetic resonance imaging data.

One of the big advantages of using multimodal DBMs for learning multimodal representations is their generative nature, which allows for an easy way to deal with missing data -even if a whole modality is missing, the model has a natural way to cope. It can also be used to generate samples of one modality in the presence of the other one, or Table 2: A summary of multimodal representation techniques. We identify three subtypes of joint representations (Section 3.1) and two subtypes of coordinated ones (Section 3.2). For modalities + indicates the modalities combined. REPRESENTATION ","[['b50', 'b156', 'b151', 'b18', 'b217'], ['b184', 'b227', 'b156', 'b151', 'b80', 'b9', 'b18', 'b145', 'b217'], ['b196', 'b151', 'b18', 'b217', 'b69'], ['b81', 'b176'], ['b197', 'b104', 'b86', 'b225'], ['b198'], ['b199', 'b156'], [None]]","[['b50', 'b156', 'b151', 'b18', 'b217'], ['b184', 'b227', 'b156', 'b151', 'b80', 'b9', 'b18', 'b145', 'b217'], ['b196', 'b151', 'b18', 'b217', 'b69'], ['b81', 'b176'], ['b197', 'b104', 'b86', 'b225'], ['b198'], ['b199', 'b156'], [None]]",29,"sent1: We start our discussion with joint representations that project unimodal representations together into a multimodal space (Equation 1).
sent2: Joint representations are mostly (but not exclusively) used in tasks where multimodal data is present both during training and inference steps.
sent3: The simplest example of a joint representation is a concatenation of individual modality features (also referred to as early fusion [50]).
sent4: In this section we discuss more advanced methods for creating joint representations starting with neural networks, followed by graphical models and recurrent neural networks (representative works can be seen in Table 2).
sent5: Neural networks have become a very popular method for unimodal data representation [18].
sent6: They are used to represent visual, acoustic, and textual data, and are increasingly used in the multimodal domain [151], [156], [217].
sent7: In this section we describe how neural networks can be used to construct a joint multimodal representation, how to train them, and what advantages they offer.
sent8: In general, neural networks are made up of successive building blocks of inner products followed by non-linear activation functions.
sent9: In order to use a neural network as Coordinated representations, on the other hand, exist in their own space, but are coordinated through a similarity (e.g. Euclidean distance) or structure constraint (e.g. partial order).a way to represent data, it is first trained to perform a specific task (e.g., recognizing objects in images).
sent10: Due to the multilayer nature of deep neural networks each successive layer is hypothesized to represent the data in a more abstract way [18], hence it is common to use the final or penultimate neural layers as a form of data representation.
sent11: To construct a multimodal representation using neural networks each modality starts with several individual neural layers followed by a hidden layer that projects the modalities into a joint space [9], [145], [156], [227].
sent12: The joint multimodal representation is then be passed through multiple hidden layers itself or used directly for prediction.
sent13: Such models can be trained end-to-end -learning both to represent the data and to perform a particular task.
sent14: This results in a close relationship between multimodal representation learning and multimodal fusion when using neural networks.
sent15: As neural networks require a lot of labeled training data, it is common to pre-train such representations using an autoencoder on unsupervised data [80].
sent16: The model proposed by Ngiam et al. [151] extended the idea of using autoencoders to the multimodal domain.
sent17: They used stacked denoising autoencoders to represent each modality individually and then fused them into a multimodal representation using another autoencoder layer.
sent18: Similarly, Silberer and Lapata [184] proposed to use a multimodal autoencoder for the task of semantic concept grounding (see Section 7.2).
sent19: In addition to using a reconstruction loss to train the representation they introduce a term into the loss function that uses the representation to predict object labels.
sent20: It is also common to fine-tune the resulting representation on a particular task at hand as the representation constructed using an autoencoder is generic and not necessarily optimal for a specific task [217].
sent21: The major advantage of neural network based joint representations comes from their often superior performance and the ability to pre-train the representations in an unsupervised manner.
sent22: The performance gain is, however, dependent on the amount of data available for training.
sent23: One of the disadvantages comes from the model not being able to handle missing data naturally -although there are ways to alleviate this issue [151], [217].
sent24: Finally, deep networks are often difficult to train [69], but the field is making progress in better training techniques [196].
sent25: Probabilistic graphical models are another popular way to construct representations through the use of latent random variables [18].
sent26: In this section we describe how probabilistic graphical models are used to represent unimodal and multimodal data.
sent27: The most popular approaches for graphical-model based representation are deep Boltzmann machines (DBM) [176], that stack restricted Boltzmann machines (RBM) [81] as building blocks.
sent28: Similar to neural networks, each successive layer of a DBM is expected to represent the data at a higher level of abstraction.
sent29: The appeal of DBMs comes from the fact that they do not need supervised data for training [176].
sent30: As they are graphical models the representation of data is probabilistic, however it is possible to convert them to a deterministic neural network -but
sent31: this loses the generative aspect of the model [176].
sent32: Work by Srivastava and Salakhutdinov [197] introduced multimodal deep belief networks as a multimodal representation.
sent33: Kim et al. [104] used a deep belief network for each modality and then combined them into joint representation for audiovisual emotion recognition.
sent34: Huang and Kingsbury [86] used a similar model for AVSR, and Wu et al. [225] for audio and skeleton joint based gesture recognition.
sent35: Multimodal deep belief networks have been extended to multimodal DBMs by Srivastava and Salakhutdinov [198].
sent36: Multimodal DBMs are capable of learning joint representations from multiple modalities by merging two or more undirected graphs using a binary layer of hidden units on top of them.
sent37: They allow for the low level representations of each modality to influence each other after the joint training due to the undirected nature of the model.
sent38: Ouyang et al. [156] explore the use of multimodal DBMs for the task of human pose estimation from multi-view data.
sent39: They demonstrate that integrating the data at a later stageafter unimodal data underwent nonlinear transformations -was beneficial for the model.
sent40: Similarly, Suk et al. [199] use multimodal DBM representation to perform Alzheimer's disease classification from positron emission tomography and magnetic resonance imaging data.
sent41: One of the big advantages of using multimodal DBMs for learning multimodal representations is their generative nature, which allows for an easy way to deal with missing data -even if a whole modality is missing, the model has a natural way to cope.
sent42: It can also be used to generate samples of one modality in the presence of the other one, or Table 2: A summary of multimodal representation techniques.
sent43: We identify three subtypes of joint representations (Section 3.1) and two subtypes of coordinated ones (Section 3.2).
sent44: For modalities + indicates the modalities combined.
sent45: REPRESENTATION"
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s4,MODALITIES REFERENCE Joint,"Neural networks Images + Audio [145], [151], [ Images + Text [32], [212], [248] Audio + Articulatory [220] both modalities from the representation. Similar to autoencoders the representation can be trained in an unsupervised manner enabling the use of unlabeled data. The major disadvantage of DBMs is the difficulty of training themhigh computational cost, and the need to use approximate variational training methods [198]. Sequential Representation. So far we have discussed models that can represent fixed length data, however, we often need to represent varying length sequences such as sentences, videos, or audio streams. In this section we describe models that can be used to represent such sequences. Recurrent neural networks (RNNs), and their variants such as long-short term memory (LSTMs) networks [82], have recently gained popularity due to their success in sequence modeling across various tasks [12], [213]. So far RNNs have mostly been used to represent unimodal sequences of words, audio, or images, with most success in the language domain. Similar to traditional neural networks, the hidden state of an RNN can be seen as a representation of the data, i.e., the hidden state of RNN at timestep t can be seen as the summarization of the sequence up to that timestep. This is especially apparent in RNN encoderdecoder frameworks where the task of an encoder is to represent a sequence in the hidden state of an RNN in such a way that a decoder could reconstruct it [12].

The use of RNN representations has not been limited to the unimodal domain. An early use of constructing a multimodal representation using RNNs comes from work by Cosi et al. [43] on AVSR. They have also been used for representing audio-visual data for affect recognition [37], [152] and to represent multi-view data such as different visual cues for human behavior analysis [166].","[['b213', 'b220', 'b151', 'b82', 'b212', 'b248', 'b198', None, 'b145', 'b12', 'b32'], ['b166', 'b43', 'b152', 'b37']]","[['b213', 'b220', 'b151', 'b82', 'b212', 'b248', 'b198', None, 'b145', 'b12', 'b32'], ['b166', 'b43', 'b152', 'b37']]",15,"sent1: Neural networks Images + Audio [145], [151], [ Images + Text [32], [212], [248] Audio + Articulatory [220]
sent2: both modalities from the representation.
sent3: Similar to autoencoders the representation can be trained in an unsupervised manner enabling the use of unlabeled data.
sent4: The major disadvantage of DBMs is the difficulty of training themhigh computational cost, and the need to use approximate variational training methods [198].
sent5: Sequential Representation. So far we have discussed models that can represent fixed length data, however, we often need to represent varying length sequences such as sentences, videos, or audio streams.
sent6: In this section we describe models that can be used to represent such sequences.
sent7: Recurrent neural networks (RNNs), and their variants such as long-short term memory (LSTMs)
sent8: networks [82], have recently gained popularity due to their success in sequence modeling across various tasks [12], [213].
sent9: So far RNNs have mostly been used to represent unimodal sequences of words, audio, or images, with most success in the language domain.
sent10: Similar to traditional neural networks, the hidden state of an RNN can be seen as a representation of the data, i.e., the hidden state of RNN at timestep t can be seen as the summarization of the sequence up to that timestep.
sent11: This is especially apparent in RNN encoderdecoder frameworks where the task of an encoder is to represent a sequence in the hidden state of an RNN in such a way that a decoder could reconstruct it [12].
sent12: The use of RNN representations has not been limited to the unimodal domain.
sent13: An early use of constructing a multimodal representation using RNNs comes from work by Cosi et al. [43] on AVSR.
sent14: They have also been used for representing audio-visual data for affect recognition [37], [152] and to represent multi-view data such as different visual cues for human behavior analysis [166]."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s2,MULTIMODAL REPRESENTATIONS,"Representing raw data in a format that a computational model can work with has always been a big challenge in machine learning. Following the work of Bengio et al. [18] we use the term feature and representation interchangeably, with each referring to a vector or tensor representation of an entity, be it an image, audio sample, individual word, or a sentence. A multimodal representation is a representation of data using information from multiple such entities. Representing multiple modalities poses many difficulties: how to combine the data from heterogeneous sources; how to deal with different levels of noise; and how to deal with missing data. The ability to represent data in a meaningful way is crucial to multimodal problems, and forms the backbone of any model.

Good representations are important for the performance of machine learning models, as evidenced behind the recent leaps in performance of speech recognition [79] and visual object classification [109] systems. Bengio et al. [18] identify a number of properties for good representations: smoothness, temporal and spatial coherence, sparsity, and natural clustering amongst others. Srivastava and Salakhutdinov [198] identify additional desirable properties for multimodal representations: similarity in the representation space should reflect the similarity of the corresponding concepts, the representation should be easy to obtain even in the absence of some modalities, and finally, it should be possible to fill-in missing modalities given the observed ones.

The development of unimodal representations has been extensively studied [5], [18], [122]. In the past decade there has been a shift from hand-designed for specific applications to data-driven. For example, one of the most famous image descriptors in the early 2000s, the scale invariant feature transform (SIFT) was hand designed [127], but currently most visual descriptions are learned from data using neural architectures such as convolutional neural networks (CNN) [109]. Similarly, in the audio domain, acoustic features such as Mel-frequency cepstral coefficients (MFCC) have been superseded by data-driven deep neural networks in speech recognition [79] and recurrent neural networks for para-linguistic analysis [207]. In natural language processing, the textual features initially relied on counting word occurrences in documents, but have been replaced datadriven word embeddings that exploit the word context [141]. While there has been a huge amount of work on unimodal representation, up until recently most multimodal representations involved simple concatenation of unimodal ones [50], but this has been rapidly changing.

To help understand the breadth of work, we propose two categories of multimodal representation: joint and coordinated. Joint representations combine the unimodal signals into the same representation space, while coordinated representations process unimodal signals separately, but enforce certain similarity constraints on them to bring them to what we term a coordinated space. An illustration of different multimodal representation types can be seen in Figure 1.

Mathematically, the joint representation is expressed as:

where the multimodal representation x m is computed using function f (e.g., a deep neural network, restricted Boltzmann machine, or a recurrent neural network) that relies on unimodal representations x 1 , . . . x n . While coordinated representation is as follows:

where each modality has a corresponding projection function (f and g above) that maps it into a coordinated multimodal space. While the projection into the multimodal space is independent for each modality, but the resulting space is coordinated between them (indicated as ∼). Examples of such coordination include minimizing cosine distance [61], maximizing correlation [7], and enforcing a partial order [212] between the resulting spaces.","[['b18'], ['b198', 'b79', 'b109', 'b18'], ['b50', 'b79', 'b5', 'b141', 'b18', 'b109', 'b207', 'b122', 'b127'], [], [], [], ['b212', 'b7', 'b61']]","[['b18'], ['b198', 'b79', 'b109', 'b18'], ['b50', 'b79', 'b5', 'b141', 'b18', 'b109', 'b207', 'b122', 'b127'], [], [], [], ['b212', 'b7', 'b61']]",17,"sent1: Representing raw data in a format that a computational model can work with has always been a big challenge in machine learning.
sent2: Following the work of Bengio et al. [18] we use the term feature and representation interchangeably, with each referring to a vector or tensor representation of an entity, be it an image, audio sample, individual word, or a sentence.
sent3: A multimodal representation is a representation of data using information from multiple such entities.
sent4: Representing multiple modalities poses many difficulties: how to combine the data from heterogeneous sources; how to deal with different levels of noise; and how to deal with missing data.
sent5: The ability to represent data in a meaningful way is crucial to multimodal problems, and forms the backbone of any model.
sent6: Good representations are important for the performance of machine learning models, as evidenced behind the recent leaps in performance of speech recognition [79] and visual object classification [109] systems.
sent7: Bengio et al. [18] identify a number of properties for good representations: smoothness, temporal and spatial coherence, sparsity, and natural clustering amongst others.
sent8: Srivastava and Salakhutdinov [198] identify additional desirable properties for multimodal representations: similarity in the representation space should reflect the similarity of the corresponding concepts, the representation should be easy to obtain even in the absence of some modalities, and finally, it should be possible to fill-in missing modalities given the observed ones.
sent9: The development of unimodal representations has been extensively studied [5], [18], [122].
sent10: In the past decade there has been a shift from hand-designed for specific applications to data-driven.
sent11: For example, one of the most famous image descriptors in the early 2000s, the scale invariant feature transform (SIFT) was hand designed [127], but currently most visual descriptions are learned from data using neural architectures such as convolutional neural networks (CNN) [109].
sent12: Similarly, in the audio domain, acoustic features such as Mel-frequency cepstral coefficients (MFCC) have been superseded by data-driven deep neural networks in speech recognition [79] and recurrent neural networks for para-linguistic analysis [207].
sent13: In natural language processing, the textual features initially relied on counting word occurrences in documents, but have been replaced datadriven word embeddings that exploit the word context [141].
sent14: While there has been a huge amount of work on unimodal representation, up until recently most multimodal representations involved simple concatenation of unimodal ones [50], but this has been rapidly changing.
sent15: To help understand the breadth of work, we propose two categories of multimodal representation: joint and coordinated.
sent16: Joint representations combine the unimodal signals into the same representation space, while coordinated representations process unimodal signals separately, but enforce certain similarity constraints on them to bring them to what we term a coordinated space.
sent17: An illustration of different multimodal representation types can be seen in Figure 1.
sent18: Mathematically, the joint representation is expressed as:where the multimodal representation x m is computed using function f (e.g., a deep neural network, restricted Boltzmann machine, or a recurrent neural network) that relies on unimodal representations x 1 , . . . x n .
sent19: While coordinated representation is as follows:where each modality has a corresponding projection function (f and g above) that maps it into a coordinated multimodal space.
sent20: While the projection into the multimodal space is independent for each modality, but the resulting space is coordinated between them (indicated as ∼).
sent21: Examples of such coordination include minimizing cosine distance [61], maximizing correlation [7], and enforcing a partial order [212] between the resulting spaces."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s7,TRANSLATION,"A big part of multimodal machine learning is concerned with translating (mapping) from one modality to another. Given an entity in one modality the task is to generate the same entity in a different modality. For example given an image we might want to generate a sentence describing it or given a textual description generate an image matching it. Multimodal translation is a long studied problem, with early work in speech synthesis [88], visual speech generation [136] video description [107], and cross-modal retrieval [169]. More recently, multimodal translation has seen renewed interest due to combined efforts of the computer vision and natural language processing (NLP) communities [19] and recent availability of large multimodal datasets [38], [205]. A particularly popular problem is visual scene description, also known as image [214] and video captioning [213], which acts as a great test bed for a number of computer vision and NLP problems. To solve it, we not only need to fully understand the visual scene and to identify its salient parts, but also to produce grammatically correct and comprehensive yet concise sentences describing it.

While the approaches to multimodal translation are very broad and are often modality specific, they share a number of unifying factors. We categorize them into two typesexample-based, and generative. Example-based models use a dictionary when translating between the modalities. Generative models, on the other hand, construct a model that is able to produce a translation. This distinction is similar to the one between non-parametric and parametric machine learning approaches and is illustrated in Figure 2, with representative examples summarized in Table 3.

Generative models are arguably more challenging to build as they require the ability to generate signals or sequences of symbols (e.g., sentences). This is difficult for any modality -visual, acoustic, or verbal, especially when temporally and structurally consistent sequences need to be generated. This led to many of the early multimodal translation systems relying on example-based translation. However, this has been changing with the advent of deep learning models that are capable of generating images [171], [210], sounds [157], [209], and text [12].","[['b88', 'b169', 'b213', 'b214', 'b136', 'b107', 'b38', 'b205', 'b19'], [], ['b157', 'b171', 'b209', 'b210', 'b12']]","[['b88', 'b169', 'b213', 'b214', 'b136', 'b107', 'b38', 'b205', 'b19'], [], ['b157', 'b171', 'b209', 'b210', 'b12']]",14,"sent1: A big part of multimodal machine learning is concerned with translating (mapping) from one modality to another.
sent2: Given an entity in one modality the task is to generate the same entity in a different modality.
sent3: For example given an image we might want to generate a sentence describing it or given a textual description generate an image matching it.
sent4: Multimodal translation is a long studied problem, with early work in speech synthesis [88], visual speech generation [136] video description [107], and cross-modal retrieval [169].
sent5: More recently, multimodal translation has seen renewed interest due to combined efforts of the computer vision and natural language processing (NLP) communities [19] and recent availability of large multimodal datasets [38], [205].
sent6: A particularly popular problem is visual scene description, also known as image [214] and video captioning [213], which acts as a great test bed for a number of computer vision and NLP problems.
sent7: To solve it, we not only need to fully understand the visual scene and to identify its salient parts, but also to produce grammatically correct and comprehensive yet concise sentences describing it.
sent8: While the approaches to multimodal translation are very broad and are often modality specific, they share a number of unifying factors.
sent9: We categorize them into two typesexample-based, and generative.
sent10: Example-based models use a dictionary when translating between the modalities.
sent11: Generative models, on the other hand, construct a model that is able to produce a translation.
sent12: This distinction is similar to the one between non-parametric and parametric machine learning approaches and is illustrated in Figure 2, with representative examples summarized in Table 3.Generative models are arguably more challenging to build as they require the ability to generate signals or sequences of symbols (e.g., sentences).
sent13: This is difficult for any modality -visual, acoustic, or verbal, especially when temporally and structurally consistent sequences need to be generated.
sent14: This led to many of the early multimodal translation systems relying on example-based translation.
sent15: However, this has been changing with the advent of deep learning models that are capable of generating images [171], [210], sounds [157], [209], and text [12]."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s8,Example-based,"Example-based algorithms are restricted by their training data -dictionary (see Figure 2a). We identify two types of such algorithms: retrieval based, and combination based.

Retrieval-based models directly use the retrieved translation without modifying it, while combination-based models rely on more complex rules to create translations based on a number of retrieved instances.

Retrieval-based models are arguably the simplest form of multimodal translation. They rely on finding the closest sample in the dictionary and using that as the translated result. The retrieval can be done in unimodal space or intermediate semantic space. Given a source modality instance to be translated, unimodal retrieval finds the closest instances in the dictionary in the space of the source -for example, visual feature space for images. Such approaches have been used for visual speech synthesis, by retrieving the closest matching visual example of the desired phoneme [26]. They have also been used in concatenative text-to-speech systems [88]. More recently, Ordonez et al. [155] used unimodal retrieval to generate image descriptions by using global image features to retrieve caption candidates [155]. Yagcioglu et al. [232] used a CNN-based image representation to retrieve visually similar images using adaptive neighborhood selection. Devlin et al. [49] demonstrated that a simple k-nearest neighbor retrieval with consensus caption selection achieves competitive translation results when compared to more complex generative approaches. The advantage of such unimodal retrieval approaches is that they only require the representation of a single modality through which we are performing retrieval. However, they often require an extra processing step such as re-ranking of retrieved translations [135], [155], [232]. This indicates a major problem with this approach -similarity in unimodal space does not always imply a good translation.

An alternative is to use an intermediate semantic space for similarity comparison during retrieval. An early example of a hand crafted semantic space is one used by Farhadi et al. [56]. They map both sentences and images to a space of object, action, scene , retrieval of relevant caption to an image is then performed in that space. In contrast to hand-crafting a representation, Socher et al. [191] learn a coordinated representation of sentences and CNN visual features (see Section 3.2 for description of coordinated spaces). They use the model for both translating from text to images and from images to text. Similarly, Xu et al. [231] used a coordinated space of videos and their descriptions for cross-modal retrieval. Jiang and Li [93] and Cao et al. [32] use cross-modal hashing to perform multimodal translation from images to sentences and back, while Hodosh et al. [83] use a multimodal KCCA space for imagesentence retrieval. Instead of aligning images and sentences globally in a common space, Karpathy et al. [99] propose a multimodal similarity metric that internally aligns image fragments (visual objects) together with sentence fragments (dependency tree relations).

Retrieval approaches in semantic space tend to perform better than their unimodal counterparts as they are retrieving examples in a more meaningful space that reflects both modalities and that is often optimized for retrieval. Furthermore, they allow for bi-directional translation, which is not straightforward with unimodal methods. However, they require manual construction or learning of such a semantic space, which often relies on the existence of large training dictionaries (datasets of paired samples). Combination-based models take the retrieval based approaches one step further. Instead of just retrieving examples from the dictionary, they combine them in a meaningful way to construct a better translation. Combination based media description approaches are motivated by the fact that sentence descriptions of images share a common and simple structure that could be exploited. Most often the rules for combinations are hand crafted or based on heuristics.

Kuznetsova et al. [114] first retrieve phrases that describe visually similar images and then combine them to generate novel descriptions of the query image by using Integer Linear Programming with a number of hand crafted rules. Gupta et al. [74] first find k images most similar to the source image, and then use the phrases extracted from their captions to generate a target sentence. Lebret et al. [119] use a CNN-based image representation to infer phrases that describe it. The predicted phrases are then combined using a trigram constrained language model. A big problem facing example-based approaches for translation is that the model is the entire dictionary -making the model large and inference slow (although, optimizations such as hashing alleviate this problem). Another issue facing example-based translation is that it is unrealistic to expect that a single comprehensive and accurate translation relevant to the source example will always exist in the dictionary -unless the task is simple or the dictionary is very large. This is partly addressed by combination models that are able to construct more complex structures. However, they are only able to perform translation in one direction, while semantic space retrieval-based models are able to perform it both ways.","[[], [], ['b88', 'b49', 'b155', 'b232', 'b26', 'b135'], ['b191', 'b56', 'b99', 'b231', 'b83', 'b93', 'b32'], [], ['b74', 'b114', 'b119']]","[[], [], ['b88', 'b49', 'b155', 'b232', 'b26', 'b135'], ['b191', 'b56', 'b99', 'b231', 'b83', 'b93', 'b32'], [], ['b74', 'b114', 'b119']]",16,"sent1: Example-based algorithms are restricted by their training data -dictionary (see Figure 2a).
sent2: We identify two types of such algorithms: retrieval based, and combination based.
sent3: Retrieval-based models directly use the retrieved translation without modifying it, while combination-based models rely on more complex rules to create translations based on a number of retrieved instances.Retrieval-based models are arguably the simplest form of multimodal translation.
sent4: They rely on finding the closest sample in the dictionary and using that as the translated result.
sent5: The retrieval can be done in unimodal space or intermediate semantic space.
sent6: Given a source modality instance to be translated, unimodal retrieval finds the closest instances in the dictionary in the space of the source -for example, visual feature space for images.
sent7: Such approaches have been used for visual speech synthesis, by retrieving the closest matching visual example of the desired phoneme [26].
sent8: They have also been used in concatenative text-to-speech systems [88].
sent9: More recently, Ordonez et al. [155] used unimodal retrieval to generate image descriptions by using global image features to retrieve caption candidates [155].
sent10: Yagcioglu et al. [232] used a CNN-based image representation to retrieve visually similar images using adaptive neighborhood selection.
sent11: Devlin et al. [49] demonstrated that a simple k-nearest neighbor retrieval with consensus caption selection achieves competitive translation results when compared to more complex generative approaches.
sent12: The advantage of such unimodal retrieval approaches is that they only require the representation of a single modality through which we are performing retrieval.
sent13: However, they often require an extra processing step such as re-ranking of retrieved translations [135], [155], [232].
sent14: This indicates a major problem with this approach -similarity in unimodal space does not always imply a good translation.
sent15: An alternative is to use an intermediate semantic space for similarity comparison during retrieval.
sent16: An early example of a hand crafted semantic space is one used by Farhadi et al. [56].
sent17: They map both sentences and images to a space of object, action, scene , retrieval of relevant caption to an image is then performed in that space.
sent18: In contrast to hand-crafting a representation, Socher et al. [191] learn a coordinated representation of sentences and CNN visual features (see Section 3.2 for description of coordinated spaces).
sent19: They use the model for both translating from text to images and from images to text.
sent20: Similarly, Xu et al. [231] used a coordinated space of videos and their descriptions for cross-modal retrieval.
sent21: Jiang and Li [93] and Cao et al. [32] use cross-modal hashing to perform multimodal translation from images to sentences and back, while Hodosh et al. [83] use a multimodal KCCA space for imagesentence retrieval.
sent22: Instead of aligning images and sentences globally in a common space, Karpathy et al. [99] propose a multimodal similarity metric that internally aligns image fragments (visual objects) together with sentence fragments (dependency tree relations).
sent23: Retrieval approaches in semantic space tend to perform better than their unimodal counterparts as they are retrieving examples in a more meaningful space that reflects both modalities and that is often optimized for retrieval.
sent24: Furthermore, they allow for bi-directional translation, which is not straightforward with unimodal methods.
sent25: However, they require manual construction or learning of such a semantic space, which often relies on the existence of large training dictionaries (datasets of paired samples).
sent26: Combination-based models take the retrieval based approaches one step further.
sent27: Instead of just retrieving examples from the dictionary, they combine them in a meaningful way to construct a better translation.
sent28: Combination based media description approaches are motivated by the fact that sentence descriptions of images share a common and simple structure that could be exploited.
sent29: Most often the rules for combinations are hand crafted or based on heuristics.
sent30: Kuznetsova et al. [114] first retrieve phrases that describe visually similar images and then combine them to generate novel descriptions of the query image by using Integer Linear Programming with a number of hand crafted rules.
sent31: Gupta et al. [74] first find k images most similar to the source image, and then use the phrases extracted from their captions to generate a target sentence.
sent32: Lebret et al. [119] use a CNN-based image representation to infer phrases that describe it.
sent33: The predicted phrases are then combined using a trigram constrained language model.
sent34: A big problem facing example-based approaches for translation is that the model is the entire dictionary -making the model large and inference slow (although, optimizations such as hashing alleviate this problem).
sent35: Another issue facing example-based translation is that it is unrealistic to expect that a single comprehensive and accurate translation relevant to the source example will always exist in the dictionary -unless the task is simple or the dictionary is very large.
sent36: This is partly addressed by combination models that are able to construct more complex structures.
sent37: However, they are only able to perform translation in one direction, while semantic space retrieval-based models are able to perform it both ways."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s9,Generative approaches,"Generative approaches to multimodal translation construct models that can perform multimodal translation given a unimodal source instance. It is a challenging problem as it requires the ability to both understand the source modality and to generate the target sequence or signal. As discussed in the following section, this also makes such methods much more difficult to evaluate, due to large space of possible correct answers.

In this survey we focus on the generation of three modalities: language, vision, and sound. Language generation has been explored for a long time [170], with a lot of recent attention for tasks such as image and video description [19]. Speech and sound generation has also seen a lot of work with a number of historical [88] and modern approaches [157], [209]. Photo-realistic image generation has been less explored, and is still in early stages [132], [171], however, there have been a number of attempts at generating abstract scenes [253], computer graphics [45], and talking heads [6].

We identify three broad categories of generative models: grammar-based, encoder-decoder, and continuous generation models. Grammar based models simplify the task by restricting the target domain by using a grammar, e.g., by generating restricted sentences based on a subject, object, verb template. Encoder-decoder models first encode the source modality to a latent representation which is then used by a decoder to generate the target modality. Continuous generation models generate the target modality continuously based on a stream of source modality inputs and are most suited for translating between temporal sequences -such as text-to-speech. Grammar-based models rely on a pre-defined grammar for generating a particular modality. They start by detecting high level concepts from the source modality, such as objects in images and actions from videos. These detections are then incorporated together with a generation procedure based on a pre-defined grammar to result in a target modality.

Kojima et al. [107] proposed a system to describe human behavior in a video using the detected position of the person's head and hands and rule based natural language generation that incorporates a hierarchy of concepts and actions. Barbu et al. [14] proposed a video description model that generates sentences of the form: who did what to whom and where and how they did it. The system was based on handcrafted object and event classifiers and used a restricted grammar suitable for the task. Guadarrama et al. [73] predict subject, verb, object triplets describing a video using semantic hierarchies that use more general words in case of uncertainty. Together with a language model their approach allows for translation of verbs and nouns not seen in the dictionary.

To describe images, Yao et al. [235] propose to use an and-or graph-based model together with domain-specific lexicalized grammar rules, targeted visual representation scheme, and a hierarchical knowledge ontology. Li et al. [121] first detect objects, visual attributes, and spatial relationships between objects. They then use an n-gram language model on the visually extracted phrases to generate subject, preposition, object style sentences. Mitchell et al. [142] use a more sophisticated tree-based language model to generate syntactic trees instead of filling in templates, leading to more diverse descriptions. A majority of approaches represent the whole image jointly as a bag of visual objects without capturing their spatial and semantic relationships. To address this, Elliott et al. [51] propose to explicitly model proximity relationships of objects for image description generation.

Some grammar-based approaches rely on graphical models to generate the target modality. An example includes BabyTalk [112], which given an image generates object, preposition, object triplets, that are used together with a conditional random field to construct the sentences. Yang et al. [233] predict a set of noun, verb, scene, preposition candidates using visual features extracted from an image and combine them into a sentence using a statistical language model and hidden Markov model style inference. A similar approach has been proposed by Thomason et al. [204], where a factor graph model is used for video description of the form subject, verb, object, place . The factor model exploits language statistics to deal with noisy visual representations. Going the other way Zitnick et al. [253] propose to use conditional random fields to generate abstract visual scenes based on language triplets extracted from sentences.

An advantage of grammar-based methods is that they are more likely to generate syntactically (in case of language) or logically correct target instances as they use predefined templates and restricted grammars. However, this limits them to producing formulaic rather than creative translations. Furthermore, grammar-based methods rely on complex pipelines for concept detection, with each concept requiring a separate model and a separate training dataset. Encoder-decoder models based on end-to-end trained neural networks are currently some of the most popular techniques for multimodal translation. The main idea behind the model is to first encode a source modality into a vectorial representation and then to use a decoder module to generate the target modality, all this in a single pass pipeline. Although, first used for machine translation [97], such models have been successfully used for image captioning [134], [214], and video description [174], [213]. So far, encoderdecoder models have been mostly used to generate text, but they can also be used to generate images [132], [171], and continuos generation of speech and sound [157], [209].

The first step of the encoder-decoder model is to encode the source object, this is done in modality specific way. Popular models to encode acoustic signals include RNNs [35] and DBNs [79]. Most of the work on encoding words sentences uses distributional semantics [141] and variants of RNNs [12]. Images are most often encoded using convolutional neural networks (CNN) [109], [185]. While learned CNN representations are common for encoding images, this is not the case for videos where hand-crafted features are still commonly used [174], [204]. While it is possible to use unimodal representations to encode the source modality, it has been shown that using a coordinated space (see Section 3.2) leads to better results [105], [159], [231].

Decoding is most often performed by an RNN or an LSTM using the encoded representation as the initial hidden state [54], [132], [214], [215]. A number of extensions have been proposed to traditional LSTM models to aid in the task of translation. A guide vector could be used to tightly couple the solutions in the image input [91]. Venugopalan et al. [213] demonstrate that it is beneficial to pre-train a decoder LSTM for image captioning before fine-tuning it to video description. Rohrbach et al. [174] explore the use of various LSTM architectures (single layer, multilayer, factored) and a number of training and regularization techniques for the task of video description.

A problem facing translation generation using an RNN is that the model has to generate a description from a single vectorial representation of the image, sentence, or video. This becomes especially difficult when generating long sequences as these models tend to forget the initial input. This has been partly addressed by neural attention models (see Section 5.2) that allow the network to focus on certain parts of an image [230], sentence [12], or video [236] during generation.

Generative attention-based RNNs have also been used for the task of generating images from sentences [132], while the results are still far from photo-realistic they show a lot of promise. More recently, a large amount of progress has been made in generating images using generative adversarial networks [71], which have been used as an alternative to RNNs for image generation from text [171].

While neural network based encoder-decoder systems have been very successful they still face a number of issues. Devlin et al. [49] suggest that it is possible that the network is memorizing the training data rather than learning how to understand the visual scene and generate it. This is based on the observation that k-nearest neighbor models perform very similarly to those based on generation. Furthermore, such models often require large quantities of data for training. Continuous generation models are intended for sequence translation and produce outputs at every timestep in an online manner. These models are useful when translating from a sequence to a sequence such as text to speech, speech to text, and video to text. A number of different techniques have been proposed for such modeling -graphical models, continuous encoder-decoder approaches, and various other regression or classification techniques. The extra difficulty that needs to be tackled by these models is the requirement of temporal consistency between modalities.

A lot of early work on sequence to sequence translation used graphical or latent variable models. Deena and Galata [47] proposed to use a shared Gaussian process latent variable model for audio-based visual speech synthesis. The model creates a shared latent space between audio and visual features that can be used to generate one space from the other, while enforcing temporal consistency of visual speech at different timesteps. Hidden Markov models (HMM) have also been used for visual speech generation [203] and textto-speech [245] tasks. They have also been extended to use cluster adaptive training to allow for training on multiple speakers, languages, and emotions allowing for more control when generating speech signal [244] or visual speech parameters [6].

Encoder-decoder models have recently become popular for sequence to sequence modeling. Owens et al. [157] used an LSTM to generate sounds resulting from drumsticks based on video. While their model is capable of generating sounds by predicting a cochleogram from CNN visual features, they found that retrieving a closest audio sample based on the predicted cochleogram led to best results. Directly modeling the raw audio signal for speech and music generation has been proposed by van den Oord et al. [209]. The authors propose using hierarchical fully convolutional neural networks, which show a large improvement over previous state-of-the-art for the task of speech synthesis. RNNs have also been used for speech to text translation (speech recognition) [72]. More recently encoder-decoder based continuous approach was shown to be good at predicting letters from a speech signal represented as a filter bank spectra [35] -allowing for more accurate recognition of rare and out of vocabulary words. Collobert et al. [42] demonstrate how to use a raw audio signal directly for speech recognition, eliminating the need for audio features.

A lot of earlier work used graphical models for multimodal translation between continuous signals. However, these methods are being replaced by neural network encoder-decoder based techniques. Especially as they have recently been shown to be able to represent and generate complex visual and acoustic signals.","[[], ['b88', 'b157', 'b132', 'b45', 'b171', 'b170', 'b209', 'b6', 'b253', 'b19'], [], ['b73', 'b107', 'b14'], ['b142', 'b235', 'b51', 'b121'], ['b233', 'b204', 'b112', None, 'b253'], ['b213', 'b132', 'b157', 'b214', 'b174', 'b171', 'b97', 'b209', 'b134'], ['b185', 'b105', 'b174', 'b159', 'b79', 'b141', 'b35', 'b231', 'b204', 'b109', 'b12'], ['b213', 'b132', 'b214', 'b174', 'b91', 'b54', 'b215'], ['b230', 'b236', 'b12'], ['b171', 'b132', 'b71'], ['b49'], ['b245', 'b47', 'b244', 'b203', 'b6'], ['b157', 'b35', 'b42', 'b209', 'b72'], []]","[[], ['b88', 'b157', 'b132', 'b45', 'b171', 'b170', 'b209', 'b6', 'b253', 'b19'], [], ['b73', 'b107', 'b14'], ['b142', 'b235', 'b51', 'b121'], ['b233', 'b204', 'b112', None, 'b253'], ['b213', 'b132', 'b157', 'b214', 'b174', 'b171', 'b97', 'b209', 'b134'], ['b185', 'b105', 'b174', 'b159', 'b79', 'b141', 'b35', 'b231', 'b204', 'b109', 'b12'], ['b213', 'b132', 'b214', 'b174', 'b91', 'b54', 'b215'], ['b230', 'b236', 'b12'], ['b171', 'b132', 'b71'], ['b49'], ['b245', 'b47', 'b244', 'b203', 'b6'], ['b157', 'b35', 'b42', 'b209', 'b72'], []]",66,"sent1: Generative approaches to multimodal translation construct models that can perform multimodal translation given a unimodal source instance.
sent2: It is a challenging problem as it requires the ability to both understand the source modality and to generate the target sequence or signal.
sent3: As discussed in the following section, this also makes such methods much more difficult to evaluate, due to large space of possible correct answers.
sent4: In this survey we focus on the generation of three modalities: language, vision, and sound.
sent5: Language generation has been explored for a long time [170], with a lot of recent attention for tasks such as image and video description [19].
sent6: Speech and sound generation has also seen a lot of work with a number of historical [88] and modern approaches [157], [209].
sent7: Photo-realistic image generation has been less explored, and is still in early stages [132], [171], however, there have been a number of attempts at generating abstract scenes [253], computer graphics [45], and talking heads [6].
sent8: We identify three broad categories of generative models: grammar-based, encoder-decoder, and continuous generation models.
sent9: Grammar based models simplify the task by restricting the target domain by using a grammar, e.g., by generating restricted sentences based on a subject, object, verb template.
sent10: Encoder-decoder models first encode the source modality to a latent representation which is then used by a decoder to generate the target modality.
sent11: Continuous generation models generate the target modality continuously based on a stream of source modality inputs and are most suited for translating between temporal sequences -such as text-to-speech.
sent12: Grammar-based models rely on a pre-defined grammar for generating a particular modality.
sent13: They start by detecting high level concepts from the source modality, such as objects in images and actions from videos.
sent14: These detections are then incorporated together with a generation procedure based on a pre-defined grammar to result in a target modality.
sent15: Kojima et al. [107] proposed a system to describe human behavior in a video using the detected position of the person's head and hands and rule based natural language generation that incorporates a hierarchy of concepts and actions.
sent16: Barbu et al. [14] proposed a video description model that generates sentences of the form: who did what to whom and where and how they did it.
sent17: The system was based on handcrafted object and event classifiers and used a restricted grammar suitable for the task.
sent18: Guadarrama et al. [73] predict subject, verb, object triplets describing a video using semantic hierarchies that use more general words in case of uncertainty.
sent19: Together with a language model their approach allows for translation of verbs and nouns not seen in the dictionary.
sent20: To describe images, Yao et al. [235] propose to use an and-or graph-based model together with domain-specific lexicalized grammar rules, targeted visual representation scheme, and a hierarchical knowledge ontology.
sent21: Li et al. [121] first detect objects, visual attributes, and spatial relationships between objects.
sent22: They then use an n-gram language model on the visually extracted phrases to generate subject, preposition, object style sentences.
sent23: Mitchell et al. [142] use a more sophisticated tree-based language model to generate syntactic trees instead of filling in templates, leading to more diverse descriptions.
sent24: A majority of approaches represent the whole image jointly as a bag of visual objects without capturing their spatial and semantic relationships.
sent25: To address this, Elliott et al. [51] propose to explicitly model proximity relationships of objects for image description generation.
sent26: Some grammar-based approaches rely on graphical models to generate the target modality.
sent27: An example includes BabyTalk [112], which given an image generates object, preposition, object triplets, that are used together with a conditional random field to construct the sentences.
sent28: Yang et al. [233] predict a set of noun, verb, scene, preposition candidates using visual features extracted from an image and combine them into a sentence using a statistical language model and hidden Markov model style inference.
sent29: A similar approach has been proposed by Thomason et al. [204], where a factor graph model is used for video description of the form subject, verb, object, place .
sent30: The factor model exploits language statistics to deal with noisy visual representations.
sent31: Going the other way Zitnick et al. [253] propose to use conditional random fields to generate abstract visual scenes based on language triplets extracted from sentences.
sent32: An advantage of grammar-based methods is that they are more likely to generate syntactically (in case of language) or logically correct target instances as they use predefined templates and restricted grammars.
sent33: However, this limits them to producing formulaic rather than creative translations.
sent34: Furthermore, grammar-based methods rely on complex pipelines for concept detection, with each concept requiring a separate model and a separate training dataset.
sent35: Encoder-decoder models based on end-to-end trained neural networks are currently some of the most popular techniques for multimodal translation.
sent36: The main idea behind the model is to first encode a source modality into a vectorial representation and then to use a decoder module to generate the target modality, all this in a single pass pipeline.
sent37: Although, first used for machine translation [97], such models have been successfully used for image captioning [134], [214], and video description [174], [213].
sent38: So far, encoderdecoder models have been mostly used to generate text, but they can also be used to generate images [132], [171], and continuos generation of speech and sound [157], [209].
sent39: The first step of the encoder-decoder model is to encode the source object, this is done in modality specific way.
sent40: Popular models to encode acoustic signals include RNNs [35] and DBNs [79].
sent41: Most of the work on encoding words sentences uses distributional semantics [141] and variants of RNNs [12].
sent42: Images are most often encoded using convolutional neural networks (CNN) [109], [185].
sent43: While learned CNN representations are common for encoding images, this is not the case for videos where hand-crafted features are still commonly used [174], [204].
sent44: While it is possible to use unimodal representations to encode the source modality, it has been shown that using a coordinated space (see Section 3.2) leads to better results [105], [159], [231].
sent45: Decoding is most often performed by an RNN or an LSTM using the encoded representation as the initial hidden state [54], [132], [214], [215].
sent46: A number of extensions have been proposed to traditional LSTM models to aid in the task of translation.
sent47: A guide vector could be used to tightly couple the solutions in the image input [91].
sent48: Venugopalan et al. [213] demonstrate that it is beneficial to pre-train a decoder LSTM for image captioning before fine-tuning it to video description.
sent49: Rohrbach et al. [174] explore the use of various LSTM architectures (single layer, multilayer, factored) and a number of training and regularization techniques for the task of video description.
sent50: A problem facing translation generation using an RNN is that the model has to generate a description from a single vectorial representation of the image, sentence, or video.
sent51: This becomes especially difficult when generating long sequences as these models tend to forget the initial input.
sent52: This has been partly addressed by neural attention models (see Section 5.2) that allow the network to focus on certain parts of an image [230], sentence [12], or video [236] during generation.
sent53: Generative attention-based RNNs have also been used for the task of generating images from sentences [132], while the results are still far from photo-realistic they show a lot of promise.
sent54: More recently, a large amount of progress has been made in generating images using generative adversarial networks [71], which have been used as an alternative to RNNs for image generation from text [171].
sent55: While neural network based encoder-decoder systems have been very successful they still face a number of issues.
sent56: Devlin et al. [49] suggest that it is possible that the network is memorizing the training data rather than learning how to understand the visual scene and generate it.
sent57: This is based on the observation that k-nearest neighbor models perform very similarly to those based on generation.
sent58: Furthermore, such models often require large quantities of data for training.
sent59: Continuous generation models are intended for sequence translation and produce outputs at every timestep in an online manner.
sent60: These models are useful when translating from a sequence to a sequence such as text to speech, speech to text, and video to text.
sent61: A number of different techniques have been proposed for such modeling -graphical models, continuous encoder-decoder approaches, and various other regression or classification techniques.
sent62: The extra difficulty that needs to be tackled by these models is the requirement of temporal consistency between modalities.
sent63: A lot of early work on sequence to sequence translation used graphical or latent variable models.
sent64: Deena and Galata [47] proposed to use a shared Gaussian process latent variable model for audio-based visual speech synthesis.
sent65: The model creates a shared latent space between audio and visual features that can be used to generate one space from the other, while enforcing temporal consistency of visual speech at different timesteps.
sent66: Hidden Markov models (HMM) have also been used for visual speech generation [203] and textto-speech [245] tasks.
sent67: They have also been extended to use cluster adaptive training to allow for training on multiple speakers, languages, and emotions allowing for more control when generating speech signal [244] or visual speech parameters [6].Encoder-decoder models have recently become popular for sequence to sequence modeling.
sent68: Owens et al. [157] used an LSTM to generate sounds resulting from drumsticks based on video.
sent69: While their model is capable of generating sounds by predicting a cochleogram from CNN visual features, they found that retrieving a closest audio sample based on the predicted cochleogram led to best results.
sent70: Directly modeling the raw audio signal for speech and music generation has been proposed by van den Oord et al. [209].
sent71: The authors propose using hierarchical fully convolutional neural networks, which show a large improvement over previous state-of-the-art for the task of speech synthesis.
sent72: RNNs have also been used for speech to text translation (speech recognition) [72].
sent73: More recently encoder-decoder based continuous approach was shown to be good at predicting letters from a speech signal represented as a filter bank spectra [35] -allowing for more accurate recognition of rare and out of vocabulary words.
sent74: Collobert et al. [42] demonstrate how to use a raw audio signal directly for speech recognition, eliminating the need for audio features.
sent75: A lot of earlier work used graphical models for multimodal translation between continuous signals.
sent76: However, these methods are being replaced by neural network encoder-decoder based techniques.
sent77: Especially as they have recently been shown to be able to represent and generate complex visual and acoustic signals."
10137425,Multimodal Machine Learning: A Survey and Taxonomy,"Computer Science, Medicine",https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,s11,ALIGNMENT,"We define multimodal alignment as finding relationships and correspondences between sub-components of instances from two or more modalities. For example, given an image and a caption we want to find the areas of the image corresponding to the caption's words or phrases [98]. Another example is, given a movie, aligning it to the script or the book chapters it was based on [252].

We categorize multimodal alignment into two typesimplicit and explicit. In explicit alignment, we are explicitly interested in aligning sub-components between modalities, e.g., aligning recipe steps with the corresponding instructional video [131]. Implicit alignment is used as an intermediate (often latent) step for another task, e.g., image retrieval Table 4: Summary of our taxonomy for the multimodal alignment challenge. For each sub-class of our taxonomy, we include reference citations and modalities aligned. ALIGNMENT ","[['b252', 'b98'], [None, 'b131']]","[['b252', 'b98'], [None, 'b131']]",4,"sent1: We define multimodal alignment as finding relationships and correspondences between sub-components of instances from two or more modalities.
sent2: For example, given an image and a caption we want to find the areas of the image corresponding to the caption's words or phrases [98].
sent3: Another example is, given a movie, aligning it to the script or the book chapters it was based on [252].
sent4: We categorize multimodal alignment into two typesimplicit and explicit.
sent5: In explicit alignment, we are explicitly interested in aligning sub-components between modalities, e.g., aligning recipe steps with the corresponding instructional video [131].
sent6: Implicit alignment is used as an intermediate (often latent) step for another task, e.g., image retrieval Table 4: Summary of our taxonomy for the multimodal alignment challenge.
sent7: For each sub-class of our taxonomy, we include reference citations and modalities aligned.
sent8: ALIGNMENT"
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s9,(9),"Radiality (Ra) Pappas et al. [10] also present radiality, which takes the diameter of a graph into account:

where D is the diameter of the graph, namely the greatest distance between any pair of nodes in the graph.

Comments on Closeness Centrality Closeness centrality and its variants (e.g., harmonic centrality, radiality) are similar to betweenness, also involving calculating the shortest paths between all pairs of nodes in a graph. One difference is that, a node with a high closeness value is usually located at the center of the graph (in terms of distance), but such a node may not have a high betweenness value because it may not be a bridging node that resides in many shortest paths connecting other nodes.

Eigenvector Centrality (EC) A widely adopted principle is that a node is important if it is connected with important nodes. For example, in a class graph, a class is important if the classes it connects with are important. This gives rise to eigenvector centrality which iteratively calculates the importance of each node v in a graph:

where N (v) is the set of v's neighbors, and λ is a constant factor for normalization. The eigenvector centrality of a node is the sum of the eigenvector centrality of its neighbors. The computation iterates over all the nodes in the graph, one round after another until convergence. Whereas this basic measure has been used in [3], its improved variants are more popular in the literature. PageRank, a well-known implementation of eigenvector centrality, is used in [20,2]. Different from the above basic measure, PageRank introduces a damping factor which is added to the centrality. Weighted PageRank, weighted HITS, or their variants are used in [24,23,22,21,4], where centrality is defined as a weighted sum. The weight of an edge between v and u indicates the strength of the connection between them; a stronger connection will transport more centrality score from u to v.

Comments on Eigenvector Centrality Eigenvector centrality and its variants (e.g., PageRank, HITS) have shown their effectiveness in many applications. However, they require iterative computation over all the nodes in a graph until convergence, which is time-consuming for large graphs.

Empirical Comparison of Centrality-based Measures It seems that the effectiveness of a centrality-based measure is related to the graph model, and may also depend on the specific ontology to be summarized as the application and the domain of an ontology provide a guideline in order to select a proper set of measures.

Specifically, according to the experiment results presented in [20], the simple degree centrality (DC) appears more effective than PageRank (i.e., EC) on some class graphs. However, Zhang et al. [23] report that weighted PageRank (i.e., EC) outperforms degree (i.e., DC) on several RDF sentence graphs; both of them are considerably better than betweenness (i.e., BC). Pappas et al. [10] find that degree (i.e., DC) and betweenness (i.e., BC, EgC, and BrC) are notably better than closeness (i.e., HC and RA) on a few class graphs.

Unfortunately, we could not draw any reliable conclusions from the current empirical results reported in the literature as they all experiment with a small number of ontologies.

Radiality (Ra) Pappas et al. [10] also present radiality, which takes the diameter of a graph into account:

where D is the diameter of the graph, namely the greatest distance between any pair of nodes in the graph.

Comments on Closeness Centrality Closeness centrality and its variants (e.g., harmonic centrality, radiality) are similar to betweenness, also involving calculating the shortest paths between all pairs of nodes in a graph. One difference is that, a node with a high closeness value is usually located at the center of the graph (in terms of distance), but such a node may not have a high betweenness value because it may not be a bridging node that resides in many shortest paths connecting other nodes.

Eigenvector Centrality (EC) A widely adopted principle is that a node is important if it is connected with important nodes. For example, in a class graph, a class is important if the classes it connects with are important. This gives rise to eigenvector centrality which iteratively calculates the importance of each node v in a graph:

where N (v) is the set of v's neighbors, and λ is a constant factor for normalization. The eigenvector centrality of a node is the sum of the eigenvector centrality of its neighbors. The computation iterates over all the nodes in the graph, one round after another until convergence. Whereas this basic measure has been used in [3], its improved variants are more popular in the literature. PageRank, a well-known implementation of eigenvector centrality, is used in [20,2]. Different from the above basic measure, PageRank introduces a damping factor which is added to the centrality. Weighted PageRank, weighted HITS, or their variants are used in [24,23,22,21,4], where centrality is defined as a weighted sum. The weight of an edge between v and u indicates the strength of the connection between them; a stronger connection will transport more centrality score from u to v.

Comments on Eigenvector Centrality Eigenvector centrality and its variants (e.g., PageRank, HITS) have shown their effectiveness in many applications. However, they require iterative computation over all the nodes in a graph until convergence, which is time-consuming for large graphs.

Empirical Comparison of Centrality-based Measures It seems that the effectiveness of a centrality-based measure is related to the graph model, and may also depend on the specific ontology to be summarized as the application and the domain of an ontology provide a guideline in order to select a proper set of measures.

Specifically, according to the experiment results presented in [20], the simple degree centrality (DC) appears more effective than PageRank (i.e., EC) on some class graphs. However, Zhang et al. [23] report that weighted PageRank (i.e., EC) outperforms degree (i.e., DC) on several RDF sentence graphs; both of them are considerably better than betweenness (i.e., BC). Pappas et al. [10] find that degree (i.e., DC) and betweenness (i.e., BC, EgC, and BrC) are notably better than closeness (i.e., HC and RA) on a few class graphs.

Unfortunately, we could not draw any reliable conclusions from the current empirical results reported in the literature as they all experiment with a small number of ontologies.","[['b9'], [], [], [], ['b3', 'b22', 'b20', 'b21', 'b1', 'b23', 'b2', 'b19'], [], [], ['b9', 'b19', 'b22'], [], ['b9'], [], [], [], ['b3', 'b22', 'b20', 'b21', 'b1', 'b23', 'b2', 'b19'], [], [], ['b9', 'b19', 'b22'], []]","[['b9'], [], [], [], ['b3', 'b22', 'b20', 'b21', 'b1', 'b23', 'b2', 'b19'], [], [], ['b9', 'b19', 'b22'], [], ['b9'], [], [], [], ['b3', 'b22', 'b20', 'b21', 'b1', 'b23', 'b2', 'b19'], [], [], ['b9', 'b19', 'b22'], []]",24,"sent1: Radiality (Ra) Pappas et al. [10] also present radiality, which takes the diameter of a graph into account:where D is the diameter of the graph, namely the greatest distance between any pair of nodes in the graph.
sent2: Comments on Closeness Centrality Closeness centrality and its variants (e.g., harmonic centrality, radiality) are similar to betweenness, also involving calculating the shortest paths between all pairs of nodes in a graph.
sent3: One difference is that, a node with a high closeness value is usually located at the center of the graph (in terms of distance), but such a node may not have a high betweenness value because it may not be a bridging node that resides in many shortest paths connecting other nodes.
sent4: Eigenvector Centrality (EC) A widely adopted principle is that a node is important if it is connected with important nodes.
sent5: For example, in a class graph, a class is important if the classes it connects with are important.
sent6: This gives rise to eigenvector centrality which iteratively calculates the importance of each node v in a graph:where N (v) is the set of v's neighbors, and λ is a constant factor for normalization.
sent7: The eigenvector centrality of a node is the sum of the eigenvector centrality of its neighbors.
sent8: The computation iterates over all the nodes in the graph, one round after another until convergence.
sent9: Whereas this basic measure has been used in [3], its improved variants are more popular in the literature.
sent10: PageRank, a well-known implementation of eigenvector centrality, is used in [20,2].
sent11: Different from the above basic measure, PageRank introduces a damping factor which is added to the centrality.
sent12: Weighted PageRank, weighted HITS, or their variants are used in [24,23,22,21,4], where centrality is defined as a weighted sum.
sent13: The weight of an edge between v and u indicates the strength of the connection between them; a stronger connection will transport more centrality score from u to v.Comments on Eigenvector Centrality Eigenvector centrality and its variants (e.g., PageRank, HITS) have shown their effectiveness in many applications.
sent14: However, they require iterative computation over all the nodes in a graph until convergence, which is time-consuming for large graphs.
sent15: Empirical Comparison of Centrality-based Measures
sent16: It seems that the effectiveness of a centrality-based measure is related to the graph model, and may also depend on the specific ontology to be summarized as the application and the domain of an ontology provide a guideline in order to select a proper set of measures.
sent17: Specifically, according to the experiment results presented in [20], the simple degree centrality (DC) appears more effective than PageRank (i.e., EC) on some class graphs.
sent18: However, Zhang et al. [23] report that weighted PageRank (i.e., EC) outperforms degree (i.e., DC) on several RDF sentence graphs; both of them are considerably better than betweenness (i.e., BC).
sent19: Pappas et al. [10] find that degree (i.e., DC) and betweenness (i.e., BC, EgC, and BrC) are notably better than closeness (i.e., HC and RA) on a few class graphs.
sent20: Unfortunately, we could not draw any reliable conclusions from the current empirical results reported in the literature as they all experiment with a small number of ontologies.
sent21: Radiality (Ra) Pappas et al. [10] also present radiality, which takes the diameter of a graph into account:where D is the diameter of the graph, namely the greatest distance between any pair of nodes in the graph.
sent22: Comments on Closeness Centrality Closeness centrality and its variants (e.g., harmonic centrality, radiality) are similar to betweenness, also involving calculating the shortest paths between all pairs of nodes in a graph.
sent23: One difference is that, a node with a high closeness value is usually located at the center of the graph (in terms of distance), but such a node may not have a high betweenness value because it may not be a bridging node that resides in many shortest paths connecting other nodes.
sent24: Eigenvector Centrality (EC) A widely adopted principle is that a node is important if it is connected with important nodes.
sent25: For example, in a class graph, a class is important if the classes it connects with are important.
sent26: This gives rise to eigenvector centrality which iteratively calculates the importance of each node v in a graph:where N (v) is the set of v's neighbors, and λ is a constant factor for normalization.
sent27: The eigenvector centrality of a node is the sum of the eigenvector centrality of its neighbors.
sent28: The computation iterates over all the nodes in the graph, one round after another until convergence.
sent29: Whereas this basic measure has been used in [3], its improved variants are more popular in the literature.
sent30: PageRank, a well-known implementation of eigenvector centrality, is used in [20,2].
sent31: Different from the above basic measure, PageRank introduces a damping factor which is added to the centrality.
sent32: Weighted PageRank, weighted HITS, or their variants are used in [24,23,22,21,4], where centrality is defined as a weighted sum.
sent33: The weight of an edge between v and u indicates the strength of the connection between them; a stronger connection will transport more centrality score from u to v.Comments on Eigenvector Centrality Eigenvector centrality and its variants (e.g., PageRank, HITS) have shown their effectiveness in many applications.
sent34: However, they require iterative computation over all the nodes in a graph until convergence, which is time-consuming for large graphs.
sent35: Empirical Comparison of Centrality-based Measures
sent36: It seems that the effectiveness of a centrality-based measure is related to the graph model, and may also depend on the specific ontology to be summarized as the application and the domain of an ontology provide a guideline in order to select a proper set of measures.
sent37: Specifically, according to the experiment results presented in [20], the simple degree centrality (DC) appears more effective than PageRank (i.e., EC) on some class graphs.
sent38: However, Zhang et al. [23] report that weighted PageRank (i.e., EC) outperforms degree (i.e., DC) on several RDF sentence graphs; both of them are considerably better than betweenness (i.e., BC).
sent39: Pappas et al. [10] find that degree (i.e., DC) and betweenness (i.e., BC, EgC, and BrC) are notably better than closeness (i.e., HC and RA) on a few class graphs.
sent40: Unfortunately, we could not draw any reliable conclusions from the current empirical results reported in the literature as they all experiment with a small number of ontologies."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s10,Coverage-based Measures,"Top-ranked nodes in a graph representation of an ontology may not form the best ontology summary. For many applications, a good summary is expected to have a good coverage of the contents of an ontology, to form a comprehensive and unbiased overview. Accordingly, the quality of a subset of nodes forming a summary is to be assessed as a whole.

Coverage (Co) Peroni et al. [12] propose the coverage criterion which aims to show how well the selected set of classes are spread over the whole class hierarchy. For each node v, let N + (v) be the set of nodes covered by v, including v and its neighbors, i.e., its subclasses and superclasses in the class hierarchy. The coverage of a set of selected nodes V is defined as the proportion of nodes in the graph that are covered by V :

where n is the number of nodes in the graph. Further, Peroni et al. [12] consider an interesting measure called balance which is directly related to coverage. It measures how balanced the selected nodes are, i.e., the degree to which each selected node contributes to the overall coverage of the set, which is characterized by standard deviation.

Diversity-based Re-ranking (Di) In [23,22], the coverage of a summary is improved by a re-ranking step after centrality-based ranking. In these approaches, nodes are iteratively selected to form a summary. In each iteration, the next node to be selected may not be the top-ranked one among the remaining nodes, which will be re-ranked such that a node similar to those selected in previous iterations will be penalized. Specifically, let score(v) be the centrality score of node v, and let sim(v, u) be the similarity between nodes v and u. Given a set of nodes V s which are already selected into the summary and a set of candidate nodes V c , the next node to be selected from V c is

Zhang et al. [23,22] use this algorithm to rank RDF sentences, where two RDF sentences are similar if they share terms. The resulting ontology summary is diversified with regard to the terms it contains.

Comments on Coverage-based Measures Coverage-based methods complement centrality-based measures, but their current implementations are suboptimal. Coverage in Eq. (12) considers the neighborhood of each node, not taking the global graph structure into account. Diversity-based re-ranking in Eq. (13) has a greedy nature, and may not find the optimum summary in terms of centrality and diversity.

Top-ranked nodes in a graph representation of an ontology may not form the best ontology summary. For many applications, a good summary is expected to have a good coverage of the contents of an ontology, to form a comprehensive and unbiased overview. Accordingly, the quality of a subset of nodes forming a summary is to be assessed as a whole.

Coverage (Co) Peroni et al. [12] propose the coverage criterion which aims to show how well the selected set of classes are spread over the whole class hierarchy. For each node v, let N + (v) be the set of nodes covered by v, including v and its neighbors, i.e., its subclasses and superclasses in the class hierarchy. The coverage of a set of selected nodes V is defined as the proportion of nodes in the graph that are covered by V :

where n is the number of nodes in the graph. Further, Peroni et al. [12] consider an interesting measure called balance which is directly related to coverage. It measures how balanced the selected nodes are, i.e., the degree to which each selected node contributes to the overall coverage of the set, which is characterized by standard deviation.

Diversity-based Re-ranking (Di) In [23,22], the coverage of a summary is improved by a re-ranking step after centrality-based ranking. In these approaches, nodes are iteratively selected to form a summary. In each iteration, the next node to be selected may not be the top-ranked one among the remaining nodes, which will be re-ranked such that a node similar to those selected in previous iterations will be penalized. Specifically, let score(v) be the centrality score of node v, and let sim(v, u) be the similarity between nodes v and u. Given a set of nodes V s which are already selected into the summary and a set of candidate nodes V c , the next node to be selected from V c is

Zhang et al. [23,22] use this algorithm to rank RDF sentences, where two RDF sentences are similar if they share terms. The resulting ontology summary is diversified with regard to the terms it contains.

Comments on Coverage-based Measures Coverage-based methods complement centrality-based measures, but their current implementations are suboptimal. Coverage in Eq. (12) considers the neighborhood of each node, not taking the global graph structure into account. Diversity-based re-ranking in Eq. (13) has a greedy nature, and may not find the optimum summary in terms of centrality and diversity.","[[], ['b11'], ['b11'], ['b21', 'b22'], ['b21', 'b22'], [], [], ['b11'], ['b11'], ['b21', 'b22'], ['b21', 'b22'], []]","[[], ['b11'], ['b11'], ['b21', 'b22'], ['b21', 'b22'], [], [], ['b11'], ['b11'], ['b21', 'b22'], ['b21', 'b22'], []]",12,"sent1: Top-ranked nodes in a graph representation of an ontology may not form the best ontology summary.
sent2: For many applications, a good summary is expected to have a good coverage of the contents of an ontology, to form a comprehensive and unbiased overview.
sent3: Accordingly, the quality of a subset of nodes forming a summary is to be assessed as a whole.
sent4: Coverage (Co) Peroni et al. [12] propose the coverage criterion which aims to show how well the selected set of classes are spread over the whole class hierarchy.
sent5: For each node v, let N + (v) be the set of nodes covered by v, including v and its neighbors, i.e., its subclasses and superclasses in the class hierarchy.
sent6: The coverage of a set of selected nodes V is defined as the proportion of nodes in the graph that are covered by V :where n is the number of nodes in the graph.
sent7: Further, Peroni et al. [12] consider an interesting measure called balance which is directly related to coverage.
sent8: It measures how balanced the selected nodes are, i.e., the degree to which each selected node contributes to the overall coverage of the set, which is characterized by standard deviation.
sent9: Diversity-based Re-ranking (Di)
sent10: In [23,22], the coverage of a summary is improved by a re-ranking step after centrality-based ranking.
sent11: In these approaches, nodes are iteratively selected to form a summary.
sent12: In each iteration, the next node to be selected may not be the top-ranked one among the remaining nodes, which will be re-ranked such that a node similar to those selected in previous iterations will be penalized.
sent13: Specifically, let score(v) be the centrality score of node v, and let sim(v, u) be the similarity between nodes v and u.
sent14: Given a set of nodes V s which are already selected into the summary and a set of candidate nodes V c , the next node to be selected from V c isZhang et al. [23,22] use this algorithm to rank RDF sentences, where two RDF sentences are similar if they share terms.
sent15: The resulting ontology summary is diversified with regard to the terms it contains.
sent16: Comments on Coverage-based Measures Coverage-based methods complement centrality-based measures, but their current implementations are suboptimal.
sent17: Coverage in Eq. (12) considers the neighborhood of each node, not taking the global graph structure into account.
sent18: Diversity-based re-ranking in Eq. (13) has a greedy nature, and may not find the optimum summary in terms of centrality and diversity.
sent19: Top-ranked nodes in a graph representation of an ontology may not form the best ontology summary.
sent20: For many applications, a good summary is expected to have a good coverage of the contents of an ontology, to form a comprehensive and unbiased overview.
sent21: Accordingly, the quality of a subset of nodes forming a summary is to be assessed as a whole.
sent22: Coverage (Co) Peroni et al. [12] propose the coverage criterion which aims to show how well the selected set of classes are spread over the whole class hierarchy.
sent23: For each node v, let N + (v) be the set of nodes covered by v, including v and its neighbors, i.e., its subclasses and superclasses in the class hierarchy.
sent24: The coverage of a set of selected nodes V is defined as the proportion of nodes in the graph that are covered by V :where n is the number of nodes in the graph.
sent25: Further, Peroni et al. [12] consider an interesting measure called balance which is directly related to coverage.
sent26: It measures how balanced the selected nodes are, i.e., the degree to which each selected node contributes to the overall coverage of the set, which is characterized by standard deviation.
sent27: Diversity-based Re-ranking (Di)
sent28: In [23,22], the coverage of a summary is improved by a re-ranking step after centrality-based ranking.
sent29: In these approaches, nodes are iteratively selected to form a summary.
sent30: In each iteration, the next node to be selected may not be the top-ranked one among the remaining nodes, which will be re-ranked such that a node similar to those selected in previous iterations will be penalized.
sent31: Specifically, let score(v) be the centrality score of node v, and let sim(v, u) be the similarity between nodes v and u.
sent32: Given a set of nodes V s which are already selected into the summary and a set of candidate nodes V c , the next node to be selected from V c isZhang et al. [23,22] use this algorithm to rank RDF sentences, where two RDF sentences are similar if they share terms.
sent33: The resulting ontology summary is diversified with regard to the terms it contains.
sent34: Comments on Coverage-based Measures Coverage-based methods complement centrality-based measures, but their current implementations are suboptimal.
sent35: Coverage in Eq. (12) considers the neighborhood of each node, not taking the global graph structure into account.
sent36: Diversity-based re-ranking in Eq. (13) has a greedy nature, and may not find the optimum summary in terms of centrality and diversity."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s11,Application-specific Measures,"The following two methods are not graph-based but are designed for specific applications.

Query Relevance (QR) A special kind of ontology summary is a snippet presented in search results pages of an ontology search engine. In this application, terms [6,2] or RDF sentences [4,11] that are relevant to a user query (e.g., containing query keywords) are prioritized for being presented in a snippet, to show the relevance of an ontology to the user's information needs.

Frequency of Correspondences (FC) Pires et al. [13] consider applications where an ontology to be summarized can be an integrated ontology obtained by merging several local ontologies. In that case, an important term in the integrated ontology is one that has a high frequency of correspondences, namely it finds correspondences to many classes in local ontologies.

The following two methods are not graph-based but are designed for specific applications.

Query Relevance (QR) A special kind of ontology summary is a snippet presented in search results pages of an ontology search engine. In this application, terms [6,2] or RDF sentences [4,11] that are relevant to a user query (e.g., containing query keywords) are prioritized for being presented in a snippet, to show the relevance of an ontology to the user's information needs.

Frequency of Correspondences (FC) Pires et al. [13] consider applications where an ontology to be summarized can be an integrated ontology obtained by merging several local ontologies. In that case, an important term in the integrated ontology is one that has a high frequency of correspondences, namely it finds correspondences to many classes in local ontologies.","[[], ['b3', 'b1', 'b5', 'b10'], ['b12'], [], ['b3', 'b1', 'b5', 'b10'], ['b12']]","[[], ['b3', 'b1', 'b5', 'b10'], ['b12'], [], ['b3', 'b1', 'b5', 'b10'], ['b12']]",10,"sent1: The following two methods are not graph-based but are designed for specific applications.
sent2: Query Relevance (QR) A special kind of ontology summary is a snippet presented in search results pages of an ontology search engine.
sent3: In this application, terms [6,2] or RDF sentences [4,11] that are relevant to a user query (e.g., containing query keywords) are prioritized for being presented in a snippet, to show the relevance of an ontology to the user's information needs.
sent4: Frequency of Correspondences (FC) Pires et al. [13] consider applications where an ontology to be summarized can be an integrated ontology obtained by merging several local ontologies.
sent5: In that case, an important term in the integrated ontology is one that has a high frequency of correspondences, namely it finds correspondences to many classes in local ontologies.
sent6: The following two methods are not graph-based but are designed for specific applications.
sent7: Query Relevance (QR) A special kind of ontology summary is a snippet presented in search results pages of an ontology search engine.
sent8: In this application, terms [6,2] or RDF sentences [4,11] that are relevant to a user query (e.g., containing query keywords) are prioritized for being presented in a snippet, to show the relevance of an ontology to the user's information needs.
sent9: Frequency of Correspondences (FC) Pires et al. [13] consider applications where an ontology to be summarized can be an integrated ontology obtained by merging several local ontologies.
sent10: In that case, an important term in the integrated ontology is one that has a high frequency of correspondences, namely it finds correspondences to many classes in local ontologies."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s12,Other Measures,"In addition to graph-based and application-specific measures, we briefly review other methods used in the literature.

Name Simplicity (NS) Peroni et al. [12] emphasize that natural categories or basic objects are good representers of an ontology. They propose that a natural category normally has a relatively simple label, and hence they assess the importance of a class by the simplicity of its name. A class having compound words in the name will be penalized.

Textual Centrality (TC) Zhang et al. [24] calculate the textual centrality of a term in an ontology. Different from the centrality-based measures discussed in Section 3.1 which are defined over graph structure, the textual centrality of a term is the similarity between its textual description and the one for the whole ontology.

Popularity (Po) The wide use of a term on the Web suggests its importance. To measure the popularity of a term, Peroni et al. [12] submit the name of the term as a keyword query to a Web search engine and resort to the number of returned results. Zhang et al. [22] calculate the number of websites hosting RDF documents where the term is instantiated.

Cohesion (Ch) Cheng et al. [4] measure the quality of a summary as a whole. Different from diversity-based re-ranking described in Section 3.2 which penalizes an ontology summary where RDF sentences share terms, such a summary will be awarded in [4] as it exhibits cohesion.

In addition to graph-based and application-specific measures, we briefly review other methods used in the literature.

Name Simplicity (NS) Peroni et al. [12] emphasize that natural categories or basic objects are good representers of an ontology. They propose that a natural category normally has a relatively simple label, and hence they assess the importance of a class by the simplicity of its name. A class having compound words in the name will be penalized.

Textual Centrality (TC) Zhang et al. [24] calculate the textual centrality of a term in an ontology. Different from the centrality-based measures discussed in Section 3.1 which are defined over graph structure, the textual centrality of a term is the similarity between its textual description and the one for the whole ontology.

Popularity (Po) The wide use of a term on the Web suggests its importance. To measure the popularity of a term, Peroni et al. [12] submit the name of the term as a keyword query to a Web search engine and resort to the number of returned results. Zhang et al. [22] calculate the number of websites hosting RDF documents where the term is instantiated.

Cohesion (Ch) Cheng et al. [4] measure the quality of a summary as a whole. Different from diversity-based re-ranking described in Section 3.2 which penalizes an ontology summary where RDF sentences share terms, such a summary will be awarded in [4] as it exhibits cohesion.","[[], ['b11'], ['b23'], ['b11', 'b21'], ['b3'], [], ['b11'], ['b23'], ['b11', 'b21'], ['b3']]","[[], ['b11'], ['b23'], ['b11', 'b21'], ['b3'], [], ['b11'], ['b23'], ['b11', 'b21'], ['b3']]",10,"sent1: In addition to graph-based and application-specific measures, we briefly review other methods used in the literature.
sent2: Name Simplicity (NS) Peroni et al. [12] emphasize that natural categories or basic objects are good representers of an ontology.
sent3: They propose that a natural category normally has a relatively simple label, and hence they assess the importance of a class by the simplicity of its name.
sent4: A class having compound words in the name will be penalized.
sent5: Textual Centrality (TC) Zhang et al. [24] calculate the textual centrality of a term in an ontology.
sent6: Different from the centrality-based measures discussed in Section 3.1 which are defined over graph structure, the textual centrality of a term is the similarity between its textual description and the one for the whole ontology.
sent7: Popularity (Po) The wide use of a term on the Web suggests its importance.
sent8: To measure the popularity of a term, Peroni et al. [12] submit the name of the term as a keyword query to a Web search engine and resort to the number of returned results.
sent9: Zhang et al. [22] calculate the number of websites hosting RDF documents where the term is instantiated.
sent10: Cohesion (Ch) Cheng et al. [4] measure the quality of a summary as a whole.
sent11: Different from diversity-based re-ranking described in Section 3.2 which penalizes an ontology summary where RDF sentences share terms, such a summary will be awarded in [4] as it exhibits cohesion.
sent12: In addition to graph-based and application-specific measures, we briefly review other methods used in the literature.
sent13: Name Simplicity (NS) Peroni et al. [12] emphasize that natural categories or basic objects are good representers of an ontology.
sent14: They propose that a natural category normally has a relatively simple label, and hence they assess the importance of a class by the simplicity of its name.
sent15: A class having compound words in the name will be penalized.
sent16: Textual Centrality (TC) Zhang et al. [24] calculate the textual centrality of a term in an ontology.
sent17: Different from the centrality-based measures discussed in Section 3.1 which are defined over graph structure, the textual centrality of a term is the similarity between its textual description and the one for the whole ontology.
sent18: Popularity (Po) The wide use of a term on the Web suggests its importance.
sent19: To measure the popularity of a term, Peroni et al. [12] submit the name of the term as a keyword query to a Web search engine and resort to the number of returned results.
sent20: Zhang et al. [22] calculate the number of websites hosting RDF documents where the term is instantiated.
sent21: Cohesion (Ch) Cheng et al. [4] measure the quality of a summary as a whole.
sent22: Different from diversity-based re-ranking described in Section 3.2 which penalizes an ontology summary where RDF sentences share terms, such a summary will be awarded in [4] as it exhibits cohesion."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s19,Class Graph,"In order to directly represent semantic relations between classes, Wu et al. [21] presented a graph model where nodes represent classes and directed arcs represent binary relations between classes, which we call a class graph. Figure 2 illustrates a class graph for the ontology in Fig. 1. Note that some axioms (e.g., owl:Restriction) are not covered by this graph representation. As to the relations between classes, if we only allow rdfs:subClassOf, the resulting graph will be a class hierarchy representing subsumption relations, as considered in [12]. More generally, a relation can also be a property defined in the ontology, connecting from its domain (which is a class) to its range (also a class).

Comments. Class graphs are close to human cognition. As classes are firstclass citizens, class graphs are particularly suitable for approaches to ranking classes. However, the expressivity of class graph is limited. It well supports binary relations between classes but not more complex axioms involving multiple classes, e.g., owl:unionOf.

In order to directly represent semantic relations between classes, Wu et al. [21] presented a graph model where nodes represent classes and directed arcs represent binary relations between classes, which we call a class graph. Figure 2 illustrates a class graph for the ontology in Fig. 1. Note that some axioms (e.g., owl:Restriction) are not covered by this graph representation. As to the relations between classes, if we only allow rdfs:subClassOf, the resulting graph will be a class hierarchy representing subsumption relations, as considered in [12]. More generally, a relation can also be a property defined in the ontology, connecting from its domain (which is a class) to its range (also a class).

Comments. Class graphs are close to human cognition. As classes are firstclass citizens, class graphs are particularly suitable for approaches to ranking classes. However, the expressivity of class graph is limited. It well supports binary relations between classes but not more complex axioms involving multiple classes, e.g., owl:unionOf.","[['b20', 'b11'], [], ['b20', 'b11'], []]","[['b20', 'b11'], [], ['b20', 'b11'], []]",4,"sent1: In order to directly represent semantic relations between classes, Wu et al. [21] presented a graph model where nodes represent classes and directed arcs represent binary relations between classes, which we call a class graph.
sent2: Figure 2 illustrates a class graph for the ontology in Fig. 1.
sent3: Note that some axioms (e.g., owl:Restriction) are not covered by this graph representation.
sent4: As to the relations between classes, if we only allow rdfs:subClassOf, the resulting graph will be a class hierarchy representing subsumption relations, as considered in [12].
sent5: More generally, a relation can also be a property defined in the ontology, connecting from its domain (which is a class) to its range (also a class).
sent6: Comments. Class graphs are close to human cognition.
sent7: As classes are firstclass citizens, class graphs are particularly suitable for approaches to ranking classes.
sent8: However, the expressivity of class graph is limited.
sent9: It well supports binary relations between classes but not more complex axioms involving multiple classes, e.g., owl:unionOf.
sent10: In order to directly represent semantic relations between classes, Wu et al. [21] presented a graph model where nodes represent classes and directed arcs represent binary relations between classes, which we call a class graph.
sent11: Figure 2 illustrates a class graph for the ontology in Fig. 1.
sent12: Note that some axioms (e.g., owl:Restriction) are not covered by this graph representation.
sent13: As to the relations between classes, if we only allow rdfs:subClassOf, the resulting graph will be a class hierarchy representing subsumption relations, as considered in [12].
sent14: More generally, a relation can also be a property defined in the ontology, connecting from its domain (which is a class) to its range (also a class).
sent15: Comments. Class graphs are close to human cognition.
sent16: As classes are firstclass citizens, class graphs are particularly suitable for approaches to ranking classes.
sent17: However, the expressivity of class graph is limited.
sent18: It well supports binary relations between classes but not more complex axioms involving multiple classes, e.g., owl:unionOf."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s22,Term-Sentence Graph,"Zhang et al. [22] present a bipartite graph model, where terms and RDF sentences are both represented by nodes, which we call a term-sentence graph. A directed arc connects an RDF sentence to a term if the term is described in that RDF sentence. Figure 5 illustrates a term-sentence graph for the ontology in Fig. 1, derived from Fig. 3. Zhang et al. [22] differentiate between three types of arcs, depending on the structural role of term in RDF sentence, which we will 

not elaborate. The model is simplified in [4], where edges are undirected and unlabeled.

Comments. A term-sentence graph is more complex than all the abovementioned models. One advantage is that, compared with an RDF sentence graph and a vocabulary dependence graph, it explicitly represents both the terms and RDF sentences in the model, thereby expanding its potential application.

Zhang et al. [22] present a bipartite graph model, where terms and RDF sentences are both represented by nodes, which we call a term-sentence graph. A directed arc connects an RDF sentence to a term if the term is described in that RDF sentence. Figure 5 illustrates a term-sentence graph for the ontology in Fig. 1, derived from Fig. 3. Zhang et al. [22] differentiate between three types of arcs, depending on the structural role of term in RDF sentence, which we will 

not elaborate. The model is simplified in [4], where edges are undirected and unlabeled.

Comments. A term-sentence graph is more complex than all the abovementioned models. One advantage is that, compared with an RDF sentence graph and a vocabulary dependence graph, it explicitly represents both the terms and RDF sentences in the model, thereby expanding its potential application.","[['b21'], ['b3'], [], ['b21'], ['b3'], []]","[['b21'], ['b3'], [], ['b21'], ['b3'], []]",4,"sent1: Zhang et al. [22] present a bipartite graph model, where terms and RDF sentences are both represented by nodes, which we call a term-sentence graph.
sent2: A directed arc connects an RDF sentence to a term if the term is described in that RDF sentence.
sent3: Figure 5 illustrates a term-sentence graph for the ontology in Fig. 1, derived from Fig. 3.
sent4: Zhang et al. [22] differentiate between three types of arcs, depending on the structural role of term in RDF sentence, which we will not elaborate.
sent5: The model is simplified in [4], where edges are undirected and unlabeled.
sent6: Comments. A term-sentence graph is more complex than all the abovementioned models.
sent7: One advantage is that, compared with an RDF sentence graph and a vocabulary dependence graph, it explicitly represents both the terms and RDF sentences in the model, thereby expanding its potential application.
sent8: Zhang et al. [22] present a bipartite graph model, where terms and RDF sentences are both represented by nodes, which we call a term-sentence graph.
sent9: A directed arc connects an RDF sentence to a term if the term is described in that RDF sentence.
sent10: Figure 5 illustrates a term-sentence graph for the ontology in Fig. 1, derived from Fig. 3.
sent11: Zhang et al. [22] differentiate between three types of arcs, depending on the structural role of term in RDF sentence, which we will not elaborate.
sent12: The model is simplified in [4], where edges are undirected and unlabeled.
sent13: Comments. A term-sentence graph is more complex than all the abovementioned models.
sent14: One advantage is that, compared with an RDF sentence graph and a vocabulary dependence graph, it explicitly represents both the terms and RDF sentences in the model, thereby expanding its potential application."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s8,Centrality-based Measures,"Centrality-based measures are used to find topologically important nodes in a graph representation of an ontology. In general, centrality-based measures are defined via the available structure of the elements of a graph including nodes and edges. These measures primarily focus on the quantitative properties of graph structure such as number of edges and position of nodes, to assess the importance of a node. Some measures take edge types into consideration. As different centrality measures highlight different topological properties of a graph, their outputs are usually not consistent.

Degree Centrality (DC) As one of the simplest centrality measures, degree centrality calculates the number of edges incident to a node v:

Pappas et al. [10] use this measure on a class graph to assess the local centrality of each class as its importance. The degree of a class indicates the richness of its description. Nodes with higher degree centrality are more important.

For a directed graph, degree centrality is divided into two categories: indegree centrality and out-degree centrality, used in [23,16]. The former counts the number of incoming arcs, and the latter counts the number of outgoing arcs.

Instead of considering all the edges incident to v, we may also count only those of specific types. More generally, different types of edges can be assigned different weights, to measure weighted degree. For example, Peroni et al. [12] define the density of a class v as the weighted sum of its number of subclasses, properties, and instances:

where w S , w P , w I are weights. Similar methods have been used in [20,8]. Pirez et al. [13] and Queiroz-Sousa et al. [16] divide edges by their types into standard (e.g., is-a, part-of, same-as) and user-defined, which are weighted separately.

Relative Cardinality (RC) Whereas in the above approaches weights are empirically configured, we highlight relative cardinality [18,19], which is a way of automatically weighting edges for calculating weighted degree. In a class graph, the cardinality of an edge which represents a property connecting two classes is the number of the corresponding instances of the classes connected with that specific type of property. Therefore, classes and properties having more instances in a knowledge base are considered more important.

Comments on Degree Centrality Degree centrality and its variants (e.g., relative cardinality) can be efficiently computed in linear time, which is important when an ontology is very large. However, to assess the importance of a node, these measures mainly use its local information, i.e., the subgraph surrounding that node. Without exploiting the global graph structure, the effectiveness of these measures is limited.

Path-based Centrality (PC) Path-based centrality calculates the number of paths that pass through a particular node. For example, Peroni et al. [12] count the number of root-leaf paths in a class hierarchy that pass through each class v as its importance: PC(v) = |Number of root-leaf paths passing through v| .

(

A class in the middle of many root-leaf paths is central.

Betweenness Centrality (BC) As a special case of path-based centrality, it makes sense to only consider shortest paths. Specifically, betweenness centrality is defined as the number of shortest paths from all nodes in a graph to all other nodes that pass through that node. Tzitzikas et al. [20] use the following implementation of betweenness to assess the importance of each node v in a class graph:

where σ st is the total number of shortest paths from node s to node t in the graph, and σ st (v) is the total number of those paths passing through node v. The same as degree centrality, a node with a higher betweenness value is considered more important. Betweenness has also been used on RDF sentence graph [23].

Ego Centrality (EgC) Alternatively, for each node v, let G v be the subgraph induced by v and its neighbors, which contains all the edges between them. Pappas et al. [10] calculate the betweenness centrality of v within G v , which is called ego centrality:

Bridging Centrality (BrC) As an improvement to betweenness, Pappas et al. [10] presented bridging centrality. A node with a high bridging centrality is one that connects densely connected components in a graph. To measure that, the bridging centrality of a node v is defined as the product of v's betweenness centrality (BC) and v's bridging coefficient (Br):

where DC(v) is the degree of node v and N (v) is the set of v's neighbors. Betweenness centrality and bridging coefficient characterize global and local features of a node, respectively.

Comments on Path-based Centrality Compared with degree centrality, pathbased centrality and its variants (e.g., betweenness centrality, bridging centrality) exploit the global graph structure, going beyond the neighborhood of a node. However, it is computationally expensive to calculate betweenness, which involves calculating the shortest paths between all pairs of nodes in a graph.

Closeness Centrality (CC) Similar to betweenness, closeness centrality is another measure for determining the importance of nodes on a global scale within a graph. A node is usually considered as a key node if it can quickly interact with all the other nodes in a graph, not only with its immediate neighbors. The closeness of a node v is originally defined as the average length of the shortest paths between v and all other nodes in a graph:

where d(v, u) is the distance between v and u, i.e., the number of edges in the shortest path between them, and n is the number of nodes in the graph. Closeness centrality is used in [16], where an improved implementation for assessing the importance of each class v in a class graph is proposed:

where score(u) is the importance score of node u determined by some other measure. This new implementation gives emphasis on the classes that are close to other important classes.

Harmonic Centrality (HC) We have seen several minor modifications made to the definition of closeness. Pappas et al. [10] present harmonic centrality, in which the average distance is replaced by the harmonic mean of all distances:

.

Centrality-based measures are used to find topologically important nodes in a graph representation of an ontology. In general, centrality-based measures are defined via the available structure of the elements of a graph including nodes and edges. These measures primarily focus on the quantitative properties of graph structure such as number of edges and position of nodes, to assess the importance of a node. Some measures take edge types into consideration. As different centrality measures highlight different topological properties of a graph, their outputs are usually not consistent.

Degree Centrality (DC) As one of the simplest centrality measures, degree centrality calculates the number of edges incident to a node v:

Pappas et al. [10] use this measure on a class graph to assess the local centrality of each class as its importance. The degree of a class indicates the richness of its description. Nodes with higher degree centrality are more important.

For a directed graph, degree centrality is divided into two categories: indegree centrality and out-degree centrality, used in [23,16]. The former counts the number of incoming arcs, and the latter counts the number of outgoing arcs.

Instead of considering all the edges incident to v, we may also count only those of specific types. More generally, different types of edges can be assigned different weights, to measure weighted degree. For example, Peroni et al. [12] define the density of a class v as the weighted sum of its number of subclasses, properties, and instances:

where w S , w P , w I are weights. Similar methods have been used in [20,8]. Pirez et al. [13] and Queiroz-Sousa et al. [16] divide edges by their types into standard (e.g., is-a, part-of, same-as) and user-defined, which are weighted separately.

Relative Cardinality (RC) Whereas in the above approaches weights are empirically configured, we highlight relative cardinality [18,19], which is a way of automatically weighting edges for calculating weighted degree. In a class graph, the cardinality of an edge which represents a property connecting two classes is the number of the corresponding instances of the classes connected with that specific type of property. Therefore, classes and properties having more instances in a knowledge base are considered more important.

Comments on Degree Centrality Degree centrality and its variants (e.g., relative cardinality) can be efficiently computed in linear time, which is important when an ontology is very large. However, to assess the importance of a node, these measures mainly use its local information, i.e., the subgraph surrounding that node. Without exploiting the global graph structure, the effectiveness of these measures is limited.

Path-based Centrality (PC) Path-based centrality calculates the number of paths that pass through a particular node. For example, Peroni et al. [12] count the number of root-leaf paths in a class hierarchy that pass through each class v as its importance: PC(v) = |Number of root-leaf paths passing through v| .

(

A class in the middle of many root-leaf paths is central.

Betweenness Centrality (BC) As a special case of path-based centrality, it makes sense to only consider shortest paths. Specifically, betweenness centrality is defined as the number of shortest paths from all nodes in a graph to all other nodes that pass through that node. Tzitzikas et al. [20] use the following implementation of betweenness to assess the importance of each node v in a class graph:

where σ st is the total number of shortest paths from node s to node t in the graph, and σ st (v) is the total number of those paths passing through node v. The same as degree centrality, a node with a higher betweenness value is considered more important. Betweenness has also been used on RDF sentence graph [23].

Ego Centrality (EgC) Alternatively, for each node v, let G v be the subgraph induced by v and its neighbors, which contains all the edges between them. Pappas et al. [10] calculate the betweenness centrality of v within G v , which is called ego centrality:

Bridging Centrality (BrC) As an improvement to betweenness, Pappas et al. [10] presented bridging centrality. A node with a high bridging centrality is one that connects densely connected components in a graph. To measure that, the bridging centrality of a node v is defined as the product of v's betweenness centrality (BC) and v's bridging coefficient (Br):

where DC(v) is the degree of node v and N (v) is the set of v's neighbors. Betweenness centrality and bridging coefficient characterize global and local features of a node, respectively.

Comments on Path-based Centrality Compared with degree centrality, pathbased centrality and its variants (e.g., betweenness centrality, bridging centrality) exploit the global graph structure, going beyond the neighborhood of a node. However, it is computationally expensive to calculate betweenness, which involves calculating the shortest paths between all pairs of nodes in a graph.

Closeness Centrality (CC) Similar to betweenness, closeness centrality is another measure for determining the importance of nodes on a global scale within a graph. A node is usually considered as a key node if it can quickly interact with all the other nodes in a graph, not only with its immediate neighbors. The closeness of a node v is originally defined as the average length of the shortest paths between v and all other nodes in a graph:

where d(v, u) is the distance between v and u, i.e., the number of edges in the shortest path between them, and n is the number of nodes in the graph. Closeness centrality is used in [16], where an improved implementation for assessing the importance of each class v in a class graph is proposed:

where score(u) is the importance score of node u determined by some other measure. This new implementation gives emphasis on the classes that are close to other important classes.

Harmonic Centrality (HC) We have seen several minor modifications made to the definition of closeness. Pappas et al. [10] present harmonic centrality, in which the average distance is replaced by the harmonic mean of all distances:

.","[[], [], ['b9'], ['b15', 'b22'], ['b11'], ['b15', 'b7', 'b12', 'b19'], ['b18', 'b17'], [], ['b11'], [], [], ['b19'], ['b22'], ['b9'], ['b9'], [], [], [], ['b15'], [], ['b9'], [], [], [], ['b9'], ['b15', 'b22'], ['b11'], ['b15', 'b7', 'b12', 'b19'], ['b18', 'b17'], [], ['b11'], [], [], ['b19'], ['b22'], ['b9'], ['b9'], [], [], [], ['b15'], [], ['b9'], []]","[[], [], ['b9'], ['b15', 'b22'], ['b11'], ['b15', 'b7', 'b12', 'b19'], ['b18', 'b17'], [], ['b11'], [], [], ['b19'], ['b22'], ['b9'], ['b9'], [], [], [], ['b15'], [], ['b9'], [], [], [], ['b9'], ['b15', 'b22'], ['b11'], ['b15', 'b7', 'b12', 'b19'], ['b18', 'b17'], [], ['b11'], [], [], ['b19'], ['b22'], ['b9'], ['b9'], [], [], [], ['b15'], [], ['b9'], []]",34,"sent1: Centrality-based measures are used to find topologically important nodes in a graph representation of an ontology.
sent2: In general, centrality-based measures are defined via the available structure of the elements of a graph including nodes and edges.
sent3: These measures primarily focus on the quantitative properties of graph structure such as number of edges and position of nodes, to assess the importance of a node.
sent4: Some measures take edge types into consideration.
sent5: As different centrality measures highlight different topological properties of a graph, their outputs are usually not consistent.
sent6: Degree Centrality (DC) As one of the simplest centrality measures, degree centrality calculates the number of edges incident to a node v:Pappas et al. [10] use this measure on a class graph to assess the local centrality of each class as its importance.
sent7: The degree of a class indicates the richness of its description.
sent8: Nodes with higher degree centrality are more important.
sent9: For a directed graph, degree centrality is divided into two categories: indegree centrality and out-degree centrality, used in [23,16].
sent10: The former counts the number of incoming arcs, and the latter counts the number of outgoing arcs.
sent11: Instead of considering all the edges incident to v, we may also count only those of specific types.
sent12: More generally, different types of edges can be assigned different weights, to measure weighted degree.
sent13: For example, Peroni et al. [12] define the density of a class v as the weighted sum of its number of subclasses, properties, and instances:where w S , w P , w I are weights.
sent14: Similar methods have been used in [20,8].
sent15: Pirez et al. [13] and Queiroz-Sousa et al. [16] divide edges by their types into standard (e.g., is-a, part-of, same-as) and user-defined, which are weighted separately.
sent16: Relative Cardinality (RC) Whereas in the above approaches weights are empirically configured, we highlight relative cardinality [18,19], which is a way of automatically weighting edges for calculating weighted degree.
sent17: In a class graph, the cardinality of an edge which represents a property connecting two classes is the number of the corresponding instances of the classes connected with that specific type of property.
sent18: Therefore, classes and properties having more instances in a knowledge base are considered more important.
sent19: Comments on Degree Centrality Degree centrality and its variants (e.g., relative cardinality) can be efficiently computed in linear time, which is important when an ontology is very large.
sent20: However, to assess the importance of a node, these measures mainly use its local information, i.e., the subgraph surrounding that node.
sent21: Without exploiting the global graph structure, the effectiveness of these measures is limited.
sent22: Path-based Centrality (PC) Path-based centrality calculates the number of paths that pass through a particular node.
sent23: For example, Peroni et al. [12] count the number of root-leaf paths in a class hierarchy that pass through each class v as its importance: PC(v) = |Number of root-leaf paths passing through v| .
sent24: (A class in the middle of many root-leaf paths is central.
sent25: Betweenness Centrality (BC) As a special case of path-based centrality, it makes sense to only consider shortest paths.
sent26: Specifically, betweenness centrality is defined as the number of shortest paths from all nodes in a graph to all other nodes that pass through that node.
sent27: Tzitzikas et al. [20] use the following implementation of betweenness to assess the importance of each node v in a class graph:where σ st is the total number of shortest paths from node s to node t in the graph, and σ st (v) is the total number of those paths passing through node v. The same as degree centrality, a node with a higher betweenness value is considered more important.
sent28: Betweenness has also been used on RDF sentence graph [23].
sent29: Ego Centrality (EgC) Alternatively, for each node v, let G v be the subgraph induced by v and its neighbors, which contains all the edges between them.
sent30: Pappas et al. [10] calculate the betweenness centrality of v within G v , which is called ego centrality:Bridging Centrality (BrC) As an improvement to betweenness, Pappas et al. [10] presented bridging centrality.
sent31: A node with a high bridging centrality is one that connects densely connected components in a graph.
sent32: To measure that, the bridging centrality of a node v is defined as the product of v's betweenness centrality (BC) and v's bridging coefficient (Br):where DC(v) is the degree of node v and N (v) is the set of v's neighbors.
sent33: Betweenness centrality and bridging coefficient characterize global and local features of a node, respectively.
sent34: Comments on Path-based Centrality Compared with degree centrality, pathbased centrality and its variants (e.g., betweenness centrality, bridging centrality) exploit the global graph structure, going beyond the neighborhood of a node.
sent35: However, it is computationally expensive to calculate betweenness, which involves calculating the shortest paths between all pairs of nodes in a graph.
sent36: Closeness Centrality (CC) Similar to betweenness, closeness centrality is another measure for determining the importance of nodes on a global scale within a graph.
sent37: A node is usually considered as a key node if it can quickly interact with all the other nodes in a graph, not only with its immediate neighbors.
sent38: The closeness of a node v is originally defined as the average length of the shortest paths between v and all other nodes in a graph:where d(v, u) is the distance between v and u, i.e., the number of edges in the shortest path between them, and n is the number of nodes in the graph.
sent39: Closeness centrality is used in [16], where an improved implementation for assessing the importance of each class v in a class graph is proposed:where score(u) is the importance score of node u determined by some other measure.
sent40: This new implementation gives emphasis on the classes that are close to other important classes.
sent41: Harmonic Centrality (HC) We have seen several minor modifications made to the definition of closeness.
sent42: Pappas et al. [10] present harmonic centrality, in which the average distance is replaced by the harmonic mean of all distances:.
sent43: Centrality-based measures are used to find topologically important nodes in a graph representation of an ontology.
sent44: In general, centrality-based measures are defined via the available structure of the elements of a graph including nodes and edges.
sent45: These measures primarily focus on the quantitative properties of graph structure such as number of edges and position of nodes, to assess the importance of a node.
sent46: Some measures take edge types into consideration.
sent47: As different centrality measures highlight different topological properties of a graph, their outputs are usually not consistent.
sent48: Degree Centrality (DC) As one of the simplest centrality measures, degree centrality calculates the number of edges incident to a node v:Pappas et al. [10] use this measure on a class graph to assess the local centrality of each class as its importance.
sent49: The degree of a class indicates the richness of its description.
sent50: Nodes with higher degree centrality are more important.
sent51: For a directed graph, degree centrality is divided into two categories: indegree centrality and out-degree centrality, used in [23,16].
sent52: The former counts the number of incoming arcs, and the latter counts the number of outgoing arcs.
sent53: Instead of considering all the edges incident to v, we may also count only those of specific types.
sent54: More generally, different types of edges can be assigned different weights, to measure weighted degree.
sent55: For example, Peroni et al. [12] define the density of a class v as the weighted sum of its number of subclasses, properties, and instances:where w S , w P , w I are weights.
sent56: Similar methods have been used in [20,8].
sent57: Pirez et al. [13] and Queiroz-Sousa et al. [16] divide edges by their types into standard (e.g., is-a, part-of, same-as) and user-defined, which are weighted separately.
sent58: Relative Cardinality (RC) Whereas in the above approaches weights are empirically configured, we highlight relative cardinality [18,19], which is a way of automatically weighting edges for calculating weighted degree.
sent59: In a class graph, the cardinality of an edge which represents a property connecting two classes is the number of the corresponding instances of the classes connected with that specific type of property.
sent60: Therefore, classes and properties having more instances in a knowledge base are considered more important.
sent61: Comments on Degree Centrality Degree centrality and its variants (e.g., relative cardinality) can be efficiently computed in linear time, which is important when an ontology is very large.
sent62: However, to assess the importance of a node, these measures mainly use its local information, i.e., the subgraph surrounding that node.
sent63: Without exploiting the global graph structure, the effectiveness of these measures is limited.
sent64: Path-based Centrality (PC) Path-based centrality calculates the number of paths that pass through a particular node.
sent65: For example, Peroni et al. [12] count the number of root-leaf paths in a class hierarchy that pass through each class v as its importance: PC(v) = |Number of root-leaf paths passing through v| .
sent66: (A class in the middle of many root-leaf paths is central.
sent67: Betweenness Centrality (BC) As a special case of path-based centrality, it makes sense to only consider shortest paths.
sent68: Specifically, betweenness centrality is defined as the number of shortest paths from all nodes in a graph to all other nodes that pass through that node.
sent69: Tzitzikas et al. [20] use the following implementation of betweenness to assess the importance of each node v in a class graph:where σ st is the total number of shortest paths from node s to node t in the graph, and σ st (v) is the total number of those paths passing through node v. The same as degree centrality, a node with a higher betweenness value is considered more important.
sent70: Betweenness has also been used on RDF sentence graph [23].
sent71: Ego Centrality (EgC) Alternatively, for each node v, let G v be the subgraph induced by v and its neighbors, which contains all the edges between them.
sent72: Pappas et al. [10] calculate the betweenness centrality of v within G v , which is called ego centrality:Bridging Centrality (BrC) As an improvement to betweenness, Pappas et al. [10] presented bridging centrality.
sent73: A node with a high bridging centrality is one that connects densely connected components in a graph.
sent74: To measure that, the bridging centrality of a node v is defined as the product of v's betweenness centrality (BC) and v's bridging coefficient (Br):where DC(v) is the degree of node v and N (v) is the set of v's neighbors.
sent75: Betweenness centrality and bridging coefficient characterize global and local features of a node, respectively.
sent76: Comments on Path-based Centrality Compared with degree centrality, pathbased centrality and its variants (e.g., betweenness centrality, bridging centrality) exploit the global graph structure, going beyond the neighborhood of a node.
sent77: However, it is computationally expensive to calculate betweenness, which involves calculating the shortest paths between all pairs of nodes in a graph.
sent78: Closeness Centrality (CC) Similar to betweenness, closeness centrality is another measure for determining the importance of nodes on a global scale within a graph.
sent79: A node is usually considered as a key node if it can quickly interact with all the other nodes in a graph, not only with its immediate neighbors.
sent80: The closeness of a node v is originally defined as the average length of the shortest paths between v and all other nodes in a graph:where d(v, u) is the distance between v and u, i.e., the number of edges in the shortest path between them, and n is the number of nodes in the graph.
sent81: Closeness centrality is used in [16], where an improved implementation for assessing the importance of each class v in a class graph is proposed:where score(u) is the importance score of node u determined by some other measure.
sent82: This new implementation gives emphasis on the classes that are close to other important classes.
sent83: Harmonic Centrality (HC) We have seen several minor modifications made to the definition of closeness.
sent84: Pappas et al. [10] present harmonic centrality, in which the average distance is replaced by the harmonic mean of all distances:."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s24,Centrality-based Measures,"Centrality-based measures are used to find topologically important nodes in a graph representation of an ontology. In general, centrality-based measures are defined via the available structure of the elements of a graph including nodes and edges. These measures primarily focus on the quantitative properties of graph structure such as number of edges and position of nodes, to assess the importance of a node. Some measures take edge types into consideration. As different centrality measures highlight different topological properties of a graph, their outputs are usually not consistent.

Degree Centrality (DC) As one of the simplest centrality measures, degree centrality calculates the number of edges incident to a node v:

Pappas et al. [10] use this measure on a class graph to assess the local centrality of each class as its importance. The degree of a class indicates the richness of its description. Nodes with higher degree centrality are more important.

For a directed graph, degree centrality is divided into two categories: indegree centrality and out-degree centrality, used in [23,16]. The former counts the number of incoming arcs, and the latter counts the number of outgoing arcs.

Instead of considering all the edges incident to v, we may also count only those of specific types. More generally, different types of edges can be assigned different weights, to measure weighted degree. For example, Peroni et al. [12] define the density of a class v as the weighted sum of its number of subclasses, properties, and instances:

where w S , w P , w I are weights. Similar methods have been used in [20,8]. Pirez et al. [13] and Queiroz-Sousa et al. [16] divide edges by their types into standard (e.g., is-a, part-of, same-as) and user-defined, which are weighted separately.

Relative Cardinality (RC) Whereas in the above approaches weights are empirically configured, we highlight relative cardinality [18,19], which is a way of automatically weighting edges for calculating weighted degree. In a class graph, the cardinality of an edge which represents a property connecting two classes is the number of the corresponding instances of the classes connected with that specific type of property. Therefore, classes and properties having more instances in a knowledge base are considered more important.

Comments on Degree Centrality Degree centrality and its variants (e.g., relative cardinality) can be efficiently computed in linear time, which is important when an ontology is very large. However, to assess the importance of a node, these measures mainly use its local information, i.e., the subgraph surrounding that node. Without exploiting the global graph structure, the effectiveness of these measures is limited.

Path-based Centrality (PC) Path-based centrality calculates the number of paths that pass through a particular node. For example, Peroni et al. [12] count the number of root-leaf paths in a class hierarchy that pass through each class v as its importance: PC(v) = |Number of root-leaf paths passing through v| .

(

A class in the middle of many root-leaf paths is central.

Betweenness Centrality (BC) As a special case of path-based centrality, it makes sense to only consider shortest paths. Specifically, betweenness centrality is defined as the number of shortest paths from all nodes in a graph to all other nodes that pass through that node. Tzitzikas et al. [20] use the following implementation of betweenness to assess the importance of each node v in a class graph:

where σ st is the total number of shortest paths from node s to node t in the graph, and σ st (v) is the total number of those paths passing through node v. The same as degree centrality, a node with a higher betweenness value is considered more important. Betweenness has also been used on RDF sentence graph [23].

Ego Centrality (EgC) Alternatively, for each node v, let G v be the subgraph induced by v and its neighbors, which contains all the edges between them. Pappas et al. [10] calculate the betweenness centrality of v within G v , which is called ego centrality:

Bridging Centrality (BrC) As an improvement to betweenness, Pappas et al. [10] presented bridging centrality. A node with a high bridging centrality is one that connects densely connected components in a graph. To measure that, the bridging centrality of a node v is defined as the product of v's betweenness centrality (BC) and v's bridging coefficient (Br):

where DC(v) is the degree of node v and N (v) is the set of v's neighbors. Betweenness centrality and bridging coefficient characterize global and local features of a node, respectively.

Comments on Path-based Centrality Compared with degree centrality, pathbased centrality and its variants (e.g., betweenness centrality, bridging centrality) exploit the global graph structure, going beyond the neighborhood of a node. However, it is computationally expensive to calculate betweenness, which involves calculating the shortest paths between all pairs of nodes in a graph.

Closeness Centrality (CC) Similar to betweenness, closeness centrality is another measure for determining the importance of nodes on a global scale within a graph. A node is usually considered as a key node if it can quickly interact with all the other nodes in a graph, not only with its immediate neighbors. The closeness of a node v is originally defined as the average length of the shortest paths between v and all other nodes in a graph:

where d(v, u) is the distance between v and u, i.e., the number of edges in the shortest path between them, and n is the number of nodes in the graph. Closeness centrality is used in [16], where an improved implementation for assessing the importance of each class v in a class graph is proposed:

where score(u) is the importance score of node u determined by some other measure. This new implementation gives emphasis on the classes that are close to other important classes.

Harmonic Centrality (HC) We have seen several minor modifications made to the definition of closeness. Pappas et al. [10] present harmonic centrality, in which the average distance is replaced by the harmonic mean of all distances:

.

Centrality-based measures are used to find topologically important nodes in a graph representation of an ontology. In general, centrality-based measures are defined via the available structure of the elements of a graph including nodes and edges. These measures primarily focus on the quantitative properties of graph structure such as number of edges and position of nodes, to assess the importance of a node. Some measures take edge types into consideration. As different centrality measures highlight different topological properties of a graph, their outputs are usually not consistent.

Degree Centrality (DC) As one of the simplest centrality measures, degree centrality calculates the number of edges incident to a node v:

Pappas et al. [10] use this measure on a class graph to assess the local centrality of each class as its importance. The degree of a class indicates the richness of its description. Nodes with higher degree centrality are more important.

For a directed graph, degree centrality is divided into two categories: indegree centrality and out-degree centrality, used in [23,16]. The former counts the number of incoming arcs, and the latter counts the number of outgoing arcs.

Instead of considering all the edges incident to v, we may also count only those of specific types. More generally, different types of edges can be assigned different weights, to measure weighted degree. For example, Peroni et al. [12] define the density of a class v as the weighted sum of its number of subclasses, properties, and instances:

where w S , w P , w I are weights. Similar methods have been used in [20,8]. Pirez et al. [13] and Queiroz-Sousa et al. [16] divide edges by their types into standard (e.g., is-a, part-of, same-as) and user-defined, which are weighted separately.

Relative Cardinality (RC) Whereas in the above approaches weights are empirically configured, we highlight relative cardinality [18,19], which is a way of automatically weighting edges for calculating weighted degree. In a class graph, the cardinality of an edge which represents a property connecting two classes is the number of the corresponding instances of the classes connected with that specific type of property. Therefore, classes and properties having more instances in a knowledge base are considered more important.

Comments on Degree Centrality Degree centrality and its variants (e.g., relative cardinality) can be efficiently computed in linear time, which is important when an ontology is very large. However, to assess the importance of a node, these measures mainly use its local information, i.e., the subgraph surrounding that node. Without exploiting the global graph structure, the effectiveness of these measures is limited.

Path-based Centrality (PC) Path-based centrality calculates the number of paths that pass through a particular node. For example, Peroni et al. [12] count the number of root-leaf paths in a class hierarchy that pass through each class v as its importance: PC(v) = |Number of root-leaf paths passing through v| .

(

A class in the middle of many root-leaf paths is central.

Betweenness Centrality (BC) As a special case of path-based centrality, it makes sense to only consider shortest paths. Specifically, betweenness centrality is defined as the number of shortest paths from all nodes in a graph to all other nodes that pass through that node. Tzitzikas et al. [20] use the following implementation of betweenness to assess the importance of each node v in a class graph:

where σ st is the total number of shortest paths from node s to node t in the graph, and σ st (v) is the total number of those paths passing through node v. The same as degree centrality, a node with a higher betweenness value is considered more important. Betweenness has also been used on RDF sentence graph [23].

Ego Centrality (EgC) Alternatively, for each node v, let G v be the subgraph induced by v and its neighbors, which contains all the edges between them. Pappas et al. [10] calculate the betweenness centrality of v within G v , which is called ego centrality:

Bridging Centrality (BrC) As an improvement to betweenness, Pappas et al. [10] presented bridging centrality. A node with a high bridging centrality is one that connects densely connected components in a graph. To measure that, the bridging centrality of a node v is defined as the product of v's betweenness centrality (BC) and v's bridging coefficient (Br):

where DC(v) is the degree of node v and N (v) is the set of v's neighbors. Betweenness centrality and bridging coefficient characterize global and local features of a node, respectively.

Comments on Path-based Centrality Compared with degree centrality, pathbased centrality and its variants (e.g., betweenness centrality, bridging centrality) exploit the global graph structure, going beyond the neighborhood of a node. However, it is computationally expensive to calculate betweenness, which involves calculating the shortest paths between all pairs of nodes in a graph.

Closeness Centrality (CC) Similar to betweenness, closeness centrality is another measure for determining the importance of nodes on a global scale within a graph. A node is usually considered as a key node if it can quickly interact with all the other nodes in a graph, not only with its immediate neighbors. The closeness of a node v is originally defined as the average length of the shortest paths between v and all other nodes in a graph:

where d(v, u) is the distance between v and u, i.e., the number of edges in the shortest path between them, and n is the number of nodes in the graph. Closeness centrality is used in [16], where an improved implementation for assessing the importance of each class v in a class graph is proposed:

where score(u) is the importance score of node u determined by some other measure. This new implementation gives emphasis on the classes that are close to other important classes.

Harmonic Centrality (HC) We have seen several minor modifications made to the definition of closeness. Pappas et al. [10] present harmonic centrality, in which the average distance is replaced by the harmonic mean of all distances:

.","[[], [], ['b9'], ['b15', 'b22'], ['b11'], ['b15', 'b7', 'b12', 'b19'], ['b18', 'b17'], [], ['b11'], [], [], ['b19'], ['b22'], ['b9'], ['b9'], [], [], [], ['b15'], [], ['b9'], [], [], [], ['b9'], ['b15', 'b22'], ['b11'], ['b15', 'b7', 'b12', 'b19'], ['b18', 'b17'], [], ['b11'], [], [], ['b19'], ['b22'], ['b9'], ['b9'], [], [], [], ['b15'], [], ['b9'], []]","[[], [], ['b9'], ['b15', 'b22'], ['b11'], ['b15', 'b7', 'b12', 'b19'], ['b18', 'b17'], [], ['b11'], [], [], ['b19'], ['b22'], ['b9'], ['b9'], [], [], [], ['b15'], [], ['b9'], [], [], [], ['b9'], ['b15', 'b22'], ['b11'], ['b15', 'b7', 'b12', 'b19'], ['b18', 'b17'], [], ['b11'], [], [], ['b19'], ['b22'], ['b9'], ['b9'], [], [], [], ['b15'], [], ['b9'], []]",34,"sent1: Centrality-based measures are used to find topologically important nodes in a graph representation of an ontology.
sent2: In general, centrality-based measures are defined via the available structure of the elements of a graph including nodes and edges.
sent3: These measures primarily focus on the quantitative properties of graph structure such as number of edges and position of nodes, to assess the importance of a node.
sent4: Some measures take edge types into consideration.
sent5: As different centrality measures highlight different topological properties of a graph, their outputs are usually not consistent.
sent6: Degree Centrality (DC) As one of the simplest centrality measures, degree centrality calculates the number of edges incident to a node v:Pappas et al. [10] use this measure on a class graph to assess the local centrality of each class as its importance.
sent7: The degree of a class indicates the richness of its description.
sent8: Nodes with higher degree centrality are more important.
sent9: For a directed graph, degree centrality is divided into two categories: indegree centrality and out-degree centrality, used in [23,16].
sent10: The former counts the number of incoming arcs, and the latter counts the number of outgoing arcs.
sent11: Instead of considering all the edges incident to v, we may also count only those of specific types.
sent12: More generally, different types of edges can be assigned different weights, to measure weighted degree.
sent13: For example, Peroni et al. [12] define the density of a class v as the weighted sum of its number of subclasses, properties, and instances:where w S , w P , w I are weights.
sent14: Similar methods have been used in [20,8].
sent15: Pirez et al. [13] and Queiroz-Sousa et al. [16] divide edges by their types into standard (e.g., is-a, part-of, same-as) and user-defined, which are weighted separately.
sent16: Relative Cardinality (RC) Whereas in the above approaches weights are empirically configured, we highlight relative cardinality [18,19], which is a way of automatically weighting edges for calculating weighted degree.
sent17: In a class graph, the cardinality of an edge which represents a property connecting two classes is the number of the corresponding instances of the classes connected with that specific type of property.
sent18: Therefore, classes and properties having more instances in a knowledge base are considered more important.
sent19: Comments on Degree Centrality Degree centrality and its variants (e.g., relative cardinality) can be efficiently computed in linear time, which is important when an ontology is very large.
sent20: However, to assess the importance of a node, these measures mainly use its local information, i.e., the subgraph surrounding that node.
sent21: Without exploiting the global graph structure, the effectiveness of these measures is limited.
sent22: Path-based Centrality (PC) Path-based centrality calculates the number of paths that pass through a particular node.
sent23: For example, Peroni et al. [12] count the number of root-leaf paths in a class hierarchy that pass through each class v as its importance: PC(v) = |Number of root-leaf paths passing through v| .
sent24: (A class in the middle of many root-leaf paths is central.
sent25: Betweenness Centrality (BC) As a special case of path-based centrality, it makes sense to only consider shortest paths.
sent26: Specifically, betweenness centrality is defined as the number of shortest paths from all nodes in a graph to all other nodes that pass through that node.
sent27: Tzitzikas et al. [20] use the following implementation of betweenness to assess the importance of each node v in a class graph:where σ st is the total number of shortest paths from node s to node t in the graph, and σ st (v) is the total number of those paths passing through node v. The same as degree centrality, a node with a higher betweenness value is considered more important.
sent28: Betweenness has also been used on RDF sentence graph [23].
sent29: Ego Centrality (EgC) Alternatively, for each node v, let G v be the subgraph induced by v and its neighbors, which contains all the edges between them.
sent30: Pappas et al. [10] calculate the betweenness centrality of v within G v , which is called ego centrality:Bridging Centrality (BrC) As an improvement to betweenness, Pappas et al. [10] presented bridging centrality.
sent31: A node with a high bridging centrality is one that connects densely connected components in a graph.
sent32: To measure that, the bridging centrality of a node v is defined as the product of v's betweenness centrality (BC) and v's bridging coefficient (Br):where DC(v) is the degree of node v and N (v) is the set of v's neighbors.
sent33: Betweenness centrality and bridging coefficient characterize global and local features of a node, respectively.
sent34: Comments on Path-based Centrality Compared with degree centrality, pathbased centrality and its variants (e.g., betweenness centrality, bridging centrality) exploit the global graph structure, going beyond the neighborhood of a node.
sent35: However, it is computationally expensive to calculate betweenness, which involves calculating the shortest paths between all pairs of nodes in a graph.
sent36: Closeness Centrality (CC) Similar to betweenness, closeness centrality is another measure for determining the importance of nodes on a global scale within a graph.
sent37: A node is usually considered as a key node if it can quickly interact with all the other nodes in a graph, not only with its immediate neighbors.
sent38: The closeness of a node v is originally defined as the average length of the shortest paths between v and all other nodes in a graph:where d(v, u) is the distance between v and u, i.e., the number of edges in the shortest path between them, and n is the number of nodes in the graph.
sent39: Closeness centrality is used in [16], where an improved implementation for assessing the importance of each class v in a class graph is proposed:where score(u) is the importance score of node u determined by some other measure.
sent40: This new implementation gives emphasis on the classes that are close to other important classes.
sent41: Harmonic Centrality (HC) We have seen several minor modifications made to the definition of closeness.
sent42: Pappas et al. [10] present harmonic centrality, in which the average distance is replaced by the harmonic mean of all distances:.
sent43: Centrality-based measures are used to find topologically important nodes in a graph representation of an ontology.
sent44: In general, centrality-based measures are defined via the available structure of the elements of a graph including nodes and edges.
sent45: These measures primarily focus on the quantitative properties of graph structure such as number of edges and position of nodes, to assess the importance of a node.
sent46: Some measures take edge types into consideration.
sent47: As different centrality measures highlight different topological properties of a graph, their outputs are usually not consistent.
sent48: Degree Centrality (DC) As one of the simplest centrality measures, degree centrality calculates the number of edges incident to a node v:Pappas et al. [10] use this measure on a class graph to assess the local centrality of each class as its importance.
sent49: The degree of a class indicates the richness of its description.
sent50: Nodes with higher degree centrality are more important.
sent51: For a directed graph, degree centrality is divided into two categories: indegree centrality and out-degree centrality, used in [23,16].
sent52: The former counts the number of incoming arcs, and the latter counts the number of outgoing arcs.
sent53: Instead of considering all the edges incident to v, we may also count only those of specific types.
sent54: More generally, different types of edges can be assigned different weights, to measure weighted degree.
sent55: For example, Peroni et al. [12] define the density of a class v as the weighted sum of its number of subclasses, properties, and instances:where w S , w P , w I are weights.
sent56: Similar methods have been used in [20,8].
sent57: Pirez et al. [13] and Queiroz-Sousa et al. [16] divide edges by their types into standard (e.g., is-a, part-of, same-as) and user-defined, which are weighted separately.
sent58: Relative Cardinality (RC) Whereas in the above approaches weights are empirically configured, we highlight relative cardinality [18,19], which is a way of automatically weighting edges for calculating weighted degree.
sent59: In a class graph, the cardinality of an edge which represents a property connecting two classes is the number of the corresponding instances of the classes connected with that specific type of property.
sent60: Therefore, classes and properties having more instances in a knowledge base are considered more important.
sent61: Comments on Degree Centrality Degree centrality and its variants (e.g., relative cardinality) can be efficiently computed in linear time, which is important when an ontology is very large.
sent62: However, to assess the importance of a node, these measures mainly use its local information, i.e., the subgraph surrounding that node.
sent63: Without exploiting the global graph structure, the effectiveness of these measures is limited.
sent64: Path-based Centrality (PC) Path-based centrality calculates the number of paths that pass through a particular node.
sent65: For example, Peroni et al. [12] count the number of root-leaf paths in a class hierarchy that pass through each class v as its importance: PC(v) = |Number of root-leaf paths passing through v| .
sent66: (A class in the middle of many root-leaf paths is central.
sent67: Betweenness Centrality (BC) As a special case of path-based centrality, it makes sense to only consider shortest paths.
sent68: Specifically, betweenness centrality is defined as the number of shortest paths from all nodes in a graph to all other nodes that pass through that node.
sent69: Tzitzikas et al. [20] use the following implementation of betweenness to assess the importance of each node v in a class graph:where σ st is the total number of shortest paths from node s to node t in the graph, and σ st (v) is the total number of those paths passing through node v. The same as degree centrality, a node with a higher betweenness value is considered more important.
sent70: Betweenness has also been used on RDF sentence graph [23].
sent71: Ego Centrality (EgC) Alternatively, for each node v, let G v be the subgraph induced by v and its neighbors, which contains all the edges between them.
sent72: Pappas et al. [10] calculate the betweenness centrality of v within G v , which is called ego centrality:Bridging Centrality (BrC) As an improvement to betweenness, Pappas et al. [10] presented bridging centrality.
sent73: A node with a high bridging centrality is one that connects densely connected components in a graph.
sent74: To measure that, the bridging centrality of a node v is defined as the product of v's betweenness centrality (BC) and v's bridging coefficient (Br):where DC(v) is the degree of node v and N (v) is the set of v's neighbors.
sent75: Betweenness centrality and bridging coefficient characterize global and local features of a node, respectively.
sent76: Comments on Path-based Centrality Compared with degree centrality, pathbased centrality and its variants (e.g., betweenness centrality, bridging centrality) exploit the global graph structure, going beyond the neighborhood of a node.
sent77: However, it is computationally expensive to calculate betweenness, which involves calculating the shortest paths between all pairs of nodes in a graph.
sent78: Closeness Centrality (CC) Similar to betweenness, closeness centrality is another measure for determining the importance of nodes on a global scale within a graph.
sent79: A node is usually considered as a key node if it can quickly interact with all the other nodes in a graph, not only with its immediate neighbors.
sent80: The closeness of a node v is originally defined as the average length of the shortest paths between v and all other nodes in a graph:where d(v, u) is the distance between v and u, i.e., the number of edges in the shortest path between them, and n is the number of nodes in the graph.
sent81: Closeness centrality is used in [16], where an improved implementation for assessing the importance of each class v in a class graph is proposed:where score(u) is the importance score of node u determined by some other measure.
sent82: This new implementation gives emphasis on the classes that are close to other important classes.
sent83: Harmonic Centrality (HC) We have seen several minor modifications made to the definition of closeness.
sent84: Pappas et al. [10] present harmonic centrality, in which the average distance is replaced by the harmonic mean of all distances:."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s25,(9),"Radiality (Ra) Pappas et al. [10] also present radiality, which takes the diameter of a graph into account:

where D is the diameter of the graph, namely the greatest distance between any pair of nodes in the graph.

Comments on Closeness Centrality Closeness centrality and its variants (e.g., harmonic centrality, radiality) are similar to betweenness, also involving calculating the shortest paths between all pairs of nodes in a graph. One difference is that, a node with a high closeness value is usually located at the center of the graph (in terms of distance), but such a node may not have a high betweenness value because it may not be a bridging node that resides in many shortest paths connecting other nodes.

Eigenvector Centrality (EC) A widely adopted principle is that a node is important if it is connected with important nodes. For example, in a class graph, a class is important if the classes it connects with are important. This gives rise to eigenvector centrality which iteratively calculates the importance of each node v in a graph:

where N (v) is the set of v's neighbors, and λ is a constant factor for normalization. The eigenvector centrality of a node is the sum of the eigenvector centrality of its neighbors. The computation iterates over all the nodes in the graph, one round after another until convergence. Whereas this basic measure has been used in [3], its improved variants are more popular in the literature. PageRank, a well-known implementation of eigenvector centrality, is used in [20,2]. Different from the above basic measure, PageRank introduces a damping factor which is added to the centrality. Weighted PageRank, weighted HITS, or their variants are used in [24,23,22,21,4], where centrality is defined as a weighted sum. The weight of an edge between v and u indicates the strength of the connection between them; a stronger connection will transport more centrality score from u to v.

Comments on Eigenvector Centrality Eigenvector centrality and its variants (e.g., PageRank, HITS) have shown their effectiveness in many applications. However, they require iterative computation over all the nodes in a graph until convergence, which is time-consuming for large graphs.

Empirical Comparison of Centrality-based Measures It seems that the effectiveness of a centrality-based measure is related to the graph model, and may also depend on the specific ontology to be summarized as the application and the domain of an ontology provide a guideline in order to select a proper set of measures.

Specifically, according to the experiment results presented in [20], the simple degree centrality (DC) appears more effective than PageRank (i.e., EC) on some class graphs. However, Zhang et al. [23] report that weighted PageRank (i.e., EC) outperforms degree (i.e., DC) on several RDF sentence graphs; both of them are considerably better than betweenness (i.e., BC). Pappas et al. [10] find that degree (i.e., DC) and betweenness (i.e., BC, EgC, and BrC) are notably better than closeness (i.e., HC and RA) on a few class graphs.

Unfortunately, we could not draw any reliable conclusions from the current empirical results reported in the literature as they all experiment with a small number of ontologies.

Radiality (Ra) Pappas et al. [10] also present radiality, which takes the diameter of a graph into account:

where D is the diameter of the graph, namely the greatest distance between any pair of nodes in the graph.

Comments on Closeness Centrality Closeness centrality and its variants (e.g., harmonic centrality, radiality) are similar to betweenness, also involving calculating the shortest paths between all pairs of nodes in a graph. One difference is that, a node with a high closeness value is usually located at the center of the graph (in terms of distance), but such a node may not have a high betweenness value because it may not be a bridging node that resides in many shortest paths connecting other nodes.

Eigenvector Centrality (EC) A widely adopted principle is that a node is important if it is connected with important nodes. For example, in a class graph, a class is important if the classes it connects with are important. This gives rise to eigenvector centrality which iteratively calculates the importance of each node v in a graph:

where N (v) is the set of v's neighbors, and λ is a constant factor for normalization. The eigenvector centrality of a node is the sum of the eigenvector centrality of its neighbors. The computation iterates over all the nodes in the graph, one round after another until convergence. Whereas this basic measure has been used in [3], its improved variants are more popular in the literature. PageRank, a well-known implementation of eigenvector centrality, is used in [20,2]. Different from the above basic measure, PageRank introduces a damping factor which is added to the centrality. Weighted PageRank, weighted HITS, or their variants are used in [24,23,22,21,4], where centrality is defined as a weighted sum. The weight of an edge between v and u indicates the strength of the connection between them; a stronger connection will transport more centrality score from u to v.

Comments on Eigenvector Centrality Eigenvector centrality and its variants (e.g., PageRank, HITS) have shown their effectiveness in many applications. However, they require iterative computation over all the nodes in a graph until convergence, which is time-consuming for large graphs.

Empirical Comparison of Centrality-based Measures It seems that the effectiveness of a centrality-based measure is related to the graph model, and may also depend on the specific ontology to be summarized as the application and the domain of an ontology provide a guideline in order to select a proper set of measures.

Specifically, according to the experiment results presented in [20], the simple degree centrality (DC) appears more effective than PageRank (i.e., EC) on some class graphs. However, Zhang et al. [23] report that weighted PageRank (i.e., EC) outperforms degree (i.e., DC) on several RDF sentence graphs; both of them are considerably better than betweenness (i.e., BC). Pappas et al. [10] find that degree (i.e., DC) and betweenness (i.e., BC, EgC, and BrC) are notably better than closeness (i.e., HC and RA) on a few class graphs.

Unfortunately, we could not draw any reliable conclusions from the current empirical results reported in the literature as they all experiment with a small number of ontologies.","[['b9'], [], [], [], ['b3', 'b22', 'b20', 'b21', 'b1', 'b23', 'b2', 'b19'], [], [], ['b9', 'b19', 'b22'], [], ['b9'], [], [], [], ['b3', 'b22', 'b20', 'b21', 'b1', 'b23', 'b2', 'b19'], [], [], ['b9', 'b19', 'b22'], []]","[['b9'], [], [], [], ['b3', 'b22', 'b20', 'b21', 'b1', 'b23', 'b2', 'b19'], [], [], ['b9', 'b19', 'b22'], [], ['b9'], [], [], [], ['b3', 'b22', 'b20', 'b21', 'b1', 'b23', 'b2', 'b19'], [], [], ['b9', 'b19', 'b22'], []]",24,"sent1: Radiality (Ra) Pappas et al. [10] also present radiality, which takes the diameter of a graph into account:where D is the diameter of the graph, namely the greatest distance between any pair of nodes in the graph.
sent2: Comments on Closeness Centrality Closeness centrality and its variants (e.g., harmonic centrality, radiality) are similar to betweenness, also involving calculating the shortest paths between all pairs of nodes in a graph.
sent3: One difference is that, a node with a high closeness value is usually located at the center of the graph (in terms of distance), but such a node may not have a high betweenness value because it may not be a bridging node that resides in many shortest paths connecting other nodes.
sent4: Eigenvector Centrality (EC) A widely adopted principle is that a node is important if it is connected with important nodes.
sent5: For example, in a class graph, a class is important if the classes it connects with are important.
sent6: This gives rise to eigenvector centrality which iteratively calculates the importance of each node v in a graph:where N (v) is the set of v's neighbors, and λ is a constant factor for normalization.
sent7: The eigenvector centrality of a node is the sum of the eigenvector centrality of its neighbors.
sent8: The computation iterates over all the nodes in the graph, one round after another until convergence.
sent9: Whereas this basic measure has been used in [3], its improved variants are more popular in the literature.
sent10: PageRank, a well-known implementation of eigenvector centrality, is used in [20,2].
sent11: Different from the above basic measure, PageRank introduces a damping factor which is added to the centrality.
sent12: Weighted PageRank, weighted HITS, or their variants are used in [24,23,22,21,4], where centrality is defined as a weighted sum.
sent13: The weight of an edge between v and u indicates the strength of the connection between them; a stronger connection will transport more centrality score from u to v.Comments on Eigenvector Centrality Eigenvector centrality and its variants (e.g., PageRank, HITS) have shown their effectiveness in many applications.
sent14: However, they require iterative computation over all the nodes in a graph until convergence, which is time-consuming for large graphs.
sent15: Empirical Comparison of Centrality-based Measures
sent16: It seems that the effectiveness of a centrality-based measure is related to the graph model, and may also depend on the specific ontology to be summarized as the application and the domain of an ontology provide a guideline in order to select a proper set of measures.
sent17: Specifically, according to the experiment results presented in [20], the simple degree centrality (DC) appears more effective than PageRank (i.e., EC) on some class graphs.
sent18: However, Zhang et al. [23] report that weighted PageRank (i.e., EC) outperforms degree (i.e., DC) on several RDF sentence graphs; both of them are considerably better than betweenness (i.e., BC).
sent19: Pappas et al. [10] find that degree (i.e., DC) and betweenness (i.e., BC, EgC, and BrC) are notably better than closeness (i.e., HC and RA) on a few class graphs.
sent20: Unfortunately, we could not draw any reliable conclusions from the current empirical results reported in the literature as they all experiment with a small number of ontologies.
sent21: Radiality (Ra) Pappas et al. [10] also present radiality, which takes the diameter of a graph into account:where D is the diameter of the graph, namely the greatest distance between any pair of nodes in the graph.
sent22: Comments on Closeness Centrality Closeness centrality and its variants (e.g., harmonic centrality, radiality) are similar to betweenness, also involving calculating the shortest paths between all pairs of nodes in a graph.
sent23: One difference is that, a node with a high closeness value is usually located at the center of the graph (in terms of distance), but such a node may not have a high betweenness value because it may not be a bridging node that resides in many shortest paths connecting other nodes.
sent24: Eigenvector Centrality (EC) A widely adopted principle is that a node is important if it is connected with important nodes.
sent25: For example, in a class graph, a class is important if the classes it connects with are important.
sent26: This gives rise to eigenvector centrality which iteratively calculates the importance of each node v in a graph:where N (v) is the set of v's neighbors, and λ is a constant factor for normalization.
sent27: The eigenvector centrality of a node is the sum of the eigenvector centrality of its neighbors.
sent28: The computation iterates over all the nodes in the graph, one round after another until convergence.
sent29: Whereas this basic measure has been used in [3], its improved variants are more popular in the literature.
sent30: PageRank, a well-known implementation of eigenvector centrality, is used in [20,2].
sent31: Different from the above basic measure, PageRank introduces a damping factor which is added to the centrality.
sent32: Weighted PageRank, weighted HITS, or their variants are used in [24,23,22,21,4], where centrality is defined as a weighted sum.
sent33: The weight of an edge between v and u indicates the strength of the connection between them; a stronger connection will transport more centrality score from u to v.Comments on Eigenvector Centrality Eigenvector centrality and its variants (e.g., PageRank, HITS) have shown their effectiveness in many applications.
sent34: However, they require iterative computation over all the nodes in a graph until convergence, which is time-consuming for large graphs.
sent35: Empirical Comparison of Centrality-based Measures
sent36: It seems that the effectiveness of a centrality-based measure is related to the graph model, and may also depend on the specific ontology to be summarized as the application and the domain of an ontology provide a guideline in order to select a proper set of measures.
sent37: Specifically, according to the experiment results presented in [20], the simple degree centrality (DC) appears more effective than PageRank (i.e., EC) on some class graphs.
sent38: However, Zhang et al. [23] report that weighted PageRank (i.e., EC) outperforms degree (i.e., DC) on several RDF sentence graphs; both of them are considerably better than betweenness (i.e., BC).
sent39: Pappas et al. [10] find that degree (i.e., DC) and betweenness (i.e., BC, EgC, and BrC) are notably better than closeness (i.e., HC and RA) on a few class graphs.
sent40: Unfortunately, we could not draw any reliable conclusions from the current empirical results reported in the literature as they all experiment with a small number of ontologies."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s26,Coverage-based Measures,"Top-ranked nodes in a graph representation of an ontology may not form the best ontology summary. For many applications, a good summary is expected to have a good coverage of the contents of an ontology, to form a comprehensive and unbiased overview. Accordingly, the quality of a subset of nodes forming a summary is to be assessed as a whole.

Coverage (Co) Peroni et al. [12] propose the coverage criterion which aims to show how well the selected set of classes are spread over the whole class hierarchy. For each node v, let N + (v) be the set of nodes covered by v, including v and its neighbors, i.e., its subclasses and superclasses in the class hierarchy. The coverage of a set of selected nodes V is defined as the proportion of nodes in the graph that are covered by V :

where n is the number of nodes in the graph. Further, Peroni et al. [12] consider an interesting measure called balance which is directly related to coverage. It measures how balanced the selected nodes are, i.e., the degree to which each selected node contributes to the overall coverage of the set, which is characterized by standard deviation.

Diversity-based Re-ranking (Di) In [23,22], the coverage of a summary is improved by a re-ranking step after centrality-based ranking. In these approaches, nodes are iteratively selected to form a summary. In each iteration, the next node to be selected may not be the top-ranked one among the remaining nodes, which will be re-ranked such that a node similar to those selected in previous iterations will be penalized. Specifically, let score(v) be the centrality score of node v, and let sim(v, u) be the similarity between nodes v and u. Given a set of nodes V s which are already selected into the summary and a set of candidate nodes V c , the next node to be selected from V c is

Zhang et al. [23,22] use this algorithm to rank RDF sentences, where two RDF sentences are similar if they share terms. The resulting ontology summary is diversified with regard to the terms it contains.

Comments on Coverage-based Measures Coverage-based methods complement centrality-based measures, but their current implementations are suboptimal. Coverage in Eq. (12) considers the neighborhood of each node, not taking the global graph structure into account. Diversity-based re-ranking in Eq. (13) has a greedy nature, and may not find the optimum summary in terms of centrality and diversity.

Top-ranked nodes in a graph representation of an ontology may not form the best ontology summary. For many applications, a good summary is expected to have a good coverage of the contents of an ontology, to form a comprehensive and unbiased overview. Accordingly, the quality of a subset of nodes forming a summary is to be assessed as a whole.

Coverage (Co) Peroni et al. [12] propose the coverage criterion which aims to show how well the selected set of classes are spread over the whole class hierarchy. For each node v, let N + (v) be the set of nodes covered by v, including v and its neighbors, i.e., its subclasses and superclasses in the class hierarchy. The coverage of a set of selected nodes V is defined as the proportion of nodes in the graph that are covered by V :

where n is the number of nodes in the graph. Further, Peroni et al. [12] consider an interesting measure called balance which is directly related to coverage. It measures how balanced the selected nodes are, i.e., the degree to which each selected node contributes to the overall coverage of the set, which is characterized by standard deviation.

Diversity-based Re-ranking (Di) In [23,22], the coverage of a summary is improved by a re-ranking step after centrality-based ranking. In these approaches, nodes are iteratively selected to form a summary. In each iteration, the next node to be selected may not be the top-ranked one among the remaining nodes, which will be re-ranked such that a node similar to those selected in previous iterations will be penalized. Specifically, let score(v) be the centrality score of node v, and let sim(v, u) be the similarity between nodes v and u. Given a set of nodes V s which are already selected into the summary and a set of candidate nodes V c , the next node to be selected from V c is

Zhang et al. [23,22] use this algorithm to rank RDF sentences, where two RDF sentences are similar if they share terms. The resulting ontology summary is diversified with regard to the terms it contains.

Comments on Coverage-based Measures Coverage-based methods complement centrality-based measures, but their current implementations are suboptimal. Coverage in Eq. (12) considers the neighborhood of each node, not taking the global graph structure into account. Diversity-based re-ranking in Eq. (13) has a greedy nature, and may not find the optimum summary in terms of centrality and diversity.","[[], ['b11'], ['b11'], ['b21', 'b22'], ['b21', 'b22'], [], [], ['b11'], ['b11'], ['b21', 'b22'], ['b21', 'b22'], []]","[[], ['b11'], ['b11'], ['b21', 'b22'], ['b21', 'b22'], [], [], ['b11'], ['b11'], ['b21', 'b22'], ['b21', 'b22'], []]",12,"sent1: Top-ranked nodes in a graph representation of an ontology may not form the best ontology summary.
sent2: For many applications, a good summary is expected to have a good coverage of the contents of an ontology, to form a comprehensive and unbiased overview.
sent3: Accordingly, the quality of a subset of nodes forming a summary is to be assessed as a whole.
sent4: Coverage (Co) Peroni et al. [12] propose the coverage criterion which aims to show how well the selected set of classes are spread over the whole class hierarchy.
sent5: For each node v, let N + (v) be the set of nodes covered by v, including v and its neighbors, i.e., its subclasses and superclasses in the class hierarchy.
sent6: The coverage of a set of selected nodes V is defined as the proportion of nodes in the graph that are covered by V :where n is the number of nodes in the graph.
sent7: Further, Peroni et al. [12] consider an interesting measure called balance which is directly related to coverage.
sent8: It measures how balanced the selected nodes are, i.e., the degree to which each selected node contributes to the overall coverage of the set, which is characterized by standard deviation.
sent9: Diversity-based Re-ranking (Di)
sent10: In [23,22], the coverage of a summary is improved by a re-ranking step after centrality-based ranking.
sent11: In these approaches, nodes are iteratively selected to form a summary.
sent12: In each iteration, the next node to be selected may not be the top-ranked one among the remaining nodes, which will be re-ranked such that a node similar to those selected in previous iterations will be penalized.
sent13: Specifically, let score(v) be the centrality score of node v, and let sim(v, u) be the similarity between nodes v and u.
sent14: Given a set of nodes V s which are already selected into the summary and a set of candidate nodes V c , the next node to be selected from V c isZhang et al. [23,22] use this algorithm to rank RDF sentences, where two RDF sentences are similar if they share terms.
sent15: The resulting ontology summary is diversified with regard to the terms it contains.
sent16: Comments on Coverage-based Measures Coverage-based methods complement centrality-based measures, but their current implementations are suboptimal.
sent17: Coverage in Eq. (12) considers the neighborhood of each node, not taking the global graph structure into account.
sent18: Diversity-based re-ranking in Eq. (13) has a greedy nature, and may not find the optimum summary in terms of centrality and diversity.
sent19: Top-ranked nodes in a graph representation of an ontology may not form the best ontology summary.
sent20: For many applications, a good summary is expected to have a good coverage of the contents of an ontology, to form a comprehensive and unbiased overview.
sent21: Accordingly, the quality of a subset of nodes forming a summary is to be assessed as a whole.
sent22: Coverage (Co) Peroni et al. [12] propose the coverage criterion which aims to show how well the selected set of classes are spread over the whole class hierarchy.
sent23: For each node v, let N + (v) be the set of nodes covered by v, including v and its neighbors, i.e., its subclasses and superclasses in the class hierarchy.
sent24: The coverage of a set of selected nodes V is defined as the proportion of nodes in the graph that are covered by V :where n is the number of nodes in the graph.
sent25: Further, Peroni et al. [12] consider an interesting measure called balance which is directly related to coverage.
sent26: It measures how balanced the selected nodes are, i.e., the degree to which each selected node contributes to the overall coverage of the set, which is characterized by standard deviation.
sent27: Diversity-based Re-ranking (Di)
sent28: In [23,22], the coverage of a summary is improved by a re-ranking step after centrality-based ranking.
sent29: In these approaches, nodes are iteratively selected to form a summary.
sent30: In each iteration, the next node to be selected may not be the top-ranked one among the remaining nodes, which will be re-ranked such that a node similar to those selected in previous iterations will be penalized.
sent31: Specifically, let score(v) be the centrality score of node v, and let sim(v, u) be the similarity between nodes v and u.
sent32: Given a set of nodes V s which are already selected into the summary and a set of candidate nodes V c , the next node to be selected from V c isZhang et al. [23,22] use this algorithm to rank RDF sentences, where two RDF sentences are similar if they share terms.
sent33: The resulting ontology summary is diversified with regard to the terms it contains.
sent34: Comments on Coverage-based Measures Coverage-based methods complement centrality-based measures, but their current implementations are suboptimal.
sent35: Coverage in Eq. (12) considers the neighborhood of each node, not taking the global graph structure into account.
sent36: Diversity-based re-ranking in Eq. (13) has a greedy nature, and may not find the optimum summary in terms of centrality and diversity."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s27,Application-specific Measures,"The following two methods are not graph-based but are designed for specific applications.

Query Relevance (QR) A special kind of ontology summary is a snippet presented in search results pages of an ontology search engine. In this application, terms [6,2] or RDF sentences [4,11] that are relevant to a user query (e.g., containing query keywords) are prioritized for being presented in a snippet, to show the relevance of an ontology to the user's information needs.

Frequency of Correspondences (FC) Pires et al. [13] consider applications where an ontology to be summarized can be an integrated ontology obtained by merging several local ontologies. In that case, an important term in the integrated ontology is one that has a high frequency of correspondences, namely it finds correspondences to many classes in local ontologies.

The following two methods are not graph-based but are designed for specific applications.

Query Relevance (QR) A special kind of ontology summary is a snippet presented in search results pages of an ontology search engine. In this application, terms [6,2] or RDF sentences [4,11] that are relevant to a user query (e.g., containing query keywords) are prioritized for being presented in a snippet, to show the relevance of an ontology to the user's information needs.

Frequency of Correspondences (FC) Pires et al. [13] consider applications where an ontology to be summarized can be an integrated ontology obtained by merging several local ontologies. In that case, an important term in the integrated ontology is one that has a high frequency of correspondences, namely it finds correspondences to many classes in local ontologies.","[[], ['b3', 'b1', 'b5', 'b10'], ['b12'], [], ['b3', 'b1', 'b5', 'b10'], ['b12']]","[[], ['b3', 'b1', 'b5', 'b10'], ['b12'], [], ['b3', 'b1', 'b5', 'b10'], ['b12']]",10,"sent1: The following two methods are not graph-based but are designed for specific applications.
sent2: Query Relevance (QR) A special kind of ontology summary is a snippet presented in search results pages of an ontology search engine.
sent3: In this application, terms [6,2] or RDF sentences [4,11] that are relevant to a user query (e.g., containing query keywords) are prioritized for being presented in a snippet, to show the relevance of an ontology to the user's information needs.
sent4: Frequency of Correspondences (FC) Pires et al. [13] consider applications where an ontology to be summarized can be an integrated ontology obtained by merging several local ontologies.
sent5: In that case, an important term in the integrated ontology is one that has a high frequency of correspondences, namely it finds correspondences to many classes in local ontologies.
sent6: The following two methods are not graph-based but are designed for specific applications.
sent7: Query Relevance (QR) A special kind of ontology summary is a snippet presented in search results pages of an ontology search engine.
sent8: In this application, terms [6,2] or RDF sentences [4,11] that are relevant to a user query (e.g., containing query keywords) are prioritized for being presented in a snippet, to show the relevance of an ontology to the user's information needs.
sent9: Frequency of Correspondences (FC) Pires et al. [13] consider applications where an ontology to be summarized can be an integrated ontology obtained by merging several local ontologies.
sent10: In that case, an important term in the integrated ontology is one that has a high frequency of correspondences, namely it finds correspondences to many classes in local ontologies."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s20,RDF Sentence Graph,"Zhang et al. [23] proposed an RDF sentence graph. An RDF sentence is a subset of RDF triples, and a set of RDF sentences form the finest partition of the triples in an RDF graph such that each blank node only appears in one block. In many cases, an RDF sentence corresponds to an axiom in OWL, since when mapping OWL ontologies into RDF graphs, blank nodes are introduced when an axiom is transformed into multiple RDF triples.  In an RDF sentence graph, nodes represent RDF sentences, which are adjacent if the terms they describe overlap. Figure 3 illustrates an RDF sentence graph for the ontology in Fig. 1 five axioms. Zhang et al. [23] differentiate between two types of arcs, depending on the structural role of the shared terms, which we will not elaborate. Penin et al. [11] further cluster textually similar RDF sentences into topic nodes.

Comments. Compared with RDF triples, there is a better correspondence between RDF sentences and OWL axioms. In an RDF sentence graph, RDF sentences (or roughly speaking, axioms) are first-class citizens, making this model particularly suitable for ranking triples/axioms. However, terms are not explicitly represented in this model, which may limit its application.

Zhang et al. [23] proposed an RDF sentence graph. An RDF sentence is a subset of RDF triples, and a set of RDF sentences form the finest partition of the triples in an RDF graph such that each blank node only appears in one block. In many cases, an RDF sentence corresponds to an axiom in OWL, since when mapping OWL ontologies into RDF graphs, blank nodes are introduced when an axiom is transformed into multiple RDF triples.  In an RDF sentence graph, nodes represent RDF sentences, which are adjacent if the terms they describe overlap. Figure 3 illustrates an RDF sentence graph for the ontology in Fig. 1 five axioms. Zhang et al. [23] differentiate between two types of arcs, depending on the structural role of the shared terms, which we will not elaborate. Penin et al. [11] further cluster textually similar RDF sentences into topic nodes.

Comments. Compared with RDF triples, there is a better correspondence between RDF sentences and OWL axioms. In an RDF sentence graph, RDF sentences (or roughly speaking, axioms) are first-class citizens, making this model particularly suitable for ranking triples/axioms. However, terms are not explicitly represented in this model, which may limit its application.","[['b10', 'b22'], [], ['b10', 'b22'], []]","[['b10', 'b22'], [], ['b10', 'b22'], []]",4,"sent1: Zhang et al. [23] proposed an RDF sentence graph.
sent2: An RDF sentence is a subset of RDF triples, and a set of RDF sentences form the finest partition of the triples in an RDF graph such that each blank node only appears in one block.
sent3: In many cases, an RDF sentence corresponds to an axiom in OWL, since when mapping OWL ontologies into RDF graphs, blank nodes are introduced when an axiom is transformed into multiple RDF triples.
sent4: In an RDF sentence graph, nodes represent RDF sentences, which are adjacent if the terms they describe overlap.
sent5: Figure 3 illustrates an RDF sentence graph for the ontology in Fig. 1 five axioms.
sent6: Zhang et al. [23] differentiate between two types of arcs, depending on the structural role of the shared terms, which we will not elaborate.
sent7: Penin et al. [11] further cluster textually similar RDF sentences into topic nodes.
sent8: Comments. Compared with RDF triples, there is a better correspondence between RDF sentences and OWL axioms.
sent9: In an RDF sentence graph, RDF sentences (or roughly speaking, axioms) are first-class citizens, making this model particularly suitable for ranking triples/axioms.
sent10: However, terms are not explicitly represented in this model, which may limit its application.
sent11: Zhang et al. [23] proposed an RDF sentence graph.
sent12: An RDF sentence is a subset of RDF triples, and a set of RDF sentences form the finest partition of the triples in an RDF graph such that each blank node only appears in one block.
sent13: In many cases, an RDF sentence corresponds to an axiom in OWL, since when mapping OWL ontologies into RDF graphs, blank nodes are introduced when an axiom is transformed into multiple RDF triples.
sent14: In an RDF sentence graph, nodes represent RDF sentences, which are adjacent if the terms they describe overlap.
sent15: Figure 3 illustrates an RDF sentence graph for the ontology in Fig. 1 five axioms.
sent16: Zhang et al. [23] differentiate between two types of arcs, depending on the structural role of the shared terms, which we will not elaborate.
sent17: Penin et al. [11] further cluster textually similar RDF sentences into topic nodes.
sent18: Comments. Compared with RDF triples, there is a better correspondence between RDF sentences and OWL axioms.
sent19: In an RDF sentence graph, RDF sentences (or roughly speaking, axioms) are first-class citizens, making this model particularly suitable for ranking triples/axioms.
sent20: However, terms are not explicitly represented in this model, which may limit its application."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s6,Term-Sentence Graph,"Zhang et al. [22] present a bipartite graph model, where terms and RDF sentences are both represented by nodes, which we call a term-sentence graph. A directed arc connects an RDF sentence to a term if the term is described in that RDF sentence. Figure 5 illustrates a term-sentence graph for the ontology in Fig. 1, derived from Fig. 3. Zhang et al. [22] differentiate between three types of arcs, depending on the structural role of term in RDF sentence, which we will 

not elaborate. The model is simplified in [4], where edges are undirected and unlabeled.

Comments. A term-sentence graph is more complex than all the abovementioned models. One advantage is that, compared with an RDF sentence graph and a vocabulary dependence graph, it explicitly represents both the terms and RDF sentences in the model, thereby expanding its potential application.

Zhang et al. [22] present a bipartite graph model, where terms and RDF sentences are both represented by nodes, which we call a term-sentence graph. A directed arc connects an RDF sentence to a term if the term is described in that RDF sentence. Figure 5 illustrates a term-sentence graph for the ontology in Fig. 1, derived from Fig. 3. Zhang et al. [22] differentiate between three types of arcs, depending on the structural role of term in RDF sentence, which we will 

not elaborate. The model is simplified in [4], where edges are undirected and unlabeled.

Comments. A term-sentence graph is more complex than all the abovementioned models. One advantage is that, compared with an RDF sentence graph and a vocabulary dependence graph, it explicitly represents both the terms and RDF sentences in the model, thereby expanding its potential application.","[['b21'], ['b3'], [], ['b21'], ['b3'], []]","[['b21'], ['b3'], [], ['b21'], ['b3'], []]",4,"sent1: Zhang et al. [22] present a bipartite graph model, where terms and RDF sentences are both represented by nodes, which we call a term-sentence graph.
sent2: A directed arc connects an RDF sentence to a term if the term is described in that RDF sentence.
sent3: Figure 5 illustrates a term-sentence graph for the ontology in Fig. 1, derived from Fig. 3.
sent4: Zhang et al. [22] differentiate between three types of arcs, depending on the structural role of term in RDF sentence, which we will not elaborate.
sent5: The model is simplified in [4], where edges are undirected and unlabeled.
sent6: Comments. A term-sentence graph is more complex than all the abovementioned models.
sent7: One advantage is that, compared with an RDF sentence graph and a vocabulary dependence graph, it explicitly represents both the terms and RDF sentences in the model, thereby expanding its potential application.
sent8: Zhang et al. [22] present a bipartite graph model, where terms and RDF sentences are both represented by nodes, which we call a term-sentence graph.
sent9: A directed arc connects an RDF sentence to a term if the term is described in that RDF sentence.
sent10: Figure 5 illustrates a term-sentence graph for the ontology in Fig. 1, derived from Fig. 3.
sent11: Zhang et al. [22] differentiate between three types of arcs, depending on the structural role of term in RDF sentence, which we will not elaborate.
sent12: The model is simplified in [4], where edges are undirected and unlabeled.
sent13: Comments. A term-sentence graph is more complex than all the abovementioned models.
sent14: One advantage is that, compared with an RDF sentence graph and a vocabulary dependence graph, it explicitly represents both the terms and RDF sentences in the model, thereby expanding its potential application."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s28,Other Measures,"In addition to graph-based and application-specific measures, we briefly review other methods used in the literature.

Name Simplicity (NS) Peroni et al. [12] emphasize that natural categories or basic objects are good representers of an ontology. They propose that a natural category normally has a relatively simple label, and hence they assess the importance of a class by the simplicity of its name. A class having compound words in the name will be penalized.

Textual Centrality (TC) Zhang et al. [24] calculate the textual centrality of a term in an ontology. Different from the centrality-based measures discussed in Section 3.1 which are defined over graph structure, the textual centrality of a term is the similarity between its textual description and the one for the whole ontology.

Popularity (Po) The wide use of a term on the Web suggests its importance. To measure the popularity of a term, Peroni et al. [12] submit the name of the term as a keyword query to a Web search engine and resort to the number of returned results. Zhang et al. [22] calculate the number of websites hosting RDF documents where the term is instantiated.

Cohesion (Ch) Cheng et al. [4] measure the quality of a summary as a whole. Different from diversity-based re-ranking described in Section 3.2 which penalizes an ontology summary where RDF sentences share terms, such a summary will be awarded in [4] as it exhibits cohesion.

In addition to graph-based and application-specific measures, we briefly review other methods used in the literature.

Name Simplicity (NS) Peroni et al. [12] emphasize that natural categories or basic objects are good representers of an ontology. They propose that a natural category normally has a relatively simple label, and hence they assess the importance of a class by the simplicity of its name. A class having compound words in the name will be penalized.

Textual Centrality (TC) Zhang et al. [24] calculate the textual centrality of a term in an ontology. Different from the centrality-based measures discussed in Section 3.1 which are defined over graph structure, the textual centrality of a term is the similarity between its textual description and the one for the whole ontology.

Popularity (Po) The wide use of a term on the Web suggests its importance. To measure the popularity of a term, Peroni et al. [12] submit the name of the term as a keyword query to a Web search engine and resort to the number of returned results. Zhang et al. [22] calculate the number of websites hosting RDF documents where the term is instantiated.

Cohesion (Ch) Cheng et al. [4] measure the quality of a summary as a whole. Different from diversity-based re-ranking described in Section 3.2 which penalizes an ontology summary where RDF sentences share terms, such a summary will be awarded in [4] as it exhibits cohesion.","[[], ['b11'], ['b23'], ['b11', 'b21'], ['b3'], [], ['b11'], ['b23'], ['b11', 'b21'], ['b3']]","[[], ['b11'], ['b23'], ['b11', 'b21'], ['b3'], [], ['b11'], ['b23'], ['b11', 'b21'], ['b3']]",10,"sent1: In addition to graph-based and application-specific measures, we briefly review other methods used in the literature.
sent2: Name Simplicity (NS) Peroni et al. [12] emphasize that natural categories or basic objects are good representers of an ontology.
sent3: They propose that a natural category normally has a relatively simple label, and hence they assess the importance of a class by the simplicity of its name.
sent4: A class having compound words in the name will be penalized.
sent5: Textual Centrality (TC) Zhang et al. [24] calculate the textual centrality of a term in an ontology.
sent6: Different from the centrality-based measures discussed in Section 3.1 which are defined over graph structure, the textual centrality of a term is the similarity between its textual description and the one for the whole ontology.
sent7: Popularity (Po) The wide use of a term on the Web suggests its importance.
sent8: To measure the popularity of a term, Peroni et al. [12] submit the name of the term as a keyword query to a Web search engine and resort to the number of returned results.
sent9: Zhang et al. [22] calculate the number of websites hosting RDF documents where the term is instantiated.
sent10: Cohesion (Ch) Cheng et al. [4] measure the quality of a summary as a whole.
sent11: Different from diversity-based re-ranking described in Section 3.2 which penalizes an ontology summary where RDF sentences share terms, such a summary will be awarded in [4] as it exhibits cohesion.
sent12: In addition to graph-based and application-specific measures, we briefly review other methods used in the literature.
sent13: Name Simplicity (NS) Peroni et al. [12] emphasize that natural categories or basic objects are good representers of an ontology.
sent14: They propose that a natural category normally has a relatively simple label, and hence they assess the importance of a class by the simplicity of its name.
sent15: A class having compound words in the name will be penalized.
sent16: Textual Centrality (TC) Zhang et al. [24] calculate the textual centrality of a term in an ontology.
sent17: Different from the centrality-based measures discussed in Section 3.1 which are defined over graph structure, the textual centrality of a term is the similarity between its textual description and the one for the whole ontology.
sent18: Popularity (Po) The wide use of a term on the Web suggests its importance.
sent19: To measure the popularity of a term, Peroni et al. [12] submit the name of the term as a keyword query to a Web search engine and resort to the number of returned results.
sent20: Zhang et al. [22] calculate the number of websites hosting RDF documents where the term is instantiated.
sent21: Cohesion (Ch) Cheng et al. [4] measure the quality of a summary as a whole.
sent22: Different from diversity-based re-ranking described in Section 3.2 which penalizes an ontology summary where RDF sentences share terms, such a summary will be awarded in [4] as it exhibits cohesion."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s3,Class Graph,"In order to directly represent semantic relations between classes, Wu et al. [21] presented a graph model where nodes represent classes and directed arcs represent binary relations between classes, which we call a class graph. Figure 2 illustrates a class graph for the ontology in Fig. 1. Note that some axioms (e.g., owl:Restriction) are not covered by this graph representation. As to the relations between classes, if we only allow rdfs:subClassOf, the resulting graph will be a class hierarchy representing subsumption relations, as considered in [12]. More generally, a relation can also be a property defined in the ontology, connecting from its domain (which is a class) to its range (also a class).

Comments. Class graphs are close to human cognition. As classes are firstclass citizens, class graphs are particularly suitable for approaches to ranking classes. However, the expressivity of class graph is limited. It well supports binary relations between classes but not more complex axioms involving multiple classes, e.g., owl:unionOf.

In order to directly represent semantic relations between classes, Wu et al. [21] presented a graph model where nodes represent classes and directed arcs represent binary relations between classes, which we call a class graph. Figure 2 illustrates a class graph for the ontology in Fig. 1. Note that some axioms (e.g., owl:Restriction) are not covered by this graph representation. As to the relations between classes, if we only allow rdfs:subClassOf, the resulting graph will be a class hierarchy representing subsumption relations, as considered in [12]. More generally, a relation can also be a property defined in the ontology, connecting from its domain (which is a class) to its range (also a class).

Comments. Class graphs are close to human cognition. As classes are firstclass citizens, class graphs are particularly suitable for approaches to ranking classes. However, the expressivity of class graph is limited. It well supports binary relations between classes but not more complex axioms involving multiple classes, e.g., owl:unionOf.","[['b20', 'b11'], [], ['b20', 'b11'], []]","[['b20', 'b11'], [], ['b20', 'b11'], []]",4,"sent1: In order to directly represent semantic relations between classes, Wu et al. [21] presented a graph model where nodes represent classes and directed arcs represent binary relations between classes, which we call a class graph.
sent2: Figure 2 illustrates a class graph for the ontology in Fig. 1.
sent3: Note that some axioms (e.g., owl:Restriction) are not covered by this graph representation.
sent4: As to the relations between classes, if we only allow rdfs:subClassOf, the resulting graph will be a class hierarchy representing subsumption relations, as considered in [12].
sent5: More generally, a relation can also be a property defined in the ontology, connecting from its domain (which is a class) to its range (also a class).
sent6: Comments. Class graphs are close to human cognition.
sent7: As classes are firstclass citizens, class graphs are particularly suitable for approaches to ranking classes.
sent8: However, the expressivity of class graph is limited.
sent9: It well supports binary relations between classes but not more complex axioms involving multiple classes, e.g., owl:unionOf.
sent10: In order to directly represent semantic relations between classes, Wu et al. [21] presented a graph model where nodes represent classes and directed arcs represent binary relations between classes, which we call a class graph.
sent11: Figure 2 illustrates a class graph for the ontology in Fig. 1.
sent12: Note that some axioms (e.g., owl:Restriction) are not covered by this graph representation.
sent13: As to the relations between classes, if we only allow rdfs:subClassOf, the resulting graph will be a class hierarchy representing subsumption relations, as considered in [12].
sent14: More generally, a relation can also be a property defined in the ontology, connecting from its domain (which is a class) to its range (also a class).
sent15: Comments. Class graphs are close to human cognition.
sent16: As classes are firstclass citizens, class graphs are particularly suitable for approaches to ranking classes.
sent17: However, the expressivity of class graph is limited.
sent18: It well supports binary relations between classes but not more complex axioms involving multiple classes, e.g., owl:unionOf."
21693765,Graph-based Ontology Summarization: A Survey,Computer Science,https://www.semanticscholar.org/paper/d66d9b9b8d5f1f7e396aef1e67d19da383f6e275,s4,RDF Sentence Graph,"Zhang et al. [23] proposed an RDF sentence graph. An RDF sentence is a subset of RDF triples, and a set of RDF sentences form the finest partition of the triples in an RDF graph such that each blank node only appears in one block. In many cases, an RDF sentence corresponds to an axiom in OWL, since when mapping OWL ontologies into RDF graphs, blank nodes are introduced when an axiom is transformed into multiple RDF triples.  In an RDF sentence graph, nodes represent RDF sentences, which are adjacent if the terms they describe overlap. Figure 3 illustrates an RDF sentence graph for the ontology in Fig. 1 five axioms. Zhang et al. [23] differentiate between two types of arcs, depending on the structural role of the shared terms, which we will not elaborate. Penin et al. [11] further cluster textually similar RDF sentences into topic nodes.

Comments. Compared with RDF triples, there is a better correspondence between RDF sentences and OWL axioms. In an RDF sentence graph, RDF sentences (or roughly speaking, axioms) are first-class citizens, making this model particularly suitable for ranking triples/axioms. However, terms are not explicitly represented in this model, which may limit its application.

Zhang et al. [23] proposed an RDF sentence graph. An RDF sentence is a subset of RDF triples, and a set of RDF sentences form the finest partition of the triples in an RDF graph such that each blank node only appears in one block. In many cases, an RDF sentence corresponds to an axiom in OWL, since when mapping OWL ontologies into RDF graphs, blank nodes are introduced when an axiom is transformed into multiple RDF triples.  In an RDF sentence graph, nodes represent RDF sentences, which are adjacent if the terms they describe overlap. Figure 3 illustrates an RDF sentence graph for the ontology in Fig. 1 five axioms. Zhang et al. [23] differentiate between two types of arcs, depending on the structural role of the shared terms, which we will not elaborate. Penin et al. [11] further cluster textually similar RDF sentences into topic nodes.

Comments. Compared with RDF triples, there is a better correspondence between RDF sentences and OWL axioms. In an RDF sentence graph, RDF sentences (or roughly speaking, axioms) are first-class citizens, making this model particularly suitable for ranking triples/axioms. However, terms are not explicitly represented in this model, which may limit its application.","[['b10', 'b22'], [], ['b10', 'b22'], []]","[['b10', 'b22'], [], ['b10', 'b22'], []]",4,"sent1: Zhang et al. [23] proposed an RDF sentence graph.
sent2: An RDF sentence is a subset of RDF triples, and a set of RDF sentences form the finest partition of the triples in an RDF graph such that each blank node only appears in one block.
sent3: In many cases, an RDF sentence corresponds to an axiom in OWL, since when mapping OWL ontologies into RDF graphs, blank nodes are introduced when an axiom is transformed into multiple RDF triples.
sent4: In an RDF sentence graph, nodes represent RDF sentences, which are adjacent if the terms they describe overlap.
sent5: Figure 3 illustrates an RDF sentence graph for the ontology in Fig. 1 five axioms.
sent6: Zhang et al. [23] differentiate between two types of arcs, depending on the structural role of the shared terms, which we will not elaborate.
sent7: Penin et al. [11] further cluster textually similar RDF sentences into topic nodes.
sent8: Comments. Compared with RDF triples, there is a better correspondence between RDF sentences and OWL axioms.
sent9: In an RDF sentence graph, RDF sentences (or roughly speaking, axioms) are first-class citizens, making this model particularly suitable for ranking triples/axioms.
sent10: However, terms are not explicitly represented in this model, which may limit its application.
sent11: Zhang et al. [23] proposed an RDF sentence graph.
sent12: An RDF sentence is a subset of RDF triples, and a set of RDF sentences form the finest partition of the triples in an RDF graph such that each blank node only appears in one block.
sent13: In many cases, an RDF sentence corresponds to an axiom in OWL, since when mapping OWL ontologies into RDF graphs, blank nodes are introduced when an axiom is transformed into multiple RDF triples.
sent14: In an RDF sentence graph, nodes represent RDF sentences, which are adjacent if the terms they describe overlap.
sent15: Figure 3 illustrates an RDF sentence graph for the ontology in Fig. 1 five axioms.
sent16: Zhang et al. [23] differentiate between two types of arcs, depending on the structural role of the shared terms, which we will not elaborate.
sent17: Penin et al. [11] further cluster textually similar RDF sentences into topic nodes.
sent18: Comments. Compared with RDF triples, there is a better correspondence between RDF sentences and OWL axioms.
sent19: In an RDF sentence graph, RDF sentences (or roughly speaking, axioms) are first-class citizens, making this model particularly suitable for ranking triples/axioms.
sent20: However, terms are not explicitly represented in this model, which may limit its application."
53114081,Overview of CAIL2018: Legal Judgment Prediction Competition,"Computer Science, Law",https://www.semanticscholar.org/paper/6556dcbb5433a01fc75711ccd4ac44522f7d7952,s7,General Architecture,"Pre-processing. For most contestants, they conduct the following pre-processing steps to transform the raw documents into the format which is suitable for their models.

• Word Segmentation. As all the documents are written in Chinese, it is important for the contestants to conduct a high-quality word segmentation. For word segmentation, the contestants usually choose jieba 2 , ICTCLAS 3 , THULAC 4 or other Chinese word segmentation tools.

• Word Embedding. After word segmentation, we need to transform the discrete word symbols into continuous word embeddings.

Generally, the contestants employ word2vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fast-Text (Joulin et al., 2017) to pre-train word embeddings on these criminal cases.

Text Classification Models. After preprocessing, we need to classify these processed fact de-scriptions into corresponding categories. For most contestants, they employ existing neural network based text classification models to extract efficient text features. The most commonly used text classification models are listed as follows:

• Text-CNN (Kim, 2014b): CNN with multiple filter widths.

• LSTM (Hochreiter and Schmidhuber, 1997)) or bidirectional LSTM.

• GRU, Gated Recurrent Unit (Cho et al., 2014).

• HAN, Hierarchical Attention Networks (Yang et al., 2016).

• RCNN, Recurrent Convolutional Neural Networks (Lai et al., 2015).

• DPCNN, Deep Pyramid Convolutional Neural Networks (Johnson and Zhang, 2017).

According to the technical reports of contestants, it has been proven that these neural models can achieve good performance in high-frequency categories.","[[], [], [], ['b20', 'b7', 'b18'], [], ['b10'], ['b3'], ['b2'], ['b27'], ['b12'], ['b6'], []]","[[], [], [], ['b20', 'b7', 'b18'], [], ['b10'], ['b3'], ['b2'], ['b27'], ['b12'], ['b6'], []]",9,"sent1: Pre-processing. For most contestants, they conduct the following pre-processing steps to transform the raw documents into the format which is suitable for their models.
sent2: • Word Segmentation. As all the documents are written in Chinese, it is important for the contestants to conduct a high-quality word segmentation.
sent3: For word segmentation, the contestants usually choose jieba 2 , ICTCLAS 3 , THULAC 4 or other Chinese word segmentation tools.
sent4: • Word Embedding. After word segmentation, we need to transform the discrete word symbols into continuous word embeddings.
sent5: Generally, the contestants employ word2vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), or Fast-Text (Joulin et al., 2017) to pre-train word embeddings on these criminal cases.
sent6: Text Classification Models. After preprocessing, we need to classify these processed fact de-scriptions into corresponding categories.
sent7: For most contestants, they employ existing neural network based text classification models to extract efficient text features.
sent8: The most commonly used text classification models are listed as follows:• Text-CNN (Kim, 2014b): CNN with multiple filter widths.
sent9: • LSTM (Hochreiter and Schmidhuber, 1997)) or bidirectional LSTM.
sent10: • GRU, Gated Recurrent Unit (Cho et al., 2014).
sent11: • HAN, Hierarchical Attention Networks (Yang et al., 2016).
sent12: • RCNN, Recurrent Convolutional Neural Networks (Lai et al., 2015).• DPCNN, Deep Pyramid Convolutional Neural Networks (Johnson and Zhang, 2017).
sent13: According to the technical reports of contestants, it has been proven that these neural models can achieve good performance in high-frequency categories."
56657817,Analysis Methods in Neural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,s2,Methods,"The most common approach for associating neural network components with linguistic properties is to predict such properties from activations of the neural network. Typically, in this approach a neural network model is trained on some task (say, MT) and its weights are frozen. Then, the trained model is used for generating feature representations for another task by running it on a corpus with linguistic annotations and recording the representations (say, hidden state activations). Another classifier is then used for predicting the property of interest (say, part-of-speech [POS] tags). The performance of this classifier is used for evaluating the quality of the generated representations, and by proxy that of the original model. This kind of approach has been used in numerous papers in recent years; see Table SM1 for references. 5 It is referred to by various names, including ''auxiliary prediction tasks'' (Adi et al., 2017b), ''diagnostic classifiers'' (Veldhoen et al., 2016), and ''probing tasks' ' (Conneau et al., 2018).

As an example of this approach, let us walk through an application to analyzing syntax in neural machine translation (NMT) by Shi et al. (2016b). In this work, two NMT models were trained on standard parallel data-English→ French and English→German. The trained models (specifically, the encoders) were run on an annotated corpus and their hidden states were used for training a logistic regression classifier that predicts different syntactic properties. The authors concluded that the NMT encoders learn significant syntactic information at both word level and sentence level. They also compared representations at different encoding layers and found that ''local features are somehow preserved in the lower layer whereas more global, abstract information tends to be stored in the upper layer.'' These results demonstrate the kind of insights that the classification analysis may lead to, especially when comparing different models or model components.

Other methods for finding correspondences between parts of the neural network and certain properties include counting how often attention weights agree with a linguistic property like anaphora resolution (Voita et al., 2018) or directly computing correlations between neural network activations and some property; for example, correlating RNN state activations with depth in a syntactic tree (Qian et al., 2016a) or with Melfrequency cepstral coefficient (MFCC) acoustic features (Wu and King, 2016). Such correspondence may also be computed indirectly. For instance, Alishahi et al. (2017) defined an ABX discrimination task to evaluate how a neural model of speech (grounded in vision) encoded phonology. Given phoneme representations from different layers in their model, and three phonemes, A, B, and X, they compared whether the model representation for X is closer to A or B. This discrimination task enabled them to draw conclusions about which layers encoder phonology better, observing that lower layers generally encode more phonological information.","[['b64', None], ['b51'], ['b73', None, 'b65']]","[['b64', None], ['b51'], ['b73', None, 'b65']]",6,"sent1: The most common approach for associating neural network components with linguistic properties is to predict such properties from activations of the neural network.
sent2: Typically, in this approach a neural network model is trained on some task (say, MT) and its weights are frozen.
sent3: Then, the trained model is used for generating feature representations for another task by running it on a corpus with linguistic annotations and recording the representations (say, hidden state activations).
sent4: Another classifier is then used for predicting the property of interest (say, part-of-speech [POS] tags).
sent5: The performance of this classifier is used for evaluating the quality of the generated representations, and by proxy that of the original model.
sent6: This kind of approach has been used in numerous papers in recent years; see Table SM1 for references.
sent7: 5 It is referred to by various names, including ''auxiliary prediction tasks'' (Adi et al., 2017b), ''diagnostic classifiers'' (Veldhoen et al., 2016), and ''probing tasks' '
sent8: (Conneau et al., 2018).As an example of this approach, let us walk through an application to analyzing syntax in neural machine translation (NMT) by Shi et al. (2016b).
sent9: In this work, two NMT models were trained on standard parallel data-English→ French and English→German.
sent10: The trained models (specifically, the encoders) were run on an annotated corpus and their hidden states were used for training a logistic regression classifier that predicts different syntactic properties.
sent11: The authors concluded that the NMT encoders learn significant syntactic information at both word level and sentence level.
sent12: They also compared representations at different encoding layers and found that ''local features are somehow preserved in the lower layer whereas more global, abstract information tends to be stored in the upper layer.''
sent13: These results demonstrate the kind of insights that the classification analysis may lead to, especially when comparing different models or model components.
sent14: Other methods for finding correspondences between parts of the neural network and certain properties include counting how often attention weights agree with a linguistic property like anaphora resolution (Voita et al., 2018) or directly computing correlations between neural network activations and some property; for example, correlating RNN state activations with depth in a syntactic tree (Qian et al., 2016a) or with Melfrequency cepstral coefficient (MFCC) acoustic features (Wu and King, 2016).
sent15: Such correspondence may also be computed indirectly.
sent16: For instance, Alishahi et al. (2017) defined an ABX discrimination task to evaluate how a neural model of speech (grounded in vision) encoded phonology.
sent17: Given phoneme representations from different layers in their model, and three phonemes, A, B, and X, they compared whether the model representation for X is closer to A or B.
sent18: This discrimination task enabled them to draw conclusions about which layers encoder phonology better, observing that lower layers generally encode more phonological information."
56657817,Analysis Methods in Neural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,s3,Linguistic Phenomena,"Different kinds of linguistic information have been analyzed, ranging from basic properties like sentence length, word position, word presence, or simple word order, to morphological, syntactic, and semantic information. Phonetic/phonemic information, speaker information, and style and accent information have been studied in neural network models for speech, or in joint audio-visual models. See Table SM1 for references.

While it is difficult to synthesize a holistic picture from this diverse body of work, it appears that neural networks are able to learn a substantial amount of information on various linguistic phenomena. These models are especially successful at capturing frequent properties, while some rare properties are more difficult to learn. Linzen et al. (2016), for instance, found that long short-term memory (LSTM) language models are able to capture subject-verb agreement in many common cases, while direct supervision is required for solving harder cases.

Another theme that emerges in several studies is the hierarchical nature of the learned representations. We have already mentioned such findings regarding NMT (Shi et al., 2016b) and a visually grounded speech model (Alishahi et al., 2017). Hierarchical representations of syntax were also reported to emerge in other RNN models (Blevins et al., 2018).

Finally, a couple of papers discovered that models trained with latent trees perform better on natural language inference (NLI) (Williams et al., 2018;Maillard and Clark, 2018) than ones trained with linguistically annotated trees. Moreover, the trees in these models do not resemble syntactic trees corresponding to known linguistic theories, which casts doubts on the importance of syntax-learning in the underlying neural network. 6","[[], ['b36'], ['b51', None], ['b72', None]]","[[], ['b36'], ['b51', None], ['b72', None]]",5,"sent1: Different kinds of linguistic information have been analyzed, ranging from basic properties like sentence length, word position, word presence, or simple word order, to morphological, syntactic, and semantic information.
sent2: Phonetic/phonemic information, speaker information, and style and accent information have been studied in neural network models for speech, or in joint audio-visual models.
sent3: See Table SM1 for references. While it is difficult to synthesize a holistic picture from this diverse body of work, it appears that neural networks are able to learn a substantial amount of information on various linguistic phenomena.
sent4: These models are especially successful at capturing frequent properties, while some rare properties are more difficult to learn.
sent5: Linzen et al. (2016), for instance, found that long short-term memory (LSTM) language models are able to capture subject-verb agreement in many common cases, while direct supervision is required for solving harder cases.
sent6: Another theme that emerges in several studies is the hierarchical nature of the learned representations.
sent7: We have already mentioned such findings regarding NMT (Shi et al., 2016b) and a visually grounded speech model (Alishahi et al., 2017).
sent8: Hierarchical representations of syntax were also reported to emerge in other RNN models (Blevins et al., 2018).
sent9: Finally, a couple of papers discovered that models trained with latent trees perform better on natural language inference (NLI) (Williams et al., 2018;Maillard and Clark, 2018) than ones trained with linguistically annotated trees.
sent10: Moreover, the trees in these models do not resemble syntactic trees corresponding to known linguistic theories, which casts doubts on the importance of syntax-learning in the underlying neural network.
sent11: 6"
56657817,Analysis Methods in Neural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,s6,Visualization,"Visualization is a valuable tool for analyzing neural networks in the language domain and beyond. Early work visualized hidden unit activations in RNNs trained on an artificial language modeling task, and observed how they correspond to certain grammatical relations such as agreement (Elman, 1991). Much recent work has focused on visualizing activations on specific examples in modern neural networks for language (Karpathy et al., 2015;Kádár et al., 2017;Qian et al., 2016a;Liu et al., 2018) and speech (Wu and King, 2016;Nagamine et al., 2015;Wang et al., 2017b). Figure 1 shows an example visualization of a neuron that captures position of words in a sentence. The heatmap uses blue and red colors for negative and positive activation values, respectively, enabling the user to quickly grasp the function of this neuron.

The attention mechanism that originated in work on NMT (Bahdanau et al., 2014) also lends itself to a natural visualization. The alignments obtained via different attention mechanisms have produced visualizations ranging from tasks like NLI (Rocktäschel et al., 2016;Yin et al., 2016), summarization (Rush et al., 2015), MT post-editing (Jauregi Unanue et al., 2018), and morphological inflection (Aharoni and Goldberg, 2017) to matching users on social media (Tay et al., 2018). Figure 2 reproduces a visualization of attention alignments from the original work by Bahdanau et al. Here grayscale values correspond to the weight of the attention between words in an English source sentence (columns) and its French translation (rows). As Bahdanau et al. explain, this visualization demonstrates that the NMT model learned a soft alignment between source and target words. Some aspects of word order may also be  Godin et al., 2018). Saliency can also be computed with respect to intermediate values, rather than input features (Ghaeini et al., 2018). 7 An instructive visualization technique is to cluster neural network activations and compare them to some linguistic property. Early work clustered RNN activations, showing that they organize in lexical categories (Elman, 1989(Elman, , 1990. Similar techniques have been followed by others. Recent examples include clustering of sentence embeddings in an RNN encoder trained in a multitask learning scenario (Brunner et al., 2017), and phoneme clusters in a joint audio-visual RNN model (Alishahi et al., 2017).

A few online tools for visualizing neural networks have recently become available. LSTMVis (Strobelt et al., 2018b) visualizes RNN activations, focusing on tracing hidden state dynamics. 8 Seq2Seq-Vis (Strobelt et al., 2018a) visualizes different modules in attention-based seq2seq models, with the goal of examining model decisions and testing alternative decisions. Another tool focused on comparing attention alignments was proposed by Rikters (2018). It also provides translation confidence scores based on the distribution of attention weights. NeuroX (Dalvi et al., 2019b) is a tool for finding and analyzing individual neurons, focusing on machine translation.

Evaluation As in much work on interpretability, evaluating visualization quality is difficult and often limited to qualitative examples. A few notable exceptions report human evaluations of visualization quality. Singh et al. (2018) showed human raters hierarchical clusterings of input words generated by two interpretation methods, and asked them to evaluate which method is more accurate, or in which method they trust more. Others reported human evaluations for attention visualization in conversation modeling (Freeman et al., 2018) and medical code prediction tasks (Mullenbach et al., 2018).

The availability of open-source tools of the sort described above will hopefully encourage users to utilize visualization in their regular research and development cycle. However, it remains to be seen how useful visualizations turn out to be.","[['b73', None, 'b70', 'b38'], ['b12', 'b61', 'b42', None, 'b75', 'b2', 'b15'], ['b54', 'b55'], ['b3', 'b53', None], []]","[['b73', None, 'b70', 'b38'], ['b12', 'b61', 'b42', None, 'b75', 'b2', 'b15'], ['b54', 'b55'], ['b3', 'b53', None], []]",16,"sent1: Visualization is a valuable tool for analyzing neural networks in the language domain and beyond.
sent2: Early work visualized hidden unit activations in RNNs trained on an artificial language modeling task, and observed how they correspond to certain grammatical relations such as agreement (Elman, 1991).
sent3: Much recent work has focused on visualizing activations on specific examples in modern neural networks for language (Karpathy et al., 2015;Kádár et al., 2017;Qian et al., 2016a;Liu et al., 2018) and speech (Wu and King, 2016;Nagamine et al., 2015;Wang et al., 2017b).
sent4: Figure 1 shows an example visualization of a neuron that captures position of words in a sentence.
sent5: The heatmap uses blue and red colors for negative and positive activation values, respectively, enabling the user to quickly grasp the function of this neuron.
sent6: The attention mechanism that originated in work on NMT (Bahdanau et al., 2014) also lends itself to a natural visualization.
sent7: The alignments obtained via different attention mechanisms have produced visualizations ranging from tasks like NLI (Rocktäschel et al., 2016;Yin et al., 2016), summarization (Rush et al., 2015), MT post-editing (Jauregi Unanue et al., 2018), and morphological inflection (Aharoni and Goldberg, 2017) to matching users on social media (Tay et al., 2018).
sent8: Figure 2 reproduces a visualization of attention alignments from the original work by Bahdanau et al.
sent9: Here grayscale values correspond to the weight of the attention between words in an English source sentence (columns) and its French translation (rows).
sent10: As Bahdanau et al. explain, this visualization demonstrates that the NMT model learned a soft alignment between source and target words.
sent11: Some aspects of word order may also be  Godin et al., 2018).
sent12: Saliency can also be computed with respect to intermediate values, rather than input features (Ghaeini et al., 2018). 7
sent13: An instructive visualization technique is to cluster neural network activations and compare them to some linguistic property.
sent14: Early work clustered RNN activations, showing that they organize in lexical categories (Elman, 1989(Elman, , 1990. Similar techniques have been followed by others. Recent examples include clustering of sentence embeddings in an RNN encoder trained in a multitask learning scenario (Brunner et al., 2017), and phoneme clusters in a joint audio-visual RNN model (Alishahi et al., 2017).
sent15: A few online tools for visualizing neural networks have recently become available.
sent16: LSTMVis (Strobelt et al., 2018b) visualizes RNN activations, focusing on tracing hidden state dynamics.
sent17: 8 Seq2Seq-Vis (Strobelt et al., 2018a) visualizes different modules in attention-based seq2seq models, with the goal of examining model decisions and testing alternative decisions.
sent18: Another tool focused on comparing attention alignments was proposed by Rikters (2018).
sent19: It also provides translation confidence scores based on the distribution of attention weights.
sent20: NeuroX (Dalvi et al., 2019b) is a tool for finding and analyzing individual neurons, focusing on machine translation.
sent21: Evaluation As in much work on interpretability, evaluating visualization quality is difficult and often limited to qualitative examples.
sent22: A few notable exceptions report human evaluations of visualization quality.
sent23: Singh et al. (2018) showed human raters hierarchical clusterings of input words generated by two interpretation methods, and asked them to evaluate which method is more accurate, or in which method they trust more.
sent24: Others reported human evaluations for attention visualization in conversation modeling (Freeman et al., 2018) and medical code prediction tasks (Mullenbach et al., 2018).
sent25: The availability of open-source tools of the sort described above will hopefully encourage users to utilize visualization in their regular research and development cycle.
sent26: However, it remains to be seen how useful visualizations turn out to be."
56657817,Analysis Methods in Neural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,s7,Challenge Sets,"The majority of benchmark datasets in NLP are drawn from text corpora, reflecting a natural frequency distribution of language phenomena. While useful in practice for evaluating system performance in the average case, such datasets may fail to capture a wide range of phenomena. An alternative evaluation framework consists of challenge sets, also known as test suites, which have been used in NLP for a long time (Lehmann et al., 1996), especially for evaluating MT systems (King and Falkedal, 1990;Isahara, 1995;Koh et al., 2001). Lehmann et al. (1996) noted several key properties of test suites: systematicity, control over data, inclusion of negative data, and exhaustivity. They contrasted such datasets with test corpora, ''whose main advantage is that they reflect naturally occurring data.'' This idea underlines much of the work on challenge sets and is echoed in more recent work . For instance, Cooper et al. (1996) constructed a semantic test suite that targets phenomena as diverse as quantifiers, plurals, anaphora, ellipsis, adjectival properties, and so on.

After a hiatus of a couple of decades, 9 challenge sets have recently gained renewed popularity in the NLP community. In this section, we include datasets used for evaluating neural network models that diverge from the common averagecase evaluation. Many of them share some of the properties noted by Lehmann et al. (1996), although negative examples (ill-formed data) are typically less utilized. The challenge datasets can be categorized along the following criteria: the task they seek to evaluate, the linguistic phenomena they aim to study, the language(s) they target, their size, their method of construction, and how performance is evaluated. 10 Table SM2 (in the supplementary materials) categorizes many recent challenge sets along these criteria. Below we discuss common trends along these lines.","[['b26', 'b30', None], ['b30', None]]","[['b26', 'b30', None], ['b30', None]]",5,"sent1: The majority of benchmark datasets in NLP are drawn from text corpora, reflecting a natural frequency distribution of language phenomena.
sent2: While useful in practice for evaluating system performance in the average case, such datasets may fail to capture a wide range of phenomena.
sent3: An alternative evaluation framework consists of challenge sets, also known as test suites, which have been used in NLP for a long time (Lehmann et al., 1996), especially for evaluating MT systems (King and Falkedal, 1990;Isahara, 1995;Koh et al., 2001).
sent4: Lehmann et al. (1996) noted several key properties of test suites: systematicity, control over data, inclusion of negative data, and exhaustivity.
sent5: They contrasted such datasets with test corpora, ''whose main advantage is that they reflect naturally occurring data.''
sent6: This idea underlines much of the work on challenge sets and is echoed in more recent work .
sent7: For instance, Cooper et al. (1996) constructed a semantic test suite that targets phenomena as diverse as quantifiers, plurals, anaphora, ellipsis, adjectival properties, and so on.
sent8: After a hiatus of a couple of decades, 9 challenge sets have recently gained renewed popularity in the NLP community.
sent9: In this section, we include datasets used for evaluating neural network models that diverge from the common averagecase evaluation.
sent10: Many of them share some of the properties noted by Lehmann et al. (1996), although negative examples (ill-formed data) are typically less utilized.
sent11: The challenge datasets can be categorized along the following criteria: the task they seek to evaluate, the linguistic phenomena they aim to study, the language(s) they target, their size, their method of construction, and how performance is evaluated.
sent12: 10 Table SM2 (in the supplementary materials) categorizes many recent challenge sets along these criteria.
sent13: Below we discuss common trends along these lines."
56657817,Analysis Methods in Neural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,s8,Task,"By far, the most targeted tasks in challenge sets are NLI and MT. This can partly be explained by the popularity of these tasks and the prevalence of neural models proposed for solving them. Perhaps more importantly, tasks like NLI and MT arguably require inferences at various linguistic levels, making the challenge set evaluation especially attractive. Still, other high-level tasks like reading comprehension or question answering have not received as much attention, and may also benefit from the careful construction of challenge sets.

A significant body of work aims to evaluate the quality of embedding models by correlating the similarity they induce on word or sentence pairs with human similarity judgments. Datasets containing such similarity scores are often used 9 One could speculate that their decrease in popularity can be attributed to the rise of large-scale quantitative evaluation of statistical NLP systems. 10 Another typology of evaluation protocols was put forth by Burlot and Yvon (2017). Their criteria are partially overlapping with ours, although they did not provide a comprehensive categorization like the one compiled here. to evaluate word embeddings (Finkelstein et al., 2002;Bruni et al., 2012;Hill et al., 2015, inter alia) or sentence embeddings; see the many shared tasks on semantic textual similarity in SemEval (Cer et al., 2017, and previous editions). Many of these datasets evaluate similarity at a coarse-grained level, but some provide a more fine-grained evaluation of similarity or relatedness. For example, some datasets are dedicated for specific word classes such as verbs (Gerz et al., 2016) or rare words (Luong et al., 2013), or for evaluating compositional knowledge in sentence embeddings (Marelli et al., 2014). Multilingual and cross-lingual versions have also been collected (Leviant and Reichart, 2015;Cer et al., 2017). Although these datasets are widely used, this kind of evaluation has been criticized for its subjectivity and questionable correlation with downstream performance (Faruqui et al., 2016).","[[], ['b40', None, 'b10', 'b32']]","[[], ['b40', None, 'b10', 'b32']]",4,"sent1: By far, the most targeted tasks in challenge sets are NLI and MT.
sent2: This can partly be explained by the popularity of these tasks and the prevalence of neural models proposed for solving them.
sent3: Perhaps more importantly, tasks like NLI and MT arguably require inferences at various linguistic levels, making the challenge set evaluation especially attractive.
sent4: Still, other high-level tasks like reading comprehension or question answering have not received as much attention, and may also benefit from the careful construction of challenge sets.
sent5: A significant body of work aims to evaluate the quality of embedding models by correlating the similarity they induce on word or sentence pairs with human similarity judgments.
sent6: Datasets containing such similarity scores are often used 9 One could speculate that their decrease in popularity can be attributed to the rise of large-scale quantitative evaluation of statistical NLP systems.
sent7: 10 Another typology of evaluation protocols was put forth by Burlot and Yvon (2017).
sent8: Their criteria are partially overlapping with ours, although they did not provide a comprehensive categorization like the one compiled here.
sent9: to evaluate word embeddings (Finkelstein et al., 2002;Bruni et al., 2012;Hill et al., 2015, inter alia) or sentence embeddings; see the many shared tasks on semantic textual similarity in SemEval (Cer et al., 2017, and previous editions).
sent10: Many of these datasets evaluate similarity at a coarse-grained level, but some provide a more fine-grained evaluation of similarity or relatedness.
sent11: For example, some datasets are dedicated for specific word classes such as verbs (Gerz et al., 2016) or rare words (Luong et al., 2013), or for evaluating compositional knowledge in sentence embeddings (Marelli et al., 2014).
sent12: Multilingual and cross-lingual versions have also been collected (Leviant and Reichart, 2015;Cer et al., 2017).
sent13: Although these datasets are widely used, this kind of evaluation has been criticized for its subjectivity and questionable correlation with downstream performance (Faruqui et al., 2016)."
56657817,Analysis Methods in Neural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,s9,Linguistic Phenomena,"One of the primary goals of challenge sets is to evaluate models on their ability to handle specific linguistic phenomena. While earlier studies emphasized exhaustivity (Cooper et al., 1996;Lehmann et al., 1996), recent ones tend to focus on a few properties of interest. For example, Sennrich (2017) introduced a challenge set for MT evaluation focusing on five properties: subject-verb agreement, noun phrase agreement, verb-particle constructions, polarity, and transliteration. Slightly more elaborated is an MT challenge set for morphology, including 14 morphological properties (Burlot and Yvon, 2017). See Table SM2 for references to datasets targeting other phenomena.

Other challenge sets cover a more diverse range of linguistic properties, in the spirit of some of the earlier work. For instance, extending the categories in Cooper et al. (1996), the GLUE analysis set for NLI covers more than 30 phenomena in four coarse categories (lexical semantics, predicate-argument structure, logic, and knowledge). In MT evaluation, Burchardt et al. (2017) reported results using a large test suite covering 120 phenomena, partly based on Lehmann et al. (1996). 11 Isabelle et al. (2017) and Isabelle and Kuhn (2018) prepared challenge sets for MT evaluation covering fine-grained phenomena at morpho-syntactic, syntactic, and lexical levels.

Generally, datasets that are constructed programmatically tend to cover less fine-grained linguistic properties, while manually constructed datasets represent more diverse phenomena.","[[None, 'b30'], [None, 'b30'], []]","[[None, 'b30'], [None, 'b30'], []]",4,"sent1: One of the primary goals of challenge sets is to evaluate models on their ability to handle specific linguistic phenomena.
sent2: While earlier studies emphasized exhaustivity (Cooper et al., 1996;Lehmann et al., 1996), recent ones tend to focus on a few properties of interest.
sent3: For example, Sennrich (2017) introduced a challenge set for MT evaluation focusing on five properties: subject-verb agreement, noun phrase agreement, verb-particle constructions, polarity, and transliteration.
sent4: Slightly more elaborated is an MT challenge set for morphology, including 14 morphological properties (Burlot and Yvon, 2017).
sent5: See Table SM2 for references to datasets targeting other phenomena.
sent6: Other challenge sets cover a more diverse range of linguistic properties, in the spirit of some of the earlier work.
sent7: For instance, extending the categories in Cooper et al. (1996), the GLUE analysis set for NLI covers more than 30 phenomena in four coarse categories (lexical semantics, predicate-argument structure, logic, and knowledge).
sent8: In MT evaluation, Burchardt et al. (2017) reported results using a large test suite covering 120 phenomena, partly based on Lehmann et al. (1996). 11 Isabelle et al. (2017) and Isabelle and Kuhn (2018) prepared challenge sets for MT evaluation covering fine-grained phenomena at morpho-syntactic, syntactic, and lexical levels.
sent9: Generally, datasets that are constructed programmatically tend to cover less fine-grained linguistic properties, while manually constructed datasets represent more diverse phenomena."
56657817,Analysis Methods in Neural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,s13,Evaluation,"Systems are typically evaluated by their performance on the challenge set examples, either with the same metric used for evaluating the system in the first place, or via a proxy, as in the contrastive pairs evaluation of Sennrich (2017). Automatic evaluation metrics are cheap to obtain and can be calculated on a large scale. However, they may miss certain aspects. Thus a few studies report human evaluation on their challenge sets, such as in MT (Isabelle et al., 2017;Burchardt et al., 2017).

We note here also that judging the quality of a model by its performance on a challenge set can be tricky. Some authors emphasize their wish to test systems on extreme or difficult cases, ''beyond normal operational capacity'' (Naik et al., 2018). However, whether one should expect systems to perform well on specially chosen cases (as opposed to the average case) may depend on one's goals. To put results in perspective, one may compare model performance to human performance on the same task (Gulordava et al., 2018).","[[None, 'b48'], ['b20', None]]","[[None, 'b48'], ['b20', None]]",4,"sent1: Systems are typically evaluated by their performance on the challenge set examples, either with the same metric used for evaluating the system in the first place, or via a proxy, as in the contrastive pairs evaluation of Sennrich (2017).
sent2: Automatic evaluation metrics are cheap to obtain and can be calculated on a large scale.
sent3: However, they may miss certain aspects.
sent4: Thus a few studies report human evaluation on their challenge sets, such as in MT (Isabelle et al., 2017;Burchardt et al., 2017).
sent5: We note here also that judging the quality of a model by its performance on a challenge set can be tricky.
sent6: Some authors emphasize their wish to test systems on extreme or difficult cases, ''beyond normal operational capacity'' (Naik et al., 2018).
sent7: However, whether one should expect systems to perform well on specially chosen cases (as opposed to the average case) may depend on one's goals.
sent8: To put results in perspective, one may compare model performance to human performance on the same task (Gulordava et al., 2018)."
56657817,Analysis Methods in Neural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,s15,Adversary's Knowledge,"Adversarial examples can be generated using access to model parameters, also known as white-box attacks, or without such access, with black-box attacks (Papernot et al., 2016a(Papernot et al., , 2017Narodytska and Kasiviswanathan, 2017;Liu et al., 2017).

White-box attacks are difficult to adapt to the text world as they typically require computing gradients with respect to the input, which would be discrete in the text case. One option is to compute gradients with respect to the input word embeddings, and perturb the embeddings. Since this may result in a vector that does not correspond to any word, one could search for the closest word embedding in a given dictionary (Papernot et al., 2016b); Cheng et al. (2018) extended this idea to seq2seq models. Others computed gradients with respect to input word embeddings to identify and rank words to be modified (Samanta and Mehta, 2017;Liang et al., 2018). Ebrahimi et al. (2018b) developed an alternative method by representing text edit operations in vector space (e.g., a binary vector specifying which characters in a word would be changed) and approximating the change in loss with the derivative along this vector.

Given the difficulty in generating white-box adversarial examples for text, much research has been devoted to black-box examples. Often, the adversarial examples are inspired by text edits that are thought to be natural or commonly generated by humans, such as typos, misspellings, and so 14 These criteria are partly taken from Yuan et al. (2017), where a more elaborate taxonomy is laid out. At present, though, the work on adversarial examples in NLP is more limited than in computer vision, so our criteria will suffice. on (Sakaguchi et al., 2017;Heigold et al., 2018;Belinkov and Bisk, 2018). Gao et al. (2018) defined scoring functions to identify tokens to modify. Their functions do not require access to model internals, but they do require the model prediction score. After identifying the important tokens, they modify characters with common edit operations. Zhao et al. (2018c) used generative adversarial networks (GANs)  to minimize the distance between latent representations of input and adversarial examples, and performed perturbations in latent space. Since the latent representations do not need to come from the attacked model, this is a black-box attack.

Finally, Alzantot et al. (2018) developed an interesting population-based genetic algorithm for crafting adversarial examples for text classification by maintaining a population of modifications of the original sentence and evaluating fitness of modifications at each generation. They do not require access to model parameters, but do use prediction scores. A similar idea was proposed by Kuleshov et al. (2018).","[['b39', None], [None, 'b35', 'b44'], ['b43', 'b82', 'b76', 'b7', None], ['b28']]","[['b39', None], [None, 'b35', 'b44'], ['b43', 'b82', 'b76', 'b7', None], ['b28']]",11,"sent1: Adversarial examples can be generated using access to model parameters, also known as white-box attacks, or without such access, with black-box attacks (Papernot et al., 2016a(Papernot et al., , 2017Narodytska and Kasiviswanathan, 2017;Liu et al., 2017).White-box attacks are difficult to adapt to the text world as they typically require computing gradients with respect to the input, which would be discrete in the text case.
sent2: One option is to compute gradients with respect to the input word embeddings, and perturb the embeddings.
sent3: Since this may result in a vector that does not correspond to any word, one could search for the closest word embedding in a given dictionary (Papernot et al., 2016b); Cheng et al. (2018) extended this idea to seq2seq models.
sent4: Others computed gradients with respect to input word embeddings to identify and rank words to be modified (Samanta and Mehta, 2017;Liang et al., 2018).
sent5: Ebrahimi et al. (2018b) developed an alternative method by representing text edit operations in vector space (e.g., a binary vector specifying which characters in a word would be changed) and approximating the change in loss with the derivative along this vector.
sent6: Given the difficulty in generating white-box adversarial examples for text, much research has been devoted to black-box examples.
sent7: Often, the adversarial examples are inspired by text edits that are thought to be natural or commonly generated by humans, such as typos, misspellings, and so 14 These criteria are partly taken from Yuan et al. (2017), where a more elaborate taxonomy is laid out.
sent8: At present, though, the work on adversarial examples in NLP is more limited than in computer vision, so our criteria will suffice.
sent9: on (Sakaguchi et al., 2017;Heigold et al., 2018;Belinkov and Bisk, 2018).
sent10: Gao et al. (2018) defined scoring functions to identify tokens to modify.
sent11: Their functions do not require access to model internals, but they do require the model prediction score.
sent12: After identifying the important tokens, they modify characters with common edit operations.
sent13: Zhao et al. (2018c) used generative adversarial networks (GANs)  to minimize the distance between latent representations of input and adversarial examples, and performed perturbations in latent space.
sent14: Since the latent representations do not need to come from the attacked model, this is a black-box attack.
sent15: Finally, Alzantot et al. (2018) developed an interesting population-based genetic algorithm for crafting adversarial examples for text classification by maintaining a population of modifications of the original sentence and evaluating fitness of modifications at each generation.
sent16: They do not require access to model parameters, but do use prediction scores.
sent17: A similar idea was proposed by Kuleshov et al. (2018)."
56657817,Analysis Methods in Neural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,s16,Attack Specificity,"Adversarial attacks can be classified to targeted vs. non-targeted attacks (Yuan et al., 2017). A targeted attack specifies a specific false class, l , while a nontargeted attack cares only that the predicted class is wrong, l = l. Targeted attacks are more difficult to generate, as they typically require knowledge of model parameters; that is, they are white-box attacks. This might explain why the majority of adversarial examples in NLP are nontargeted (see Table SM3). A few targeted attacks include Liang et al. (2018), which specified a desired class to fool a text classifier, and Chen et al. (2018a), which specified words or captions to generate in an image captioning model. Others targeted specific words to omit, replace, or include when attacking seq2seq models (Cheng et al., 2018;Ebrahimi et al., 2018a).

Methods for generating targeted attacks in NLP could possibly take more inspiration from adversarial attacks in other fields. For instance, in attacking malware detection systems, several studies developed targeted attacks in a blackbox scenario (Yuan et al., 2017). A black-box targeted attack for MT was proposed by Zhao et al. (2018c), who used GANs to search for attacks on Google's MT system after mapping sentences into continuous space with adversarially regularized autoencoders (Zhao et al., 2018b).","[['b76', 'b35', None], ['b76', 'b81', 'b82']]","[['b76', 'b35', None], ['b76', 'b81', 'b82']]",6,"sent1: Adversarial attacks can be classified to targeted vs. non-targeted attacks (Yuan et al., 2017).
sent2: A targeted attack specifies a specific false class, l , while a nontargeted attack cares only that the predicted class is wrong, l = l. Targeted attacks are more difficult to generate, as they typically require knowledge of model parameters; that is, they are white-box attacks.
sent3: This might explain why the majority of adversarial examples in NLP are nontargeted (see Table SM3).
sent4: A few targeted attacks include Liang et al. (2018), which specified a desired class to fool a text classifier, and Chen et al. (2018a), which specified words or captions to generate in an image captioning model.
sent5: Others targeted specific words to omit, replace, or include when attacking seq2seq models (Cheng et al., 2018;Ebrahimi et al., 2018a).Methods for generating targeted attacks in NLP could possibly take more inspiration from adversarial attacks in other fields.
sent6: For instance, in attacking malware detection systems, several studies developed targeted attacks in a blackbox scenario (Yuan et al., 2017).
sent7: A black-box targeted attack for MT was proposed by Zhao et al. (2018c), who used GANs to search for attacks on Google's MT system after mapping sentences into continuous space with adversarially regularized autoencoders (Zhao et al., 2018b)."
56657817,Analysis Methods in Neural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,s19,Coherence and Perturbation Measurement,"In adversarial image examples, it is fairly straightforward to measure the perturbation, either by measuring distance in pixel space, say ||x − x || under some norm, or with alternative measures that are better correlated with human perception (Rozsa et al., 2016). It is also visually compelling to present an adversarial image with imperceptible difference from its source image.

In the text domain, measuring distance is not as straightforward, and even small changes to the text may be perceptible by humans. Thus, evaluation of attacks is fairly tricky. Some studies imposed constraints on adversarial examples to have a small number of edit operations (Gao et al., 2018). Others ensured syntactic or semantic coherence in different ways, such as filtering replacements by word similarity or sentence similarity (Alzantot et al., 2018;Kuleshov et al., 2018), or by using synonyms and other word lists (Samanta and Mehta, 2017;Yang et al., 2018). Some reported whether a human can classify the adversarial example correctly (Yang et al., 2018), but this does not indicate how perceptible the changes are. More informative human studies evaluate grammaticality or similarity of the adversarial examples to the original ones (Zhao et al., 2018c;Alzantot et al., 2018). Given the inherent difficulty in generating imperceptible changes in text, more such evaluations are needed.","[[None], ['b82', 'b28', 'b74', 'b7', None, 'b44']]","[[None], ['b82', 'b28', 'b74', 'b7', None, 'b44']]",7,"sent1: In adversarial image examples, it is fairly straightforward to measure the perturbation, either by measuring distance in pixel space, say ||x − x || under some norm, or with alternative measures that are better correlated with human perception (Rozsa et al., 2016).
sent2: It is also visually compelling to present an adversarial image with imperceptible difference from its source image.
sent3: In the text domain, measuring distance is not as straightforward, and even small changes to the text may be perceptible by humans.
sent4: Thus, evaluation of attacks is fairly tricky.
sent5: Some studies imposed constraints on adversarial examples to have a small number of edit operations (Gao et al., 2018).
sent6: Others ensured syntactic or semantic coherence in different ways, such as filtering replacements by word similarity or sentence similarity (Alzantot et al., 2018;Kuleshov et al., 2018), or by using synonyms and other word lists (Samanta and Mehta, 2017;Yang et al., 2018).
sent7: Some reported whether a human can classify the adversarial example correctly (Yang et al., 2018), but this does not indicate how perceptible the changes are.
sent8: More informative human studies evaluate grammaticality or similarity of the adversarial examples to the original ones (Zhao et al., 2018c;Alzantot et al., 2018).
sent9: Given the inherent difficulty in generating imperceptible changes in text, more such evaluations are needed."
56657817,Analysis Methods in Neural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,s20,Explaining Predictions,"Explaining specific predictions is recognized as a desideratum in intereptability work (Lipton, 2016), argued to increase the accountability of machine learning systems (Doshi-Velez et al., 2017). However, explaining why a deep, highly non-linear neural network makes a certain prediction is not trivial. One solution is to ask the model to generate explanations along with its primary prediction (Zaidan et al., 2007;Zhang et al., 2016), 15 but this approach requires manual annotations of explanations, which may be hard to collect.

An alternative approach is to use parts of the input as explanations. For example, Lei et al. (2016) defined a generator that learns a distribution over text fragments as candidate rationales for justifying predictions, evaluated on sentiment analysis. Alvarez-Melis and Jaakkola (2017) discovered input-output associations in a sequence-to-sequence learning scenario, by perturbing the input and finding the most relevant associations. Gupta and Schütze (2018) inspected how information is accumulated in RNNs towards a prediction, and associated peaks in prediction scores with important input segments. As these methods use input segments to explain predictions, they do not shed much light on the internal computations that take place in the network.

At present, despite the recognized importance for interpretability, our ability to explain predictions of neural networks in NLP is still limited.","[[None, 'b79', 'b77', 'b37'], ['b31', 'b22'], []]","[[None, 'b79', 'b77', 'b37'], ['b31', 'b22'], []]",6,"sent1: Explaining specific predictions is recognized as a desideratum in intereptability work (Lipton, 2016), argued to increase the accountability of machine learning systems (Doshi-Velez et al., 2017).
sent2: However, explaining why a deep, highly non-linear neural network makes a certain prediction is not trivial.
sent3: One solution is to ask the model to generate explanations along with its primary prediction (Zaidan et al., 2007;Zhang et al., 2016), 15 but this approach requires manual annotations of explanations, which may be hard to collect.
sent4: An alternative approach is to use parts of the input as explanations.
sent5: For example, Lei et al. (2016) defined a generator that learns a distribution over text fragments as candidate rationales for justifying predictions, evaluated on sentiment analysis.
sent6: Alvarez-Melis and Jaakkola (2017) discovered input-output associations in a sequence-to-sequence learning scenario, by perturbing the input and finding the most relevant associations.
sent7: Gupta and Schütze (2018) inspected how information is accumulated in RNNs towards a prediction, and associated peaks in prediction scores with important input segments.
sent8: As these methods use input segments to explain predictions, they do not shed much light on the internal computations that take place in the network.
sent9: At present, despite the recognized importance for interpretability, our ability to explain predictions of neural networks in NLP is still limited."
56657817,Analysis Methods in Neural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,s21,Other Methods,"We briefly mention here several analysis methods that do not fall neatly into the previous sections.

A number of studies evaluated the effect of erasing or masking certain neural network components, such as word embedding dimensions, hidden units, or even full words (Li et al., 2016b;Feng et al., 2018;Khandelwal et al., 2018;Bau et al., 2018). For example, Li et al. (2016b) erased specific dimensions in word embeddings or hidden states and computed the change in probability assigned to different labels. Their experiments revealed interesting differences between word embedding models, where in some models information is more focused in individual dimensions. They also found that information is more distributed in hidden layers than in the input layer, and erased entire words to find important words in a sentiment analysis task.

Several studies conducted behavioral experiments to interpret word embeddings by defining intrusion tasks, where humans need to identify an intruder word, chosen based on difference in word embedding dimensions (Murphy et al., 2012;Fyshe et al., 2015;Faruqui et al., 2015). 16 In this kind of work, a word embedding model may be deemed more interpretable if humans are better able to identify the intruding words. Since the evaluation is costly for high-dimensional representations, alternative automatic metrics were considered (Park et al., 2017;Senel et al., 2018).

A long tradition in work on neural networks is to evaluate and analyze their ability to learn different formal languages (Das et al., 1992;Casey, 1996;Gers and Schmidhuber, 2001;Bodén and Wiles, 2002;Chalup and Blair, 2003). This trend continues today, with research into modern architectures and what formal languages they can learn (Weiss et al., 2018;Bernardy, 2018;Suzgun et al., 2019), or the formal properties they possess (Chen et al., 2018b).","[[], [None, 'b34'], ['b4', 'b47', None], ['b58', None, 'b9', 'b71']]","[[], [None, 'b34'], ['b4', 'b47', None], ['b58', None, 'b9', 'b71']]",9,"sent1: We briefly mention here several analysis methods that do not fall neatly into the previous sections.
sent2: A number of studies evaluated the effect of erasing or masking certain neural network components, such as word embedding dimensions, hidden units, or even full words (Li et al., 2016b;Feng et al., 2018;Khandelwal et al., 2018;Bau et al., 2018).
sent3: For example, Li et al. (2016b) erased specific dimensions in word embeddings or hidden states and computed the change in probability assigned to different labels.
sent4: Their experiments revealed interesting differences between word embedding models, where in some models information is more focused in individual dimensions.
sent5: They also found that information is more distributed in hidden layers than in the input layer, and erased entire words to find important words in a sentiment analysis task.
sent6: Several studies conducted behavioral experiments to interpret word embeddings by defining intrusion tasks, where humans need to identify an intruder word, chosen based on difference in word embedding dimensions (Murphy et al., 2012;Fyshe et al., 2015;Faruqui et al., 2015).
sent7: 16 In this kind of work, a word embedding model may be deemed more interpretable if humans are better able to identify the intruding words.
sent8: Since the evaluation is costly for high-dimensional representations, alternative automatic metrics were considered (Park et al., 2017;Senel et al., 2018).
sent9: A long tradition in work on neural networks is to evaluate and analyze their ability to learn different formal languages (Das et al., 1992;Casey, 1996;Gers and Schmidhuber, 2001;Bodén and Wiles, 2002;Chalup and Blair, 2003).
sent10: This trend continues today, with research into modern architectures and what formal languages they can learn (Weiss et al., 2018;Bernardy, 2018;Suzgun et al., 2019), or the formal properties they possess (Chen et al., 2018b)."
56657817,Analysis Methods in Neural Language Processing: A Survey,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f,s12,Construction Method,"Challenge sets are usually created either programmatically or manually, by handcrafting specific examples. Often, semi-automatic methods are used to compile an initial list of examples that is manually verified by annotators. The specific method also affects the kind of language use and how natural or artificial/synthetic the examples are. We describe here some trends in dataset construction methods in the hope that they may be useful for researchers contemplating new datasets. Several datasets were constructed by modifying or extracting examples from existing datasets. For instance, Sanchez et al. (2018) and Glockner et al. (2018) extracted examples from SNLI (Bowman et al., 2015) and replaced specific words such as hypernyms, synonyms, and antonyms, followed by manual verification. Linzen et al. (2016), on the other hand, extracted examples of subject-verb agreement from raw texts using heuristics, resulting in a large-scale dataset. Gulordava et al. (2018) extended this to other agreement phenomena, but they relied on syntactic information available in treebanks, resulting in a smaller dataset.

Several challenge sets utilize existing test suites, either as a direct source of examples (Burchardt et al., 2017) or for searching similar naturally occurring examples . 12 Sennrich (2017) introduced a method for evaluating NMT systems via contrastive translation pairs, where the system is asked to estimate the probability of two candidate translations that are designed to reflect specific linguistic properties. Sennrich generated such pairs programmatically by applying simple heuristics, such as changing gender and number to induce agreement errors, resulting in a large-scale challenge set of close to 100 thousand examples. This framework was extended to evaluate other properties, but often requiring more sophisticated generation methods like using morphological analyzers/ generators (Burlot and Yvon, 2017) or more manual involvement in generation (Bawden et al., 2018) or verification (Rios Gonzales et al., 2017).

Finally, a few studies define templates that capture certain linguistic properties and instantiate them with word lists (Dasgupta et al., 2018;Rudinger et al., 2018;Zhao et al., 2018a). Template-based generation has the advantage of providing more control, for example for obtaining a specific vocabulary distribution, but this comes at the expense of how natural the examples are.","[['b14', 'b45', 'b20', None, 'b36'], [None], ['b80', None]]","[['b14', 'b45', 'b20', None, 'b36'], [None], ['b80', None]]",8,"sent1: Challenge sets are usually created either programmatically or manually, by handcrafting specific examples.
sent2: Often, semi-automatic methods are used to compile an initial list of examples that is manually verified by annotators.
sent3: The specific method also affects the kind of language use and how natural or artificial/synthetic the examples are.
sent4: We describe here some trends in dataset construction methods in the hope that they may be useful for researchers contemplating new datasets.
sent5: Several datasets were constructed by modifying or extracting examples from existing datasets.
sent6: For instance, Sanchez et al. (2018) and Glockner et al. (2018) extracted examples from SNLI (Bowman et al., 2015) and replaced specific words such as hypernyms, synonyms, and antonyms, followed by manual verification.
sent7: Linzen et al. (2016), on the other hand, extracted examples of subject-verb agreement from raw texts using heuristics, resulting in a large-scale dataset.
sent8: Gulordava et al. (2018) extended this to other agreement phenomena, but they relied on syntactic information available in treebanks, resulting in a smaller dataset.
sent9: Several challenge sets utilize existing test suites, either as a direct source of examples (Burchardt et al., 2017) or for searching similar naturally occurring examples .
sent10: 12 Sennrich (2017) introduced a method for evaluating NMT systems via contrastive translation pairs, where the system is asked to estimate the probability of two candidate translations that are designed to reflect specific linguistic properties.
sent11: Sennrich generated such pairs programmatically by applying simple heuristics, such as changing gender and number to induce agreement errors, resulting in a large-scale challenge set of close to 100 thousand examples.
sent12: This framework was extended to evaluate other properties, but often requiring more sophisticated generation methods like using morphological analyzers/ generators (Burlot and Yvon, 2017) or more manual involvement in generation (Bawden et al., 2018) or verification (Rios Gonzales et al., 2017).
sent13: Finally, a few studies define templates that capture certain linguistic properties and instantiate them with word lists (Dasgupta et al., 2018;Rudinger et al., 2018;Zhao et al., 2018a).
sent14: Template-based generation has the advantage of providing more control, for example for obtaining a specific vocabulary distribution, but this comes at the expense of how natural the examples are."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s2,Automatic identification of travel blog entries,"Travel blogs 1 are defined as travel journals written by bloggers in diary form. Travel blogs are considered useful for obtaining travel information, because many bloggers' travel experiences are written in this form.

There are various portal sites for travel blogs, which we will describe in Section 6. At these sites, travel blogs are manually registered by bloggers themselves, and the blogs are classified according to travel destination. However, there are many more travel blogs in the blogosphere, beyond these portal sites. In an attempt to construct an exhaustive database of travel blogs, Nanba et al. [25] identified travel blog entries written in Japanese in a blog database. 2 Blog entries that contain cue phrases, such as ""travel"", ""sightseeing"", or ""tour"", have a high degree of probability of being travel blogs. However, not every travel blog contains such cue phrases. For example, if a blogger describes his/her journey to Norway in multiple blog entries, the blog might state ""We traveled to Norway"" in the first entry, while only writing ""We ate wild sheep!"" in the second entry. In this case, because the second entry does not contain any expressions related to travel, it is difficult to identify it as a travel blog entry. Therefore, Nanba et al. focused not only on each blog entry but also on the surrounding entries for the identification of travel blog entries. They formulated the identification of travel blog entries as a sequence-labeling problem, and solved it using machine learning. For the machine learning method, they examined the Conditional Random Fields (CRF) method [20]; its empirical success has been reported recently in the field of natural language processing. The CRF-based method identifies the tag 3 of each entry. Features and tags are given in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry (see Figure 1). They used the value of k = 4, which was determined in a pilot study. Here, they used the following features for machine learning: whether an entry contains any of 416 cue phrases, such as ""ÅL (travel)"", ""Ä¢ü (tour)"", and ""úz (departure)"", and the number of location names in each entry.

Using the above method, Nanba et al. identified 17,268 travel blog entries from 1,100,000 blog entries, and constructed a system that plotted travel blog entries on a Google map (see Figure 2). 4 In this figure, travel blog entries are shown as icons. If the user clicks an icon, the corresponding blog entry is shown in a pop-up window.

Travel blogs 1 are defined as travel journals written by bloggers in diary form. Travel blogs are considered useful for obtaining travel information, because many bloggers' travel experiences are written in this form.

There are various portal sites for travel blogs, which we will describe in Section 6. At these sites, travel blogs are manually registered by bloggers themselves, and the blogs are classified according to travel destination. However, there are many more travel blogs in the blogosphere, beyond these portal sites. In an attempt to construct an exhaustive database of travel blogs, Nanba et al. [25] identified travel blog entries written in Japanese in a blog database. 2 Blog entries that contain cue phrases, such as ""travel"", ""sightseeing"", or ""tour"", have a high degree of probability of being travel blogs. However, not every travel blog contains such cue phrases. For example, if a blogger describes his/her journey to Norway in multiple blog entries, the blog might state ""We traveled to Norway"" in the first entry, while only writing ""We ate wild sheep!"" in the second entry. In this case, because the second entry does not contain any expressions related to travel, it is difficult to identify it as a travel blog entry. Therefore, Nanba et al. focused not only on each blog entry but also on the surrounding entries for the identification of travel blog entries. They formulated the identification of travel blog entries as a sequence-labeling problem, and solved it using machine learning. For the machine learning method, they examined the Conditional Random Fields (CRF) method [20]; its empirical success has been reported recently in the field of natural language processing. The CRF-based method identifies the tag 3 of each entry. Features and tags are given in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry (see Figure 1). They used the value of k = 4, which was determined in a pilot study. Here, they used the following features for machine learning: whether an entry contains any of 416 cue phrases, such as ""ÅL (travel)"", ""Ä¢ü (tour)"", and ""úz (departure)"", and the number of location names in each entry.

Using the above method, Nanba et al. identified 17,268 travel blog entries from 1,100,000 blog entries, and constructed a system that plotted travel blog entries on a Google map (see Figure 2). 4 In this figure, travel blog entries are shown as icons. If the user clicks an icon, the corresponding blog entry is shown in a pop-up window.","[[], ['b1', 'b19', 'b26'], ['b3'], [], ['b1', 'b19', 'b26'], ['b3']]","[[], ['b1', 'b19', 'b26'], ['b3'], [], ['b1', 'b19', 'b26'], ['b3']]",8,"sent1: Travel blogs 1 are defined as travel journals written by bloggers in diary form.
sent2: Travel blogs are considered useful for obtaining travel information, because many bloggers' travel experiences are written in this form.
sent3: There are various portal sites for travel blogs, which we will describe in Section 6.
sent4: At these sites, travel blogs are manually registered by bloggers themselves, and the blogs are classified according to travel destination.
sent5: However, there are many more travel blogs in the blogosphere, beyond these portal sites.
sent6: In an attempt to construct an exhaustive database of travel blogs, Nanba et al. [25] identified travel blog entries written in Japanese in a blog database.
sent7: 2 Blog entries that contain cue phrases, such as ""travel"", ""sightseeing"", or ""tour"", have a high degree of probability of being travel blogs.
sent8: However, not every travel blog contains such cue phrases.
sent9: For example, if a blogger describes his/her journey to Norway in multiple blog entries, the blog might state ""We traveled to Norway"" in the first entry, while only writing ""We ate wild sheep!""
sent10: in the second entry. In this case, because the second entry does not contain any expressions related to travel, it is difficult to identify it as a travel blog entry.
sent11: Therefore, Nanba et al. focused not only on each blog entry but also on the surrounding entries for the identification of travel blog entries.
sent12: They formulated the identification of travel blog entries as a sequence-labeling problem, and solved it using machine learning.
sent13: For the machine learning method, they examined the Conditional Random Fields (CRF) method [20]; its empirical success has been reported recently in the field of natural language processing.
sent14: The CRF-based method identifies the tag 3 of each entry.
sent15: Features and tags are given in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry (see Figure 1).
sent16: They used the value of k = 4, which was determined in a pilot study.
sent17: Here, they used the following features for machine learning: whether an entry contains any of 416 cue phrases, such as ""ÅL (travel)"", ""Ä¢ü (tour)"", and ""úz (departure)"", and the number of location names in each entry.
sent18: Using the above method, Nanba et al. identified 17,268 travel blog entries from 1,100,000 blog entries, and constructed a system that plotted travel blog entries on a Google map (see Figure 2). 4 In this figure, travel blog entries are shown as icons.
sent19: If the user clicks an icon, the corresponding blog entry is shown in a pop-up window.
sent20: Travel blogs 1 are defined as travel journals written by bloggers in diary form.
sent21: Travel blogs are considered useful for obtaining travel information, because many bloggers' travel experiences are written in this form.
sent22: There are various portal sites for travel blogs, which we will describe in Section 6.
sent23: At these sites, travel blogs are manually registered by bloggers themselves, and the blogs are classified according to travel destination.
sent24: However, there are many more travel blogs in the blogosphere, beyond these portal sites.
sent25: In an attempt to construct an exhaustive database of travel blogs, Nanba et al. [25] identified travel blog entries written in Japanese in a blog database.
sent26: 2 Blog entries that contain cue phrases, such as ""travel"", ""sightseeing"", or ""tour"", have a high degree of probability of being travel blogs.
sent27: However, not every travel blog contains such cue phrases.
sent28: For example, if a blogger describes his/her journey to Norway in multiple blog entries, the blog might state ""We traveled to Norway"" in the first entry, while only writing ""We ate wild sheep!""
sent29: in the second entry. In this case, because the second entry does not contain any expressions related to travel, it is difficult to identify it as a travel blog entry.
sent30: Therefore, Nanba et al. focused not only on each blog entry but also on the surrounding entries for the identification of travel blog entries.
sent31: They formulated the identification of travel blog entries as a sequence-labeling problem, and solved it using machine learning.
sent32: For the machine learning method, they examined the Conditional Random Fields (CRF) method [20]; its empirical success has been reported recently in the field of natural language processing.
sent33: The CRF-based method identifies the tag 3 of each entry.
sent34: Features and tags are given in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry (see Figure 1).
sent35: They used the value of k = 4, which was determined in a pilot study.
sent36: Here, they used the following features for machine learning: whether an entry contains any of 416 cue phrases, such as ""ÅL (travel)"", ""Ä¢ü (tour)"", and ""úz (departure)"", and the number of location names in each entry.
sent37: Using the above method, Nanba et al. identified 17,268 travel blog entries from 1,100,000 blog entries, and constructed a system that plotted travel blog entries on a Google map (see Figure 2). 4 In this figure, travel blog entries are shown as icons.
sent38: If the user clicks an icon, the corresponding blog entry is shown in a pop-up window."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s3,Automatic extraction of travel information from texts,"Nakatoh et al. [24] proposed a method for extracting names of local culinary dishes from travel blogs written in Japanese, which were identified when the blog entry included both the name of a sightseeing destination and the word ""tourism"". They extracted local dishes by gathering nouns that are dependent on the verb ""'ßy‹"" (eat). Tsai and Chou [32] also proposed a method for extracting dish names from restaurant review blogs written in Chinese using a machine learning (CRF) technique.  In the following, we explain the detail of the bootstrapping-based and machine learning-based information extraction approaches based on Nanba's work [25]. Nanba et al. extracted pairs comprising a location name and a local product from travel blogs written in Japanese, which were identified using the method described in Section 2.1. For the efficient extraction of travel information, they employed a bootstrapping method.

First, they prepared 482 pairs as seeds for the bootstrapping. These pairs were obtained automatically from a ""Web Japanese N-gram"" database provided by Google, Inc. The database comprises N-grams (N = 1-7) extracted from 20 billion Japanese sentences on the Web. They applied the pattern ""[0 ] i [ i] "" ([slot of ""location name""] local product [slot of ""local product""] ) to the database, and extracted location names and local products from each corresponding slot, thereby obtaining the 482 pairs. Second, they applied a machine learning-based information extraction technique to the travel blogs identified in the previous step, and obtained new pairs. In this step, they prepared training data for the machine learning in the following three steps.

1. Select 200 sentences that contain both a location name and a local product from the 482 pairs. Then automatically create 200 tagged sentences, to which both ""location"" and ""product"" tags are assigned. 5 2. Prepare another 200 sentences that contain only a location name. Then create 200 tagged sentences, to which the ""location"" tag is assigned.

3. Apply machine learning to the 400 tagged sentences, and obtain a system that automatically allocates ""location"" and ""product"" tags to given sentences.

As a machine learning method, they used CRF. The CRF-based method identifies the class of each word in a given sentence. Features and tags are given in the CRF method as follows: (1) k tags occur before a target word; (2) k features occur before a target word; and (3) k features follow a target word. They used the value of k = 2, which was determined in a pilot study. They used the following six features for machine learning.

• Word.

• The part of speech to which the word belongs (noun, verb, adjective, etc.)

• Whether the word is a quotation mark.

• Whether the word is a cue word, such as "" i"", "" #"", ""y#"" (local product), ""˜Ó"" (famous confection), or "" #"" (souvenir).

• Whether the word is a surface case.

• Whether the word is frequently used in the names of local products or souvenirs, such as ""cake"" or ""noodle"".

Nakatoh et al. [24] proposed a method for extracting names of local culinary dishes from travel blogs written in Japanese, which were identified when the blog entry included both the name of a sightseeing destination and the word ""tourism"". They extracted local dishes by gathering nouns that are dependent on the verb ""'ßy‹"" (eat). Tsai and Chou [32] also proposed a method for extracting dish names from restaurant review blogs written in Chinese using a machine learning (CRF) technique.  In the following, we explain the detail of the bootstrapping-based and machine learning-based information extraction approaches based on Nanba's work [25]. Nanba et al. extracted pairs comprising a location name and a local product from travel blogs written in Japanese, which were identified using the method described in Section 2.1. For the efficient extraction of travel information, they employed a bootstrapping method.

First, they prepared 482 pairs as seeds for the bootstrapping. These pairs were obtained automatically from a ""Web Japanese N-gram"" database provided by Google, Inc. The database comprises N-grams (N = 1-7) extracted from 20 billion Japanese sentences on the Web. They applied the pattern ""[0 ] i [ i] "" ([slot of ""location name""] local product [slot of ""local product""] ) to the database, and extracted location names and local products from each corresponding slot, thereby obtaining the 482 pairs. Second, they applied a machine learning-based information extraction technique to the travel blogs identified in the previous step, and obtained new pairs. In this step, they prepared training data for the machine learning in the following three steps.

1. Select 200 sentences that contain both a location name and a local product from the 482 pairs. Then automatically create 200 tagged sentences, to which both ""location"" and ""product"" tags are assigned. 5 2. Prepare another 200 sentences that contain only a location name. Then create 200 tagged sentences, to which the ""location"" tag is assigned.

3. Apply machine learning to the 400 tagged sentences, and obtain a system that automatically allocates ""location"" and ""product"" tags to given sentences.

As a machine learning method, they used CRF. The CRF-based method identifies the class of each word in a given sentence. Features and tags are given in the CRF method as follows: (1) k tags occur before a target word; (2) k features occur before a target word; and (3) k features follow a target word. They used the value of k = 2, which was determined in a pilot study. They used the following six features for machine learning.

• Word.

• The part of speech to which the word belongs (noun, verb, adjective, etc.)

• Whether the word is a quotation mark.

• Whether the word is a cue word, such as "" i"", "" #"", ""y#"" (local product), ""˜Ó"" (famous confection), or "" #"" (souvenir).

• Whether the word is a surface case.

• Whether the word is frequently used in the names of local products or souvenirs, such as ""cake"" or ""noodle"".","[['b25', 'b33', 'b26'], [], ['b4'], [], [], [], [], [], [], [], [], ['b25', 'b33', 'b26'], [], ['b4'], [], [], [], [], [], [], [], []]","[['b25', 'b33', 'b26'], [], ['b4'], [], [], [], [], [], [], [], [], ['b25', 'b33', 'b26'], [], ['b4'], [], [], [], [], [], [], [], []]",8,"sent1: Nakatoh et al. [24] proposed a method for extracting names of local culinary dishes from travel blogs written in Japanese, which were identified when the blog entry included both the name of a sightseeing destination and the word ""tourism"".
sent2: They extracted local dishes by gathering nouns that are dependent on the verb ""'ßy‹"" (eat).
sent3: Tsai and Chou [32] also proposed a method for extracting dish names from restaurant review blogs written in Chinese using a machine learning (CRF) technique.
sent4: In the following, we explain the detail of the bootstrapping-based and machine learning-based information extraction approaches based on Nanba's work [25].
sent5: Nanba et al. extracted pairs comprising a location name and a local product from travel blogs written in Japanese, which were identified using the method described in Section 2.1.
sent6: For the efficient extraction of travel information, they employed a bootstrapping method.
sent7: First, they prepared 482 pairs as seeds for the bootstrapping.
sent8: These pairs were obtained automatically from a ""Web Japanese N-gram"" database provided by Google, Inc.
sent9: The database comprises N-grams (N = 1-7) extracted from 20 billion Japanese sentences on the Web.
sent10: They applied the pattern ""[0 ] i [ i] "" ([slot of ""location name""] local product [slot of ""local product""] ) to the database, and extracted location names and local products from each corresponding slot, thereby obtaining the 482 pairs.
sent11: Second, they applied a machine learning-based information extraction technique to the travel blogs identified in the previous step, and obtained new pairs.
sent12: In this step, they prepared training data for the machine learning in the following three steps.
sent13: 1. Select 200 sentences that contain both a location name and a local product from the 482 pairs.
sent14: Then automatically create 200 tagged sentences, to which both ""location"" and ""product"" tags are assigned.
sent15: 5 2. Prepare another 200 sentences that contain only a location name.
sent16: Then create 200 tagged sentences, to which the ""location"" tag is assigned.
sent17: 3. Apply machine learning to the 400 tagged sentences, and obtain a system that automatically allocates ""location"" and ""product"" tags to given sentences.
sent18: As a machine learning method, they used CRF.
sent19: The CRF-based method identifies the class of each word in a given sentence.
sent20: Features and tags are given in the CRF method as follows: (1) k tags occur before a target word; (2) k features occur before a target word; and (3) k features follow a target word.
sent21: They used the value of k = 2, which was determined in a pilot study.
sent22: They used the following six features for machine learning.
sent23: • Word. • The part of speech to which the word belongs (noun, verb, adjective, etc.)• Whether the word is a quotation mark.
sent24: • Whether the word is a cue word, such as "" i"", "" #"", ""y#"" (local product), ""˜Ó"" (famous confection), or "" #"" (souvenir).
sent25: • Whether the word is a surface case.
sent26: • Whether the word is frequently used in the names of local products or souvenirs, such as ""cake"" or ""noodle"".
sent27: Nakatoh et al. [24] proposed a method for extracting names of local culinary dishes from travel blogs written in Japanese, which were identified when the blog entry included both the name of a sightseeing destination and the word ""tourism"".
sent28: They extracted local dishes by gathering nouns that are dependent on the verb ""'ßy‹"" (eat).
sent29: Tsai and Chou [32] also proposed a method for extracting dish names from restaurant review blogs written in Chinese using a machine learning (CRF) technique.
sent30: In the following, we explain the detail of the bootstrapping-based and machine learning-based information extraction approaches based on Nanba's work [25].
sent31: Nanba et al. extracted pairs comprising a location name and a local product from travel blogs written in Japanese, which were identified using the method described in Section 2.1.
sent32: For the efficient extraction of travel information, they employed a bootstrapping method.
sent33: First, they prepared 482 pairs as seeds for the bootstrapping.
sent34: These pairs were obtained automatically from a ""Web Japanese N-gram"" database provided by Google, Inc.
sent35: The database comprises N-grams (N = 1-7) extracted from 20 billion Japanese sentences on the Web.
sent36: They applied the pattern ""[0 ] i [ i] "" ([slot of ""location name""] local product [slot of ""local product""] ) to the database, and extracted location names and local products from each corresponding slot, thereby obtaining the 482 pairs.
sent37: Second, they applied a machine learning-based information extraction technique to the travel blogs identified in the previous step, and obtained new pairs.
sent38: In this step, they prepared training data for the machine learning in the following three steps.
sent39: 1. Select 200 sentences that contain both a location name and a local product from the 482 pairs.
sent40: Then automatically create 200 tagged sentences, to which both ""location"" and ""product"" tags are assigned.
sent41: 5 2. Prepare another 200 sentences that contain only a location name.
sent42: Then create 200 tagged sentences, to which the ""location"" tag is assigned.
sent43: 3. Apply machine learning to the 400 tagged sentences, and obtain a system that automatically allocates ""location"" and ""product"" tags to given sentences.
sent44: As a machine learning method, they used CRF.
sent45: The CRF-based method identifies the class of each word in a given sentence.
sent46: Features and tags are given in the CRF method as follows: (1) k tags occur before a target word; (2) k features occur before a target word; and (3) k features follow a target word.
sent47: They used the value of k = 2, which was determined in a pilot study.
sent48: They used the following six features for machine learning.
sent49: • Word. • The part of speech to which the word belongs (noun, verb, adjective, etc.)• Whether the word is a quotation mark.
sent50: • Whether the word is a cue word, such as "" i"", "" #"", ""y#"" (local product), ""˜Ó"" (famous confection), or "" #"" (souvenir).
sent51: • Whether the word is a surface case.
sent52: • Whether the word is frequently used in the names of local products or souvenirs, such as ""cake"" or ""noodle""."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s8,Travelers' behavior analysis,"The analysis of people's transportation information is considered an important issue in various fields, such as city planning, architectural planning, car navigation, sightseeing administration, crime prevention, and tracing the spread of infection of epidemics. In this section, we focus on the analysis of travelers' behavior.

Ishino et al. [15] proposed a method to extract people's transportation information from automatically identified travel blogs written in Japanese [25]. They used machine learning to extract information, such as ""departure place"", ""destination"", or ""transportation device"", from travel blog entries. First, the tags used in their examination are defined.

• FROM tag indicates the departure place.

• TO tag indicates the destination.

• VIA tag indicates the route.

• METHOD tag indicates the transportation device.

• TIME tag indicates the time of transportation.

The following is a tagged example.

It took <TIME>five hours< /TIME> to travel from <FROM>Hiroshima< /FROM> to<TO>Osaka< /TO> by <METHOD>bus< /METHOD>.

They formulated the task of identifying the class of each word in a given sentence and solved it using machine learning. For the machine learning method, they used CRF [20], in the same way as Nanba et al. [25], which we mentioned in Section 2.2. The CRF-based method identifies the class of each entry. Features and tags are used in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry. They used the value k = 4 7 , which was determined via a pilot study. They used the following features for machine learning.

• A word.

• The part of speech to which the word belongs (noun, verb, adjective, etc.).

• Whether the word is a quotation mark.

• Whether the word is a cue phrase.

The details of cue phrases, together with the number of cue phrases of the given type, are shown as follows.

The analysis of people's transportation information is considered an important issue in various fields, such as city planning, architectural planning, car navigation, sightseeing administration, crime prevention, and tracing the spread of infection of epidemics. In this section, we focus on the analysis of travelers' behavior.

Ishino et al. [15] proposed a method to extract people's transportation information from automatically identified travel blogs written in Japanese [25]. They used machine learning to extract information, such as ""departure place"", ""destination"", or ""transportation device"", from travel blog entries. First, the tags used in their examination are defined.

• FROM tag indicates the departure place.

• TO tag indicates the destination.

• VIA tag indicates the route.

• METHOD tag indicates the transportation device.

• TIME tag indicates the time of transportation.

The following is a tagged example.

It took <TIME>five hours< /TIME> to travel from <FROM>Hiroshima< /FROM> to<TO>Osaka< /TO> by <METHOD>bus< /METHOD>.

They formulated the task of identifying the class of each word in a given sentence and solved it using machine learning. For the machine learning method, they used CRF [20], in the same way as Nanba et al. [25], which we mentioned in Section 2.2. The CRF-based method identifies the class of each entry. Features and tags are used in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry. They used the value k = 4 7 , which was determined via a pilot study. They used the following features for machine learning.

• A word.

• The part of speech to which the word belongs (noun, verb, adjective, etc.).

• Whether the word is a quotation mark.

• Whether the word is a cue phrase.

The details of cue phrases, together with the number of cue phrases of the given type, are shown as follows.","[[], ['b14', 'b26'], [], [], [], [], [], [], [], ['b26', 'b19'], [], [], [], [], [], [], ['b14', 'b26'], [], [], [], [], [], [], [], ['b26', 'b19'], [], [], [], [], []]","[[], ['b14', 'b26'], [], [], [], [], [], [], [], ['b26', 'b19'], [], [], [], [], [], [], ['b14', 'b26'], [], [], [], [], [], [], [], ['b26', 'b19'], [], [], [], [], []]",8,"sent1: The analysis of people's transportation information is considered an important issue in various fields, such as city planning, architectural planning, car navigation, sightseeing administration, crime prevention, and tracing the spread of infection of epidemics.
sent2: In this section, we focus on the analysis of travelers' behavior.
sent3: Ishino et al. [15] proposed a method to extract people's transportation information from automatically identified travel blogs written in Japanese [25].
sent4: They used machine learning to extract information, such as ""departure place"", ""destination"", or ""transportation device"", from travel blog entries.
sent5: First, the tags used in their examination are defined.
sent6: • FROM tag indicates the departure place.
sent7: • TO tag indicates the destination.
sent8: • VIA tag indicates the route. • METHOD tag indicates the transportation device.
sent9: • TIME tag indicates the time of transportation.
sent10: The following is a tagged example.
sent11: It took <TIME>five hours< /TIME> to travel from <FROM>Hiroshima< /FROM> to<TO>Osaka< /TO>
sent12: by <METHOD>bus< /METHOD>. They formulated the task of identifying the class of each word in a given sentence and solved it using machine learning.
sent13: For the machine learning method, they used CRF [20], in the same way as Nanba et al. [25], which we mentioned in Section 2.2.
sent14: The CRF-based method identifies the class of each entry.
sent15: Features and tags are used in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry.
sent16: They used the value k = 4 7 , which was determined via a pilot study.
sent17: They used the following features for machine learning.
sent18: • A word. • The part of speech to which the word belongs (noun, verb, adjective, etc.).• Whether the word is a quotation mark.
sent19: • Whether the word is a cue phrase.
sent20: The details of cue phrases, together with the number of cue phrases of the given type, are shown as follows.
sent21: The analysis of people's transportation information is considered an important issue in various fields, such as city planning, architectural planning, car navigation, sightseeing administration, crime prevention, and tracing the spread of infection of epidemics.
sent22: In this section, we focus on the analysis of travelers' behavior.
sent23: Ishino et al. [15] proposed a method to extract people's transportation information from automatically identified travel blogs written in Japanese [25].
sent24: They used machine learning to extract information, such as ""departure place"", ""destination"", or ""transportation device"", from travel blog entries.
sent25: First, the tags used in their examination are defined.
sent26: • FROM tag indicates the departure place.
sent27: • TO tag indicates the destination.
sent28: • VIA tag indicates the route. • METHOD tag indicates the transportation device.
sent29: • TIME tag indicates the time of transportation.
sent30: The following is a tagged example.
sent31: It took <TIME>five hours< /TIME> to travel from <FROM>Hiroshima< /FROM> to<TO>Osaka< /TO>
sent32: by <METHOD>bus< /METHOD>. They formulated the task of identifying the class of each word in a given sentence and solved it using machine learning.
sent33: For the machine learning method, they used CRF [20], in the same way as Nanba et al. [25], which we mentioned in Section 2.2.
sent34: The CRF-based method identifies the class of each entry.
sent35: Features and tags are used in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry.
sent36: They used the value k = 4 7 , which was determined via a pilot study.
sent37: They used the following features for machine learning.
sent38: • A word. • The part of speech to which the word belongs (noun, verb, adjective, etc.).• Whether the word is a quotation mark.
sent39: • Whether the word is a cue phrase.
sent40: The details of cue phrases, together with the number of cue phrases of the given type, are shown as follows."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s16,Recommending tourist spots,"Recommending tourist spots 8 has been well studied in the multimedia field. Movies and images are used as information sources in addition to texts. In this section, we describe two multimedia studies.

Hao et al. [10] proposed a method for mining location-representative knowledge from travel blogs based on a probabilistic topic model (the Location-Topic model). Using this model,  Figure 5 shows an example of the system output. In this figure, a travel blog segment 9 is enriched with three images that depict its most informative parts. Each image's original tags and the words in the text to which it corresponds are also presented.

Wu et al. [34] proposed a system that summarized tourism-related information. When a user (traveler) entered a query, such as ""What is the historical background of Tian Tan?"", the system searched for and obtained information from Wikipedia, Flickr, YouTube, and official tourism Web sites using the tourist spot name as a query. The system also classified the query as belonging to one of five categories-""general"", ""history"", ""landscape"", ""indoor scenery"", and ""outdoor scenery""-in order to provide users with more relevant information. For example, when a query is classified as belonging to the ""history"" category, the information is obtained from texts, while for a query regarding ""outdoor scenery"", the information is obtained from photos and videos. 

Recommending tourist spots 8 has been well studied in the multimedia field. Movies and images are used as information sources in addition to texts. In this section, we describe two multimedia studies.

Hao et al. [10] proposed a method for mining location-representative knowledge from travel blogs based on a probabilistic topic model (the Location-Topic model). Using this model,  Figure 5 shows an example of the system output. In this figure, a travel blog segment 9 is enriched with three images that depict its most informative parts. Each image's original tags and the words in the text to which it corresponds are also presented.

Wu et al. [34] proposed a system that summarized tourism-related information. When a user (traveler) entered a query, such as ""What is the historical background of Tian Tan?"", the system searched for and obtained information from Wikipedia, Flickr, YouTube, and official tourism Web sites using the tourist spot name as a query. The system also classified the query as belonging to one of five categories-""general"", ""history"", ""landscape"", ""indoor scenery"", and ""outdoor scenery""-in order to provide users with more relevant information. For example, when a query is classified as belonging to the ""history"" category, the information is obtained from texts, while for a query regarding ""outdoor scenery"", the information is obtained from photos and videos. ","[[], ['b9'], ['b35'], [], ['b9'], ['b35']]","[[], ['b9'], ['b35'], [], ['b9'], ['b35']]",4,"sent1: Recommending tourist spots 8 has been well studied in the multimedia field.
sent2: Movies and images are used as information sources in addition to texts.
sent3: In this section, we describe two multimedia studies.
sent4: Hao et al. [10] proposed a method for mining location-representative knowledge from travel blogs based on a probabilistic topic model (the Location-Topic model).
sent5: Using this model,  Figure 5 shows an example of the system output.
sent6: In this figure, a travel blog segment 9 is enriched with three images that depict its most informative parts.
sent7: Each image's original tags and the words in the text to which it corresponds are also presented.
sent8: Wu et al. [34] proposed a system that summarized tourism-related information.
sent9: When a user (traveler) entered a query, such as ""What is the historical background of Tian Tan?"", the system searched for and obtained information from Wikipedia, Flickr, YouTube, and official tourism Web sites using the tourist spot name as a query.
sent10: The system also classified the query as belonging to one of five categories-""general"", ""history"", ""landscape"", ""indoor scenery"", and ""outdoor scenery""-in order to provide users with more relevant information.
sent11: For example, when a query is classified as belonging to the ""history"" category, the information is obtained from texts, while for a query regarding ""outdoor scenery"", the information is obtained from photos and videos.
sent12: Recommending tourist spots 8 has been well studied in the multimedia field.
sent13: Movies and images are used as information sources in addition to texts.
sent14: In this section, we describe two multimedia studies.
sent15: Hao et al. [10] proposed a method for mining location-representative knowledge from travel blogs based on a probabilistic topic model (the Location-Topic model).
sent16: Using this model,  Figure 5 shows an example of the system output.
sent17: In this figure, a travel blog segment 9 is enriched with three images that depict its most informative parts.
sent18: Each image's original tags and the words in the text to which it corresponds are also presented.
sent19: Wu et al. [34] proposed a system that summarized tourism-related information.
sent20: When a user (traveler) entered a query, such as ""What is the historical background of Tian Tan?"", the system searched for and obtained information from Wikipedia, Flickr, YouTube, and official tourism Web sites using the tourist spot name as a query.
sent21: The system also classified the query as belonging to one of five categories-""general"", ""history"", ""landscape"", ""indoor scenery"", and ""outdoor scenery""-in order to provide users with more relevant information.
sent22: For example, when a query is classified as belonging to the ""history"" category, the information is obtained from texts, while for a query regarding ""outdoor scenery"", the information is obtained from photos and videos."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s17,Recommending landmarks,"Finding and recommending landmarks is considered an important research topic in the multimedia field, along with recommending tourist spots. Abbasi et al. [1] focused on the photo-sharing system Flickr, and proposed a method to identify landmark photos using tags and social Flickr groups. Gao et al. [7] also proposed a method to identify landmarks using Flickr and the Yahoo Travel Guide.

Ji et al. [17] proposed another method for finding landmarks. They adopted the method of clustering blog photos relating to a particular tourist site, such as Louvre Museum in Paris. 10 Then they represented these photos as a graph based on the clustering results, and detected landmarks using link analysis methods, such as the PageRank [3] and HITS [19] algorithms.

Finding and recommending landmarks is considered an important research topic in the multimedia field, along with recommending tourist spots. Abbasi et al. [1] focused on the photo-sharing system Flickr, and proposed a method to identify landmark photos using tags and social Flickr groups. Gao et al. [7] also proposed a method to identify landmarks using Flickr and the Yahoo Travel Guide.

Ji et al. [17] proposed another method for finding landmarks. They adopted the method of clustering blog photos relating to a particular tourist site, such as Louvre Museum in Paris. 10 Then they represented these photos as a graph based on the clustering results, and detected landmarks using link analysis methods, such as the PageRank [3] and HITS [19] algorithms.","[['b6', 'b0'], ['b18', 'b9', 'b2', 'b16'], ['b6', 'b0'], ['b18', 'b9', 'b2', 'b16']]","[['b6', 'b0'], ['b18', 'b9', 'b2', 'b16'], ['b6', 'b0'], ['b18', 'b9', 'b2', 'b16']]",12,"sent1: Finding and recommending landmarks is considered an important research topic in the multimedia field, along with recommending tourist spots.
sent2: Abbasi et al. [1] focused on the photo-sharing system Flickr, and proposed a method to identify landmark photos using tags and social Flickr groups.
sent3: Gao et al. [7] also proposed a method to identify landmarks using Flickr and the Yahoo Travel Guide.
sent4: Ji et al. [17] proposed another method for finding landmarks.
sent5: They adopted the method of clustering blog photos relating to a particular tourist site, such as Louvre Museum in Paris.
sent6: 10 Then they represented these photos as a graph based on the clustering results, and detected landmarks using link analysis methods, such as the PageRank [3] and HITS [19] algorithms.
sent7: Finding and recommending landmarks is considered an important research topic in the multimedia field, along with recommending tourist spots.
sent8: Abbasi et al. [1] focused on the photo-sharing system Flickr, and proposed a method to identify landmark photos using tags and social Flickr groups.
sent9: Gao et al. [7] also proposed a method to identify landmarks using Flickr and the Yahoo Travel Guide.
sent10: Ji et al. [17] proposed another method for finding landmarks.
sent11: They adopted the method of clustering blog photos relating to a particular tourist site, such as Louvre Museum in Paris.
sent12: 10 Then they represented these photos as a graph based on the clustering results, and detected landmarks using link analysis methods, such as the PageRank [3] and HITS [19] algorithms."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s18,Recommending travel products,"Ishino et al. [14] proposed a method that added links to advertisements for travel products to the travel information links that were described in Section 2.3. 11 The procedure for providing ad links is as follows.

1. Input a link type and the citing areas of a travel information link.

2. Extract keywords from the citing areas.

3. Extract product data containing all keywords, and calculate the similarity between the citing areas of a travel information link and the product data.

4. Provide the ad link to the product data having the highest similarity to the travel information link.

They extracted keywords for travel products corresponding to the link type. They used the same cues to classify travel information links [15] (see Section 2.3), and then extracted keywords from the citing areas of links of types S (Spot) and R (Restaurant). 10 For calculating the similarity between two photos, they used the Bag-of-Visual-Words representation [18,26], which represents an image as a set of salient regions (visual words), called Bag-of-Visual-Words vectors. Then the similarity between photos is measured based on the cosine distance between their Bag-of-Visual-Words vectors. In addition to the features in each image, they also used textual information for each photo, such as the title, description, and surrounding text. 11 http://www.ls.info.hiroshima-cu.ac.jp/travel/ First, the method for extracting keywords from the citing areas of links of type S is described. The cues for type S, such as tourist spots collected from Wikipedia and words frequently used in the names of tourist spots, tend to become keywords. Therefore, they registered these cues as candidate keywords for links of type S. If the citing areas of these links contained candidate keywords, they extracted the candidates as keywords. In addition, if citing areas contained names of places, they extracted the names as keywords.

The cues for type R, such as dish names and cooking styles, also tend to become keywords. Therefore, they registered these cues as candidate keywords for links of type R. If the citing areas for links of type R contained candidate keywords, they extracted them as keywords. Titov and McDonald [31] proposed an aspect-based summarization system, and applied the method to the summarization of hotel reviews. The system took as input a set of user reviews for a specific product or service with a numeric rating (left side in Figure 6), and produced a set of relevant aspects, which they called an aspect-based summary (right side in Figure  6). To extract all relevant mentions in each review for each aspect, they introduced a topic model. They applied their method to hotel reviews on the TripAdvisor Web site 12 , and obtained aspect-based summaries for each hotel. To obtain more reliable hotel reviews, opinion spams should be detected and eliminated. Opinion spams are fictitious opinions that have been deliberately written to sound authentic. Ott et al. [27] proposed a method to detect opinion spam among consumer reviews of hotels. They created 400 deceptive opinions using the Amazon Mechanical Turk (AMT) crowdsourcing service 13 by asking anonymous online workers (Turkers) to create the opinion spam for 20 chosen hotels. In addition to these spam messages, they selected 6,977 truthful opinions from TripAdvisor, and used both groups for their task.

Ishino et al. [14] proposed a method that added links to advertisements for travel products to the travel information links that were described in Section 2.3. 11 The procedure for providing ad links is as follows.

1. Input a link type and the citing areas of a travel information link.

2. Extract keywords from the citing areas.

3. Extract product data containing all keywords, and calculate the similarity between the citing areas of a travel information link and the product data.

4. Provide the ad link to the product data having the highest similarity to the travel information link.

They extracted keywords for travel products corresponding to the link type. They used the same cues to classify travel information links [15] (see Section 2.3), and then extracted keywords from the citing areas of links of types S (Spot) and R (Restaurant). 10 For calculating the similarity between two photos, they used the Bag-of-Visual-Words representation [18,26], which represents an image as a set of salient regions (visual words), called Bag-of-Visual-Words vectors. Then the similarity between photos is measured based on the cosine distance between their Bag-of-Visual-Words vectors. In addition to the features in each image, they also used textual information for each photo, such as the title, description, and surrounding text. 11 http://www.ls.info.hiroshima-cu.ac.jp/travel/ First, the method for extracting keywords from the citing areas of links of type S is described. The cues for type S, such as tourist spots collected from Wikipedia and words frequently used in the names of tourist spots, tend to become keywords. Therefore, they registered these cues as candidate keywords for links of type S. If the citing areas of these links contained candidate keywords, they extracted the candidates as keywords. In addition, if citing areas contained names of places, they extracted the names as keywords.

The cues for type R, such as dish names and cooking styles, also tend to become keywords. Therefore, they registered these cues as candidate keywords for links of type R. If the citing areas for links of type R contained candidate keywords, they extracted them as keywords. Titov and McDonald [31] proposed an aspect-based summarization system, and applied the method to the summarization of hotel reviews. The system took as input a set of user reviews for a specific product or service with a numeric rating (left side in Figure 6), and produced a set of relevant aspects, which they called an aspect-based summary (right side in Figure  6). To extract all relevant mentions in each review for each aspect, they introduced a topic model. They applied their method to hotel reviews on the TripAdvisor Web site 12 , and obtained aspect-based summaries for each hotel. To obtain more reliable hotel reviews, opinion spams should be detected and eliminated. Opinion spams are fictitious opinions that have been deliberately written to sound authentic. Ott et al. [27] proposed a method to detect opinion spam among consumer reviews of hotels. They created 400 deceptive opinions using the Amazon Mechanical Turk (AMT) crowdsourcing service 13 by asking anonymous online workers (Turkers) to create the opinion spam for 20 chosen hotels. In addition to these spam messages, they selected 6,977 truthful opinions from TripAdvisor, and used both groups for their task.","[['b10', 'b13'], [], [], [], [], ['b27', 'b14', 'b9', 'b17'], [None, 'b28'], ['b10', 'b13'], [], [], [], [], ['b27', 'b14', 'b9', 'b17'], [None, 'b28']]","[['b10', 'b13'], [], [], [], [], ['b27', 'b14', 'b9', 'b17'], [None, 'b28'], ['b10', 'b13'], [], [], [], [], ['b27', 'b14', 'b9', 'b17'], [None, 'b28']]",16,"sent1: Ishino et al. [14] proposed a method that added links to advertisements for travel products to the travel information links that were described in Section 2.3.
sent2: 11 The procedure for providing ad links is as follows.
sent3: 1. Input a link type and the citing areas of a travel information link.
sent4: 2. Extract keywords from the citing areas.
sent5: 3. Extract product data containing all keywords, and calculate the similarity between the citing areas of a travel information link and the product data.4.
sent6: Provide the ad link to the product data having the highest similarity to the travel information link.
sent7: They extracted keywords for travel products corresponding to the link type.
sent8: They used the same cues to classify travel information links [15] (see Section 2.3), and then extracted keywords from the citing areas of links of types S (Spot) and R (Restaurant). 10 For calculating the similarity between two photos, they used the Bag-of-Visual-Words representation [18,26], which represents an image as a set of salient regions (visual words), called Bag-of-Visual-Words vectors.
sent9: Then the similarity between photos is measured based on the cosine distance between their Bag-of-Visual-Words vectors.
sent10: In addition to the features in each image, they also used textual information for each photo, such as the title, description, and surrounding text.
sent11: 11 http://www.ls.info.hiroshima-cu.ac.jp/travel/ First, the method for extracting keywords from the citing areas of links of type S is described.
sent12: The cues for type S, such as tourist spots collected from Wikipedia and words frequently used in the names of tourist spots, tend to become keywords.
sent13: Therefore, they registered these cues as candidate keywords for links of type S.
sent14: If the citing areas of these links contained candidate keywords, they extracted the candidates as keywords.
sent15: In addition, if citing areas contained names of places, they extracted the names as keywords.
sent16: The cues for type R, such as dish names and cooking styles, also tend to become keywords.
sent17: Therefore, they registered these cues as candidate keywords for links of type R.
sent18: If the citing areas for links of type R contained candidate keywords, they extracted them as keywords.
sent19: Titov and McDonald [31] proposed an aspect-based summarization system, and applied the method to the summarization of hotel reviews.
sent20: The system took as input a set of user reviews for a specific product or service with a numeric rating (left side in Figure 6), and produced a set of relevant aspects, which they called an aspect-based summary (right side in Figure  6).
sent21: To extract all relevant mentions in each review for each aspect, they introduced a topic model.
sent22: They applied their method to hotel reviews on the TripAdvisor Web site 12 , and obtained aspect-based summaries for each hotel.
sent23: To obtain more reliable hotel reviews, opinion spams should be detected and eliminated.
sent24: Opinion spams are fictitious opinions that have been deliberately written to sound authentic.
sent25: Ott et al. [27] proposed a method to detect opinion spam among consumer reviews of hotels.
sent26: They created 400 deceptive opinions using the Amazon Mechanical Turk (AMT) crowdsourcing service 13 by asking anonymous online workers (Turkers) to create the opinion spam for 20 chosen hotels.
sent27: In addition to these spam messages, they selected 6,977 truthful opinions from TripAdvisor, and used both groups for their task.
sent28: Ishino et al. [14] proposed a method that added links to advertisements for travel products to the travel information links that were described in Section 2.3.
sent29: 11 The procedure for providing ad links is as follows.
sent30: 1. Input a link type and the citing areas of a travel information link.
sent31: 2. Extract keywords from the citing areas.
sent32: 3. Extract product data containing all keywords, and calculate the similarity between the citing areas of a travel information link and the product data.4.
sent33: Provide the ad link to the product data having the highest similarity to the travel information link.
sent34: They extracted keywords for travel products corresponding to the link type.
sent35: They used the same cues to classify travel information links [15] (see Section 2.3), and then extracted keywords from the citing areas of links of types S (Spot) and R (Restaurant). 10 For calculating the similarity between two photos, they used the Bag-of-Visual-Words representation [18,26], which represents an image as a set of salient regions (visual words), called Bag-of-Visual-Words vectors.
sent36: Then the similarity between photos is measured based on the cosine distance between their Bag-of-Visual-Words vectors.
sent37: In addition to the features in each image, they also used textual information for each photo, such as the title, description, and surrounding text.
sent38: 11 http://www.ls.info.hiroshima-cu.ac.jp/travel/ First, the method for extracting keywords from the citing areas of links of type S is described.
sent39: The cues for type S, such as tourist spots collected from Wikipedia and words frequently used in the names of tourist spots, tend to become keywords.
sent40: Therefore, they registered these cues as candidate keywords for links of type S.
sent41: If the citing areas of these links contained candidate keywords, they extracted the candidates as keywords.
sent42: In addition, if citing areas contained names of places, they extracted the names as keywords.
sent43: The cues for type R, such as dish names and cooking styles, also tend to become keywords.
sent44: Therefore, they registered these cues as candidate keywords for links of type R.
sent45: If the citing areas for links of type R contained candidate keywords, they extracted them as keywords.
sent46: Titov and McDonald [31] proposed an aspect-based summarization system, and applied the method to the summarization of hotel reviews.
sent47: The system took as input a set of user reviews for a specific product or service with a numeric rating (left side in Figure 6), and produced a set of relevant aspects, which they called an aspect-based summary (right side in Figure  6).
sent48: To extract all relevant mentions in each review for each aspect, they introduced a topic model.
sent49: They applied their method to hotel reviews on the TripAdvisor Web site 12 , and obtained aspect-based summaries for each hotel.
sent50: To obtain more reliable hotel reviews, opinion spams should be detected and eliminated.
sent51: Opinion spams are fictitious opinions that have been deliberately written to sound authentic.
sent52: Ott et al. [27] proposed a method to detect opinion spam among consumer reviews of hotels.
sent53: They created 400 deceptive opinions using the Amazon Mechanical Turk (AMT) crowdsourcing service 13 by asking anonymous online workers (Turkers) to create the opinion spam for 20 chosen hotels.
sent54: In addition to these spam messages, they selected 6,977 truthful opinions from TripAdvisor, and used both groups for their task."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s22,Providing travel information along streetcar lines,"Ishino et al. [13] proposed a method for collecting blog entries about the Hiroshima Electric Railway (Hiroden) from a blog database. 15 Hiroden blog entries were defined as travel journals that provide regional information for streetcar stations in Hiroshima. The task of collecting Hiroden blog entries was divided into two steps: (1) collection of blog entries; and (2) identification of Hiroden blog entries. Figure 7 shows a route map used by the system for providing travel information along the Hiroden streetcar lines. The route map shows Hiroden streetcar stations and major tourist spots. The steps in the search procedure are as follows.

• (Step 1) Click the Hiroden streetcar station, such as ""Ÿ Éü àM"" (Atomic Bomb Dome), in Figure 7 to generate a list of links to Hiroden blog entries ( Figure 8).

• (Step 2) Click the link to a Hiroden blog entry to display it.

Ishino et al. [13] proposed a method for collecting blog entries about the Hiroshima Electric Railway (Hiroden) from a blog database. 15 Hiroden blog entries were defined as travel journals that provide regional information for streetcar stations in Hiroshima. The task of collecting Hiroden blog entries was divided into two steps: (1) collection of blog entries; and (2) identification of Hiroden blog entries. Figure 7 shows a route map used by the system for providing travel information along the Hiroden streetcar lines. The route map shows Hiroden streetcar stations and major tourist spots. The steps in the search procedure are as follows.

• (Step 1) Click the Hiroden streetcar station, such as ""Ÿ Éü àM"" (Atomic Bomb Dome), in Figure 7 to generate a list of links to Hiroden blog entries ( Figure 8).

• (Step 2) Click the link to a Hiroden blog entry to display it.","[['b14', 'b12'], [], [], ['b14', 'b12'], [], []]","[['b14', 'b12'], [], [], ['b14', 'b12'], [], []]",4,"sent1: Ishino et al. [13] proposed a method for collecting blog entries about the Hiroshima Electric Railway (Hiroden) from a blog database.
sent2: 15 Hiroden blog entries were defined as travel journals that provide regional information for streetcar stations in Hiroshima.
sent3: The task of collecting Hiroden blog entries was divided into two steps: (1) collection of blog entries; and (2) identification of Hiroden blog entries.
sent4: Figure 7 shows a route map used by the system for providing travel information along the Hiroden streetcar lines.
sent5: The route map shows Hiroden streetcar stations and major tourist spots.
sent6: The steps in the search procedure are as follows.
sent7: • (Step 1) Click the Hiroden streetcar station, such as ""Ÿ Éü àM"" (Atomic Bomb Dome), in Figure 7 to generate a list of links to Hiroden blog entries ( Figure 8).
sent8: • (Step 2) Click the link to a Hiroden blog entry to display it.
sent9: Ishino et al. [13] proposed a method for collecting blog entries about the Hiroshima Electric Railway (Hiroden) from a blog database.
sent10: 15 Hiroden blog entries were defined as travel journals that provide regional information for streetcar stations in Hiroshima.
sent11: The task of collecting Hiroden blog entries was divided into two steps: (1) collection of blog entries; and (2) identification of Hiroden blog entries.
sent12: Figure 7 shows a route map used by the system for providing travel information along the Hiroden streetcar lines.
sent13: The route map shows Hiroden streetcar stations and major tourist spots.
sent14: The steps in the search procedure are as follows.
sent15: • (Step 1) Click the Hiroden streetcar station, such as ""Ÿ Éü àM"" (Atomic Bomb Dome), in Figure 7 to generate a list of links to Hiroden blog entries ( Figure 8).
sent16: • (Step 2) Click the link to a Hiroden blog entry to display it."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s23,Natural language interface for accessing databases,"Several ontologies for e-tourism have been developed (see Section 6). Unfortunately, the gap between human users who want to retrieve information and the Semantic Web is yet to be cloased. Ruiz-Martínez et al. [30] proposed a method for querying ontological knowledge bases using natural language sentences. For example, when the user inputted the query ""I want to visit the most important tourist attractions in Paris"", the system conducted part-of-speech tagging, lemmatizing, and modification of query terms by synonyms, and finally searched the ontology. 14 Bressan et al. used images that were categorized into 44 classes as training data for visual categorization. Each class was given a short text name, such as ""clouds and sky"" or ""beach"". When an image was categorized as belonging to classes A and B using the visual categorizer, the short texts given to each class were assigned as keywords of the image. 15 

Several ontologies for e-tourism have been developed (see Section 6). Unfortunately, the gap between human users who want to retrieve information and the Semantic Web is yet to be cloased. Ruiz-Martínez et al. [30] proposed a method for querying ontological knowledge bases using natural language sentences. For example, when the user inputted the query ""I want to visit the most important tourist attractions in Paris"", the system conducted part-of-speech tagging, lemmatizing, and modification of query terms by synonyms, and finally searched the ontology. 14 Bressan et al. used images that were categorized into 44 classes as training data for visual categorization. Each class was given a short text name, such as ""clouds and sky"" or ""beach"". When an image was categorized as belonging to classes A and B using the visual categorizer, the short texts given to each class were assigned as keywords of the image. 15 ","[['b14', 'b31', 'b13'], ['b14', 'b31', 'b13']]","[['b14', 'b31', 'b13'], ['b14', 'b31', 'b13']]",6,"sent1: Several ontologies for e-tourism have been developed (see Section 6).
sent2: Unfortunately, the gap between human users who want to retrieve information and the Semantic Web is yet to be cloased.
sent3: Ruiz-Martínez et al. [30] proposed a method for querying ontological knowledge bases using natural language sentences.
sent4: For example, when the user inputted the query ""I want to visit the most important tourist attractions in Paris"", the system conducted part-of-speech tagging, lemmatizing, and modification of query terms by synonyms, and finally searched the ontology.
sent5: 14 Bressan et al. used images that were categorized into 44 classes as training data for visual categorization.
sent6: Each class was given a short text name, such as ""clouds and sky"" or ""beach"".
sent7: When an image was categorized as belonging to classes A and B using the visual categorizer, the short texts given to each class were assigned as keywords of the image.
sent8: 15 Several ontologies for e-tourism have been developed (see Section 6).
sent9: Unfortunately, the gap between human users who want to retrieve information and the Semantic Web is yet to be cloased.
sent10: Ruiz-Martínez et al. [30] proposed a method for querying ontological knowledge bases using natural language sentences.
sent11: For example, when the user inputted the query ""I want to visit the most important tourist attractions in Paris"", the system conducted part-of-speech tagging, lemmatizing, and modification of query terms by synonyms, and finally searched the ontology.
sent12: 14 Bressan et al. used images that were categorized into 44 classes as training data for visual categorization.
sent13: Each class was given a short text name, such as ""clouds and sky"" or ""beach"".
sent14: When an image was categorized as belonging to classes A and B using the visual categorizer, the short texts given to each class were assigned as keywords of the image.
sent15: 15"
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s26,Useful Sites or Services for Travel,"• Yahoo Travel Guide: http://travel.yahoo.com/ This site provides an area-based recommendation service. For each country, several main cities are listed.

• WikiTravel: http://wikitravel.org The travel recommendation system contributed by ""WikiTravellers"". For each destination, the articles in WikiTravel generally include all or parts of the following information: history, climate, landmarks, work information, shopping information, food, and how to get there.

technologies, such as automatic acquisition of synonyms [5,29,35,36] and word sense disambiguation [23], are available.

• Recommending landmarks (landmark finding) is a standard research topic in image processing using Flickr. In this chapter, we mentioned three studies [1,7,17] that relied mainly on image processing and tag-based recommendation techniques rather than natural language processing. The authors believe that there is still room to improve the methods of recommending landmarks by natural language processing, because sentiment analysis techniques, such as those used for recommending accommodation, have not yet been used for recommending landmarks.

• Yahoo Travel Guide: http://travel.yahoo.com/ This site provides an area-based recommendation service. For each country, several main cities are listed.

• WikiTravel: http://wikitravel.org The travel recommendation system contributed by ""WikiTravellers"". For each destination, the articles in WikiTravel generally include all or parts of the following information: history, climate, landmarks, work information, shopping information, food, and how to get there.

technologies, such as automatic acquisition of synonyms [5,29,35,36] and word sense disambiguation [23], are available.

• Recommending landmarks (landmark finding) is a standard research topic in image processing using Flickr. In this chapter, we mentioned three studies [1,7,17] that relied mainly on image processing and tag-based recommendation techniques rather than natural language processing. The authors believe that there is still room to improve the methods of recommending landmarks by natural language processing, because sentiment analysis techniques, such as those used for recommending accommodation, have not yet been used for recommending landmarks.","[[], [], ['b4', 'b37', 'b36', 'b30', 'b24'], ['b16', 'b6', 'b0'], [], [], ['b4', 'b37', 'b36', 'b30', 'b24'], ['b16', 'b6', 'b0']]","[[], [], ['b4', 'b37', 'b36', 'b30', 'b24'], ['b16', 'b6', 'b0'], [], [], ['b4', 'b37', 'b36', 'b30', 'b24'], ['b16', 'b6', 'b0']]",16,"sent1: • Yahoo Travel Guide: http://travel.yahoo.com/
sent2: This site provides an area-based recommendation service.
sent3: For each country, several main cities are listed.
sent4: • WikiTravel: http://wikitravel.org The travel recommendation system contributed by ""WikiTravellers"".
sent5: For each destination, the articles in WikiTravel generally include all or parts of the following information: history, climate, landmarks, work information, shopping information, food, and how to get there.technologies, such as automatic acquisition of synonyms [5,29,35,36] and word sense disambiguation [23], are available.
sent6: • Recommending landmarks (landmark finding) is a standard research topic in image processing using Flickr.
sent7: In this chapter, we mentioned three studies [1,7,17] that relied mainly on image processing and tag-based recommendation techniques rather than natural language processing.
sent8: The authors believe that there is still room to improve the methods of recommending landmarks by natural language processing, because sentiment analysis techniques, such as those used for recommending accommodation, have not yet been used for recommending landmarks.
sent9: • Yahoo Travel Guide: http://travel.yahoo.com/
sent10: This site provides an area-based recommendation service.
sent11: For each country, several main cities are listed.
sent12: • WikiTravel: http://wikitravel.org The travel recommendation system contributed by ""WikiTravellers"".
sent13: For each destination, the articles in WikiTravel generally include all or parts of the following information: history, climate, landmarks, work information, shopping information, food, and how to get there.technologies, such as automatic acquisition of synonyms [5,29,35,36] and word sense disambiguation [23], are available.
sent14: • Recommending landmarks (landmark finding) is a standard research topic in image processing using Flickr.
sent15: In this chapter, we mentioned three studies [1,7,17] that relied mainly on image processing and tag-based recommendation techniques rather than natural language processing.
sent16: The authors believe that there is still room to improve the methods of recommending landmarks by natural language processing, because sentiment analysis techniques, such as those used for recommending accommodation, have not yet been used for recommending landmarks."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s39,Automatic extraction of travel information from texts,"Nakatoh et al. [24] proposed a method for extracting names of local culinary dishes from travel blogs written in Japanese, which were identified when the blog entry included both the name of a sightseeing destination and the word ""tourism"". They extracted local dishes by gathering nouns that are dependent on the verb ""'ßy‹"" (eat). Tsai and Chou [32] also proposed a method for extracting dish names from restaurant review blogs written in Chinese using a machine learning (CRF) technique.  In the following, we explain the detail of the bootstrapping-based and machine learning-based information extraction approaches based on Nanba's work [25]. Nanba et al. extracted pairs comprising a location name and a local product from travel blogs written in Japanese, which were identified using the method described in Section 2.1. For the efficient extraction of travel information, they employed a bootstrapping method.

First, they prepared 482 pairs as seeds for the bootstrapping. These pairs were obtained automatically from a ""Web Japanese N-gram"" database provided by Google, Inc. The database comprises N-grams (N = 1-7) extracted from 20 billion Japanese sentences on the Web. They applied the pattern ""[0 ] i [ i] "" ([slot of ""location name""] local product [slot of ""local product""] ) to the database, and extracted location names and local products from each corresponding slot, thereby obtaining the 482 pairs. Second, they applied a machine learning-based information extraction technique to the travel blogs identified in the previous step, and obtained new pairs. In this step, they prepared training data for the machine learning in the following three steps.

1. Select 200 sentences that contain both a location name and a local product from the 482 pairs. Then automatically create 200 tagged sentences, to which both ""location"" and ""product"" tags are assigned. 5 2. Prepare another 200 sentences that contain only a location name. Then create 200 tagged sentences, to which the ""location"" tag is assigned.

3. Apply machine learning to the 400 tagged sentences, and obtain a system that automatically allocates ""location"" and ""product"" tags to given sentences.

As a machine learning method, they used CRF. The CRF-based method identifies the class of each word in a given sentence. Features and tags are given in the CRF method as follows: (1) k tags occur before a target word; (2) k features occur before a target word; and (3) k features follow a target word. They used the value of k = 2, which was determined in a pilot study. They used the following six features for machine learning.

• Word.

• The part of speech to which the word belongs (noun, verb, adjective, etc.)

• Whether the word is a quotation mark.

• Whether the word is a cue word, such as "" i"", "" #"", ""y#"" (local product), ""˜Ó"" (famous confection), or "" #"" (souvenir).

• Whether the word is a surface case.

• Whether the word is frequently used in the names of local products or souvenirs, such as ""cake"" or ""noodle"".

Nakatoh et al. [24] proposed a method for extracting names of local culinary dishes from travel blogs written in Japanese, which were identified when the blog entry included both the name of a sightseeing destination and the word ""tourism"". They extracted local dishes by gathering nouns that are dependent on the verb ""'ßy‹"" (eat). Tsai and Chou [32] also proposed a method for extracting dish names from restaurant review blogs written in Chinese using a machine learning (CRF) technique.  In the following, we explain the detail of the bootstrapping-based and machine learning-based information extraction approaches based on Nanba's work [25]. Nanba et al. extracted pairs comprising a location name and a local product from travel blogs written in Japanese, which were identified using the method described in Section 2.1. For the efficient extraction of travel information, they employed a bootstrapping method.

First, they prepared 482 pairs as seeds for the bootstrapping. These pairs were obtained automatically from a ""Web Japanese N-gram"" database provided by Google, Inc. The database comprises N-grams (N = 1-7) extracted from 20 billion Japanese sentences on the Web. They applied the pattern ""[0 ] i [ i] "" ([slot of ""location name""] local product [slot of ""local product""] ) to the database, and extracted location names and local products from each corresponding slot, thereby obtaining the 482 pairs. Second, they applied a machine learning-based information extraction technique to the travel blogs identified in the previous step, and obtained new pairs. In this step, they prepared training data for the machine learning in the following three steps.

1. Select 200 sentences that contain both a location name and a local product from the 482 pairs. Then automatically create 200 tagged sentences, to which both ""location"" and ""product"" tags are assigned. 5 2. Prepare another 200 sentences that contain only a location name. Then create 200 tagged sentences, to which the ""location"" tag is assigned.

3. Apply machine learning to the 400 tagged sentences, and obtain a system that automatically allocates ""location"" and ""product"" tags to given sentences.

As a machine learning method, they used CRF. The CRF-based method identifies the class of each word in a given sentence. Features and tags are given in the CRF method as follows: (1) k tags occur before a target word; (2) k features occur before a target word; and (3) k features follow a target word. They used the value of k = 2, which was determined in a pilot study. They used the following six features for machine learning.

• Word.

• The part of speech to which the word belongs (noun, verb, adjective, etc.)

• Whether the word is a quotation mark.

• Whether the word is a cue word, such as "" i"", "" #"", ""y#"" (local product), ""˜Ó"" (famous confection), or "" #"" (souvenir).

• Whether the word is a surface case.

• Whether the word is frequently used in the names of local products or souvenirs, such as ""cake"" or ""noodle"".","[['b25', 'b33', 'b26'], [], ['b4'], [], [], [], [], [], [], [], [], ['b25', 'b33', 'b26'], [], ['b4'], [], [], [], [], [], [], [], []]","[['b25', 'b33', 'b26'], [], ['b4'], [], [], [], [], [], [], [], [], ['b25', 'b33', 'b26'], [], ['b4'], [], [], [], [], [], [], [], []]",8,"sent1: Nakatoh et al. [24] proposed a method for extracting names of local culinary dishes from travel blogs written in Japanese, which were identified when the blog entry included both the name of a sightseeing destination and the word ""tourism"".
sent2: They extracted local dishes by gathering nouns that are dependent on the verb ""'ßy‹"" (eat).
sent3: Tsai and Chou [32] also proposed a method for extracting dish names from restaurant review blogs written in Chinese using a machine learning (CRF) technique.
sent4: In the following, we explain the detail of the bootstrapping-based and machine learning-based information extraction approaches based on Nanba's work [25].
sent5: Nanba et al. extracted pairs comprising a location name and a local product from travel blogs written in Japanese, which were identified using the method described in Section 2.1.
sent6: For the efficient extraction of travel information, they employed a bootstrapping method.
sent7: First, they prepared 482 pairs as seeds for the bootstrapping.
sent8: These pairs were obtained automatically from a ""Web Japanese N-gram"" database provided by Google, Inc.
sent9: The database comprises N-grams (N = 1-7) extracted from 20 billion Japanese sentences on the Web.
sent10: They applied the pattern ""[0 ] i [ i] "" ([slot of ""location name""] local product [slot of ""local product""] ) to the database, and extracted location names and local products from each corresponding slot, thereby obtaining the 482 pairs.
sent11: Second, they applied a machine learning-based information extraction technique to the travel blogs identified in the previous step, and obtained new pairs.
sent12: In this step, they prepared training data for the machine learning in the following three steps.
sent13: 1. Select 200 sentences that contain both a location name and a local product from the 482 pairs.
sent14: Then automatically create 200 tagged sentences, to which both ""location"" and ""product"" tags are assigned.
sent15: 5 2. Prepare another 200 sentences that contain only a location name.
sent16: Then create 200 tagged sentences, to which the ""location"" tag is assigned.
sent17: 3. Apply machine learning to the 400 tagged sentences, and obtain a system that automatically allocates ""location"" and ""product"" tags to given sentences.
sent18: As a machine learning method, they used CRF.
sent19: The CRF-based method identifies the class of each word in a given sentence.
sent20: Features and tags are given in the CRF method as follows: (1) k tags occur before a target word; (2) k features occur before a target word; and (3) k features follow a target word.
sent21: They used the value of k = 2, which was determined in a pilot study.
sent22: They used the following six features for machine learning.
sent23: • Word. • The part of speech to which the word belongs (noun, verb, adjective, etc.)• Whether the word is a quotation mark.
sent24: • Whether the word is a cue word, such as "" i"", "" #"", ""y#"" (local product), ""˜Ó"" (famous confection), or "" #"" (souvenir).
sent25: • Whether the word is a surface case.
sent26: • Whether the word is frequently used in the names of local products or souvenirs, such as ""cake"" or ""noodle"".
sent27: Nakatoh et al. [24] proposed a method for extracting names of local culinary dishes from travel blogs written in Japanese, which were identified when the blog entry included both the name of a sightseeing destination and the word ""tourism"".
sent28: They extracted local dishes by gathering nouns that are dependent on the verb ""'ßy‹"" (eat).
sent29: Tsai and Chou [32] also proposed a method for extracting dish names from restaurant review blogs written in Chinese using a machine learning (CRF) technique.
sent30: In the following, we explain the detail of the bootstrapping-based and machine learning-based information extraction approaches based on Nanba's work [25].
sent31: Nanba et al. extracted pairs comprising a location name and a local product from travel blogs written in Japanese, which were identified using the method described in Section 2.1.
sent32: For the efficient extraction of travel information, they employed a bootstrapping method.
sent33: First, they prepared 482 pairs as seeds for the bootstrapping.
sent34: These pairs were obtained automatically from a ""Web Japanese N-gram"" database provided by Google, Inc.
sent35: The database comprises N-grams (N = 1-7) extracted from 20 billion Japanese sentences on the Web.
sent36: They applied the pattern ""[0 ] i [ i] "" ([slot of ""location name""] local product [slot of ""local product""] ) to the database, and extracted location names and local products from each corresponding slot, thereby obtaining the 482 pairs.
sent37: Second, they applied a machine learning-based information extraction technique to the travel blogs identified in the previous step, and obtained new pairs.
sent38: In this step, they prepared training data for the machine learning in the following three steps.
sent39: 1. Select 200 sentences that contain both a location name and a local product from the 482 pairs.
sent40: Then automatically create 200 tagged sentences, to which both ""location"" and ""product"" tags are assigned.
sent41: 5 2. Prepare another 200 sentences that contain only a location name.
sent42: Then create 200 tagged sentences, to which the ""location"" tag is assigned.
sent43: 3. Apply machine learning to the 400 tagged sentences, and obtain a system that automatically allocates ""location"" and ""product"" tags to given sentences.
sent44: As a machine learning method, they used CRF.
sent45: The CRF-based method identifies the class of each word in a given sentence.
sent46: Features and tags are given in the CRF method as follows: (1) k tags occur before a target word; (2) k features occur before a target word; and (3) k features follow a target word.
sent47: They used the value of k = 2, which was determined in a pilot study.
sent48: They used the following six features for machine learning.
sent49: • Word. • The part of speech to which the word belongs (noun, verb, adjective, etc.)• Whether the word is a quotation mark.
sent50: • Whether the word is a cue word, such as "" i"", "" #"", ""y#"" (local product), ""˜Ó"" (famous confection), or "" #"" (souvenir).
sent51: • Whether the word is a surface case.
sent52: • Whether the word is frequently used in the names of local products or souvenirs, such as ""cake"" or ""noodle""."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s44,Travelers' behavior analysis,"The analysis of people's transportation information is considered an important issue in various fields, such as city planning, architectural planning, car navigation, sightseeing administration, crime prevention, and tracing the spread of infection of epidemics. In this section, we focus on the analysis of travelers' behavior.

Ishino et al. [15] proposed a method to extract people's transportation information from automatically identified travel blogs written in Japanese [25]. They used machine learning to extract information, such as ""departure place"", ""destination"", or ""transportation device"", from travel blog entries. First, the tags used in their examination are defined.

• FROM tag indicates the departure place.

• TO tag indicates the destination.

• VIA tag indicates the route.

• METHOD tag indicates the transportation device.

• TIME tag indicates the time of transportation.

The following is a tagged example.

It took <TIME>five hours< /TIME> to travel from <FROM>Hiroshima< /FROM> to<TO>Osaka< /TO> by <METHOD>bus< /METHOD>.

They formulated the task of identifying the class of each word in a given sentence and solved it using machine learning. For the machine learning method, they used CRF [20], in the same way as Nanba et al. [25], which we mentioned in Section 2.2. The CRF-based method identifies the class of each entry. Features and tags are used in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry. They used the value k = 4 7 , which was determined via a pilot study. They used the following features for machine learning.

• A word.

• The part of speech to which the word belongs (noun, verb, adjective, etc.).

• Whether the word is a quotation mark.

• Whether the word is a cue phrase.

The details of cue phrases, together with the number of cue phrases of the given type, are shown as follows.

The analysis of people's transportation information is considered an important issue in various fields, such as city planning, architectural planning, car navigation, sightseeing administration, crime prevention, and tracing the spread of infection of epidemics. In this section, we focus on the analysis of travelers' behavior.

Ishino et al. [15] proposed a method to extract people's transportation information from automatically identified travel blogs written in Japanese [25]. They used machine learning to extract information, such as ""departure place"", ""destination"", or ""transportation device"", from travel blog entries. First, the tags used in their examination are defined.

• FROM tag indicates the departure place.

• TO tag indicates the destination.

• VIA tag indicates the route.

• METHOD tag indicates the transportation device.

• TIME tag indicates the time of transportation.

The following is a tagged example.

It took <TIME>five hours< /TIME> to travel from <FROM>Hiroshima< /FROM> to<TO>Osaka< /TO> by <METHOD>bus< /METHOD>.

They formulated the task of identifying the class of each word in a given sentence and solved it using machine learning. For the machine learning method, they used CRF [20], in the same way as Nanba et al. [25], which we mentioned in Section 2.2. The CRF-based method identifies the class of each entry. Features and tags are used in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry. They used the value k = 4 7 , which was determined via a pilot study. They used the following features for machine learning.

• A word.

• The part of speech to which the word belongs (noun, verb, adjective, etc.).

• Whether the word is a quotation mark.

• Whether the word is a cue phrase.

The details of cue phrases, together with the number of cue phrases of the given type, are shown as follows.","[[], ['b14', 'b26'], [], [], [], [], [], [], [], ['b26', 'b19'], [], [], [], [], [], [], ['b14', 'b26'], [], [], [], [], [], [], [], ['b26', 'b19'], [], [], [], [], []]","[[], ['b14', 'b26'], [], [], [], [], [], [], [], ['b26', 'b19'], [], [], [], [], [], [], ['b14', 'b26'], [], [], [], [], [], [], [], ['b26', 'b19'], [], [], [], [], []]",8,"sent1: The analysis of people's transportation information is considered an important issue in various fields, such as city planning, architectural planning, car navigation, sightseeing administration, crime prevention, and tracing the spread of infection of epidemics.
sent2: In this section, we focus on the analysis of travelers' behavior.
sent3: Ishino et al. [15] proposed a method to extract people's transportation information from automatically identified travel blogs written in Japanese [25].
sent4: They used machine learning to extract information, such as ""departure place"", ""destination"", or ""transportation device"", from travel blog entries.
sent5: First, the tags used in their examination are defined.
sent6: • FROM tag indicates the departure place.
sent7: • TO tag indicates the destination.
sent8: • VIA tag indicates the route. • METHOD tag indicates the transportation device.
sent9: • TIME tag indicates the time of transportation.
sent10: The following is a tagged example.
sent11: It took <TIME>five hours< /TIME> to travel from <FROM>Hiroshima< /FROM> to<TO>Osaka< /TO>
sent12: by <METHOD>bus< /METHOD>. They formulated the task of identifying the class of each word in a given sentence and solved it using machine learning.
sent13: For the machine learning method, they used CRF [20], in the same way as Nanba et al. [25], which we mentioned in Section 2.2.
sent14: The CRF-based method identifies the class of each entry.
sent15: Features and tags are used in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry.
sent16: They used the value k = 4 7 , which was determined via a pilot study.
sent17: They used the following features for machine learning.
sent18: • A word. • The part of speech to which the word belongs (noun, verb, adjective, etc.).• Whether the word is a quotation mark.
sent19: • Whether the word is a cue phrase.
sent20: The details of cue phrases, together with the number of cue phrases of the given type, are shown as follows.
sent21: The analysis of people's transportation information is considered an important issue in various fields, such as city planning, architectural planning, car navigation, sightseeing administration, crime prevention, and tracing the spread of infection of epidemics.
sent22: In this section, we focus on the analysis of travelers' behavior.
sent23: Ishino et al. [15] proposed a method to extract people's transportation information from automatically identified travel blogs written in Japanese [25].
sent24: They used machine learning to extract information, such as ""departure place"", ""destination"", or ""transportation device"", from travel blog entries.
sent25: First, the tags used in their examination are defined.
sent26: • FROM tag indicates the departure place.
sent27: • TO tag indicates the destination.
sent28: • VIA tag indicates the route. • METHOD tag indicates the transportation device.
sent29: • TIME tag indicates the time of transportation.
sent30: The following is a tagged example.
sent31: It took <TIME>five hours< /TIME> to travel from <FROM>Hiroshima< /FROM> to<TO>Osaka< /TO>
sent32: by <METHOD>bus< /METHOD>. They formulated the task of identifying the class of each word in a given sentence and solved it using machine learning.
sent33: For the machine learning method, they used CRF [20], in the same way as Nanba et al. [25], which we mentioned in Section 2.2.
sent34: The CRF-based method identifies the class of each entry.
sent35: Features and tags are used in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry.
sent36: They used the value k = 4 7 , which was determined via a pilot study.
sent37: They used the following features for machine learning.
sent38: • A word. • The part of speech to which the word belongs (noun, verb, adjective, etc.).• Whether the word is a quotation mark.
sent39: • Whether the word is a cue phrase.
sent40: The details of cue phrases, together with the number of cue phrases of the given type, are shown as follows."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s53,Recommending landmarks,"Finding and recommending landmarks is considered an important research topic in the multimedia field, along with recommending tourist spots. Abbasi et al. [1] focused on the photo-sharing system Flickr, and proposed a method to identify landmark photos using tags and social Flickr groups. Gao et al. [7] also proposed a method to identify landmarks using Flickr and the Yahoo Travel Guide.

Ji et al. [17] proposed another method for finding landmarks. They adopted the method of clustering blog photos relating to a particular tourist site, such as Louvre Museum in Paris. 10 Then they represented these photos as a graph based on the clustering results, and detected landmarks using link analysis methods, such as the PageRank [3] and HITS [19] algorithms.

Finding and recommending landmarks is considered an important research topic in the multimedia field, along with recommending tourist spots. Abbasi et al. [1] focused on the photo-sharing system Flickr, and proposed a method to identify landmark photos using tags and social Flickr groups. Gao et al. [7] also proposed a method to identify landmarks using Flickr and the Yahoo Travel Guide.

Ji et al. [17] proposed another method for finding landmarks. They adopted the method of clustering blog photos relating to a particular tourist site, such as Louvre Museum in Paris. 10 Then they represented these photos as a graph based on the clustering results, and detected landmarks using link analysis methods, such as the PageRank [3] and HITS [19] algorithms.","[['b6', 'b0'], ['b18', 'b9', 'b2', 'b16'], ['b6', 'b0'], ['b18', 'b9', 'b2', 'b16']]","[['b6', 'b0'], ['b18', 'b9', 'b2', 'b16'], ['b6', 'b0'], ['b18', 'b9', 'b2', 'b16']]",12,"sent1: Finding and recommending landmarks is considered an important research topic in the multimedia field, along with recommending tourist spots.
sent2: Abbasi et al. [1] focused on the photo-sharing system Flickr, and proposed a method to identify landmark photos using tags and social Flickr groups.
sent3: Gao et al. [7] also proposed a method to identify landmarks using Flickr and the Yahoo Travel Guide.
sent4: Ji et al. [17] proposed another method for finding landmarks.
sent5: They adopted the method of clustering blog photos relating to a particular tourist site, such as Louvre Museum in Paris.
sent6: 10 Then they represented these photos as a graph based on the clustering results, and detected landmarks using link analysis methods, such as the PageRank [3] and HITS [19] algorithms.
sent7: Finding and recommending landmarks is considered an important research topic in the multimedia field, along with recommending tourist spots.
sent8: Abbasi et al. [1] focused on the photo-sharing system Flickr, and proposed a method to identify landmark photos using tags and social Flickr groups.
sent9: Gao et al. [7] also proposed a method to identify landmarks using Flickr and the Yahoo Travel Guide.
sent10: Ji et al. [17] proposed another method for finding landmarks.
sent11: They adopted the method of clustering blog photos relating to a particular tourist site, such as Louvre Museum in Paris.
sent12: 10 Then they represented these photos as a graph based on the clustering results, and detected landmarks using link analysis methods, such as the PageRank [3] and HITS [19] algorithms."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s54,Recommending travel products,"Ishino et al. [14] proposed a method that added links to advertisements for travel products to the travel information links that were described in Section 2.3. 11 The procedure for providing ad links is as follows.

1. Input a link type and the citing areas of a travel information link.

2. Extract keywords from the citing areas.

3. Extract product data containing all keywords, and calculate the similarity between the citing areas of a travel information link and the product data.

4. Provide the ad link to the product data having the highest similarity to the travel information link.

They extracted keywords for travel products corresponding to the link type. They used the same cues to classify travel information links [15] (see Section 2.3), and then extracted keywords from the citing areas of links of types S (Spot) and R (Restaurant). 10 For calculating the similarity between two photos, they used the Bag-of-Visual-Words representation [18,26], which represents an image as a set of salient regions (visual words), called Bag-of-Visual-Words vectors. Then the similarity between photos is measured based on the cosine distance between their Bag-of-Visual-Words vectors. In addition to the features in each image, they also used textual information for each photo, such as the title, description, and surrounding text. 11 http://www.ls.info.hiroshima-cu.ac.jp/travel/ First, the method for extracting keywords from the citing areas of links of type S is described. The cues for type S, such as tourist spots collected from Wikipedia and words frequently used in the names of tourist spots, tend to become keywords. Therefore, they registered these cues as candidate keywords for links of type S. If the citing areas of these links contained candidate keywords, they extracted the candidates as keywords. In addition, if citing areas contained names of places, they extracted the names as keywords.

The cues for type R, such as dish names and cooking styles, also tend to become keywords. Therefore, they registered these cues as candidate keywords for links of type R. If the citing areas for links of type R contained candidate keywords, they extracted them as keywords. Titov and McDonald [31] proposed an aspect-based summarization system, and applied the method to the summarization of hotel reviews. The system took as input a set of user reviews for a specific product or service with a numeric rating (left side in Figure 6), and produced a set of relevant aspects, which they called an aspect-based summary (right side in Figure  6). To extract all relevant mentions in each review for each aspect, they introduced a topic model. They applied their method to hotel reviews on the TripAdvisor Web site 12 , and obtained aspect-based summaries for each hotel. To obtain more reliable hotel reviews, opinion spams should be detected and eliminated. Opinion spams are fictitious opinions that have been deliberately written to sound authentic. Ott et al. [27] proposed a method to detect opinion spam among consumer reviews of hotels. They created 400 deceptive opinions using the Amazon Mechanical Turk (AMT) crowdsourcing service 13 by asking anonymous online workers (Turkers) to create the opinion spam for 20 chosen hotels. In addition to these spam messages, they selected 6,977 truthful opinions from TripAdvisor, and used both groups for their task.

Ishino et al. [14] proposed a method that added links to advertisements for travel products to the travel information links that were described in Section 2.3. 11 The procedure for providing ad links is as follows.

1. Input a link type and the citing areas of a travel information link.

2. Extract keywords from the citing areas.

3. Extract product data containing all keywords, and calculate the similarity between the citing areas of a travel information link and the product data.

4. Provide the ad link to the product data having the highest similarity to the travel information link.

They extracted keywords for travel products corresponding to the link type. They used the same cues to classify travel information links [15] (see Section 2.3), and then extracted keywords from the citing areas of links of types S (Spot) and R (Restaurant). 10 For calculating the similarity between two photos, they used the Bag-of-Visual-Words representation [18,26], which represents an image as a set of salient regions (visual words), called Bag-of-Visual-Words vectors. Then the similarity between photos is measured based on the cosine distance between their Bag-of-Visual-Words vectors. In addition to the features in each image, they also used textual information for each photo, such as the title, description, and surrounding text. 11 http://www.ls.info.hiroshima-cu.ac.jp/travel/ First, the method for extracting keywords from the citing areas of links of type S is described. The cues for type S, such as tourist spots collected from Wikipedia and words frequently used in the names of tourist spots, tend to become keywords. Therefore, they registered these cues as candidate keywords for links of type S. If the citing areas of these links contained candidate keywords, they extracted the candidates as keywords. In addition, if citing areas contained names of places, they extracted the names as keywords.

The cues for type R, such as dish names and cooking styles, also tend to become keywords. Therefore, they registered these cues as candidate keywords for links of type R. If the citing areas for links of type R contained candidate keywords, they extracted them as keywords. Titov and McDonald [31] proposed an aspect-based summarization system, and applied the method to the summarization of hotel reviews. The system took as input a set of user reviews for a specific product or service with a numeric rating (left side in Figure 6), and produced a set of relevant aspects, which they called an aspect-based summary (right side in Figure  6). To extract all relevant mentions in each review for each aspect, they introduced a topic model. They applied their method to hotel reviews on the TripAdvisor Web site 12 , and obtained aspect-based summaries for each hotel. To obtain more reliable hotel reviews, opinion spams should be detected and eliminated. Opinion spams are fictitious opinions that have been deliberately written to sound authentic. Ott et al. [27] proposed a method to detect opinion spam among consumer reviews of hotels. They created 400 deceptive opinions using the Amazon Mechanical Turk (AMT) crowdsourcing service 13 by asking anonymous online workers (Turkers) to create the opinion spam for 20 chosen hotels. In addition to these spam messages, they selected 6,977 truthful opinions from TripAdvisor, and used both groups for their task.","[['b10', 'b13'], [], [], [], [], ['b27', 'b14', 'b9', 'b17'], [None, 'b28'], ['b10', 'b13'], [], [], [], [], ['b27', 'b14', 'b9', 'b17'], [None, 'b28']]","[['b10', 'b13'], [], [], [], [], ['b27', 'b14', 'b9', 'b17'], [None, 'b28'], ['b10', 'b13'], [], [], [], [], ['b27', 'b14', 'b9', 'b17'], [None, 'b28']]",16,"sent1: Ishino et al. [14] proposed a method that added links to advertisements for travel products to the travel information links that were described in Section 2.3.
sent2: 11 The procedure for providing ad links is as follows.
sent3: 1. Input a link type and the citing areas of a travel information link.
sent4: 2. Extract keywords from the citing areas.
sent5: 3. Extract product data containing all keywords, and calculate the similarity between the citing areas of a travel information link and the product data.4.
sent6: Provide the ad link to the product data having the highest similarity to the travel information link.
sent7: They extracted keywords for travel products corresponding to the link type.
sent8: They used the same cues to classify travel information links [15] (see Section 2.3), and then extracted keywords from the citing areas of links of types S (Spot) and R (Restaurant). 10 For calculating the similarity between two photos, they used the Bag-of-Visual-Words representation [18,26], which represents an image as a set of salient regions (visual words), called Bag-of-Visual-Words vectors.
sent9: Then the similarity between photos is measured based on the cosine distance between their Bag-of-Visual-Words vectors.
sent10: In addition to the features in each image, they also used textual information for each photo, such as the title, description, and surrounding text.
sent11: 11 http://www.ls.info.hiroshima-cu.ac.jp/travel/ First, the method for extracting keywords from the citing areas of links of type S is described.
sent12: The cues for type S, such as tourist spots collected from Wikipedia and words frequently used in the names of tourist spots, tend to become keywords.
sent13: Therefore, they registered these cues as candidate keywords for links of type S.
sent14: If the citing areas of these links contained candidate keywords, they extracted the candidates as keywords.
sent15: In addition, if citing areas contained names of places, they extracted the names as keywords.
sent16: The cues for type R, such as dish names and cooking styles, also tend to become keywords.
sent17: Therefore, they registered these cues as candidate keywords for links of type R.
sent18: If the citing areas for links of type R contained candidate keywords, they extracted them as keywords.
sent19: Titov and McDonald [31] proposed an aspect-based summarization system, and applied the method to the summarization of hotel reviews.
sent20: The system took as input a set of user reviews for a specific product or service with a numeric rating (left side in Figure 6), and produced a set of relevant aspects, which they called an aspect-based summary (right side in Figure  6).
sent21: To extract all relevant mentions in each review for each aspect, they introduced a topic model.
sent22: They applied their method to hotel reviews on the TripAdvisor Web site 12 , and obtained aspect-based summaries for each hotel.
sent23: To obtain more reliable hotel reviews, opinion spams should be detected and eliminated.
sent24: Opinion spams are fictitious opinions that have been deliberately written to sound authentic.
sent25: Ott et al. [27] proposed a method to detect opinion spam among consumer reviews of hotels.
sent26: They created 400 deceptive opinions using the Amazon Mechanical Turk (AMT) crowdsourcing service 13 by asking anonymous online workers (Turkers) to create the opinion spam for 20 chosen hotels.
sent27: In addition to these spam messages, they selected 6,977 truthful opinions from TripAdvisor, and used both groups for their task.
sent28: Ishino et al. [14] proposed a method that added links to advertisements for travel products to the travel information links that were described in Section 2.3.
sent29: 11 The procedure for providing ad links is as follows.
sent30: 1. Input a link type and the citing areas of a travel information link.
sent31: 2. Extract keywords from the citing areas.
sent32: 3. Extract product data containing all keywords, and calculate the similarity between the citing areas of a travel information link and the product data.4.
sent33: Provide the ad link to the product data having the highest similarity to the travel information link.
sent34: They extracted keywords for travel products corresponding to the link type.
sent35: They used the same cues to classify travel information links [15] (see Section 2.3), and then extracted keywords from the citing areas of links of types S (Spot) and R (Restaurant). 10 For calculating the similarity between two photos, they used the Bag-of-Visual-Words representation [18,26], which represents an image as a set of salient regions (visual words), called Bag-of-Visual-Words vectors.
sent36: Then the similarity between photos is measured based on the cosine distance between their Bag-of-Visual-Words vectors.
sent37: In addition to the features in each image, they also used textual information for each photo, such as the title, description, and surrounding text.
sent38: 11 http://www.ls.info.hiroshima-cu.ac.jp/travel/ First, the method for extracting keywords from the citing areas of links of type S is described.
sent39: The cues for type S, such as tourist spots collected from Wikipedia and words frequently used in the names of tourist spots, tend to become keywords.
sent40: Therefore, they registered these cues as candidate keywords for links of type S.
sent41: If the citing areas of these links contained candidate keywords, they extracted the candidates as keywords.
sent42: In addition, if citing areas contained names of places, they extracted the names as keywords.
sent43: The cues for type R, such as dish names and cooking styles, also tend to become keywords.
sent44: Therefore, they registered these cues as candidate keywords for links of type R.
sent45: If the citing areas for links of type R contained candidate keywords, they extracted them as keywords.
sent46: Titov and McDonald [31] proposed an aspect-based summarization system, and applied the method to the summarization of hotel reviews.
sent47: The system took as input a set of user reviews for a specific product or service with a numeric rating (left side in Figure 6), and produced a set of relevant aspects, which they called an aspect-based summary (right side in Figure  6).
sent48: To extract all relevant mentions in each review for each aspect, they introduced a topic model.
sent49: They applied their method to hotel reviews on the TripAdvisor Web site 12 , and obtained aspect-based summaries for each hotel.
sent50: To obtain more reliable hotel reviews, opinion spams should be detected and eliminated.
sent51: Opinion spams are fictitious opinions that have been deliberately written to sound authentic.
sent52: Ott et al. [27] proposed a method to detect opinion spam among consumer reviews of hotels.
sent53: They created 400 deceptive opinions using the Amazon Mechanical Turk (AMT) crowdsourcing service 13 by asking anonymous online workers (Turkers) to create the opinion spam for 20 chosen hotels.
sent54: In addition to these spam messages, they selected 6,977 truthful opinions from TripAdvisor, and used both groups for their task."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s58,Providing travel information along streetcar lines,"Ishino et al. [13] proposed a method for collecting blog entries about the Hiroshima Electric Railway (Hiroden) from a blog database. 15 Hiroden blog entries were defined as travel journals that provide regional information for streetcar stations in Hiroshima. The task of collecting Hiroden blog entries was divided into two steps: (1) collection of blog entries; and (2) identification of Hiroden blog entries. Figure 7 shows a route map used by the system for providing travel information along the Hiroden streetcar lines. The route map shows Hiroden streetcar stations and major tourist spots. The steps in the search procedure are as follows.

• (Step 1) Click the Hiroden streetcar station, such as ""Ÿ Éü àM"" (Atomic Bomb Dome), in Figure 7 to generate a list of links to Hiroden blog entries ( Figure 8).

• (Step 2) Click the link to a Hiroden blog entry to display it.

Ishino et al. [13] proposed a method for collecting blog entries about the Hiroshima Electric Railway (Hiroden) from a blog database. 15 Hiroden blog entries were defined as travel journals that provide regional information for streetcar stations in Hiroshima. The task of collecting Hiroden blog entries was divided into two steps: (1) collection of blog entries; and (2) identification of Hiroden blog entries. Figure 7 shows a route map used by the system for providing travel information along the Hiroden streetcar lines. The route map shows Hiroden streetcar stations and major tourist spots. The steps in the search procedure are as follows.

• (Step 1) Click the Hiroden streetcar station, such as ""Ÿ Éü àM"" (Atomic Bomb Dome), in Figure 7 to generate a list of links to Hiroden blog entries ( Figure 8).

• (Step 2) Click the link to a Hiroden blog entry to display it.","[['b14', 'b12'], [], [], ['b14', 'b12'], [], []]","[['b14', 'b12'], [], [], ['b14', 'b12'], [], []]",4,"sent1: Ishino et al. [13] proposed a method for collecting blog entries about the Hiroshima Electric Railway (Hiroden) from a blog database.
sent2: 15 Hiroden blog entries were defined as travel journals that provide regional information for streetcar stations in Hiroshima.
sent3: The task of collecting Hiroden blog entries was divided into two steps: (1) collection of blog entries; and (2) identification of Hiroden blog entries.
sent4: Figure 7 shows a route map used by the system for providing travel information along the Hiroden streetcar lines.
sent5: The route map shows Hiroden streetcar stations and major tourist spots.
sent6: The steps in the search procedure are as follows.
sent7: • (Step 1) Click the Hiroden streetcar station, such as ""Ÿ Éü àM"" (Atomic Bomb Dome), in Figure 7 to generate a list of links to Hiroden blog entries ( Figure 8).
sent8: • (Step 2) Click the link to a Hiroden blog entry to display it.
sent9: Ishino et al. [13] proposed a method for collecting blog entries about the Hiroshima Electric Railway (Hiroden) from a blog database.
sent10: 15 Hiroden blog entries were defined as travel journals that provide regional information for streetcar stations in Hiroshima.
sent11: The task of collecting Hiroden blog entries was divided into two steps: (1) collection of blog entries; and (2) identification of Hiroden blog entries.
sent12: Figure 7 shows a route map used by the system for providing travel information along the Hiroden streetcar lines.
sent13: The route map shows Hiroden streetcar stations and major tourist spots.
sent14: The steps in the search procedure are as follows.
sent15: • (Step 1) Click the Hiroden streetcar station, such as ""Ÿ Éü àM"" (Atomic Bomb Dome), in Figure 7 to generate a list of links to Hiroden blog entries ( Figure 8).
sent16: • (Step 2) Click the link to a Hiroden blog entry to display it."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s59,Natural language interface for accessing databases,"Several ontologies for e-tourism have been developed (see Section 6). Unfortunately, the gap between human users who want to retrieve information and the Semantic Web is yet to be cloased. Ruiz-Martínez et al. [30] proposed a method for querying ontological knowledge bases using natural language sentences. For example, when the user inputted the query ""I want to visit the most important tourist attractions in Paris"", the system conducted part-of-speech tagging, lemmatizing, and modification of query terms by synonyms, and finally searched the ontology. 14 Bressan et al. used images that were categorized into 44 classes as training data for visual categorization. Each class was given a short text name, such as ""clouds and sky"" or ""beach"". When an image was categorized as belonging to classes A and B using the visual categorizer, the short texts given to each class were assigned as keywords of the image. 15 

Several ontologies for e-tourism have been developed (see Section 6). Unfortunately, the gap between human users who want to retrieve information and the Semantic Web is yet to be cloased. Ruiz-Martínez et al. [30] proposed a method for querying ontological knowledge bases using natural language sentences. For example, when the user inputted the query ""I want to visit the most important tourist attractions in Paris"", the system conducted part-of-speech tagging, lemmatizing, and modification of query terms by synonyms, and finally searched the ontology. 14 Bressan et al. used images that were categorized into 44 classes as training data for visual categorization. Each class was given a short text name, such as ""clouds and sky"" or ""beach"". When an image was categorized as belonging to classes A and B using the visual categorizer, the short texts given to each class were assigned as keywords of the image. 15 ","[['b14', 'b31', 'b13'], ['b14', 'b31', 'b13']]","[['b14', 'b31', 'b13'], ['b14', 'b31', 'b13']]",6,"sent1: Several ontologies for e-tourism have been developed (see Section 6).
sent2: Unfortunately, the gap between human users who want to retrieve information and the Semantic Web is yet to be cloased.
sent3: Ruiz-Martínez et al. [30] proposed a method for querying ontological knowledge bases using natural language sentences.
sent4: For example, when the user inputted the query ""I want to visit the most important tourist attractions in Paris"", the system conducted part-of-speech tagging, lemmatizing, and modification of query terms by synonyms, and finally searched the ontology.
sent5: 14 Bressan et al. used images that were categorized into 44 classes as training data for visual categorization.
sent6: Each class was given a short text name, such as ""clouds and sky"" or ""beach"".
sent7: When an image was categorized as belonging to classes A and B using the visual categorizer, the short texts given to each class were assigned as keywords of the image.
sent8: 15 Several ontologies for e-tourism have been developed (see Section 6).
sent9: Unfortunately, the gap between human users who want to retrieve information and the Semantic Web is yet to be cloased.
sent10: Ruiz-Martínez et al. [30] proposed a method for querying ontological knowledge bases using natural language sentences.
sent11: For example, when the user inputted the query ""I want to visit the most important tourist attractions in Paris"", the system conducted part-of-speech tagging, lemmatizing, and modification of query terms by synonyms, and finally searched the ontology.
sent12: 14 Bressan et al. used images that were categorized into 44 classes as training data for visual categorization.
sent13: Each class was given a short text name, such as ""clouds and sky"" or ""beach"".
sent14: When an image was categorized as belonging to classes A and B using the visual categorizer, the short texts given to each class were assigned as keywords of the image.
sent15: 15"
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s62,Useful Sites or Services for Travel,"• Yahoo Travel Guide: http://travel.yahoo.com/ This site provides an area-based recommendation service. For each country, several main cities are listed.

• WikiTravel: http://wikitravel.org The travel recommendation system contributed by ""WikiTravellers"". For each destination, the articles in WikiTravel generally include all or parts of the following information: history, climate, landmarks, work information, shopping information, food, and how to get there.

technologies, such as automatic acquisition of synonyms [5,29,35,36] and word sense disambiguation [23], are available.

• Recommending landmarks (landmark finding) is a standard research topic in image processing using Flickr. In this chapter, we mentioned three studies [1,7,17] that relied mainly on image processing and tag-based recommendation techniques rather than natural language processing. The authors believe that there is still room to improve the methods of recommending landmarks by natural language processing, because sentiment analysis techniques, such as those used for recommending accommodation, have not yet been used for recommending landmarks.

• Yahoo Travel Guide: http://travel.yahoo.com/ This site provides an area-based recommendation service. For each country, several main cities are listed.

• WikiTravel: http://wikitravel.org The travel recommendation system contributed by ""WikiTravellers"". For each destination, the articles in WikiTravel generally include all or parts of the following information: history, climate, landmarks, work information, shopping information, food, and how to get there.

technologies, such as automatic acquisition of synonyms [5,29,35,36] and word sense disambiguation [23], are available.

• Recommending landmarks (landmark finding) is a standard research topic in image processing using Flickr. In this chapter, we mentioned three studies [1,7,17] that relied mainly on image processing and tag-based recommendation techniques rather than natural language processing. The authors believe that there is still room to improve the methods of recommending landmarks by natural language processing, because sentiment analysis techniques, such as those used for recommending accommodation, have not yet been used for recommending landmarks.","[[], [], ['b4', 'b37', 'b36', 'b30', 'b24'], ['b16', 'b6', 'b0'], [], [], ['b4', 'b37', 'b36', 'b30', 'b24'], ['b16', 'b6', 'b0']]","[[], [], ['b4', 'b37', 'b36', 'b30', 'b24'], ['b16', 'b6', 'b0'], [], [], ['b4', 'b37', 'b36', 'b30', 'b24'], ['b16', 'b6', 'b0']]",16,"sent1: • Yahoo Travel Guide: http://travel.yahoo.com/
sent2: This site provides an area-based recommendation service.
sent3: For each country, several main cities are listed.
sent4: • WikiTravel: http://wikitravel.org The travel recommendation system contributed by ""WikiTravellers"".
sent5: For each destination, the articles in WikiTravel generally include all or parts of the following information: history, climate, landmarks, work information, shopping information, food, and how to get there.technologies, such as automatic acquisition of synonyms [5,29,35,36] and word sense disambiguation [23], are available.
sent6: • Recommending landmarks (landmark finding) is a standard research topic in image processing using Flickr.
sent7: In this chapter, we mentioned three studies [1,7,17] that relied mainly on image processing and tag-based recommendation techniques rather than natural language processing.
sent8: The authors believe that there is still room to improve the methods of recommending landmarks by natural language processing, because sentiment analysis techniques, such as those used for recommending accommodation, have not yet been used for recommending landmarks.
sent9: • Yahoo Travel Guide: http://travel.yahoo.com/
sent10: This site provides an area-based recommendation service.
sent11: For each country, several main cities are listed.
sent12: • WikiTravel: http://wikitravel.org The travel recommendation system contributed by ""WikiTravellers"".
sent13: For each destination, the articles in WikiTravel generally include all or parts of the following information: history, climate, landmarks, work information, shopping information, food, and how to get there.technologies, such as automatic acquisition of synonyms [5,29,35,36] and word sense disambiguation [23], are available.
sent14: • Recommending landmarks (landmark finding) is a standard research topic in image processing using Flickr.
sent15: In this chapter, we mentioned three studies [1,7,17] that relied mainly on image processing and tag-based recommendation techniques rather than natural language processing.
sent16: The authors believe that there is still room to improve the methods of recommending landmarks by natural language processing, because sentiment analysis techniques, such as those used for recommending accommodation, have not yet been used for recommending landmarks."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s38,Automatic identification of travel blog entries,"Travel blogs 1 are defined as travel journals written by bloggers in diary form. Travel blogs are considered useful for obtaining travel information, because many bloggers' travel experiences are written in this form.

There are various portal sites for travel blogs, which we will describe in Section 6. At these sites, travel blogs are manually registered by bloggers themselves, and the blogs are classified according to travel destination. However, there are many more travel blogs in the blogosphere, beyond these portal sites. In an attempt to construct an exhaustive database of travel blogs, Nanba et al. [25] identified travel blog entries written in Japanese in a blog database. 2 Blog entries that contain cue phrases, such as ""travel"", ""sightseeing"", or ""tour"", have a high degree of probability of being travel blogs. However, not every travel blog contains such cue phrases. For example, if a blogger describes his/her journey to Norway in multiple blog entries, the blog might state ""We traveled to Norway"" in the first entry, while only writing ""We ate wild sheep!"" in the second entry. In this case, because the second entry does not contain any expressions related to travel, it is difficult to identify it as a travel blog entry. Therefore, Nanba et al. focused not only on each blog entry but also on the surrounding entries for the identification of travel blog entries. They formulated the identification of travel blog entries as a sequence-labeling problem, and solved it using machine learning. For the machine learning method, they examined the Conditional Random Fields (CRF) method [20]; its empirical success has been reported recently in the field of natural language processing. The CRF-based method identifies the tag 3 of each entry. Features and tags are given in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry (see Figure 1). They used the value of k = 4, which was determined in a pilot study. Here, they used the following features for machine learning: whether an entry contains any of 416 cue phrases, such as ""ÅL (travel)"", ""Ä¢ü (tour)"", and ""úz (departure)"", and the number of location names in each entry.

Using the above method, Nanba et al. identified 17,268 travel blog entries from 1,100,000 blog entries, and constructed a system that plotted travel blog entries on a Google map (see Figure 2). 4 In this figure, travel blog entries are shown as icons. If the user clicks an icon, the corresponding blog entry is shown in a pop-up window.

Travel blogs 1 are defined as travel journals written by bloggers in diary form. Travel blogs are considered useful for obtaining travel information, because many bloggers' travel experiences are written in this form.

There are various portal sites for travel blogs, which we will describe in Section 6. At these sites, travel blogs are manually registered by bloggers themselves, and the blogs are classified according to travel destination. However, there are many more travel blogs in the blogosphere, beyond these portal sites. In an attempt to construct an exhaustive database of travel blogs, Nanba et al. [25] identified travel blog entries written in Japanese in a blog database. 2 Blog entries that contain cue phrases, such as ""travel"", ""sightseeing"", or ""tour"", have a high degree of probability of being travel blogs. However, not every travel blog contains such cue phrases. For example, if a blogger describes his/her journey to Norway in multiple blog entries, the blog might state ""We traveled to Norway"" in the first entry, while only writing ""We ate wild sheep!"" in the second entry. In this case, because the second entry does not contain any expressions related to travel, it is difficult to identify it as a travel blog entry. Therefore, Nanba et al. focused not only on each blog entry but also on the surrounding entries for the identification of travel blog entries. They formulated the identification of travel blog entries as a sequence-labeling problem, and solved it using machine learning. For the machine learning method, they examined the Conditional Random Fields (CRF) method [20]; its empirical success has been reported recently in the field of natural language processing. The CRF-based method identifies the tag 3 of each entry. Features and tags are given in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry (see Figure 1). They used the value of k = 4, which was determined in a pilot study. Here, they used the following features for machine learning: whether an entry contains any of 416 cue phrases, such as ""ÅL (travel)"", ""Ä¢ü (tour)"", and ""úz (departure)"", and the number of location names in each entry.

Using the above method, Nanba et al. identified 17,268 travel blog entries from 1,100,000 blog entries, and constructed a system that plotted travel blog entries on a Google map (see Figure 2). 4 In this figure, travel blog entries are shown as icons. If the user clicks an icon, the corresponding blog entry is shown in a pop-up window.","[[], ['b1', 'b19', 'b26'], ['b3'], [], ['b1', 'b19', 'b26'], ['b3']]","[[], ['b1', 'b19', 'b26'], ['b3'], [], ['b1', 'b19', 'b26'], ['b3']]",8,"sent1: Travel blogs 1 are defined as travel journals written by bloggers in diary form.
sent2: Travel blogs are considered useful for obtaining travel information, because many bloggers' travel experiences are written in this form.
sent3: There are various portal sites for travel blogs, which we will describe in Section 6.
sent4: At these sites, travel blogs are manually registered by bloggers themselves, and the blogs are classified according to travel destination.
sent5: However, there are many more travel blogs in the blogosphere, beyond these portal sites.
sent6: In an attempt to construct an exhaustive database of travel blogs, Nanba et al. [25] identified travel blog entries written in Japanese in a blog database.
sent7: 2 Blog entries that contain cue phrases, such as ""travel"", ""sightseeing"", or ""tour"", have a high degree of probability of being travel blogs.
sent8: However, not every travel blog contains such cue phrases.
sent9: For example, if a blogger describes his/her journey to Norway in multiple blog entries, the blog might state ""We traveled to Norway"" in the first entry, while only writing ""We ate wild sheep!""
sent10: in the second entry. In this case, because the second entry does not contain any expressions related to travel, it is difficult to identify it as a travel blog entry.
sent11: Therefore, Nanba et al. focused not only on each blog entry but also on the surrounding entries for the identification of travel blog entries.
sent12: They formulated the identification of travel blog entries as a sequence-labeling problem, and solved it using machine learning.
sent13: For the machine learning method, they examined the Conditional Random Fields (CRF) method [20]; its empirical success has been reported recently in the field of natural language processing.
sent14: The CRF-based method identifies the tag 3 of each entry.
sent15: Features and tags are given in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry (see Figure 1).
sent16: They used the value of k = 4, which was determined in a pilot study.
sent17: Here, they used the following features for machine learning: whether an entry contains any of 416 cue phrases, such as ""ÅL (travel)"", ""Ä¢ü (tour)"", and ""úz (departure)"", and the number of location names in each entry.
sent18: Using the above method, Nanba et al. identified 17,268 travel blog entries from 1,100,000 blog entries, and constructed a system that plotted travel blog entries on a Google map (see Figure 2). 4 In this figure, travel blog entries are shown as icons.
sent19: If the user clicks an icon, the corresponding blog entry is shown in a pop-up window.
sent20: Travel blogs 1 are defined as travel journals written by bloggers in diary form.
sent21: Travel blogs are considered useful for obtaining travel information, because many bloggers' travel experiences are written in this form.
sent22: There are various portal sites for travel blogs, which we will describe in Section 6.
sent23: At these sites, travel blogs are manually registered by bloggers themselves, and the blogs are classified according to travel destination.
sent24: However, there are many more travel blogs in the blogosphere, beyond these portal sites.
sent25: In an attempt to construct an exhaustive database of travel blogs, Nanba et al. [25] identified travel blog entries written in Japanese in a blog database.
sent26: 2 Blog entries that contain cue phrases, such as ""travel"", ""sightseeing"", or ""tour"", have a high degree of probability of being travel blogs.
sent27: However, not every travel blog contains such cue phrases.
sent28: For example, if a blogger describes his/her journey to Norway in multiple blog entries, the blog might state ""We traveled to Norway"" in the first entry, while only writing ""We ate wild sheep!""
sent29: in the second entry. In this case, because the second entry does not contain any expressions related to travel, it is difficult to identify it as a travel blog entry.
sent30: Therefore, Nanba et al. focused not only on each blog entry but also on the surrounding entries for the identification of travel blog entries.
sent31: They formulated the identification of travel blog entries as a sequence-labeling problem, and solved it using machine learning.
sent32: For the machine learning method, they examined the Conditional Random Fields (CRF) method [20]; its empirical success has been reported recently in the field of natural language processing.
sent33: The CRF-based method identifies the tag 3 of each entry.
sent34: Features and tags are given in the CRF method as follows: (1) k tags occur before a target entry; (2) k features occur before a target entry; and (3) k features follow a target entry (see Figure 1).
sent35: They used the value of k = 4, which was determined in a pilot study.
sent36: Here, they used the following features for machine learning: whether an entry contains any of 416 cue phrases, such as ""ÅL (travel)"", ""Ä¢ü (tour)"", and ""úz (departure)"", and the number of location names in each entry.
sent37: Using the above method, Nanba et al. identified 17,268 travel blog entries from 1,100,000 blog entries, and constructed a system that plotted travel blog entries on a Google map (see Figure 2). 4 In this figure, travel blog entries are shown as icons.
sent38: If the user clicks an icon, the corresponding blog entry is shown in a pop-up window."
58789753,Automatic Compilation of Travel Information from Texts: A Survey Automatic Compilation of Travel Information from Texts: A Survey,"Computer Science, Geography",https://www.semanticscholar.org/paper/222e128a140e5a6cd180e79280928150cd6d782e,s52,Recommending tourist spots,"Recommending tourist spots 8 has been well studied in the multimedia field. Movies and images are used as information sources in addition to texts. In this section, we describe two multimedia studies.

Hao et al. [10] proposed a method for mining location-representative knowledge from travel blogs based on a probabilistic topic model (the Location-Topic model). Using this model,  Figure 5 shows an example of the system output. In this figure, a travel blog segment 9 is enriched with three images that depict its most informative parts. Each image's original tags and the words in the text to which it corresponds are also presented.

Wu et al. [34] proposed a system that summarized tourism-related information. When a user (traveler) entered a query, such as ""What is the historical background of Tian Tan?"", the system searched for and obtained information from Wikipedia, Flickr, YouTube, and official tourism Web sites using the tourist spot name as a query. The system also classified the query as belonging to one of five categories-""general"", ""history"", ""landscape"", ""indoor scenery"", and ""outdoor scenery""-in order to provide users with more relevant information. For example, when a query is classified as belonging to the ""history"" category, the information is obtained from texts, while for a query regarding ""outdoor scenery"", the information is obtained from photos and videos. 

Recommending tourist spots 8 has been well studied in the multimedia field. Movies and images are used as information sources in addition to texts. In this section, we describe two multimedia studies.

Hao et al. [10] proposed a method for mining location-representative knowledge from travel blogs based on a probabilistic topic model (the Location-Topic model). Using this model,  Figure 5 shows an example of the system output. In this figure, a travel blog segment 9 is enriched with three images that depict its most informative parts. Each image's original tags and the words in the text to which it corresponds are also presented.

Wu et al. [34] proposed a system that summarized tourism-related information. When a user (traveler) entered a query, such as ""What is the historical background of Tian Tan?"", the system searched for and obtained information from Wikipedia, Flickr, YouTube, and official tourism Web sites using the tourist spot name as a query. The system also classified the query as belonging to one of five categories-""general"", ""history"", ""landscape"", ""indoor scenery"", and ""outdoor scenery""-in order to provide users with more relevant information. For example, when a query is classified as belonging to the ""history"" category, the information is obtained from texts, while for a query regarding ""outdoor scenery"", the information is obtained from photos and videos. ","[[], ['b9'], ['b35'], [], ['b9'], ['b35']]","[[], ['b9'], ['b35'], [], ['b9'], ['b35']]",4,"sent1: Recommending tourist spots 8 has been well studied in the multimedia field.
sent2: Movies and images are used as information sources in addition to texts.
sent3: In this section, we describe two multimedia studies.
sent4: Hao et al. [10] proposed a method for mining location-representative knowledge from travel blogs based on a probabilistic topic model (the Location-Topic model).
sent5: Using this model,  Figure 5 shows an example of the system output.
sent6: In this figure, a travel blog segment 9 is enriched with three images that depict its most informative parts.
sent7: Each image's original tags and the words in the text to which it corresponds are also presented.
sent8: Wu et al. [34] proposed a system that summarized tourism-related information.
sent9: When a user (traveler) entered a query, such as ""What is the historical background of Tian Tan?"", the system searched for and obtained information from Wikipedia, Flickr, YouTube, and official tourism Web sites using the tourist spot name as a query.
sent10: The system also classified the query as belonging to one of five categories-""general"", ""history"", ""landscape"", ""indoor scenery"", and ""outdoor scenery""-in order to provide users with more relevant information.
sent11: For example, when a query is classified as belonging to the ""history"" category, the information is obtained from texts, while for a query regarding ""outdoor scenery"", the information is obtained from photos and videos.
sent12: Recommending tourist spots 8 has been well studied in the multimedia field.
sent13: Movies and images are used as information sources in addition to texts.
sent14: In this section, we describe two multimedia studies.
sent15: Hao et al. [10] proposed a method for mining location-representative knowledge from travel blogs based on a probabilistic topic model (the Location-Topic model).
sent16: Using this model,  Figure 5 shows an example of the system output.
sent17: In this figure, a travel blog segment 9 is enriched with three images that depict its most informative parts.
sent18: Each image's original tags and the words in the text to which it corresponds are also presented.
sent19: Wu et al. [34] proposed a system that summarized tourism-related information.
sent20: When a user (traveler) entered a query, such as ""What is the historical background of Tian Tan?"", the system searched for and obtained information from Wikipedia, Flickr, YouTube, and official tourism Web sites using the tourist spot name as a query.
sent21: The system also classified the query as belonging to one of five categories-""general"", ""history"", ""landscape"", ""indoor scenery"", and ""outdoor scenery""-in order to provide users with more relevant information.
sent22: For example, when a query is classified as belonging to the ""history"" category, the information is obtained from texts, while for a query regarding ""outdoor scenery"", the information is obtained from photos and videos."
61066716,A Corpus-based Survey of Four Electronic Swahili-English Bilingual Dictionaries,Computer Science,https://www.semanticscholar.org/paper/b416759e1941fae9b6474541448dec676636a9fe,s4,3.2,"The Freedict Swahili-English Dictionary [Freedict] The Freedict Swahili-English Dictionary is an attempt to unify and homogenize existing bilingual dictionaries (Bański and Wójtowicz 2009).It is based on a previously published electronic dictionary (Dict 2009) and also includes entries from a Freedict dictionary (Freedict 2009), and a Swahili-Esperanto-English dictionary (Ergane 2009).It uses the open-source Freedict architecture for development and dissemination and sources are therefore freely downloadable.

The latest version includes 2 600 entries, associated with an English translation equivalent and a part-of-speech tag. Figure 2 illustrates the typical layout of the entries.While the dictionary itself is very small and the information provided is scarce, the developers seem to have tried as much as possible to provide single-word translation equivalents that bode well in a machine translation environment.","[['b8', None, 'b7', 'b0'], []]","[['b8', None, 'b7', 'b0'], []]",4,"sent1: The Freedict Swahili-English Dictionary [Freedict]
sent2: The Freedict Swahili-English Dictionary is an attempt to unify and homogenize existing bilingual dictionaries (Bański and Wójtowicz 2009).It is based on a previously published electronic dictionary (Dict 2009) and also includes entries from a Freedict dictionary (Freedict 2009), and a Swahili-Esperanto-English dictionary (Ergane 2009).It uses the open-source Freedict architecture for development and dissemination and sources are therefore freely downloadable.
sent3: The latest version includes 2 600 entries, associated with an English translation equivalent and a part-of-speech tag.
sent4: Figure 2 illustrates the typical layout of the entries.
sent5: While the dictionary itself is very small and the information provided is scarce, the developers seem to have tried as much as possible to provide single-word translation equivalents that bode well in a machine translation environment."
61066716,A Corpus-based Survey of Four Electronic Swahili-English Bilingual Dictionaries,Computer Science,https://www.semanticscholar.org/paper/b416759e1941fae9b6474541448dec676636a9fe,s9,Monolingual corpus,"We used several textual sources to compute the coverage of the dictionaries.These include:

-The Helsinki Corpus of Swahili, HCS (Hurskainen 2004a) consisting of more than 9 million words.-Wikipedia in Swahili: almost 12 000 Internet pages, good for more than 1 million words.

We pre-processed the texts by uniformly converting them into UTF-8, tokenizing the data and lemmatizing them using the automatic morphological analyzer described in Section 4. The data was also part-of-speech tagged using the method described in De Pauw et al. (2006).The Helsinki Corpus of Swahili already has lemmatization and part-of-speech tag information available.We nevertheless chose to process it again using our own techniques, for reasons of annotation accuracy and consistency across the data sets.

We then proceeded to compute coverage.We used a purely quantitative approach for this, which checks for each word in the corpus whether its lemma (for the given part-of-speech) can be retrieved in the dictionaries.Table 3 displays the scores for the different dictionaries and corpora.The ILSD has the highest coverage across the board, but loses a lot of coverage for the more recent texts in the recently developed and noisier Wikipedia pages.TUKI follows the same trend, while the TeDJe-SED dictionary hardly loses coverage.The latter's smaller set of lemmas consistently covers the most frequent words in the corpus and is therefore not as vulnerable to change of register and publication date.The complete consolidated dictionary database performs quite well with an overall coverage of about 90.2%.

To study the effect of publication date, we calculated the coverage per year of the periodicals included in the Helsinki Corpus of Swahili (1990Swahili ( → 2002Swahili ( , no data for 1995Swahili ( , 1996Swahili ( , 1997)).The downward trend in coverage is visible for all four dictionaries; see Figure 5.The most frequent items not covered by the dictionaries are named entities, foreign words and IT terminology.The need to update dictionaries consistently is therefore high.The open architectures of the Freedict and ILSD projects are in this sense suitable solutions.Interestingly, the scores reported here differ significantly from those in Hurskainen (2004).Overall, coverage scores are lower than those reported in the previous survey, which may be due to differences in evaluation metrics.Stranger however is that Hurskainen (2004) observes higher recall scores for more recent documents, whereas Figure 5 shows a definite downward trend over time.These discrepancies warrant further investigation.

We also calculated the coverage of the dictionaries disregarding the frequency of the lemma.In this calculation, covering a highly frequent word like lakini 'but; however; nevertheless' scores the same as covering a hapax.Table 4 shows that in this experiment ILSD is trailing TUKI, indicating that even though ILSD contains many more entries, TUKI seems to cater for a wider range of words.

A final experiment counts for how many lemmas in the respective dictionaries evidence can be found in the corpus.The last line of Table 4 shows that TeDJe-SED has all lemmas covered by real-world data.About 30% of the entries in TUKI are not found in the data, while only two thirds of ILSD is covered by the corpus.Comparing the data in Tables 3 and 4 shows that while TUKI trails in comparison to ILSD in terms of raw coverage (Table 3), it does http://lexikos.journals.ac.za seem to strike a better balance in terms of both lexical richness and empirical evidence (Table 4).Owing to the massive amount of data, it is impossible to check whether the lemma retrieved in the dictionary is indeed the one intended in the text.It might indeed be the case that a particular lemma-tag combination retrieved in the dictionary, does not describe the correct meaning in its actual context.We manually checked a small section of the corpus (±2 000 words) and found only two occasions of such an error.We are therefore confident that our scores are reliable in the context of the comparison between the dictionaries.

In De Pauw and De Schryver ( 2008) we presented our morphological analyzer as a way to unearth undiscovered lemmas in the corpus data.Our approach indeed has the distinct advantage that it is not dependent on a preset list of roots or lemmas, and is thus capable of lemmatizing word forms for previously unseen lemmas.The experiments outlined in this section have further underlined this property, as we now have at our disposal a list of word forms and associated candidate lemmas (roughly put the remaining 10% not covered by the consolidated dictionary) that need to be lexicographically described.","[[], ['b12'], [None], [], [None, 'b11'], [], [], []]","[[], ['b12'], [None], [], [None, 'b11'], [], [], []]",4,"sent1: We used several textual sources to compute the coverage of the dictionaries.
sent2: These include:-The Helsinki Corpus of Swahili, HCS (Hurskainen 2004a) consisting of more than 9 million words.-Wikipedia in Swahili: almost 12 000 Internet pages, good for more than 1 million words.
sent3: We pre-processed the texts by uniformly converting them into UTF-8, tokenizing the data and lemmatizing them using the automatic morphological analyzer described in Section 4.
sent4: The data was also part-of-speech tagged using the method described in De Pauw et al. (2006).The Helsinki Corpus of Swahili already has lemmatization and part-of-speech tag information available.
sent5: We nevertheless chose to process it again using our own techniques, for reasons of annotation accuracy and consistency across the data sets.
sent6: We then proceeded to compute coverage.
sent7: We used a purely quantitative approach for this, which checks for each word in the corpus whether its lemma (for the given part-of-speech) can be retrieved in the dictionaries.
sent8: Table 3 displays the scores for the different dictionaries and corpora.
sent9: The ILSD has the highest coverage across the board, but loses a lot of coverage for the more recent texts in the recently developed and noisier Wikipedia pages.
sent10: TUKI follows the same trend, while the TeDJe-SED dictionary hardly loses coverage.
sent11: The latter's smaller set of lemmas consistently covers the most frequent words in the corpus and is therefore not as vulnerable to change of register and publication date.
sent12: The complete consolidated dictionary database performs quite well with an overall coverage of about 90.2%.
sent13: To study the effect of publication date, we calculated the coverage per year of the periodicals included in the Helsinki Corpus of Swahili (1990Swahili ( → 2002Swahili ( , no data for 1995Swahili ( , 1996Swahili ( , 1997)).The downward trend in coverage is visible for all four dictionaries; see Figure 5.The most frequent items not covered by the dictionaries are named entities, foreign words and IT terminology.
sent14: The need to update dictionaries consistently is therefore high.
sent15: The open architectures of the Freedict and ILSD projects are in this sense suitable solutions.
sent16: Interestingly, the scores reported here differ significantly from those in Hurskainen (2004).Overall, coverage scores are lower than those reported in the previous survey, which may be due to differences in evaluation metrics.
sent17: Stranger however is that Hurskainen (2004) observes higher recall scores for more recent documents, whereas Figure 5 shows a definite downward trend over time.
sent18: These discrepancies warrant further investigation.
sent19: We also calculated the coverage of the dictionaries disregarding the frequency of the lemma.
sent20: In this calculation, covering a highly frequent word like lakini 'but; however; nevertheless' scores the same as covering a hapax.
sent21: Table 4 shows that in this experiment ILSD is trailing TUKI, indicating that even though ILSD contains many more entries, TUKI seems to cater for a wider range of words.
sent22: A final experiment counts for how many lemmas in the respective dictionaries evidence can be found in the corpus.
sent23: The last line of Table 4 shows that TeDJe-SED has all lemmas covered by real-world data.
sent24: About 30% of the entries in TUKI are not found in the data, while only two thirds of ILSD is covered by the corpus.
sent25: Comparing the data in Tables 3 and 4 shows that while TUKI trails in comparison to ILSD in terms of raw coverage (Table 3), it does http://lexikos.journals.ac.za seem to strike a better balance in terms of both lexical richness and empirical evidence (Table 4).Owing to the massive amount of data, it is impossible to check whether the lemma retrieved in the dictionary is indeed the one intended in the text.
sent26: It might indeed be the case that a particular lemma-tag combination retrieved in the dictionary, does not describe the correct meaning in its actual context.
sent27: We manually checked a small section of the corpus (±2 000 words) and found only two occasions of such an error.
sent28: We are therefore confident that our scores are reliable in the context of the comparison between the dictionaries.
sent29: In De Pauw and De Schryver ( 2008) we presented our morphological analyzer as a way to unearth undiscovered lemmas in the corpus data.
sent30: Our approach indeed has the distinct advantage that it is not dependent on a preset list of roots or lemmas, and is thus capable of lemmatizing word forms for previously unseen lemmas.
sent31: The experiments outlined in this section have further underlined this property, as we now have at our disposal a list of word forms and associated candidate lemmas (roughly put the remaining 10% not covered by the consolidated dictionary) that need to be lexicographically described."
65219850,A Survey of Text Mining in Social Media: Facebook and Twitter Perspectives Establish algorithm for to determine an authorized PW error View project Further Investigations on Developing an Arabic Sentiment Lexicon View project Said Salloum British University in Dubai 3 PUBLICATIONS 2 CITATIONS SEE PROFILE,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/94d255e0b2a729e918a320ab99e503cfc04e1fc8,s7,Text mining in Twitter,"A significant size of research has been occupied by the Twitter data analysis over the last couple of years [54]. Large spectrums of domains are using this data, some of which are using it for academic research and others for applications [55]. New improvements regarding twitter data are presented by this section. The document collection from various resources triggers the ""Text Mining"" process. A particular document would be retrieved by Text mining tool and this document is pre-processed by checking the character sets and format [56]. Subsequently, a text analysis phase would monitor the document. Semantic analysis is used to derive high-quality information from text; this is referred to ""Text analysis"". The market has a lot of text analysis techniques. Professionals can use combinations of techniques subject to the goal of the organization. Researchers tend to repeat the text analysis techniques till the time information is acquired. A management information system is capable of incorporating the resulting information, and as a result, significant knowledge is produced for the user of that information system [57]. A key issue in text mining is intricacy of natural language. The ambiguity problem is much dense in the natural language. There are multiple meanings of a single word and multiple words can possess same meaning. Ambiguity is referred to as the understanding of a word which has more than one possible meaning. Noise has emerged in extracted information as a result of this ambiguity. Since usability and flexibility are the main parts of ambiguity, it cannot be removed from the natural language. One phrase or sentence can have multiple understandings, so there is a chance we can obtain a number of meanings. The work is still undeveloped and a particular domain is correlated with the suggested approach while the experts have attempted to resolve the ambiguity problem by performing a number of research studies. As there is uncertainty/vagueness in the semantic meanings of many discovered words, so it is very difficult to answer the requirements of the user.

Scholars of [58] developed and formulated an automatic classification technique through which potentially abuseindicating user posts could be identified and evaluating the likelihood of social media usage as a source for automatic monitoring of drug medication abuse. In this regard, Twitter user posts (tweets) were collected and these were linked with three commonly abused medications (Oxycodone, Adderall, and Quetiapine). Besides interpreting a control medication (metformin), which is not the subject of abuse due to its process, nearly 6400 tweets were manually annotated, where these three medications were pointed out. The annotated data was qualitatively and quantitatively analyzed to determine as to whether or not signals of drug medication abuse are presented in Twitter posts. To sum up, Twitter's value was assessed in exploring the patterns of abuse over time and an automatic supervised classification technique was also designed, in which the purpose was to observe and separate the posts containing signals of medication abuse from those that do not. According to the findings of investigations, Twitter posts have yielded clear signals of medication abuse. As compared to the proportion for the control medication (i.e., metformin: 0.3 %), there is a very high ratio of tweets containing abuse signals for the three case medications (Adderall: 23 %, oxycodone: 12 %, quetiapine: 5.0 %). In addition, almost 82 % accuracy (medication abuse class recall: 0.51, precision: 0.41, F-measure: 0.46) has been achieved through the automatic classification approach. The Study demonstrated how the abuse patterns over time can be analyzed by using the classification data and its goal is to illustrate the effectiveness of automatic classification. As a result, it is found that abuse-related information for medications can be significantly acquired from social media, and the research indicates that natural language processing and supervised classification are the automatic approaches that have potentials for future monitoring and intervention assignments. With respect to supervised learning, the lack of sufficient training data is believed to be the largest shortcoming of the study. Both annotation and automatic classification are hindered by the lack of context and ambiguity in tweets. During the course of annotations, many ambiguous tweets were found and services of pharmacology expert were hired to address these issues. As a result of these ambiguities, the undefined situation is observed in the binary classification process and this inadequacy will continue until the time fine-tuned annotation rules could be specified by the future annotation rules.

A study by [59] applied the text mining approach on a large dataset of tweets. The complete Twitter timelines of 10 academic libraries were used to collect the dataset for this research. Nearly 23,707 tweets formed the total dataset, where there were 7625 hashtags, 17,848 mentions, and 5974 retweets. Inconsistency among academic libraries is found in the distribution of tweets. ""Open"" was the most repeated word that was used by the academic libraries in different perspectives. It was observed that ""special collections"" was the most frequent bigram (two-word sequence) in the aggregated tweets. While ""save the date"" was the most recurrent tri-gram (three-word sequence). In the semantic analysis, words such as ""insight, knowledge, and information about cultural and personal relations"" were the most frequent word categories. Moreover, ""Resources"" was the most widespread category of the tweets among all the selected academic libraries. The significance of data and text-mining approaches are reported within the study and their purpose is to gain an insight with the aggregate social data of academic libraries so that the process of decision-making and strategic planning could become facilitated for marketing of services and patron outreach. The 10 academic libraries from top global universities have undergone the text mining approach. The study aimed to illustrate their Twitter usage and to examine their tweet content.

As far as social media is concerned, decision-making is supported and user-generated text is analyzed through text mining and content analysis [60]. By employing an archiving service (twimemachine.com) in December 2014, the complete Twitter timelines of 10 academic libraries were taken into account to collect the dataset for this research. The libraries of 10 highestranking universities from the global Shanghai Ranking were chosen for that purpose. The language of the university must be English-based, which was the condition for selection and selection was restricted to only one library if there was more than one library in the university. Certain weaknesses were found in the study, for example, all of the libraries are English-language libraries in the sample and only 10 academic libraries were considered for the analysis. This gap must be filled in future by applying the analysis to a dataset from diversified academic libraries, including non-English language libraries. Consequently, a complete understanding of tweet patterns would be acknowledged. The future inquiry can also incorporate the international or crosscultural comparisons. Any discrepancy among libraries in their tweets' content affected by the number and interaction of followers could be highlighted by the analysis and its findings. The accuracy of the tweet categorization tool has yielded the inadequate findings, and the said tool needs to be substantiated through other machine-learning models along with their applications.

Researchers of [55] demonstrated in a smoking cessation nicotine patch study an innovative Twitter recruitment system that is deployed by the group. The study aimed to describe the methodology and used to address the issue of digital recruitment. Furthermore, designing a rule-based system with the provision of system specification besides representing the data mining approaches and algorithms (classification and association analysis) using Twitter data. Twitter's streaming API captured two sets of streaming tweets, which were collected for the study. Ten search terms, (i.e. quitting, quit, nicotine, smoking, smoke, patches, cig, cigarette, ecig, cigs, marijuana) were used to gather the first set. The second set of tweets contains 30 terms, in which the terms from the first set were included. Moreover, the second set is a superset of the first one. A number of studies have been conducted to review the information gathering methods. As unstructured data sets are in the textual format, the use of various procedures of text mining has been tackled by many research studies. Nonetheless, the data sets on the social networking websites are not mainly discussed by these studies. A study by [50] applied various text mining techniques. The study would describe the application of these strategies in the social networking websites. In the field of intelligent text analysis, the latest improvements would also be examined in the survey. The study focused on two key techniques pertaining to the text mining field, namely classification and clustering. Usually, they are operated for the study of the unstructured text accessible on the extensive scale frameworks. Prior to the start of World Cup, a total of approximately 30,000 tweets were used by [61]. Moreover, an algorithm was used for integrating the consensus matrix and the DBSCAN algorithm. Consequently, the concerned tweets on those prevailing topics were available to him. Afterward, the clustering analysis was applied to seek the topics discussed by the tweets. The tweets were grouped utilizing the k-means [62], Non-Negative Matrix Factorization (NMF), and a popular clustering algorithm. After that, the results were compared. Similar results were delivered by both algorithms. However, NMF became faster and the researchers could easily interpret the outcomes.

A study by [1] initiated a workflow to gain an insight into both the large-scale data mining methods and qualitative analysis. Twitter posts of engineering students were the primary concern. The basic goal was to identify their issues in their academic experiences. The study conducted a qualitative analysis of samples obtained from around 25,000 tweets that were associated with the engineering students and their college life. The encounter troubles of engineering students were discovered during the study. For example, a large volume of study, sleep deprivation and lack of social engagement. Considering these outcomes, a multi-label classification algorithm was implemented to categorize tweets in lieu of students' queries. The algorithm was applied on approximately 35,000 tweets streamed at the geo-location of Purdue University. At the first instance, the concerned authorities have addressed the experiences and issues of the students and social media data was used to expose the issues. Moreover, a study by [1] also developed a multi-label classifier so that tweets founded within the content evaluation phase could be organized. A number of renowned classifiers are significantly consumed in machine learning domain and data mining process. With Comparison to other state-of-the-art multi-label classifiers, the Naïve Bayes classifiers were found proficient on the dataset.

A study by [63] discussed the clustering technique, the execution of correlation and association analyses to social media. The investigation of insurance Twitter posts was carried out to assess this matter. Consequently, recognizing theories and keywords in the social media data has become an easy task, due to which the information by insurers and its application would be facilitated. After having a detailed analysis, client queries and the potential market would be proactively addressed with usefulness and the findings of the analysis are to be effectively implemented in suitable fields. According to this evaluation, the overall 68,370 tweets were utilized. Two additional kinds of evaluation need to be applied to the data. The first is the clustering analysis, through which the tweets depending on their similarities or dissimilarities would be merged. An Association Analysis is the second one whereas the occurrences of particular composed words were discovered.

Authors of [64] stated that sentiment analysis through social media usage has witnessed a huge interest from scholars in the last few years. In that, the authors discussed the influence of tweets' sentiment on elections and the impact of the elections' results on web sentiment.","[['b54', 'b53', 'b56', 'b55'], ['b57'], ['b58'], ['b59'], ['b54', 'b61', 'b49', 'b60'], ['b0'], ['b62'], ['b63']]","[['b54', 'b53', 'b56', 'b55'], ['b57'], ['b58'], ['b59'], ['b54', 'b61', 'b49', 'b60'], ['b0'], ['b62'], ['b63']]",14,"sent1: A significant size of research has been occupied by the Twitter data analysis over the last couple of years [54].
sent2: Large spectrums of domains are using this data, some of which are using it for academic research and others for applications [55].
sent3: New improvements regarding twitter data are presented by this section.
sent4: The document collection from various resources triggers the ""Text Mining"" process.
sent5: A particular document would be retrieved by Text mining tool and this document is pre-processed by checking the character sets and format [56].
sent6: Subsequently, a text analysis phase would monitor the document.
sent7: Semantic analysis is used to derive high-quality information from text; this is referred to ""Text analysis"".
sent8: The market has a lot of text analysis techniques.
sent9: Professionals can use combinations of techniques subject to the goal of the organization.
sent10: Researchers tend to repeat the text analysis techniques till the time information is acquired.
sent11: A management information system is capable of incorporating the resulting information, and as a result, significant knowledge is produced for the user of that information system [57].
sent12: A key issue in text mining is intricacy of natural language.
sent13: The ambiguity problem is much dense in the natural language.
sent14: There are multiple meanings of a single word and multiple words can possess same meaning.
sent15: Ambiguity is referred to as the understanding of a word which has more than one possible meaning.
sent16: Noise has emerged in extracted information as a result of this ambiguity.
sent17: Since usability and flexibility are the main parts of ambiguity, it cannot be removed from the natural language.
sent18: One phrase or sentence can have multiple understandings, so there is a chance we can obtain a number of meanings.
sent19: The work is still undeveloped and a particular domain is correlated with the suggested approach while the experts have attempted to resolve the ambiguity problem by performing a number of research studies.
sent20: As there is uncertainty/vagueness in the semantic meanings of many discovered words, so it is very difficult to answer the requirements of the user.
sent21: Scholars of [58] developed and formulated an automatic classification technique through which potentially abuseindicating user posts could be identified and evaluating the likelihood of social media usage as a source for automatic monitoring of drug medication abuse.
sent22: In this regard, Twitter user posts (tweets) were collected and these were linked with three commonly abused medications (Oxycodone, Adderall, and Quetiapine).
sent23: Besides interpreting a control medication (metformin), which is not the subject of abuse due to its process, nearly 6400 tweets were manually annotated, where these three medications were pointed out.
sent24: The annotated data was qualitatively and quantitatively analyzed to determine as to whether or not signals of drug medication abuse are presented in Twitter posts.
sent25: To sum up, Twitter's value was assessed in exploring the patterns of abuse over time and an automatic supervised classification technique was also designed, in which the purpose was to observe and separate the posts containing signals of medication abuse from those that do not.
sent26: According to the findings of investigations, Twitter posts have yielded clear signals of medication abuse.
sent27: As compared to the proportion for the control medication (i.e., metformin: 0.3 %), there is a very high ratio of tweets containing abuse signals for the three case medications (Adderall: 23 %, oxycodone: 12 %, quetiapine: 5.0 %).
sent28: In addition, almost 82 % accuracy (medication abuse class recall: 0.51, precision: 0.41, F-measure: 0.46) has been achieved through the automatic classification approach.
sent29: The Study demonstrated how the abuse patterns over time can be analyzed by using the classification data and its goal is to illustrate the effectiveness of automatic classification.
sent30: As a result, it is found that abuse-related information for medications can be significantly acquired from social media, and the research indicates that natural language processing and supervised classification are the automatic approaches that have potentials for future monitoring and intervention assignments.
sent31: With respect to supervised learning, the lack of sufficient training data is believed to be the largest shortcoming of the study.
sent32: Both annotation and automatic classification are hindered by the lack of context and ambiguity in tweets.
sent33: During the course of annotations, many ambiguous tweets were found and services of pharmacology expert were hired to address these issues.
sent34: As a result of these ambiguities, the undefined situation is observed in the binary classification process and this inadequacy will continue until the time fine-tuned annotation rules could be specified by the future annotation rules.
sent35: A study by [59] applied the text mining approach on a large dataset of tweets.
sent36: The complete Twitter timelines of 10 academic libraries were used to collect the dataset for this research.
sent37: Nearly 23,707 tweets formed the total dataset, where there were 7625 hashtags, 17,848 mentions, and 5974 retweets.
sent38: Inconsistency among academic libraries is found in the distribution of tweets.
sent39: ""Open"" was the most repeated word that was used by the academic libraries in different perspectives.
sent40: It was observed that ""special collections"" was the most frequent bigram (two-word sequence) in the aggregated tweets.
sent41: While ""save the date"" was the most recurrent tri-gram (three-word sequence).
sent42: In the semantic analysis, words such as ""insight, knowledge, and information about cultural and personal relations"" were the most frequent word categories.
sent43: Moreover, ""Resources"" was the most widespread category of the tweets among all the selected academic libraries.
sent44: The significance of data and text-mining approaches are reported within the study and their purpose is to gain an insight with the aggregate social data of academic libraries so that the process of decision-making and strategic planning could become facilitated for marketing of services and patron outreach.
sent45: The 10 academic libraries from top global universities have undergone the text mining approach.
sent46: The study aimed to illustrate their Twitter usage and to examine their tweet content.
sent47: As far as social media is concerned, decision-making is supported and user-generated text is analyzed through text mining and content analysis [60].
sent48: By employing an archiving service (twimemachine.com) in December 2014, the complete Twitter timelines of 10 academic libraries were taken into account to collect the dataset for this research.
sent49: The libraries of 10 highestranking universities from the global Shanghai Ranking were chosen for that purpose.
sent50: The language of the university must be English-based, which was the condition for selection and selection was restricted to only one library if there was more than one library in the university.
sent51: Certain weaknesses were found in the study, for example, all of the libraries are English-language libraries in the sample and only 10 academic libraries were considered for the analysis.
sent52: This gap must be filled in future by applying the analysis to a dataset from diversified academic libraries, including non-English language libraries.
sent53: Consequently, a complete understanding of tweet patterns would be acknowledged.
sent54: The future inquiry can also incorporate the international or crosscultural comparisons.
sent55: Any discrepancy among libraries in their tweets' content affected by the number and interaction of followers could be highlighted by the analysis and its findings.
sent56: The accuracy of the tweet categorization tool has yielded the inadequate findings, and the said tool needs to be substantiated through other machine-learning models along with their applications.
sent57: Researchers of [55] demonstrated in a smoking cessation nicotine patch study an innovative Twitter recruitment system that is deployed by the group.
sent58: The study aimed to describe the methodology and used to address the issue of digital recruitment.
sent59: Furthermore, designing a rule-based system with the provision of system specification besides representing the data mining approaches and algorithms (classification and association analysis) using Twitter data.
sent60: Twitter's streaming API captured two sets of streaming tweets, which were collected for the study.
sent61: Ten search terms, (i.e. quitting, quit, nicotine, smoking, smoke, patches, cig, cigarette, ecig, cigs, marijuana) were used to gather the first set.
sent62: The second set of tweets contains 30 terms, in which the terms from the first set were included.
sent63: Moreover, the second set is a superset of the first one.
sent64: A number of studies have been conducted to review the information gathering methods.
sent65: As unstructured data sets are in the textual format, the use of various procedures of text mining has been tackled by many research studies.
sent66: Nonetheless, the data sets on the social networking websites are not mainly discussed by these studies.
sent67: A study by [50] applied various text mining techniques.
sent68: The study would describe the application of these strategies in the social networking websites.
sent69: In the field of intelligent text analysis, the latest improvements would also be examined in the survey.
sent70: The study focused on two key techniques pertaining to the text mining field, namely classification and clustering.
sent71: Usually, they are operated for the study of the unstructured text accessible on the extensive scale frameworks.
sent72: Prior to the start of World Cup, a total of approximately 30,000 tweets were used by [61].
sent73: Moreover, an algorithm was used for integrating the consensus matrix and the DBSCAN algorithm.
sent74: Consequently, the concerned tweets on those prevailing topics were available to him.
sent75: Afterward, the clustering analysis was applied to seek the topics discussed by the tweets.
sent76: The tweets were grouped utilizing the k-means [62], Non-Negative Matrix Factorization (NMF), and a popular clustering algorithm.
sent77: After that, the results were compared.
sent78: Similar results were delivered by both algorithms.
sent79: However, NMF became faster and the researchers could easily interpret the outcomes.
sent80: A study by [1] initiated a workflow to gain an insight into both the large-scale data mining methods and qualitative analysis.
sent81: Twitter posts of engineering students were the primary concern.
sent82: The basic goal was to identify their issues in their academic experiences.
sent83: The study conducted a qualitative analysis of samples obtained from around 25,000 tweets that were associated with the engineering students and their college life.
sent84: The encounter troubles of engineering students were discovered during the study.
sent85: For example, a large volume of study, sleep deprivation and lack of social engagement.
sent86: Considering these outcomes, a multi-label classification algorithm was implemented to categorize tweets in lieu of students' queries.
sent87: The algorithm was applied on approximately 35,000 tweets streamed at the geo-location of Purdue University.
sent88: At the first instance, the concerned authorities have addressed the experiences and issues of the students and social media data was used to expose the issues.
sent89: Moreover, a study by [1] also developed a multi-label classifier so that tweets founded within the content evaluation phase could be organized.
sent90: A number of renowned classifiers are significantly consumed in machine learning domain and data mining process.
sent91: With Comparison to other state-of-the-art multi-label classifiers, the Naïve Bayes classifiers were found proficient on the dataset.
sent92: A study by [63] discussed the clustering technique, the execution of correlation and association analyses to social media.
sent93: The investigation of insurance Twitter posts was carried out to assess this matter.
sent94: Consequently, recognizing theories and keywords in the social media data has become an easy task, due to which the information by insurers and its application would be facilitated.
sent95: After having a detailed analysis, client queries and the potential market would be proactively addressed with usefulness and the findings of the analysis are to be effectively implemented in suitable fields.
sent96: According to this evaluation, the overall 68,370 tweets were utilized.
sent97: Two additional kinds of evaluation need to be applied to the data.
sent98: The first is the clustering analysis, through which the tweets depending on their similarities or dissimilarities would be merged.
sent99: An Association Analysis is the second one whereas the occurrences of particular composed words were discovered.
sent100: Authors of [64] stated that sentiment analysis through social media usage has witnessed a huge interest from scholars in the last few years.
sent101: In that, the authors discussed the influence of tweets' sentiment on elections and the impact of the elections' results on web sentiment."
65219850,A Survey of Text Mining in Social Media: Facebook and Twitter Perspectives Establish algorithm for to determine an authorized PW error View project Further Investigations on Developing an Arabic Sentiment Lexicon View project Said Salloum British University in Dubai 3 PUBLICATIONS 2 CITATIONS SEE PROFILE,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/94d255e0b2a729e918a320ab99e503cfc04e1fc8,s6,Text mining in Facebook,"The social networks are growing at a rapid rate without a break. Most importantly, the unstructured data is being stored on these networks as they act as a large pool and this data pertains to a host of domains containing governments, businesses, and health. Data mining techniques tend to transform the unstructured data for its placement within a systematic arrangement [47]. Nowadays, Facebook is one of the most popular social media. This media is used by a large number of people on earth for expressing their ideas, thoughts, sorrows, pleasures and poems [48]. Researchers had chosen a number of Facebook variables that were expected to develop the right situation for carrying out our investigations. The valuable statistics of user's personality is provided by the Facebook profiles and activities, which exposes the actual objects instead of projected or idealized character [49]. The digital data has currently witnessed an enormous growth. The key area of interest among professionals is now data mining and knowledge discovery. Moreover, a strong need has been felt to transform such data into useful knowledge and information. A number of applications like business management and market analysis have realized the benefits from the information and knowledge extracted out of large scale data. Information is stored in text form across various applications so one of the up-to-date areas for research is text mining. The hard issue is extracting the user required information. The knowledge discovery process has an important step which is believed to be the Text Mining. The hidden information is extracted from unstructured to semistructured data in this process. Extracting information from a number of written resources and its automatic discovery is called as Text mining. Moreover, computers are also used for the needful and to meet this goal.

Scholars of [50] illustrated the text mining techniques, methods, and challenges. These successful techniques would be described to give usefulness over information acquisition during text mining. The study discussed the situations where each technology could be beneficial for a different number of users. A number of business organizations would be examined by mining data that has been exposed by their employees on LinkedIn, Facebook, and other openly available sources. A network of informal social connections among employees is extracted through web crawler developed for this purpose. According to the findings, leadership roles can be identified within the organization and this could be achieved absolutely by using machine learning techniques besides centrality analysis. Clustering the social network of an organization and collecting available information within each cluster can result in the valuable non-trivial perceptions. A key asset or a considerable threat to the primary organization can be the knowledge about the network of informal relationships. Besides analyzing social networks of the organizations, algorithms and methods used to gather data from freely available sources would be presented by this paper. A web crawler was developed to obtain profiles of employees from six targeted organizations and this was done by collecting the Facebook data. A social network topology was created for each organization, and machine-learning algorithms and centrality measures were implemented so that the hidden leadership positions within each company could be discovered. Moreover, the social community clusters inside these organizations were also revealed by the algorithms, which gave us understanding about the communication network of each company in addition to the structure of the organization.

According to a study by [51], it has become clear that social media data is simply susceptible to misuse. The scheme encompasses structured approach and its application. Furthermore, it entails performing a statistical cluster analysis in addition to the comprehensive analysis of social media comments so that researchers could determine the inter-relationships among key factors. The qualitative social media data can be quantified by these schemes and subsequently cluster them based on their similar features, and then they can be used as decision-making tools. The SAMSUNG Mobile Facebook page, where Samsung smartphones were introduced, was used for the data acquisition process. The comment published by Facebook users on the captioned Facebook page is referred to as the ""Data"". In a period of 3 months, almost 128371 comments were downloaded. The English comments only were undergone through the analysis process. Afterward, the conceptual analysis was used by the content analysis and ultimately statistical cluster analysis was performed by carrying out relational analysis. Hence, social media data is integrated by applying the statistical cluster analysis and it is performed based on the output of the conceptual analysis. The researchers are consequently enabled to categorize a large dataset into many subsets, at times, referred to as objects. One of the disciplines of its application is marketing. Factors that can be manageable in some cases are also minimized by these types of techniques.

A study by [52] explored the social data as a systematic data mining architecture. Findings indicated that Facebook as a social networking site is the major source of data. Besides this approach, information on ""my wall"" post regarding myself, age and comments from the Facebook all are emphasized by the author. It has been taken as a raw data, which is applied later to study and monitor the analytical tactics. In addition, the study investigated images for the advertisement of their products and for the decisionmaking process. A number of data mining techniques precede the coercion of intellectual knowledge from social data. Mainly, it organizes the key information and other applied activities in which users are attributed regarding their colleagues on social networking sites (i.e. Facebook). For the recovery on Facebook user database, Facebook API performs Application Secret key and Facebook API Key are executed by Facebook API. As a result, WEKA files and data mining techniques are supported to collect certain data into the secondary database, while the text data is represented by the detached data.

Researchers of [41] explored the applicability of representing user's personality based on the extracted features from the Facebook data. The classification techniques and their utilities were completely analyzed with regard to the inspirational research outcomes. A sample of 250 user instances from Facebook formed the research study and this sample was from about 10,000 status updates, which was delivered by the My Personality project [53]. The study has the following two interconnected objectives: (1) having knowledge about the pertinent personality-correlated indicators that presents user data implicitly or explicitly in Facebook, and (2) identifying the feasibility of prognostic character demonstration so that upcoming intelligent systems could be supported. The study emphasized on the promotion of pertinent features in a model, through which the enhanced output of the classifiers under evaluation could be observed.","[['b47', 'b46', 'b48'], ['b49'], ['b50'], ['b51'], ['b40', 'b52', 'b0']]","[['b47', 'b46', 'b48'], ['b49'], ['b50'], ['b51'], ['b40', 'b52', 'b0']]",9,"sent1: The social networks are growing at a rapid rate without a break.
sent2: Most importantly, the unstructured data is being stored on these networks as they act as a large pool and this data pertains to a host of domains containing governments, businesses, and health.
sent3: Data mining techniques tend to transform the unstructured data for its placement within a systematic arrangement [47].
sent4: Nowadays, Facebook is one of the most popular social media.
sent5: This media is used by a large number of people on earth for expressing their ideas, thoughts, sorrows, pleasures and poems [48].
sent6: Researchers had chosen a number of Facebook variables that were expected to develop the right situation for carrying out our investigations.
sent7: The valuable statistics of user's personality is provided by the Facebook profiles and activities, which exposes the actual objects instead of projected or idealized character [49].
sent8: The digital data has currently witnessed an enormous growth.
sent9: The key area of interest among professionals is now data mining and knowledge discovery.
sent10: Moreover, a strong need has been felt to transform such data into useful knowledge and information.
sent11: A number of applications like business management and market analysis have realized the benefits from the information and knowledge extracted out of large scale data.
sent12: Information is stored in text form across various applications so one of the up-to-date areas for research is text mining.
sent13: The hard issue is extracting the user required information.
sent14: The knowledge discovery process has an important step which is believed to be the Text Mining.
sent15: The hidden information is extracted from unstructured to semistructured data in this process.
sent16: Extracting information from a number of written resources and its automatic discovery is called as Text mining.
sent17: Moreover, computers are also used for the needful and to meet this goal.
sent18: Scholars of [50] illustrated the text mining techniques, methods, and challenges.
sent19: These successful techniques would be described to give usefulness over information acquisition during text mining.
sent20: The study discussed the situations where each technology could be beneficial for a different number of users.
sent21: A number of business organizations would be examined by mining data that has been exposed by their employees on LinkedIn, Facebook, and other openly available sources.
sent22: A network of informal social connections among employees is extracted through web crawler developed for this purpose.
sent23: According to the findings, leadership roles can be identified within the organization and this could be achieved absolutely by using machine learning techniques besides centrality analysis.
sent24: Clustering the social network of an organization and collecting available information within each cluster can result in the valuable non-trivial perceptions.
sent25: A key asset or a considerable threat to the primary organization can be the knowledge about the network of informal relationships.
sent26: Besides analyzing social networks of the organizations, algorithms and methods used to gather data from freely available sources would be presented by this paper.
sent27: A web crawler was developed to obtain profiles of employees from six targeted organizations and this was done by collecting the Facebook data.
sent28: A social network topology was created for each organization, and machine-learning algorithms and centrality measures were implemented so that the hidden leadership positions within each company could be discovered.
sent29: Moreover, the social community clusters inside these organizations were also revealed by the algorithms, which gave us understanding about the communication network of each company in addition to the structure of the organization.
sent30: According to a study by [51], it has become clear that social media data is simply susceptible to misuse.
sent31: The scheme encompasses structured approach and its application.
sent32: Furthermore, it entails performing a statistical cluster analysis in addition to the comprehensive analysis of social media comments so that researchers could determine the inter-relationships among key factors.
sent33: The qualitative social media data can be quantified by these schemes and subsequently cluster them based on their similar features, and then they can be used as decision-making tools.
sent34: The SAMSUNG Mobile Facebook page, where Samsung smartphones were introduced, was used for the data acquisition process.
sent35: The comment published by Facebook users on the captioned Facebook page is referred to as the ""Data"".
sent36: In a period of 3 months, almost 128371 comments were downloaded.
sent37: The English comments only were undergone through the analysis process.
sent38: Afterward, the conceptual analysis was used by the content analysis and ultimately statistical cluster analysis was performed by carrying out relational analysis.
sent39: Hence, social media data is integrated by applying the statistical cluster analysis and it is performed based on the output of the conceptual analysis.
sent40: The researchers are consequently enabled to categorize a large dataset into many subsets, at times, referred to as objects.
sent41: One of the disciplines of its application is marketing.
sent42: Factors that can be manageable in some cases are also minimized by these types of techniques.
sent43: A study by [52] explored the social data as a systematic data mining architecture.
sent44: Findings indicated that Facebook as a social networking site is the major source of data.
sent45: Besides this approach, information on ""my wall"" post regarding myself, age and comments from the Facebook all are emphasized by the author.
sent46: It has been taken as a raw data, which is applied later to study and monitor the analytical tactics.
sent47: In addition, the study investigated images for the advertisement of their products and for the decisionmaking process.
sent48: A number of data mining techniques precede the coercion of intellectual knowledge from social data.
sent49: Mainly, it organizes the key information and other applied activities in which users are attributed regarding their colleagues on social networking sites (i.e. Facebook).
sent50: For the recovery on Facebook user database, Facebook API performs Application Secret key and Facebook API Key are executed by Facebook API.
sent51: As a result, WEKA files and data mining techniques are supported to collect certain data into the secondary database, while the text data is represented by the detached data.
sent52: Researchers of [41] explored the applicability of representing user's personality based on the extracted features from the Facebook data.
sent53: The classification techniques and their utilities were completely analyzed with regard to the inspirational research outcomes.
sent54: A sample of 250 user instances from Facebook formed the research study and this sample was from about 10,000 status updates, which was delivered by the My Personality project [53].
sent55: The study has the following two interconnected objectives: (1) having knowledge about the pertinent personality-correlated indicators that presents user data implicitly or explicitly in Facebook, and (2) identifying the feasibility of prognostic character demonstration so that upcoming intelligent systems could be supported.
sent56: The study emphasized on the promotion of pertinent features in a model, through which the enhanced output of the classifiers under evaluation could be observed."
65219850,A Survey of Text Mining in Social Media: Facebook and Twitter Perspectives Establish algorithm for to determine an authorized PW error View project Further Investigations on Developing an Arabic Sentiment Lexicon View project Said Salloum British University in Dubai 3 PUBLICATIONS 2 CITATIONS SEE PROFILE,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/94d255e0b2a729e918a320ab99e503cfc04e1fc8,s5,Text mining efforts in resolving various NLP issues,"A study by [44] stated that text mining is responsible for structuring the irregular data patterns written in the human language. As most of the people interact with each other in the form of text so for those people who are not able to share structured form of data, text mining is the best technique to handle these situations. Among others, NLP is considered as the most amazing research field. The main goal of NLP is to seek information regarding how the computer systems are examining and getting information from the languages of human beings to create applications of high quality [17]. The art of sharing meaningful information with the help of uncommon and meaningless data is truly a good thing. Text mining technique as described by [45] examines the content for extracting the meaningful data which can be used for particular purposes. It looks like text mining that is going to include the overall NLP scheme [46] in its system in order to effectively examine the human language and to structure the unstructured data patterns accordingly. As the technology is advancing day by day, text mining system will get better and better and this is what all people are looking for.","[['b45', 'b43', 'b16', 'b44']]","[['b45', 'b43', 'b16', 'b44']]",4,"sent1: A study by [44] stated that text mining is responsible for structuring the irregular data patterns written in the human language.
sent2: As most of the people interact with each other in the form of text so for those people who are not able to share structured form of data, text mining is the best technique to handle these situations.
sent3: Among others, NLP is considered as the most amazing research field.
sent4: The main goal of NLP is to seek information regarding how the computer systems are examining and getting information from the languages of human beings to create applications of high quality [17].
sent5: The art of sharing meaningful information with the help of uncommon and meaningless data is truly a good thing.
sent6: Text mining technique as described by [45] examines the content for extracting the meaningful data which can be used for particular purposes.
sent7: It looks like text mining that is going to include the overall NLP scheme [46] in its system in order to effectively examine the human language and to structure the unstructured data patterns accordingly.
sent8: As the technology is advancing day by day, text mining system will get better and better and this is what all people are looking for."
65219850,A Survey of Text Mining in Social Media: Facebook and Twitter Perspectives Establish algorithm for to determine an authorized PW error View project Further Investigations on Developing an Arabic Sentiment Lexicon View project Said Salloum British University in Dubai 3 PUBLICATIONS 2 CITATIONS SEE PROFILE,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/94d255e0b2a729e918a320ab99e503cfc04e1fc8,s4,Text mining in social networks,"The importance of text mining has been increased due to the significant contributions in the field of technology. Data mining as reported by [37] is also important but due to the advancement, text mining is taking its place. It is really a big effort to convey valuable information and knowledge [38] through powerful handling and mining processes from the irregular form of information. In this era, structured data has lost its importance and the unstructured data has gained the popularity. Most of the organizations are going towards text mining and forgetting the concept of data mining [39]. Scholars of [40] reported that all the social networking sites are providing a great space to individuals to facilitate interaction and share their views and opinions. The best thing which these sites are doing is that it has become easy for the individuals to understand a particular person depending upon his or her activities. Through all these activities, people related to different customs and values have come closer to each other because of having the better understanding of each other's emotions, perceptions and areas of interest. At this time, user interfaces are going to be equipped with personality based qualities [41]. Personalized designs were used in e-commerce [42], [43], e-learning, and information filtering for enhancing different styles and skills.","[['b41', 'b37', 'b42', 'b40', 'b38', 'b36', 'b39']]","[['b41', 'b37', 'b42', 'b40', 'b38', 'b36', 'b39']]",7,"sent1: The importance of text mining has been increased due to the significant contributions in the field of technology.
sent2: Data mining as reported by [37] is also important but due to the advancement, text mining is taking its place.
sent3: It is really a big effort to convey valuable information and knowledge [38] through powerful handling and mining processes from the irregular form of information.
sent4: In this era, structured data has lost its importance and the unstructured data has gained the popularity.
sent5: Most of the organizations are going towards text mining and forgetting the concept of data mining [39].
sent6: Scholars of [40] reported that all the social networking sites are providing a great space to individuals to facilitate interaction and share their views and opinions.
sent7: The best thing which these sites are doing is that it has become easy for the individuals to understand a particular person depending upon his or her activities.
sent8: Through all these activities, people related to different customs and values have come closer to each other because of having the better understanding of each other's emotions, perceptions and areas of interest.
sent9: At this time, user interfaces are going to be equipped with personality based qualities [41].
sent10: Personalized designs were used in e-commerce [42], [43], e-learning, and information filtering for enhancing different styles and skills."
65219850,A Survey of Text Mining in Social Media: Facebook and Twitter Perspectives Establish algorithm for to determine an authorized PW error View project Further Investigations on Developing an Arabic Sentiment Lexicon View project Said Salloum British University in Dubai 3 PUBLICATIONS 2 CITATIONS SEE PROFILE,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/94d255e0b2a729e918a320ab99e503cfc04e1fc8,s3,Text Mining vs. Data Mining,"In the case of posting comments on any post on different social networking sites, there is not a single structured technique available which causes problems in the direct usage of the data. Data available in the text format has much more importance and that is why text mining is generating much business value [32]. A study by [33] stated that data mining represents the derivation of a meaningful pattern or principles from a spatial database for determining a particular issue or issues. Data mining is different from text mining [34]. A study by [35] pointed out that text mining is much more complex than data mining because it contains irregular and unstructured data patterns, whereas data mining is dealing with the structured sets of data. The tools that were used in data mining were only dealing with structured data [34]. Text mining is like an intelligence system which is extracting proper words or sentences from the improper words and then transforming those words into the particular suggestions. Text mining is basically a new field having the main purpose of data recovery, machine learning, information mining and computational linguistics [36].","[['b35', 'b31', 'b34', 'b33', 'b32']]","[['b35', 'b31', 'b34', 'b33', 'b32']]",5,"sent1: In the case of posting comments on any post on different social networking sites, there is not a single structured technique available which causes problems in the direct usage of the data.
sent2: Data available in the text format has much more importance and that is why text mining is generating much business value [32].
sent3: A study by [33] stated that data mining represents the derivation of a meaningful pattern or principles from a spatial database for determining a particular issue or issues.
sent4: Data mining is different from text mining [34].
sent5: A study by [35] pointed out that text mining is much more complex than data mining because it contains irregular and unstructured data patterns, whereas data mining is dealing with the structured sets of data.
sent6: The tools that were used in data mining were only dealing with structured data [34].
sent7: Text mining is like an intelligence system which is extracting proper words or sentences from the improper words and then transforming those words into the particular suggestions.
sent8: Text mining is basically a new field having the main purpose of data recovery, machine learning, information mining and computational linguistics [36]."
65219850,A Survey of Text Mining in Social Media: Facebook and Twitter Perspectives Establish algorithm for to determine an authorized PW error View project Further Investigations on Developing an Arabic Sentiment Lexicon View project Said Salloum British University in Dubai 3 PUBLICATIONS 2 CITATIONS SEE PROFILE,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/94d255e0b2a729e918a320ab99e503cfc04e1fc8,s1,Background,"Businesses have identified data-driven approaches as the ideal blueprint for their growth. It is easier to understand this theory. After all, wouldn't it benefit a company to get an idea about the perception of its products in the market without having to consult individual reviews from everyone? Wouldn't it be better if they could gauge which political candidate is ideal for their public image without having to analyze them all individually? This is why market study and research are some of the most highly invested fields in the world right now. Social networking sites like Twitter and Facebook are ideal for this purpose. Posts or messages shared by people on these platforms with their friends remain freely accessible or are kept confidential. They give businesses the chance to scoop up public sentiments [20], [21] about topics that they are interested to share by a large group of people.

The processing of surveys and public impressions using specially designed computational systems is a shared objective of inter-connected fields like subjectivity analysis, opinion mining [22], and sentiment analysis. Creating problem-solving techniques or methods to define the structure and precedence or for summarizing opinionated messages for particular topics [23], occasions or products is another target of the survey. For example, these methods could be used for gauging support for particular occasions or items, or determining thumbs down or thumbs up votes for specific movies based on their reviews.","[['b20', 'b19'], ['b21', 'b22']]","[['b20', 'b19'], ['b21', 'b22']]",4,"sent1: Businesses have identified data-driven approaches as the ideal blueprint for their growth.
sent2: It is easier to understand this theory.
sent3: After all, wouldn't it benefit a company to get an idea about the perception of its products in the market without having to consult individual reviews from everyone?
sent4: Wouldn't it be better if they could gauge which political candidate is ideal for their public image without having to analyze them all individually?
sent5: This is why market study and research are some of the most highly invested fields in the world right now.
sent6: Social networking sites like Twitter and Facebook are ideal for this purpose.
sent7: Posts or messages shared by people on these platforms with their friends remain freely accessible or are kept confidential.
sent8: They give businesses the chance to scoop up public sentiments [20], [21] about topics that they are interested to share by a large group of people.
sent9: The processing of surveys and public impressions using specially designed computational systems is a shared objective of inter-connected fields like subjectivity analysis, opinion mining [22], and sentiment analysis.
sent10: Creating problem-solving techniques or methods to define the structure and precedence or for summarizing opinionated messages for particular topics [23], occasions or products is another target of the survey.
sent11: For example, these methods could be used for gauging support for particular occasions or items, or determining thumbs down or thumbs up votes for specific movies based on their reviews."
65219850,A Survey of Text Mining in Social Media: Facebook and Twitter Perspectives Establish algorithm for to determine an authorized PW error View project Further Investigations on Developing an Arabic Sentiment Lexicon View project Said Salloum British University in Dubai 3 PUBLICATIONS 2 CITATIONS SEE PROFILE,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/94d255e0b2a729e918a320ab99e503cfc04e1fc8,s2,Text Mining,"Text mining makes it easy to obtain a meaningful and structured data from the irregular data patterns [24], [25], and [26]. It is really not an easy task for the computers to understand the unstructured data [27], [28] and make it structured. Human beings can perform this task without any further efforts due to the availability of different linguistic techniques. However, human beings are limited in terms of speed and space as comparing to computers. That is, computers are much better than humans to do these tasks. Most of the existing data in any organization is represented in a text format, so if we compare data mining with text mining then text mining is more important [29]. But as text mining is used for structuring the unstructured text data then this task is more demanding as compared to data mining. In general, the data related to social media sites is not collected for the research purpose [30], it is mandatory to change the structure of the data coming from the social media. 80% of the available text on the web is unstructured while only 20% is structured [31]. ","[['b27', 'b29', 'b28', 'b25', 'b26', 'b23', 'b30', 'b24']]","[['b27', 'b29', 'b28', 'b25', 'b26', 'b23', 'b30', 'b24']]",8,"sent1: Text mining makes it easy to obtain a meaningful and structured data from the irregular data patterns [24], [25], and [26].
sent2: It is really not an easy task for the computers to understand the unstructured data [27], [28] and make it structured.
sent3: Human beings can perform this task without any further efforts due to the availability of different linguistic techniques.
sent4: However, human beings are limited in terms of speed and space as comparing to computers.
sent5: That is, computers are much better than humans to do these tasks.
sent6: Most of the existing data in any organization is represented in a text format, so if we compare data mining with text mining then text mining is more important [29].
sent7: But as text mining is used for structuring the unstructured text data then this task is more demanding as compared to data mining.
sent8: In general, the data related to social media sites is not collected for the research purpose [30], it is mandatory to change the structure of the data coming from the social media.
sent9: 80% of the available text on the web is unstructured while only 20% is structured [31]."
112487687,Overview of the 2013 ALTA Shared Task,"Engineering, Computer Science",https://www.semanticscholar.org/paper/200d6e8c0aae03704c5c3f51823532597e36d325,s2,The Training and Test Sets,"We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.

The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus. Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems. The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard. The second test set was a ""private"" test set that was used to determine the final scores. By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day. As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.

To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 . We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words. We lowercased the tokens and removed those that matched our list of punctuation marks.

The Wikipedia training data consisted of 18 files with a total of 306,445 words. The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.

We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.

The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus. Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems. The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard. The second test set was a ""private"" test set that was used to determine the final scores. By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day. As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.

To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 . We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words. We lowercased the tokens and removed those that matched our list of punctuation marks.

The Wikipedia training data consisted of 18 files with a total of 306,445 words. The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.","[['b0'], ['b0'], ['b1'], ['b0'], ['b0'], ['b0'], ['b1'], ['b0']]","[['b0'], ['b0'], ['b1'], ['b0'], ['b0'], ['b0'], ['b1'], ['b0']]",8,"sent1: We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.
sent2: The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus.
sent3: Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems.
sent4: The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard.
sent5: The second test set was a ""private"" test set that was used to determine the final scores.
sent6: By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day.
sent7: As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.
sent8: To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 .
sent9: We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words.
sent10: We lowercased the tokens and removed those that matched our list of punctuation marks.
sent11: The Wikipedia training data consisted of 18 files with a total of 306,445 words.
sent12: The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.
sent13: We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.
sent14: The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus.
sent15: Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems.
sent16: The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard.
sent17: The second test set was a ""private"" test set that was used to determine the final scores.
sent18: By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day.
sent19: As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.
sent20: To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 .
sent21: We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words.
sent22: We lowercased the tokens and removed those that matched our list of punctuation marks.
sent23: The Wikipedia training data consisted of 18 files with a total of 306,445 words.
sent24: The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words."
112487687,Overview of the 2013 ALTA Shared Task,"Engineering, Computer Science",https://www.semanticscholar.org/paper/200d6e8c0aae03704c5c3f51823532597e36d325,s13,The Training and Test Sets,"We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.

The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus. Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems. The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard. The second test set was a ""private"" test set that was used to determine the final scores. By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day. As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.

To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 . We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words. We lowercased the tokens and removed those that matched our list of punctuation marks.

The Wikipedia training data consisted of 18 files with a total of 306,445 words. The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.

We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.

The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus. Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems. The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard. The second test set was a ""private"" test set that was used to determine the final scores. By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day. As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.

To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 . We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words. We lowercased the tokens and removed those that matched our list of punctuation marks.

The Wikipedia training data consisted of 18 files with a total of 306,445 words. The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.","[['b0'], ['b0'], ['b1'], ['b0'], ['b0'], ['b0'], ['b1'], ['b0']]","[['b0'], ['b0'], ['b1'], ['b0'], ['b0'], ['b0'], ['b1'], ['b0']]",8,"sent1: We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.
sent2: The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus.
sent3: Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems.
sent4: The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard.
sent5: The second test set was a ""private"" test set that was used to determine the final scores.
sent6: By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day.
sent7: As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.
sent8: To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 .
sent9: We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words.
sent10: We lowercased the tokens and removed those that matched our list of punctuation marks.
sent11: The Wikipedia training data consisted of 18 files with a total of 306,445 words.
sent12: The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words.
sent13: We used the data by Baldwin and Joseph (2009) to produce a training set and two test sets, plus text from Wikipedia to produce additional training data.
sent14: The data by Baldwin and Joseph (2009) are from the AP Newswire (APW) and New York Times (NYT) sections of the English Gigaword Corpus.
sent15: Of the two test sets, one was used as a ""public"" test set that participants could use to check their progress in the development of their systems.
sent16: The participants did not have access to the target output but they could submit the output of their systems and they would receive instant feedback with the results and how they compare against other participants in the leaderboard.
sent17: The second test set was a ""private"" test set that was used to determine the final scores.
sent18: By having separate ""public"" and ""private"" test sets we aimed to reduce the risk of some systems overfitting to the actual test set, since each participant could submit up to two runs every day.
sent19: As training data we used the third partition from Baldwin and Joseph (2009) plus an extract from Wikipedia.
sent20: To download the Wikipedia text, shuffle the paragraphs, and split the contents into smaller files we used a method and scripts based on a blog post 1 .
sent21: We then used the Python NLTK toolkit (Bird et al., 2009) to tokenise the words.
sent22: We lowercased the tokens and removed those that matched our list of punctuation marks.
sent23: The Wikipedia training data consisted of 18 files with a total of 306,445 words.
sent24: The data from Baldwin and Joseph (2009) consisted of a ""train"" file with 66,371 words, the ""public"" test file with 64,072 words, and the ""private"" test file with 65,903 words."
149117098,Article Respondent Robotics: Simulating Responses to Likert-Scale Survey Items,Computer Science,https://www.semanticscholar.org/paper/30d7c07ec6575bf8b1e7b132dbd2d5ae10c087ee,s2,Semantics and Correlations,"Rensis Likert assumed that his scales delivered measures of attitude strength (Likert, 1932). Statistic modeling of such data in classic psychometrics viewed survey responses as basically composed of a true score and an error component. The error component of the score would reflect random influences on the response, and these could be minimized by averaging scores of semantically related questions for each variable (Nunnally & Bernstein, 2010). The error variance is assumed to converge around 0, making average scale scores a better expression of the true attitude strength of the respondents. The relationships among other surveyed variables should however not be determined by the semantics of the items, but instead only covary to the extent that they are empirically related. A frequent way of demonstrating this relative independence has been done by applying factor analytical techniques (Abdi, 2003;Hu & Bentler, 1999). In short, the prevalent psychometric practices have until now been treating the systematic variation among items as expression of attitude strength toward topics in the survey.

The STSR proposes a contrasting view. Here, the relationships among items and among survey variables are first and foremost semantic (Arnulf et al., 2014), a view corroborated by independent researchers (Nimon, Shuck, & Zigarmi, 2016). Every respondent may begin the survey by expressing attitude strength toward the surveyed topic in the form of a score on the Likert-type scale. However, in the succeeding responses, the scores on the coming items may be predominantly determined by the degree to which these items are semantically similar. This was earlier argued and documented by Feldman and Lynch (1988). A slightly different version of this hypothesis was also formulated by Schwarz (1999). However, both these precursors to STSR were speculating that calculation of responses may be exceptional to situations where people hold no real attitudes, or become unduly influenced in their response patterns by recent responses to other items. The formulation of STSR was the first claim that semantic calculation may actually be the fundamental mechanism explaining systematic variance among items.

Another antecedent to STSR is ""unfolding theory"" as described by Coombs (Coombs, 1964;Coombs & Kao, 1960) and later by Michell (1994). We will deal with unfolding theory in some detail as it has direct consequences for creating algorithms to mimic the real responses. A practical example may be a job satisfaction item, such as ""I like working here."" When respondents choose to answer this on a scale from 1 to 5, it may be hard to explain what the number means. To quantify an attitude, one could split the statement in discreet answering categories such as the extremely positive attitude: ""I would prefer working here to any other job or even leisure activity."" A neutral attitude could be a statement such as ""I do not care if I work here or not,"" or the negative statement ""I would take any other job to get away from this one."" The central point in unfolding theory is that any respondent's preferred response would be the point at which item response scale ""folds."" Folding implies that the response alternatives need to be sorted in their mutual distance from the preferred option. If someone picks the option 4 on a scale from 1 to 5, it would mean that the options 3 and 5 are about equally distant from 4, but that 2 and certainly 1 would be further away from the preferred statement. In this way, the scale is said to be ""folding"" around the preferred value 4, which determines the distance of all other responses from the folding point. Michell (1994) showed mathematically and experimentally that the quantitative properties of surveys stem from these semantic distinctions. Just as Coombs claimed, all respondents need to understand the common semantic properties-the meaning-of any survey item to attach numerical values to the questions in the survey. For two respondents to rate an item such as ""I like to work here"" with 1 or 5, they need to agree on the meaning of this response-the one respondent likes his job, the other does not, but both need to understand the meaning of the other response alternatives for one's own response to be quantitatively comparable. Michell showed how any survey scale needs to fold along a ""dominant path"" -the mutual meaning of items and response options used in a scale. This ""dominant path"" will affect the responses to other items if they are semantically related. Take the following simple example measuring job satisfaction and turnover intention, two commonly measured variables in OB research: One item measuring job satisfaction is the item ""I like working here,"" and one item measuring turnover intention is ""I will probably look for a new job in the next weeks."" A person who answers 5 to ""I like working here"" is by semantic implication less likely to look for a new job in the next week than someone who scores 1, and vice versa. Less obvious is the effect of what Michell called the ""dominant path"": If someone has a slightly positive attitude toward the job without giving it full score, this person will be slightly inclined, but maybe not determined, to turn down offers for a new job. The dominant path of such items will make the respondents rank the mutual answering alternatives in an ""unfolding way."" Not only are the extreme points of the Likert-type scales semantically linked but people also appear to rank the response option of all items in mutual order. A third item measuring organizational citizenship behavior (OCB), for example, is ""I frequently attend to problems that really are not part of my job."" The semantic specification of responses to this scale may be negative items such as ""I only do as little as possible so I don't get fired"" or positive items such as ""I feel capable and responsible for correcting any problem that may arise.""

According to unfolding theory, people will respond such that their response pattern is semantically coherent, that is, consistent with an unfolding of the semantic properties of items. The dominant path will prevent most people from choosing answer alternatives that are not semantically coherent.

Any survey will need a semantically invariant structure to attain reliably different but consistent responses from different people. Coombs and Kao showed experimentally that there is a necessary structure in all surveys emanating from how respondents commonly understand the survey (Coombs & Kao, 1960;Habing, Finch, & Roberts, 2005;Roysamb & Strype, 2002).

In STSR, correlations among survey items are primarily explained by the likelihood that they evoke similar meanings. As we will show below, the semantic relationships among survey items contain information isomorphic to the correlations among the same items in a survey. This implies that individual responses are shaped-and thereby principally computable-because the semantics of items are given and possible to estimate a priori to administering the survey.

To the extent that this is possible, current-day analytical techniques risk treating attitude strength as error variance. This is contrary to what is commonly believed, as the tradition of ""construct validation"" in survey research rests on the assumption that attitude strength across samples of respondents is the source of measures informing the empirical research (Bagozzi, 2011;Lamiell, 2013;MacKenzie, Podsakoff, & Podsakoff, 2011;Michell, 2013;Slaney, 2017;Slaney & Racine, 2013a, 2013b.

Other researchers have reported that the survey structure itself may create distinct factors for items that were originally devised as ""reversed"" or negatively phrased items (Roysamb & Strype, 2002;van Schuur & Kiers, 1994). One reason for this is the uncertain relationship between the actual measurements obtained from the survey and the assumed quantifiable nature of the latent construct in question. Kathleen Slaney's (2017) recent review of construct validation procedures shows how ""measurement"" of attitudes may come about by imposing numbers on an unknown structure. As shown by Andrew Maul (2017), acceptable psychometric properties of scales are obtainable even if keywords in the items are replaced by nonsensical words. The psychometric properties were largely retained even if the item texts were replaced by totally meaningless sentences or even by entirely empty items carrying nothing but response alternatives. The survey structure seems to be a powerful source of methods effects, imposing structure on response statistics.

The purpose here is to reconstruct survey responses using semantic information and other a priori known information about the survey structure. Semantic information about the semantic content of items is precisely void of knowledge about attitude strength. If this type of information can be used to create artificial responses with meaningful characteristics akin to the original ones, it will substantiate the claims of STSR. In particular, it will deliver empirical evidence that common psychometric practices may risk treating attitude strength as error variance, leaving mostly semantic relationships in the statistics. This attempt is exploratory in nature, and we will therefore not derive hypotheses but instead seek to explore the research question from various angles. The following exploration is undertaken as two independent studies: Study 1 is an in-depth study of the MLQ, containing the main procedures to investigate and explore. Study 2 is a brief application of the same procedure to a different, shorter scale, and another sample of respondents.","[['b21', 'b31', 'b15', 'b0'], ['b4', 'b39', 'b30', 'b11'], ['b9', 'b10', 'b24'], [], ['b36', 'b14', 'b10'], [], ['b43', 'b22', 'b25', 'b45', 'b18', 'b44', 'b6'], ['b23', 'b36', 'b49'], []]","[['b21', 'b31', 'b15', 'b0'], ['b4', 'b39', 'b30', 'b11'], ['b9', 'b10', 'b24'], [], ['b36', 'b14', 'b10'], [], ['b43', 'b22', 'b25', 'b45', 'b18', 'b44', 'b6'], ['b23', 'b36', 'b49'], []]",24,"sent1: Rensis Likert assumed that his scales delivered measures of attitude strength (Likert, 1932).
sent2: Statistic modeling of such data in classic psychometrics viewed survey responses as basically composed of a true score and an error component.
sent3: The error component of the score would reflect random influences on the response, and these could be minimized by averaging scores of semantically related questions for each variable (Nunnally & Bernstein, 2010).
sent4: The error variance is assumed to converge around 0, making average scale scores a better expression of the true attitude strength of the respondents.
sent5: The relationships among other surveyed variables should however not be determined by the semantics of the items, but instead only covary to the extent that they are empirically related.
sent6: A frequent way of demonstrating this relative independence has been done by applying factor analytical techniques (Abdi, 2003;Hu & Bentler, 1999).
sent7: In short, the prevalent psychometric practices have until now been treating the systematic variation among items as expression of attitude strength toward topics in the survey.
sent8: The STSR proposes a contrasting view.
sent9: Here, the relationships among items and among survey variables are first and foremost semantic (Arnulf et al., 2014), a view corroborated by independent researchers (Nimon, Shuck, & Zigarmi, 2016).
sent10: Every respondent may begin the survey by expressing attitude strength toward the surveyed topic in the form of a score on the Likert-type scale.
sent11: However, in the succeeding responses, the scores on the coming items may be predominantly determined by the degree to which these items are semantically similar.
sent12: This was earlier argued and documented by Feldman and Lynch (1988).
sent13: A slightly different version of this hypothesis was also formulated by Schwarz (1999).
sent14: However, both these precursors to STSR were speculating that calculation of responses may be exceptional to situations where people hold no real attitudes, or become unduly influenced in their response patterns by recent responses to other items.
sent15: The formulation of STSR was the first claim that semantic calculation may actually be the fundamental mechanism explaining systematic variance among items.
sent16: Another antecedent to STSR is ""unfolding theory"" as described by Coombs (Coombs, 1964;Coombs & Kao, 1960) and later by Michell (1994).
sent17: We will deal with unfolding theory in some detail as it has direct consequences for creating algorithms to mimic the real responses.
sent18: A practical example may be a job satisfaction item, such as ""I like working here.""
sent19: When respondents choose to answer this on a scale from 1 to 5, it may be hard to explain what the number means.
sent20: To quantify an attitude, one could split the statement in discreet answering categories such as the extremely positive attitude: ""I would prefer working here to any other job or even leisure activity.""
sent21: A neutral attitude could be a statement such as ""I do not care if I work here or not,"" or the negative statement ""I would take any other job to get away from this one.""
sent22: The central point in unfolding theory is that any respondent's preferred response would be the point at which item response scale ""folds.""
sent23: Folding implies that the response alternatives need to be sorted in their mutual distance from the preferred option.
sent24: If someone picks the option 4 on a scale from 1 to 5, it would mean that the options 3 and 5 are about equally distant from 4, but that 2 and certainly 1 would be further away from the preferred statement.
sent25: In this way, the scale is said to be ""folding"" around the preferred value 4, which determines the distance of all other responses from the folding point.
sent26: Michell (1994) showed mathematically and experimentally that the quantitative properties of surveys stem from these semantic distinctions.
sent27: Just as Coombs claimed, all respondents need to understand the common semantic properties-the meaning-of any survey item to attach numerical values to the questions in the survey.
sent28: For two respondents to rate an item such as ""I like to work here"" with 1 or 5, they need to agree on the meaning of this response-the one respondent likes his job, the other does not, but both need to understand the meaning of the other response alternatives for one's own response to be quantitatively comparable.
sent29: Michell showed how any survey scale needs to fold along a ""dominant path"" -the mutual meaning of items and response options used in a scale.
sent30: This ""dominant path"" will affect the responses to other items if they are semantically related.
sent31: Take the following simple example measuring job satisfaction and turnover intention, two commonly measured variables in OB research: One item measuring job satisfaction is the item ""I like working here,"" and one item measuring turnover intention is ""I will probably look for a new job in the next weeks.""
sent32: A person who answers 5 to ""I like working here"" is by semantic implication less likely to look for a new job in the next week than someone who scores 1, and vice versa.
sent33: Less obvious is the effect of what Michell called the ""dominant path"": If someone has a slightly positive attitude toward the job without giving it full score, this person will be slightly inclined, but maybe not determined, to turn down offers for a new job.
sent34: The dominant path of such items will make the respondents rank the mutual answering alternatives in an ""unfolding way.""
sent35: Not only are the extreme points of the Likert-type scales semantically linked but people also appear to rank the response option of all items in mutual order.
sent36: A third item measuring organizational citizenship behavior (OCB), for example, is ""I frequently attend to problems that really are not part of my job.""
sent37: The semantic specification of responses to this scale may be negative items such as ""I only do as little as possible so I don't get fired"" or positive items such as ""I feel capable and responsible for correcting any problem that may arise.
sent38: ""According to unfolding theory, people will respond such that their response pattern is semantically coherent, that is, consistent with an unfolding of the semantic properties of items.
sent39: The dominant path will prevent most people from choosing answer alternatives that are not semantically coherent.
sent40: Any survey will need a semantically invariant structure to attain reliably different but consistent responses from different people.
sent41: Coombs and Kao showed experimentally that there is a necessary structure in all surveys emanating from how respondents commonly understand the survey (Coombs & Kao, 1960;Habing, Finch, & Roberts, 2005;Roysamb & Strype, 2002).
sent42: In STSR, correlations among survey items are primarily explained by the likelihood that they evoke similar meanings.
sent43: As we will show below, the semantic relationships among survey items contain information isomorphic to the correlations among the same items in a survey.
sent44: This implies that individual responses are shaped-and thereby principally computable-because the semantics of items are given and possible to estimate a priori to administering the survey.
sent45: To the extent that this is possible, current-day analytical techniques risk treating attitude strength as error variance.
sent46: This is contrary to what is commonly believed, as the tradition of ""construct validation"" in survey research rests on the assumption that attitude strength across samples of respondents is the source of measures informing the empirical research (Bagozzi, 2011;Lamiell, 2013;MacKenzie, Podsakoff, & Podsakoff, 2011;Michell, 2013;Slaney, 2017;Slaney & Racine, 2013a, 2013b.
sent47: Other researchers have reported that the survey structure itself may create distinct factors for items that were originally devised as ""reversed"" or negatively phrased items (Roysamb & Strype, 2002;van Schuur & Kiers, 1994).
sent48: One reason for this is the uncertain relationship between the actual measurements obtained from the survey and the assumed quantifiable nature of the latent construct in question.
sent49: Kathleen Slaney's (2017) recent review of construct validation procedures shows how ""measurement"" of attitudes may come about by imposing numbers on an unknown structure.
sent50: As shown by Andrew Maul (2017), acceptable psychometric properties of scales are obtainable even if keywords in the items are replaced by nonsensical words.
sent51: The psychometric properties were largely retained even if the item texts were replaced by totally meaningless sentences or even by entirely empty items carrying nothing but response alternatives.
sent52: The survey structure seems to be a powerful source of methods effects, imposing structure on response statistics.
sent53: The purpose here is to reconstruct survey responses using semantic information and other a priori known information about the survey structure.
sent54: Semantic information about the semantic content of items is precisely void of knowledge about attitude strength.
sent55: If this type of information can be used to create artificial responses with meaningful characteristics akin to the original ones, it will substantiate the claims of STSR.
sent56: In particular, it will deliver empirical evidence that common psychometric practices may risk treating attitude strength as error variance, leaving mostly semantic relationships in the statistics.
sent57: This attempt is exploratory in nature, and we will therefore not derive hypotheses but instead seek to explore the research question from various angles.
sent58: The following exploration is undertaken as two independent studies: Study 1 is an in-depth study of the MLQ, containing the main procedures to investigate and explore.
sent59: Study 2 is a brief application of the same procedure to a different, shorter scale, and another sample of respondents."
149117098,Article Respondent Robotics: Simulating Responses to Likert-Scale Survey Items,Computer Science,https://www.semanticscholar.org/paper/30d7c07ec6575bf8b1e7b132dbd2d5ae10c087ee,s5,Estimating Item Semantics,"A number of algorithms exist that allow computing the similarity of the survey items. Here, we have chosen one termed ""MI"" (Mihalcea, Corley, & Strapparava, 2006;Mohler & Mihalcea, 2009). MI is chosen because it has been previously published, is well understood, and allows easy replication. The Arnulf et al. study in 2014 also showed that MI values are probably closer to everyday language than some LSA-generated values that may carry specialized domain knowledge.

The MI algorithm derives its knowledge about words from a lexical database called WordNet, containing information about 147,278 unique words that were encoded by a team of linguists between 1990 and 2007 (Leacock, Miller, & Chodorow, 1998;Miller, 1995;Poli, Healy, & Kameas, 2010). Building on knowledge about each single word in WordNet as its point of departure, MI computes a similarity measure for two candidate sentences: S1 and S2. It identifies part of speech (POS), beginning with tokenization, and POS tagging of all the words in the survey item with their respective word classes (noun, verb, adverb, adjective, and cardinal, which play a very important role in text understanding). It then calculates word similarity by measuring each word in the sentence against all the words from the other sentence. This identifies the highest semantic similarity (maxSim) from six word-similarity metrics originally created to measure concept likeness (instead of word likeness). The metrics are adapted here to compute word similarity by computing the shortest distance of given words' synsets in the WordNet hierarchy. The word-word similarity measure is directional. It begins with each word in S1 being computed against each word in S2, and then vice versa. The algorithm finally considers sentence similarity by normalizing the highest semantic similarity (maxSim) for each word in the sentences by applying ""inverse document frequency"" (IDF) to the British National Corpus to weight rare and common terms. The normalized scores are then summed up for a sentence similarity score, SimMI, as follows:

where maxSim(w, S2) is the score of the most similar word in S2 to w, and IDF (w) is the IDF of word w. The final output of MI is a numeric value between 0 and 1, where 0 indicates no semantic overlap, and numbers approaching 1 indicate identical meaning of the two sentences. These numbers serve as the input to our simulating algorithm for constructing artificial responses. Note that the information in the MI values is entirely lexical and syntactic.

It contains no knowledge about surveys, leadership, or respondent behavior. The MLQ has 45 items. This yields (45 × (45 − 1)) / 2 or 990 unique item pairs, for which we obtain MI values.

One special problem concerns the direction of signs. In the MLQ, 264 of 990 pairs of items are negatively correlated. Theory suggests that two scales, Laissez-faire and Passive Management by Exception, are likely to relate negatively to effective leadership. The problem has been treated extensively elsewhere (Arnulf et al., 2014), so we will only offer a brief explanation here. MI does not take negative values, and does not differentiate well between positive and negative statements about the same content. For two items describing how (a) a manager is unapproachable when called for and (b) that the same person uses appropriate methods of leadership, the surveyed responses correlate at -.42 in the present sample, while the MI value is .38. The chosen solution is to allow MI values to be negative for all pairs of items from Laissezfaire and Passive Management by Exception (correctly identifying 255 of the 264 negative correlations, p < .001).","[['b29', 'b26', None], ['b20', 'b34', 'b28'], [], [], ['b4']]","[['b29', 'b26', None], ['b20', 'b34', 'b28'], [], [], ['b4']]",7,"sent1: A number of algorithms exist that allow computing the similarity of the survey items.
sent2: Here, we have chosen one termed ""MI"" (Mihalcea, Corley, & Strapparava, 2006;Mohler & Mihalcea, 2009). MI is chosen because it has been previously published, is well understood, and allows easy replication.
sent3: The Arnulf et al. study in 2014 also showed that MI values are probably closer to everyday language than some LSA-generated values that may carry specialized domain knowledge.
sent4: The MI algorithm derives its knowledge about words from a lexical database called WordNet, containing information about 147,278 unique words that were encoded by a team of linguists between 1990 and 2007 (Leacock, Miller, & Chodorow, 1998;Miller, 1995;Poli, Healy, & Kameas, 2010).
sent5: Building on knowledge about each single word in WordNet as its point of departure, MI computes a similarity measure for two candidate sentences: S1 and S2.
sent6: It identifies part of speech (POS), beginning with tokenization, and POS tagging of all the words in the survey item with their respective word classes (noun, verb, adverb, adjective, and cardinal, which play a very important role in text understanding).
sent7: It then calculates word similarity by measuring each word in the sentence against all the words from the other sentence.
sent8: This identifies the highest semantic similarity (maxSim) from six word-similarity metrics originally created to measure concept likeness (instead of word likeness).
sent9: The metrics are adapted here to compute word similarity by computing the shortest distance of given words' synsets in the WordNet hierarchy.
sent10: The word-word similarity measure is directional.
sent11: It begins with each word in S1 being computed against each word in S2, and then vice versa.
sent12: The algorithm finally considers sentence similarity by normalizing the highest semantic similarity (maxSim) for each word in the sentences by applying ""inverse document frequency"" (IDF) to the British National Corpus to weight rare and common terms.
sent13: The normalized scores are then summed up for a sentence similarity score, SimMI, as follows:where maxSim(w, S2) is the score of the most similar word in S2 to w, and IDF (w) is the IDF of word
sent14: w. The final output of MI is a numeric value between 0 and 1, where 0 indicates no semantic overlap, and numbers approaching 1 indicate identical meaning of the two sentences.
sent15: These numbers serve as the input to our simulating algorithm for constructing artificial responses.
sent16: Note that the information in the MI values is entirely lexical and syntactic.
sent17: It contains no knowledge about surveys, leadership, or respondent behavior.
sent18: The MLQ has 45 items. This yields (45 × (45 − 1)) /
sent19: 2 or 990 unique item pairs, for which we obtain MI values.
sent20: One special problem concerns the direction of signs.
sent21: In the MLQ, 264 of 990 pairs of items are negatively correlated.
sent22: Theory suggests that two scales, Laissez-faire and Passive Management by Exception, are likely to relate negatively to effective leadership.
sent23: The problem has been treated extensively elsewhere (Arnulf et al., 2014), so we will only offer a brief explanation here.
sent24: MI does not take negative values, and does not differentiate well between positive and negative statements about the same content.
sent25: For two items describing how (a) a manager is unapproachable when called for and (b) that the same person uses appropriate methods of leadership, the surveyed responses correlate at -.42 in the present sample, while the MI value is .38.
sent26: The chosen solution is to allow MI values to be negative for all pairs of items from Laissezfaire and Passive Management by Exception (correctly identifying 255 of the 264 negative correlations, p < .001)."
149117098,Article Respondent Robotics: Simulating Responses to Likert-Scale Survey Items,Computer Science,https://www.semanticscholar.org/paper/30d7c07ec6575bf8b1e7b132dbd2d5ae10c087ee,s7,Simulating Responses,"Based on the consideration above, it is possible to hypothesize that a given respondent's responses are not free to vary. Once the respondent has chosen a response to the initial items, the subsequent responses should be determined by the semantic relationships of the items (Arnulf et al., 2014;Nimon et al., 2016) and the structure of the survey, most notably the response categories (Maul, 2017;Slaney, 2017) and the unfolding patterns following from expected negative correlations (Michell, 1994;Roysamb & Strype, 2002;van Schuur & Kiers, 1994).

Ideally, it should be possible to predict any given response based on the knowledge of the semantic matrix and a minimum of initial responses. In our simulations, we can see that any response in the MLQ is predictable by using other known responses and knowledge about the distances between items. The R 2 s of these predictions are in the range of .86 to .94.  Step 2) Were Regressed on the Average Score Differences (N = 990).

Step 1

Step As the semantic MI values correlate at -.79 and predict the distances significantly (R 2 = .63), it should theoretically be possible to substitute the distances with the semantic values, and thus predict later responses with a minimum of initial responses.

The perfect formula is yet to be found, but we have created a preliminary algorithm that can possibly mimic real responses to the MLQ. The present approach is explicitly aiming at reproducing existing responses as this gives us the best opportunity to compare simulated with real responses.

The rationale for the algorithm combines semantics and unfolding theory as follows: x -0.79. However, the distances were computed as absolute measures; that is, the absolute distance from 3 to 5 = 2, but so is 5 to 3. In practice, though, the algorithm may need to predict a high number from a low number or vice versa. The constant will therefore not ""anchor"" the distance at the right point in the scale. 4. We therefore need to tie the estimated point to the value of Item A. We have tested several approaches to this, and the formula that seems to work best for calculating any response B is to simply replace the constant with the value for Item A, thus Value(Item B) = Value(Item A) + (MI for Item A and Item B) x − 0.79. 5. This formula does impose the structure of semantic values on the subsequent numbers. It also seems counterintuitive because if MI increases (indicating higher similarity), the term will grow in absolute numbers. However, the beta is negative, and the resulting number will be smaller. The impact on the ensuing calculations now comes from the unfolding operations, depending on whether Response B is higher or lower than A. To comply with predictions from unfolding theory, the formula above keeps its positive form if the respondent's first three responses indicate a positive evaluation (biasing the item distances in a positive direction) but should be negative if the unfolding pattern appears to be negative. This information is picked up by comparing the responses of Items 1, 2, and 3. While Items 1 and 2 are descriptions of positive leadership, Item 3 contains a negative appreciation. 6. In the case that the Items A and B are assumed to be negatively related (this was discussed in the explanation of MI values above), the same relationship between MI and distances hold. However, the estimated value should logically be at the other end of the Likert-type scale (in a perfect negative correlation, a score of 5 on A indicates that the score for B is 1). So in the case of expected negative correlations, the direction of the algorithm formula is reversed within the 5-point Likert-type scale, such that In this way, our algorithm is based on the complete pattern of semantic distances for every item with all other items, as well as a hypothesis on the direction of scale unfolding based on the initial three responses. It is admittedly explorative and based on an incomplete understanding of the issues involved, and our intention is to invite criticism and improvements from others. One questionable feature of this algorithm is the tendency for positive evaluations to escalate positively and vice versa, probably due to a deficiency of the formula in

Step 4. In the course of all 990 iterations however, these tendencies seem to balance each other out, and fix the averaged responses as dictated by the mutual pattern of semantic distances. We have also checked that this formula performs better than simply using averages of the known values instead of semantics, thus substantiating the use of semantics in the formula. A further contrasting procedure will be described below. The MLQ has 45 items. Of these, 36 measure different types of leadership behaviors, and the nine last items measure how well the rated person's work group does, commonly treated as ""outcome"" variables. The Arnulf et al. (2014) study found the ""outcome"" variables to be determined by the responses to the preceding items. We will therefore start by trying to predict the individual cases of these by deleting them from real response sets. By deleting progressive numbers of items, we will then explore how well the semantics will perform to predict the missing responses.

Therefore, our first simulated step will be concerned with predicting outcomes training the algorithm on the first 36 items. In the next steps, we simply subtract remaining half of the survey until all real responses are deleted, offering the algorithm diminishing amounts of training information. In this way, we can evaluate the degree to which the computed values still bear resemblance to the original values.

Contrast validation procedure. Algorithms like this may create artificial structures that are not due to the semantic MI values but simply artifacts created by the algorithm procedures themselves. To control for this, we have created similar sets of responses with the same numbers of missing values, where the MI values in the algorithm are replaced by randomly generated values in the same range as the MI values (from −1 to +1). If similarities between artificial and real responses are created by biases in the algorithmic procedure and not by semantics, the output of randomly generated numbers should also be able to reproduce numbers resembling the original scores. The difference between the output of random and semantically created numbers expresses the value of (present-day) semantics in predicting real responses.","[['b4', 'b43', 'b49', 'b23', 'b36', 'b30', 'b24'], [], [], [], [], [], ['b4'], [], []]","[['b4', 'b43', 'b49', 'b23', 'b36', 'b30', 'b24'], [], [], [], [], [], ['b4'], [], []]",8,"sent1: Based on the consideration above, it is possible to hypothesize that a given respondent's responses are not free to vary.
sent2: Once the respondent has chosen a response to the initial items, the subsequent responses should be determined by the semantic relationships of the items (Arnulf et al., 2014;Nimon et al., 2016) and the structure of the survey, most notably the response categories (Maul, 2017;Slaney, 2017) and the unfolding patterns following from expected negative correlations (Michell, 1994;Roysamb & Strype, 2002;van Schuur & Kiers, 1994).
sent3: Ideally, it should be possible to predict any given response based on the knowledge of the semantic matrix and a minimum of initial responses.
sent4: In our simulations, we can see that any response in the MLQ is predictable by using other known responses and knowledge about the distances between items.
sent5: The R 2 s of these predictions are in the range of .86 to .94.
sent6: Step 2) Were Regressed on the Average Score Differences (N = 990).
sent7: Step 1Step As the semantic MI values correlate at -.79 and predict the distances significantly (R 2 = .63), it should theoretically be possible to substitute the distances with the semantic values, and thus predict later responses with a minimum of initial responses.
sent8: The perfect formula is yet to be found, but we have created a preliminary algorithm that can possibly mimic real responses to the MLQ.
sent9: The present approach is explicitly aiming at reproducing existing responses as this gives us the best opportunity to compare simulated with real responses.
sent10: The rationale for the algorithm combines semantics and unfolding theory as follows: x -0.79.
sent11: However, the distances were computed as absolute measures; that is, the absolute distance from 3 to 5 = 2, but so is 5 to 3.
sent12: In practice, though, the algorithm may need to predict a high number from a low number or vice versa.
sent13: The constant will therefore not ""anchor"" the distance at the right point in the scale.
sent14: 4. We therefore need to tie the estimated point to the value of Item A. We have tested several approaches to this, and the formula that seems to work best for calculating any response B is to simply replace the constant with the value for Item A, thus Value(Item B) = Value(Item A) + (MI for Item A and Item B) x − 0.79. 5.
sent15: This formula does impose the structure of semantic values on the subsequent numbers.
sent16: It also seems counterintuitive because if MI increases (indicating higher similarity), the term will grow in absolute numbers.
sent17: However, the beta is negative, and the resulting number will be smaller.
sent18: The impact on the ensuing calculations now comes from the unfolding operations, depending on whether Response B is higher or lower than A. To comply with predictions from unfolding theory, the formula above keeps its positive form if the respondent's first three responses indicate a positive evaluation (biasing the item distances in a positive direction) but should be negative if the unfolding pattern appears to be negative.
sent19: This information is picked up by comparing the responses of Items 1, 2, and 3.
sent20: While Items 1 and 2 are descriptions of positive leadership, Item 3 contains a negative appreciation.
sent21: 6. In the case that the Items A and B are assumed to be negatively related (this was discussed in the explanation of MI values above), the same relationship between MI and distances hold.
sent22: However, the estimated value should logically be at the other end of the Likert-type scale (in a perfect negative correlation, a score of 5 on A indicates that the score for B is 1).
sent23: So in the case of expected negative correlations, the direction of the algorithm formula is reversed within the 5-point Likert-type scale, such that In this way, our algorithm is based on the complete pattern of semantic distances for every item with all other items, as well as a hypothesis on the direction of scale unfolding based on the initial three responses.
sent24: It is admittedly explorative and based on an incomplete understanding of the issues involved, and our intention is to invite criticism and improvements from others.
sent25: One questionable feature of this algorithm is the tendency for positive evaluations to escalate positively and vice versa, probably due to a deficiency of the formula inStep 4.
sent26: In the course of all 990 iterations however, these tendencies seem to balance each other out, and fix the averaged responses as dictated by the mutual pattern of semantic distances.
sent27: We have also checked that this formula performs better than simply using averages of the known values instead of semantics, thus substantiating the use of semantics in the formula.
sent28: A further contrasting procedure will be described below.
sent29: The MLQ has 45 items. Of these, 36 measure different types of leadership behaviors, and the nine last items measure how well the rated person's work group does, commonly treated as ""outcome"" variables.
sent30: The Arnulf et al. (2014) study found the ""outcome"" variables to be determined by the responses to the preceding items.
sent31: We will therefore start by trying to predict the individual cases of these by deleting them from real response sets.
sent32: By deleting progressive numbers of items, we will then explore how well the semantics will perform to predict the missing responses.
sent33: Therefore, our first simulated step will be concerned with predicting outcomes training the algorithm on the first 36 items.
sent34: In the next steps, we simply subtract remaining half of the survey until all real responses are deleted, offering the algorithm diminishing amounts of training information.
sent35: In this way, we can evaluate the degree to which the computed values still bear resemblance to the original values.
sent36: Contrast validation procedure. Algorithms like this may create artificial structures that are not due to the semantic MI values but simply artifacts created by the algorithm procedures themselves.
sent37: To control for this, we have created similar sets of responses with the same numbers of missing values, where the MI values in the algorithm are replaced by randomly generated values in the same range as the MI values (from −1 to +1).
sent38: If similarities between artificial and real responses are created by biases in the algorithmic procedure and not by semantics, the output of randomly generated numbers should also be able to reproduce numbers resembling the original scores.
sent39: The difference between the output of random and semantically created numbers expresses the value of (present-day) semantics in predicting real responses."
181562553,A Systematic Literature Review on Image Captioning,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/92ccf5a39c63cb5e1639be518e6db2e357acd58e,s13,Human-Like Feeling,"In the last year, two keywords have come into the vocabulary of almost every article written under the image captioning topic-novel and semantics. These keywords are important for solving the biggest challenge in this exercise i.e. generating a caption in a way that it would be inseparable from human written ones. Semantics implementation [49] is supposed to design a clean way of injecting sentiment into the current image captioning system. Novel objects must be included for the expansion of scenarios. There have been several insights on why this is still an open issue. First of all, usually models are built on very specific datasets, which do not cover all possible scenarios and are not applicable in describing diverse environment. The same with vocabulary as it has a limited number of words and their combinations. Second, models are usually thought to perform on one specific task, while humans are able to work on many tasks simultaneously. Ref. [35] has already tried to overcome this problem and has provided a solution although it was not further continued. Another great approach for dealing with unseen data, as it is currently impossible to feed all existing data into the machine, was proposed in ref. [56,96]. Lifelong learning is based on a questioning approach i.e. making a discussion directly with the user or inside the model. This approach relies on a natural way of human communication; from early childhood children mostly learn by asking questions. The model is intended to learn also like a child-by asking specific questions and learning from the answers. This method falls under the question answering topic-a literature research in depth might be done on this topic as here we have presented only what appeared during this study on image captioning. This can be targeted as a separate problem, but it also makes a great impact in image captioning.","[['b58', 'b51', 'b98', 'b37']]","[['b58', 'b51', 'b98', 'b37']]",4,"sent1: In the last year, two keywords have come into the vocabulary of almost every article written under the image captioning topic-novel and semantics.
sent2: These keywords are important for solving the biggest challenge in this exercise i.e. generating a caption in a way that it would be inseparable from human written ones.
sent3: Semantics implementation [49] is supposed to design a clean way of injecting sentiment into the current image captioning system.
sent4: Novel objects must be included for the expansion of scenarios.
sent5: There have been several insights on why this is still an open issue.
sent6: First of all, usually models are built on very specific datasets, which do not cover all possible scenarios and are not applicable in describing diverse environment.
sent7: The same with vocabulary as it has a limited number of words and their combinations.
sent8: Second, models are usually thought to perform on one specific task, while humans are able to work on many tasks simultaneously.
sent9: Ref. [35] has already tried to overcome this problem and has provided a solution although it was not further continued.
sent10: Another great approach for dealing with unseen data, as it is currently impossible to feed all existing data into the machine, was proposed in ref.
sent11: [56,96]. Lifelong learning is based on a questioning approach i.e. making a discussion directly with the user or inside the model.
sent12: This approach relies on a natural way of human communication; from early childhood children mostly learn by asking questions.
sent13: The model is intended to learn also like a child-by asking specific questions and learning from the answers.
sent14: This method falls under the question answering topic-a literature research in depth might be done on this topic as here we have presented only what appeared during this study on image captioning.
sent15: This can be targeted as a separate problem, but it also makes a great impact in image captioning."
181562553,A Systematic Literature Review on Image Captioning,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/92ccf5a39c63cb5e1639be518e6db2e357acd58e,s12,Datasets,"Most of the works are evaluated on Flickr30k [90] and MSCOCO [91] datasets. Both datasets are rich in the number of images and each image has five captions assigned which makes it very suitable to train and test the models. It is of course necessary to continuously compare models with the same datasets in order to check the performance, however, they are very limited in the object classes and scenarios presented. The need of new datasets has always been an open question in image captioning. Ref. [92] proposed a method for gathering large datasets of images from the internet which might be helpful for replacing MS COCO or Flickr datasets which were used in most of the previous researches. There have been several other datasets used for model evaluation, such as Lifelog dataset [10], Visual Genome dataset [20,36], IAPRTC-12 [45], OpenImages and Visual Relationship Detection datasets [36], but they were just single cases.

Recently the popularity in novel image scenarios has grown which has increased the demand of newer datasets even more. In ref. [93] the first rigorous and large-scale data set for novel object captioning, which contains more than 500 novel object classes, was introduced. Another realistic dataset was introduced in ref. [94]. It contains news images and their actual captions, along with their associated news articles, news categories, and keyword labels. Moreover, it is clear, that social networks are highly integrated into people's lifestyle. There are more and more images appearing on the social media, especially from the young generation, so it is important to analyze this data as well-for the most natural background, for the newest trends to be interpreted by machines, and to start learning and improving on those as well. Ref. [95] proposed a novel deep feature learning paradigm based on social collective intelligence, which can be acquired from the inexhaustible social multimedia content on the Web, particularly largely social images and tags, however, it was not further continued, at least to our knowledge.","[['b10', 'b47', 'b21', 'b38', 'b93', 'b92', 'b94'], ['b95', 'b96', 'b97']]","[['b10', 'b47', 'b21', 'b38', 'b93', 'b92', 'b94'], ['b95', 'b96', 'b97']]",10,"sent1: Most of the works are evaluated on Flickr30k [90] and MSCOCO [91] datasets.
sent2: Both datasets are rich in the number of images and each image has five captions assigned which makes it very suitable to train and test the models.
sent3: It is of course necessary to continuously compare models with the same datasets in order to check the performance, however, they are very limited in the object classes and scenarios presented.
sent4: The need of new datasets has always been an open question in image captioning.
sent5: Ref. [92] proposed a method for gathering large datasets of images from the internet which might be helpful for replacing MS COCO or Flickr datasets which were used in most of the previous researches.
sent6: There have been several other datasets used for model evaluation, such as Lifelog dataset [10], Visual Genome dataset [20,36], IAPRTC-12 [45], OpenImages and Visual Relationship Detection datasets [36], but they were just single cases.
sent7: Recently the popularity in novel image scenarios has grown which has increased the demand of newer datasets even more.
sent8: In ref. [93] the first rigorous and large-scale data set for novel object captioning, which contains more than 500 novel object classes, was introduced.
sent9: Another realistic dataset was introduced in ref.[94].
sent10: It contains news images and their actual captions, along with their associated news articles, news categories, and keyword labels.
sent11: Moreover, it is clear, that social networks are highly integrated into people's lifestyle.
sent12: There are more and more images appearing on the social media, especially from the young generation, so it is important to analyze this data as well-for the most natural background, for the newest trends to be interpreted by machines, and to start learning and improving on those as well.
sent13: Ref. [95] proposed a novel deep feature learning paradigm based on social collective intelligence, which can be acquired from the inexhaustible social multimedia content on the Web, particularly largely social images and tags, however, it was not further continued, at least to our knowledge."
181562553,A Systematic Literature Review on Image Captioning,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/92ccf5a39c63cb5e1639be518e6db2e357acd58e,s1,SLR Methodology,"The SLR has become a great help in the dynamic, data driven world of today, with massive data volume growth. It is sometimes very difficult to consume all currently existing information before starting to delve into a specific field. In this case, when we talk about image captioning and, as already said, having so much meaning in this task, it was found that there is much literature, which is hard to summarize and thus stay up to date with the newest achievements. There are only a few SLRs that have been conducted for image captioning until now [6][7][8][9], though with such fast progress and increasing popularity in this field we find it necessary that they continue to be undertaken. Moreover, results of image captioning models in previous reviews were not as detailed as they are in this paper. Researchers dedicated time to detailed study of most articles in image captioning-digital libraries, which store most of the articles, were identified, search questions carefully formulated, all articles found were precisely analyzed, and results presented together with important challenges which were captured through the review process. This work follows ref. [6] as a guideline due to the easily understandable structure of their work and the similar ideas.","[['b9', 'b8', 'b7', 'b6']]","[['b9', 'b8', 'b7', 'b6']]",4,"sent1: The SLR has become a great help in the dynamic, data driven world of today, with massive data volume growth.
sent2: It is sometimes very difficult to consume all currently existing information before starting to delve into a specific field.
sent3: In this case, when we talk about image captioning and, as already said, having so much meaning in this task, it was found that there is much literature, which is hard to summarize and thus stay up to date with the newest achievements.
sent4: There are only a few SLRs that have been conducted for image captioning until now [6][7][8][9], though with such fast progress and increasing popularity in this field we find it necessary that they continue to be undertaken.
sent5: Moreover, results of image captioning models in previous reviews were not as detailed as they are in this paper.
sent6: Researchers dedicated time to detailed study of most articles in image captioning-digital libraries, which store most of the articles, were identified, search questions carefully formulated, all articles found were precisely analyzed, and results presented together with important challenges which were captured through the review process.
sent7: This work follows ref. [6] as a guideline due to the easily understandable structure of their work and the similar ideas."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s43,TR Discover,"TR Discover (Song et al, 2015) is a system providing an NLI which translates the input question in form of an English sentence (or sentence fragment) into SQL or SPARQL. It is either used for relational databases or ontologies, but does not need an ontology to work for relational databases. During the translation steps, TR Discover uses a First Order Logic (FOL) representation as an intermediate language. Furthermore, it provides auto-suggestions based on the user input. There are two types of suggestions: autocompletion and prediction.

TR Discover helps the users in formulating the question through an auto-suggestion feature. For example, assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1). When the users start typing 'p', TR Discover will not only suggest 'person' but also longer phrases like 'person directing' (autocomplete). After 'person directing' is selected (or typed), TR Discover will again suggest phrases, like 'movies' or even specific movies like 'Inglourious Basterds' (prediction). For input question Q1, the input could be 'person directing Inglourious Basterds'.

The suggestions are based upon the relationships and entities in the dataset and use the linguistic constraints encoded in a feature-based context-free grammar (FCFG). The grammar consists of grammar rules (G1-3) and lexical entries (L1-2). For the sample world (and the input question Q1), the following rules could be defined: The suggestions are computed based on the idea of left-corner parsing: given a query segment, it finds all grammar rules whose left corner on the right side matches the left side of the lexical entry of the query segment. Then, all leaf nodes (lexical entries) in the grammar that can be reached by using the adjacent element are found. For example, while typing 'person' (Q1), the lexical entries L1 and L2 are found and provided to the user.

TR Discover uses three steps to translate the English sentence or fragment of a sentence into a SQL or SPARQL query. The first step parses the input question into a FOL representation. The query parsing uses the FCFG. For the example input, the token 'person' Fig. 12 The parse tree for the FOL representation of the input question 'person directing Inglourious Basterds'. will be parsed by the lexical entry L1 and the token 'directing' will be parsed with the lexical entry L2. This leads to the FOL representation:

How exactly the phrase 'Inglourious Basterds' is matched to the base data and therefore can be used as part of the lexical entry L2 and how it is resolved, is not explained by Song et al (2015). If there are multiple possibilities to parse the input question, the first one is chosen.

The next step is to translate the generated FOL into a parse tree. The FOL parser takes a grammar and the FOL representation from the previous step, and generates a parse tree ( Figure 12) using ANTLER for implementation.

In the third step, an in-order traversal of the parse tree (provided by the previous step) is performed to translate it into an executable SQL or SPARQL query. While traversing the parse tree, the atomic logical conditions and connectors are put on a stack. After traversing, the constraints are popped from the stack to build the correct query constraints. The predicates are mapped to their corresponding attribute names (SQL) or ontology properties (SPARQL).

The strengths of TR Discover are the auto-suggestion and the possibility to translate natural language into different query languages such as SQL and SPARQL, because FOL is used as an intermediate language.

The weaknesses of TR Discover are that quantifiers (e.g., Q3: 'grossed most') cannot be used, synonyms are not properly handled, and negations only work for SPARQL. Song et al (2015) suggest extending TR Discover with a ranking system for multiple parses in the first step and to improve the synonym handling. Furthermore, they pointed out the possibility of applying user query logs to improve auto-suggestions.

TR Discover (Song et al, 2015) is a system providing an NLI which translates the input question in form of an English sentence (or sentence fragment) into SQL or SPARQL. It is either used for relational databases or ontologies, but does not need an ontology to work for relational databases. During the translation steps, TR Discover uses a First Order Logic (FOL) representation as an intermediate language. Furthermore, it provides auto-suggestions based on the user input. There are two types of suggestions: autocompletion and prediction.

TR Discover helps the users in formulating the question through an auto-suggestion feature. For example, assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1). When the users start typing 'p', TR Discover will not only suggest 'person' but also longer phrases like 'person directing' (autocomplete). After 'person directing' is selected (or typed), TR Discover will again suggest phrases, like 'movies' or even specific movies like 'Inglourious Basterds' (prediction). For input question Q1, the input could be 'person directing Inglourious Basterds'.

The suggestions are based upon the relationships and entities in the dataset and use the linguistic constraints encoded in a feature-based context-free grammar (FCFG). The grammar consists of grammar rules (G1-3) and lexical entries (L1-2). For the sample world (and the input question Q1), the following rules could be defined: The suggestions are computed based on the idea of left-corner parsing: given a query segment, it finds all grammar rules whose left corner on the right side matches the left side of the lexical entry of the query segment. Then, all leaf nodes (lexical entries) in the grammar that can be reached by using the adjacent element are found. For example, while typing 'person' (Q1), the lexical entries L1 and L2 are found and provided to the user.

TR Discover uses three steps to translate the English sentence or fragment of a sentence into a SQL or SPARQL query. The first step parses the input question into a FOL representation. The query parsing uses the FCFG. For the example input, the token 'person' Fig. 12 The parse tree for the FOL representation of the input question 'person directing Inglourious Basterds'. will be parsed by the lexical entry L1 and the token 'directing' will be parsed with the lexical entry L2. This leads to the FOL representation:

How exactly the phrase 'Inglourious Basterds' is matched to the base data and therefore can be used as part of the lexical entry L2 and how it is resolved, is not explained by Song et al (2015). If there are multiple possibilities to parse the input question, the first one is chosen.

The next step is to translate the generated FOL into a parse tree. The FOL parser takes a grammar and the FOL representation from the previous step, and generates a parse tree ( Figure 12) using ANTLER for implementation.

In the third step, an in-order traversal of the parse tree (provided by the previous step) is performed to translate it into an executable SQL or SPARQL query. While traversing the parse tree, the atomic logical conditions and connectors are put on a stack. After traversing, the constraints are popped from the stack to build the correct query constraints. The predicates are mapped to their corresponding attribute names (SQL) or ontology properties (SPARQL).

The strengths of TR Discover are the auto-suggestion and the possibility to translate natural language into different query languages such as SQL and SPARQL, because FOL is used as an intermediate language.

The weaknesses of TR Discover are that quantifiers (e.g., Q3: 'grossed most') cannot be used, synonyms are not properly handled, and negations only work for SPARQL. Song et al (2015) suggest extending TR Discover with a ranking system for multiple parses in the first step and to improve the synonym handling. Furthermore, they pointed out the possibility of applying user query logs to improve auto-suggestions.","[['b49'], [], [], [], ['b49'], [], [], [], ['b49'], ['b49'], [], [], [], ['b49'], [], [], [], ['b49']]","[['b49'], [], [], [], ['b49'], [], [], [], ['b49'], ['b49'], [], [], [], ['b49'], [], [], [], ['b49']]",6,"sent1: TR Discover (Song et al, 2015) is a system providing an NLI which translates the input question in form of an English sentence (or sentence fragment) into SQL or SPARQL.
sent2: It is either used for relational databases or ontologies, but does not need an ontology to work for relational databases.
sent3: During the translation steps, TR Discover uses a First Order Logic (FOL) representation as an intermediate language.
sent4: Furthermore, it provides auto-suggestions based on the user input.
sent5: There are two types of suggestions: autocompletion and prediction.
sent6: TR Discover helps the users in formulating the question through an auto-suggestion feature.
sent7: For example, assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1).
sent8: When the users start typing 'p', TR Discover will not only suggest 'person' but also longer phrases like 'person directing' (autocomplete).
sent9: After 'person directing' is selected (or typed), TR Discover will again suggest phrases, like 'movies' or even specific movies like 'Inglourious Basterds' (prediction).
sent10: For input question Q1, the input could be 'person directing Inglourious Basterds'.
sent11: The suggestions are based upon the relationships and entities in the dataset and use the linguistic constraints encoded in a feature-based context-free grammar (FCFG).
sent12: The grammar consists of grammar rules (G1-3) and lexical entries (L1-2).
sent13: For the sample world (and the input question Q1), the following rules could be defined: The suggestions are computed based on the idea of left-corner parsing: given a query segment, it finds all grammar rules whose left corner on the right side matches the left side of the lexical entry of the query segment.
sent14: Then, all leaf nodes (lexical entries) in the grammar that can be reached by using the adjacent element are found.
sent15: For example, while typing 'person' (Q1), the lexical entries L1 and L2 are found and provided to the user.
sent16: TR Discover uses three steps to translate the English sentence or fragment of a sentence into a SQL or SPARQL query.
sent17: The first step parses the input question into a FOL representation.
sent18: The query parsing uses the FCFG.
sent19: For the example input, the token 'person' Fig. 12 The parse tree for the FOL representation of the input question 'person directing Inglourious Basterds'. will be parsed by the lexical entry L1 and the token 'directing' will be parsed with the lexical entry L2.
sent20: This leads to the FOL representation:How exactly the phrase 'Inglourious Basterds' is matched to the base data and therefore can be used as part of the lexical entry L2 and how it is resolved, is not explained by Song et al (2015).
sent21: If there are multiple possibilities to parse the input question, the first one is chosen.
sent22: The next step is to translate the generated FOL into a parse tree.
sent23: The FOL parser takes a grammar and the FOL representation from the previous step, and generates a parse tree ( Figure 12) using ANTLER for implementation.
sent24: In the third step, an in-order traversal of the parse tree (provided by the previous step) is performed to translate it into an executable SQL or SPARQL query.
sent25: While traversing the parse tree, the atomic logical conditions and connectors are put on a stack.
sent26: After traversing, the constraints are popped from the stack to build the correct query constraints.
sent27: The predicates are mapped to their corresponding attribute names (SQL) or ontology properties (SPARQL).
sent28: The strengths of TR Discover are the auto-suggestion and the possibility to translate natural language into different query languages such as SQL and SPARQL, because FOL is used as an intermediate language.
sent29: The weaknesses of TR Discover are that quantifiers (e.g., Q3: 'grossed most') cannot be used, synonyms are not properly handled, and negations only work for SPARQL.
sent30: Song et al (2015) suggest extending TR Discover with a ranking system for multiple parses in the first step and to improve the synonym handling.
sent31: Furthermore, they pointed out the possibility of applying user query logs to improve auto-suggestions.
sent32: TR Discover (Song et al, 2015) is a system providing an NLI which translates the input question in form of an English sentence (or sentence fragment) into SQL or SPARQL.
sent33: It is either used for relational databases or ontologies, but does not need an ontology to work for relational databases.
sent34: During the translation steps, TR Discover uses a First Order Logic (FOL) representation as an intermediate language.
sent35: Furthermore, it provides auto-suggestions based on the user input.
sent36: There are two types of suggestions: autocompletion and prediction.
sent37: TR Discover helps the users in formulating the question through an auto-suggestion feature.
sent38: For example, assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1).
sent39: When the users start typing 'p', TR Discover will not only suggest 'person' but also longer phrases like 'person directing' (autocomplete).
sent40: After 'person directing' is selected (or typed), TR Discover will again suggest phrases, like 'movies' or even specific movies like 'Inglourious Basterds' (prediction).
sent41: For input question Q1, the input could be 'person directing Inglourious Basterds'.
sent42: The suggestions are based upon the relationships and entities in the dataset and use the linguistic constraints encoded in a feature-based context-free grammar (FCFG).
sent43: The grammar consists of grammar rules (G1-3) and lexical entries (L1-2).
sent44: For the sample world (and the input question Q1), the following rules could be defined: The suggestions are computed based on the idea of left-corner parsing: given a query segment, it finds all grammar rules whose left corner on the right side matches the left side of the lexical entry of the query segment.
sent45: Then, all leaf nodes (lexical entries) in the grammar that can be reached by using the adjacent element are found.
sent46: For example, while typing 'person' (Q1), the lexical entries L1 and L2 are found and provided to the user.
sent47: TR Discover uses three steps to translate the English sentence or fragment of a sentence into a SQL or SPARQL query.
sent48: The first step parses the input question into a FOL representation.
sent49: The query parsing uses the FCFG.
sent50: For the example input, the token 'person' Fig. 12 The parse tree for the FOL representation of the input question 'person directing Inglourious Basterds'. will be parsed by the lexical entry L1 and the token 'directing' will be parsed with the lexical entry L2.
sent51: This leads to the FOL representation:How exactly the phrase 'Inglourious Basterds' is matched to the base data and therefore can be used as part of the lexical entry L2 and how it is resolved, is not explained by Song et al (2015).
sent52: If there are multiple possibilities to parse the input question, the first one is chosen.
sent53: The next step is to translate the generated FOL into a parse tree.
sent54: The FOL parser takes a grammar and the FOL representation from the previous step, and generates a parse tree ( Figure 12) using ANTLER for implementation.
sent55: In the third step, an in-order traversal of the parse tree (provided by the previous step) is performed to translate it into an executable SQL or SPARQL query.
sent56: While traversing the parse tree, the atomic logical conditions and connectors are put on a stack.
sent57: After traversing, the constraints are popped from the stack to build the correct query constraints.
sent58: The predicates are mapped to their corresponding attribute names (SQL) or ontology properties (SPARQL).
sent59: The strengths of TR Discover are the auto-suggestion and the possibility to translate natural language into different query languages such as SQL and SPARQL, because FOL is used as an intermediate language.
sent60: The weaknesses of TR Discover are that quantifiers (e.g., Q3: 'grossed most') cannot be used, synonyms are not properly handled, and negations only work for SPARQL.
sent61: Song et al (2015) suggest extending TR Discover with a ranking system for multiple parses in the first step and to improve the synonym handling.
sent62: Furthermore, they pointed out the possibility of applying user query logs to improve auto-suggestions."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s45,SQUALL (Semantic Query and Update High-Level Language),"SQUALL (Ferré, 2014(Ferré, , 2011 is an NLI for searching and updating an RDF store. It uses the style of Montague grammars (context-free generative grammar) as an intermediate language (similar to TR Discover) to split the translation process in two parts: translating the natural language input question into a logical language and translating the logical language into a query language. Because of that, the second part is getting easier: the logical language and the query language share the same semantics and level of detail. The grammar of SQUALL consists of about 120 domain-independent rules.

The translation into the logical form is done in three steps. In the first step, the keywords are recognized (lookup step). The second step is a syntactic analysis based on a descending parser, which is fed with the grammar rules. Afterwards, the next step can generate the logical language based on the definition in the grammar. After the translation into the logical language, the translation in to the chosen formal language can be done.

The strength of SQUALL is that it is able to translate any type of input question, including aggregations, negations and subqueries. The weakness of SQUALL is that the users have to know the RDF vocabulary (e.g., classes and properties). For example, the input question Q1 needs to be formulated as 'Who is the director of Inglourious Basterds? '.

SQUALL (Ferré, 2014(Ferré, , 2011 is an NLI for searching and updating an RDF store. It uses the style of Montague grammars (context-free generative grammar) as an intermediate language (similar to TR Discover) to split the translation process in two parts: translating the natural language input question into a logical language and translating the logical language into a query language. Because of that, the second part is getting easier: the logical language and the query language share the same semantics and level of detail. The grammar of SQUALL consists of about 120 domain-independent rules.

The translation into the logical form is done in three steps. In the first step, the keywords are recognized (lookup step). The second step is a syntactic analysis based on a descending parser, which is fed with the grammar rules. Afterwards, the next step can generate the logical language based on the definition in the grammar. After the translation into the logical language, the translation in to the chosen formal language can be done.

The strength of SQUALL is that it is able to translate any type of input question, including aggregations, negations and subqueries. The weakness of SQUALL is that the users have to know the RDF vocabulary (e.g., classes and properties). For example, the input question Q1 needs to be formulated as 'Who is the director of Inglourious Basterds? '.","[['b16', 'b17'], [], [], ['b16', 'b17'], [], []]","[['b16', 'b17'], [], [], ['b16', 'b17'], [], []]",4,"sent1: SQUALL (Ferré, 2014(Ferré, , 2011 is an NLI for searching and updating an RDF store. It uses the style of Montague grammars (context-free generative grammar) as an intermediate language (similar to TR Discover) to split the translation process in two parts: translating the natural language input question into a logical language and translating the logical language into a query language.
sent2: Because of that, the second part is getting easier: the logical language and the query language share the same semantics and level of detail.
sent3: The grammar of SQUALL consists of about 120 domain-independent rules.
sent4: The translation into the logical form is done in three steps.
sent5: In the first step, the keywords are recognized (lookup step).
sent6: The second step is a syntactic analysis based on a descending parser, which is fed with the grammar rules.
sent7: Afterwards, the next step can generate the logical language based on the definition in the grammar.
sent8: After the translation into the logical language, the translation in to the chosen formal language can be done.
sent9: The strength of SQUALL is that it is able to translate any type of input question, including aggregations, negations and subqueries.
sent10: The weakness of SQUALL is that the users have to know the RDF vocabulary (e.g., classes and properties).
sent11: For example, the input question Q1 needs to be formulated as 'Who is the director of Inglourious Basterds? '.
sent12: SQUALL (Ferré, 2014(Ferré, , 2011 is an NLI for searching and updating an RDF store. It uses the style of Montague grammars (context-free generative grammar) as an intermediate language (similar to TR Discover) to split the translation process in two parts: translating the natural language input question into a logical language and translating the logical language into a query language.
sent13: Because of that, the second part is getting easier: the logical language and the query language share the same semantics and level of detail.
sent14: The grammar of SQUALL consists of about 120 domain-independent rules.
sent15: The translation into the logical form is done in three steps.
sent16: In the first step, the keywords are recognized (lookup step).
sent17: The second step is a syntactic analysis based on a descending parser, which is fed with the grammar rules.
sent18: Afterwards, the next step can generate the logical language based on the definition in the grammar.
sent19: After the translation into the logical language, the translation in to the chosen formal language can be done.
sent20: The strength of SQUALL is that it is able to translate any type of input question, including aggregations, negations and subqueries.
sent21: The weakness of SQUALL is that the users have to know the RDF vocabulary (e.g., classes and properties).
sent22: For example, the input question Q1 needs to be formulated as 'Who is the director of Inglourious Basterds? '."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s51,Evaluation of 24 recently developed NLIs,"In this section, we provide a systematic analysis of the major NLIs. We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity. Each system category, based on its technical approach, has its own strengths and weaknesses. There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.

We evaluate the systems based on what is reported in the papers. If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3. If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L). If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with . If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3. In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1). This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them. Therefore, they do not expect any complexer questions like Q4 or Q7. Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7). However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10). For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery. Trigger words are not sufficient to identify the range of each subquery.

Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR). This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified. Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.

Grammar-based systems offer the possibility to guide the users during the formulation of their questions. Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language. The huge disadvantage of grammar-based systems is that they need handcrafted rules. There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL). The general rules can be used for other domains and therefore increase the adaptability of the system. Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).

We will now analyze how well the different systems can handle the ten sample questions. The first question is a basic filter question and can be solved by all NLIs as shown in Table 3. The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others). Complex questions (e.g., aggregations) cannot be phrased with keywords only. Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences. In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords. Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible. They adapted their systems so that they can handle different forms of user input. Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution. This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors. Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms. This approach can be used to answer questions formulated as keywords or as complete sentences. Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR). The identification of possible subqueries is necessary to answer questions like Q9 and Q10.

Based on the sample questions, SQUALL, SPARK-LIS, NaLIR and ATHENA are the systems that per- 

In this section, we provide a systematic analysis of the major NLIs. We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity. Each system category, based on its technical approach, has its own strengths and weaknesses. There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.

We evaluate the systems based on what is reported in the papers. If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3. If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L). If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with . If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3. In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1). This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them. Therefore, they do not expect any complexer questions like Q4 or Q7. Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7). However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10). For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery. Trigger words are not sufficient to identify the range of each subquery.

Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR). This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified. Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.

Grammar-based systems offer the possibility to guide the users during the formulation of their questions. Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language. The huge disadvantage of grammar-based systems is that they need handcrafted rules. There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL). The general rules can be used for other domains and therefore increase the adaptability of the system. Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).

We will now analyze how well the different systems can handle the ten sample questions. The first question is a basic filter question and can be solved by all NLIs as shown in Table 3. The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others). Complex questions (e.g., aggregations) cannot be phrased with keywords only. Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences. In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords. Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible. They adapted their systems so that they can handle different forms of user input. Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution. This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors. Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms. This approach can be used to answer questions formulated as keywords or as complete sentences. Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR). The identification of possible subqueries is necessary to answer questions like Q9 and Q10.

Based on the sample questions, SQUALL, SPARK-LIS, NaLIR and ATHENA are the systems that per- ","[[], [], [], [], ['b53', 'b30'], [], [], [], [], [], ['b53', 'b30'], []]","[[], [], [], [], ['b53', 'b30'], [], [], [], [], [], ['b53', 'b30'], []]",4,"sent1: In this section, we provide a systematic analysis of the major NLIs.
sent2: We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity.
sent3: Each system category, based on its technical approach, has its own strengths and weaknesses.
sent4: There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.
sent5: We evaluate the systems based on what is reported in the papers.
sent6: If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3.
sent7: If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L).
sent8: If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with .
sent9: If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3.
sent10: In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1).
sent11: This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them.
sent12: Therefore, they do not expect any complexer questions like Q4 or Q7.
sent13: Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7).
sent14: However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10).
sent15: For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery.
sent16: Trigger words are not sufficient to identify the range of each subquery.
sent17: Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR).
sent18: This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified.
sent19: Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.
sent20: Grammar-based systems offer the possibility to guide the users during the formulation of their questions.
sent21: Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language.
sent22: The huge disadvantage of grammar-based systems is that they need handcrafted rules.
sent23: There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL).
sent24: The general rules can be used for other domains and therefore increase the adaptability of the system.
sent25: Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).
sent26: We will now analyze how well the different systems can handle the ten sample questions.
sent27: The first question is a basic filter question and can be solved by all NLIs as shown in Table 3.
sent28: The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others).
sent29: Complex questions (e.g., aggregations) cannot be phrased with keywords only.
sent30: Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences.
sent31: In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords.
sent32: Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible.
sent33: They adapted their systems so that they can handle different forms of user input.
sent34: Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution.
sent35: This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors.
sent36: Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms.
sent37: This approach can be used to answer questions formulated as keywords or as complete sentences.
sent38: Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR).
sent39: The identification of possible subqueries is necessary to answer questions like Q9 and Q10.
sent40: Based on the sample questions, SQUALL, SPARK-LIS, NaLIR and ATHENA are the systems that per-
sent41: In this section, we provide a systematic analysis of the major NLIs.
sent42: We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity.
sent43: Each system category, based on its technical approach, has its own strengths and weaknesses.
sent44: There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.
sent45: We evaluate the systems based on what is reported in the papers.
sent46: If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3.
sent47: If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L).
sent48: If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with .
sent49: If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3.
sent50: In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1).
sent51: This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them.
sent52: Therefore, they do not expect any complexer questions like Q4 or Q7.
sent53: Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7).
sent54: However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10).
sent55: For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery.
sent56: Trigger words are not sufficient to identify the range of each subquery.
sent57: Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR).
sent58: This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified.
sent59: Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.
sent60: Grammar-based systems offer the possibility to guide the users during the formulation of their questions.
sent61: Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language.
sent62: The huge disadvantage of grammar-based systems is that they need handcrafted rules.
sent63: There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL).
sent64: The general rules can be used for other domains and therefore increase the adaptability of the system.
sent65: Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).
sent66: We will now analyze how well the different systems can handle the ten sample questions.
sent67: The first question is a basic filter question and can be solved by all NLIs as shown in Table 3.
sent68: The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others).
sent69: Complex questions (e.g., aggregations) cannot be phrased with keywords only.
sent70: Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences.
sent71: In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords.
sent72: Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible.
sent73: They adapted their systems so that they can handle different forms of user input.
sent74: Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution.
sent75: This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors.
sent76: Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms.
sent77: This approach can be used to answer questions formulated as keywords or as complete sentences.
sent78: Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR).
sent79: The identification of possible subqueries is necessary to answer questions like Q9 and Q10.
sent80: Based on the sample questions, SQUALL, SPARK-LIS, NaLIR and ATHENA are the systems that per-"
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s53,Machine Learning Approaches for NLIs,"In current research, more and more systems include machine learning (ML) in their translation process (e.g., MEANS) or ranking (e.g., Aqqu).

KBQA (Cui et al, 2017) learns templates as a kind of question representation. It supports binary factoid questions as well as complex questions which are composed of binary factoid questions. OQA (Fader et al, 2014) approaches to leverage both curated and extracted KBs, by mining millions of rules from an unlabeled question corpus and across multiple KBs.

AMUSE (Hakimov et al, 2017) uses ML to determine the most probable meaning of a question and can handle different languages at the same time. Xser (Xu et al, 2014) divides the translation task into a KB-independent and a KB-related task. In the KBindependent task, they are developing a Directed Acyclic Graph parser to capture the structure of the query intentions and trained it on human labeled data.

A new promising avenue of research is to use deep learning techniques as the foundation for NLIDBs. The basic idea is to formulate the translation of natural language (NL) to SQL as an end-to-end machine translation problem (Sutskever et al (2014); Dong and Lapata (2016); Jia and Liang (2016)). The approach is often called neural semantic parsing (Wang et al (2015)). In other words, translating from NL to SQL can be formulated as a supervised machine learning problem on pairs of natural language and SQL queries. In particular, machine translation can be modeled as a sequenceto-sequence problem where the input sequence is represented by the words (tokens) of the NL and the output sequence by the tokens of SQL. The main goal is given an input sequence of tokens, predict the output sequence based on observed patterns in the past.

The main advantage of machine learning based approaches over traditional NLIDBs is that they support a richer linguistic variability in query expressions and thus users can formulate queries with greater flexibility. However, one of the major challenges of supervised machine learning approaches is that they require a large training data set in order to achieve good accuracy on the translation task.

The most commonly used approach for sequenceto-sequence modeling is based on recurrent neural networks (RNNs, Elman (1990)) with an input encoder and an output decoder. The encoder and decoder are implemented as bi-directional LSTMs (Long Short Term Memory) by Hochreiter and Schmidhuber (1997). However, before an NL can be encoded, the tokens need to be represented as a vector that in turn can be processed by the neural network. A widely used approach is to use word embeddings where the vocabulary of the NL is mapped to a high-dimensional vector (Pennington et al (2014)). In order to improve the accuracy of the translation, attention models are often applied (Luong et al (2015)).

One of the currently most advanced neural machine translation systems was introduced by Iyer et al (2017). Their approach uses an encoder-decoder model with global attention similar to Luong et al (2015) and a bi-directional LSTM network to encode the input tokens. In order to improve the translation accuracy from NL to SQL compared to traditional machine translation approaches, the database schema is taken into account. Moreover, external paraphrases are used to increase the linguistic variance of the training data. In the first step of training the neural machine translation system, the process of generating training data is bootstrapped by manually-handcrafted templates for the NL and SQL query pairs. In the next phase, the training set is extended by adding linguistic variations of the input questions and parts to the query are replaced with synonyms or paraphrases of the query. The advantage of this approach is that it is query language independent and could in principle also be used to translate from NL to SPARQL. However, the disadvantage is that a large, manually handcrafted a training set is necessary.

Zhong et al (2017) introduce a system called Seq2SQL. Their approach uses a deep neural network architecture with reinforcement learning to translate from NL to SQL. The authors released WikiSQL -a new data set based on Wikipedia consisting of 24,241 tables and 80,654 hand-annotated NL-SQL-pairs. However, their approach was only demonstrated to work on simple single-table queries without joins. SQLNet by Xu et al (2017) uses a more traditional machine translation approach without reinforcement learning. However, even though SQLNet shows better accuracy than Seq2SQL, the experiments are also based on the WikiSQL data set and it is not clear how this approach would handle join queries against more realistic database settings with multiple tables. Finally, Yavuz et al (2018) show further improvements over SQLNet by incorporating both the information about the database schema as well as the base data. The paper in particular focuses on the generation of WHERE-clauses, which the authors identified as major problem of the relative low accuracy of SQL queries generated by Seq2SQL and SQLNet.

DBPal by Basik et al (2018) overcomes shortcomings of manually labeling large training data sets by synthetically generating a training set that only requires minimal annotations in the database. Similar to Iyer et al (2017), DBPal uses the database schema and query templates to describe NL/SQL-pairs. Moreover, inspired by Wang et al (2015), DBPal augments an initial set of NL queries using a paraphrasing approach based on a paraphrase dictionary. The results show that on a single-table data set DPal performs better than the semantic parsing approach of Iyer et al (2017). However, for a multi-table data set with join queries, DBPal performs worse. The reason is that the limited training set does not seem to generalize well for join-queries. The authors attributed the good performance of the system introduced by Iyer et al (2017) to overfitting. Soru et al (2017) use neural machine translation approach similar to Iyer et al (2017). However, the major difference is that they translate natural language to SPARQL rather than to SQL. Moreover, they do not apply an attention mechanism. In a subsequent white paper Hartmann et al (2018) present an approach to automatically generate a large set of training data consisting of 894,499 training examples based on set of 5000 natural language queries. The approach is very similar to the approach used by DBPal.

In general, these new approaches show promising results, but they have either only been demonstrated to work for single-table data sets or require large amounts training data. Hence, the practical usage in realistic database settings still needs to be shown.

Another interesting new trend is in the area of conversational systems such as Williams et al (2015); John et al (2017) that often apply neural machine translation techniques. However, a detailed discussion on these systems is beyond the scope of this paper.

In current research, more and more systems include machine learning (ML) in their translation process (e.g., MEANS) or ranking (e.g., Aqqu).

KBQA (Cui et al, 2017) learns templates as a kind of question representation. It supports binary factoid questions as well as complex questions which are composed of binary factoid questions. OQA (Fader et al, 2014) approaches to leverage both curated and extracted KBs, by mining millions of rules from an unlabeled question corpus and across multiple KBs.

AMUSE (Hakimov et al, 2017) uses ML to determine the most probable meaning of a question and can handle different languages at the same time. Xser (Xu et al, 2014) divides the translation task into a KB-independent and a KB-related task. In the KBindependent task, they are developing a Directed Acyclic Graph parser to capture the structure of the query intentions and trained it on human labeled data.

A new promising avenue of research is to use deep learning techniques as the foundation for NLIDBs. The basic idea is to formulate the translation of natural language (NL) to SQL as an end-to-end machine translation problem (Sutskever et al (2014); Dong and Lapata (2016); Jia and Liang (2016)). The approach is often called neural semantic parsing (Wang et al (2015)). In other words, translating from NL to SQL can be formulated as a supervised machine learning problem on pairs of natural language and SQL queries. In particular, machine translation can be modeled as a sequenceto-sequence problem where the input sequence is represented by the words (tokens) of the NL and the output sequence by the tokens of SQL. The main goal is given an input sequence of tokens, predict the output sequence based on observed patterns in the past.

The main advantage of machine learning based approaches over traditional NLIDBs is that they support a richer linguistic variability in query expressions and thus users can formulate queries with greater flexibility. However, one of the major challenges of supervised machine learning approaches is that they require a large training data set in order to achieve good accuracy on the translation task.

The most commonly used approach for sequenceto-sequence modeling is based on recurrent neural networks (RNNs, Elman (1990)) with an input encoder and an output decoder. The encoder and decoder are implemented as bi-directional LSTMs (Long Short Term Memory) by Hochreiter and Schmidhuber (1997). However, before an NL can be encoded, the tokens need to be represented as a vector that in turn can be processed by the neural network. A widely used approach is to use word embeddings where the vocabulary of the NL is mapped to a high-dimensional vector (Pennington et al (2014)). In order to improve the accuracy of the translation, attention models are often applied (Luong et al (2015)).

One of the currently most advanced neural machine translation systems was introduced by Iyer et al (2017). Their approach uses an encoder-decoder model with global attention similar to Luong et al (2015) and a bi-directional LSTM network to encode the input tokens. In order to improve the translation accuracy from NL to SQL compared to traditional machine translation approaches, the database schema is taken into account. Moreover, external paraphrases are used to increase the linguistic variance of the training data. In the first step of training the neural machine translation system, the process of generating training data is bootstrapped by manually-handcrafted templates for the NL and SQL query pairs. In the next phase, the training set is extended by adding linguistic variations of the input questions and parts to the query are replaced with synonyms or paraphrases of the query. The advantage of this approach is that it is query language independent and could in principle also be used to translate from NL to SPARQL. However, the disadvantage is that a large, manually handcrafted a training set is necessary.

Zhong et al (2017) introduce a system called Seq2SQL. Their approach uses a deep neural network architecture with reinforcement learning to translate from NL to SQL. The authors released WikiSQL -a new data set based on Wikipedia consisting of 24,241 tables and 80,654 hand-annotated NL-SQL-pairs. However, their approach was only demonstrated to work on simple single-table queries without joins. SQLNet by Xu et al (2017) uses a more traditional machine translation approach without reinforcement learning. However, even though SQLNet shows better accuracy than Seq2SQL, the experiments are also based on the WikiSQL data set and it is not clear how this approach would handle join queries against more realistic database settings with multiple tables. Finally, Yavuz et al (2018) show further improvements over SQLNet by incorporating both the information about the database schema as well as the base data. The paper in particular focuses on the generation of WHERE-clauses, which the authors identified as major problem of the relative low accuracy of SQL queries generated by Seq2SQL and SQLNet.

DBPal by Basik et al (2018) overcomes shortcomings of manually labeling large training data sets by synthetically generating a training set that only requires minimal annotations in the database. Similar to Iyer et al (2017), DBPal uses the database schema and query templates to describe NL/SQL-pairs. Moreover, inspired by Wang et al (2015), DBPal augments an initial set of NL queries using a paraphrasing approach based on a paraphrase dictionary. The results show that on a single-table data set DPal performs better than the semantic parsing approach of Iyer et al (2017). However, for a multi-table data set with join queries, DBPal performs worse. The reason is that the limited training set does not seem to generalize well for join-queries. The authors attributed the good performance of the system introduced by Iyer et al (2017) to overfitting. Soru et al (2017) use neural machine translation approach similar to Iyer et al (2017). However, the major difference is that they translate natural language to SPARQL rather than to SQL. Moreover, they do not apply an attention mechanism. In a subsequent white paper Hartmann et al (2018) present an approach to automatically generate a large set of training data consisting of 894,499 training examples based on set of 5000 natural language queries. The approach is very similar to the approach used by DBPal.

In general, these new approaches show promising results, but they have either only been demonstrated to work for single-table data sets or require large amounts training data. Hence, the practical usage in realistic database settings still needs to be shown.

Another interesting new trend is in the area of conversational systems such as Williams et al (2015); John et al (2017) that often apply neural machine translation techniques. However, a detailed discussion on these systems is beyond the scope of this paper.","[[], ['b9', 'b15'], ['b61', 'b22'], ['b56', 'b12', 'b26'], [], ['b14', 'b36', 'b42', 'b24'], ['b36'], ['b62', 'b63'], ['b50', 'b56', 'b25', 'b23', 'b0'], [], ['b27', 'b59'], [], ['b9', 'b15'], ['b61', 'b22'], ['b56', 'b12', 'b26'], [], ['b14', 'b36', 'b42', 'b24'], ['b36'], ['b62', 'b63'], ['b50', 'b56', 'b25', 'b23', 'b0'], [], ['b27', 'b59']]","[[], ['b9', 'b15'], ['b61', 'b22'], ['b56', 'b12', 'b26'], [], ['b14', 'b36', 'b42', 'b24'], ['b36'], ['b62', 'b63'], ['b50', 'b56', 'b25', 'b23', 'b0'], [], ['b27', 'b59'], [], ['b9', 'b15'], ['b61', 'b22'], ['b56', 'b12', 'b26'], [], ['b14', 'b36', 'b42', 'b24'], ['b36'], ['b62', 'b63'], ['b50', 'b56', 'b25', 'b23', 'b0'], [], ['b27', 'b59']]",42,"sent1: In current research, more and more systems include machine learning (ML) in their translation process (e.g., MEANS) or ranking (e.g., Aqqu).
sent2: KBQA (Cui et al, 2017) learns templates as a kind of question representation.
sent3: It supports binary factoid questions as well as complex questions which are composed of binary factoid questions.
sent4: OQA (Fader et al, 2014) approaches to leverage both curated and extracted KBs, by mining millions of rules from an unlabeled question corpus and across multiple KBs.
sent5: AMUSE (Hakimov et al, 2017) uses ML to determine the most probable meaning of a question and can handle different languages at the same time.
sent6: Xser (Xu et al, 2014) divides the translation task into a KB-independent and a KB-related task.
sent7: In the KBindependent task, they are developing a Directed Acyclic Graph parser to capture the structure of the query intentions and trained it on human labeled data.
sent8: A new promising avenue of research is to use deep learning techniques as the foundation for NLIDBs.
sent9: The basic idea is to formulate the translation of natural language (NL) to SQL as an end-to-end machine translation problem (Sutskever et al (2014); Dong and Lapata (2016); Jia and Liang (2016)).
sent10: The approach is often called neural semantic parsing (Wang et al (2015)).
sent11: In other words, translating from NL to SQL can be formulated as a supervised machine learning problem on pairs of natural language and SQL queries.
sent12: In particular, machine translation can be modeled as a sequenceto-sequence problem where the input sequence is represented by the words (tokens) of the NL and the output sequence by the tokens of SQL.
sent13: The main goal is given an input sequence of tokens, predict the output sequence based on observed patterns in the past.
sent14: The main advantage of machine learning based approaches over traditional NLIDBs is that they support a richer linguistic variability in query expressions and thus users can formulate queries with greater flexibility.
sent15: However, one of the major challenges of supervised machine learning approaches is that they require a large training data set in order to achieve good accuracy on the translation task.
sent16: The most commonly used approach for sequenceto-sequence modeling is based on recurrent neural networks (RNNs, Elman (1990)) with an input encoder and an output decoder.
sent17: The encoder and decoder are implemented as bi-directional LSTMs (Long Short Term Memory) by Hochreiter and Schmidhuber (1997).
sent18: However, before an NL can be encoded, the tokens need to be represented as a vector that in turn can be processed by the neural network.
sent19: A widely used approach is to use word embeddings where the vocabulary of the NL is mapped to a high-dimensional vector (Pennington et al (2014)).
sent20: In order to improve the accuracy of the translation, attention models are often applied (Luong et al (2015)).
sent21: One of the currently most advanced neural machine translation systems was introduced by Iyer et al (2017).
sent22: Their approach uses an encoder-decoder model with global attention similar to Luong et al (2015) and a bi-directional LSTM network to encode the input tokens.
sent23: In order to improve the translation accuracy from NL to SQL compared to traditional machine translation approaches, the database schema is taken into account.
sent24: Moreover, external paraphrases are used to increase the linguistic variance of the training data.
sent25: In the first step of training the neural machine translation system, the process of generating training data is bootstrapped by manually-handcrafted templates for the NL and SQL query pairs.
sent26: In the next phase, the training set is extended by adding linguistic variations of the input questions and parts to the query are replaced with synonyms or paraphrases of the query.
sent27: The advantage of this approach is that it is query language independent and could in principle also be used to translate from NL to SPARQL.
sent28: However, the disadvantage is that a large, manually handcrafted a training set is necessary.
sent29: Zhong et al (2017) introduce a system called Seq2SQL.
sent30: Their approach uses a deep neural network architecture with reinforcement learning to translate from NL to SQL.
sent31: The authors released WikiSQL -a new data set based on Wikipedia consisting of 24,241 tables and 80,654 hand-annotated NL-SQL-pairs.
sent32: However, their approach was only demonstrated to work on simple single-table queries without joins.
sent33: SQLNet by Xu et al (2017) uses a more traditional machine translation approach without reinforcement learning.
sent34: However, even though SQLNet shows better accuracy than Seq2SQL, the experiments are also based on the WikiSQL data set and it is not clear how this approach would handle join queries against more realistic database settings with multiple tables.
sent35: Finally, Yavuz et al (2018) show further improvements over SQLNet by incorporating both the information about the database schema as well as the base data.
sent36: The paper in particular focuses on the generation of WHERE-clauses, which the authors identified as major problem of the relative low accuracy of SQL queries generated by Seq2SQL and SQLNet.
sent37: DBPal by Basik et al (2018) overcomes shortcomings of manually labeling large training data sets by synthetically generating a training set that only requires minimal annotations in the database.
sent38: Similar to Iyer et al (2017), DBPal uses the database schema and query templates to describe NL/SQL-pairs.
sent39: Moreover, inspired by Wang et al (2015), DBPal augments an initial set of NL queries using a paraphrasing approach based on a paraphrase dictionary.
sent40: The results show that on a single-table data set DPal performs better than the semantic parsing approach of Iyer et al (2017).
sent41: However, for a multi-table data set with join queries, DBPal performs worse.
sent42: The reason is that the limited training set does not seem to generalize well for join-queries.
sent43: The authors attributed the good performance of the system introduced by Iyer et al (2017) to overfitting.
sent44: Soru et al (2017) use neural machine translation approach similar to Iyer et al (2017).
sent45: However, the major difference is that they translate natural language to SPARQL rather than to SQL.
sent46: Moreover, they do not apply an attention mechanism.
sent47: In a subsequent white paper Hartmann et al (2018) present an approach to automatically generate a large set of training data consisting of 894,499 training examples based on set of 5000 natural language queries.
sent48: The approach is very similar to the approach used by DBPal.
sent49: In general, these new approaches show promising results, but they have either only been demonstrated to work for single-table data sets or require large amounts training data.
sent50: Hence, the practical usage in realistic database settings still needs to be shown.
sent51: Another interesting new trend is in the area of conversational systems such as Williams et al (2015); John et al (2017) that often apply neural machine translation techniques.
sent52: However, a detailed discussion on these systems is beyond the scope of this paper.
sent53: In current research, more and more systems include machine learning (ML) in their translation process (e.g., MEANS) or ranking (e.g., Aqqu).
sent54: KBQA (Cui et al, 2017) learns templates as a kind of question representation.
sent55: It supports binary factoid questions as well as complex questions which are composed of binary factoid questions.
sent56: OQA (Fader et al, 2014) approaches to leverage both curated and extracted KBs, by mining millions of rules from an unlabeled question corpus and across multiple KBs.
sent57: AMUSE (Hakimov et al, 2017) uses ML to determine the most probable meaning of a question and can handle different languages at the same time.
sent58: Xser (Xu et al, 2014) divides the translation task into a KB-independent and a KB-related task.
sent59: In the KBindependent task, they are developing a Directed Acyclic Graph parser to capture the structure of the query intentions and trained it on human labeled data.
sent60: A new promising avenue of research is to use deep learning techniques as the foundation for NLIDBs.
sent61: The basic idea is to formulate the translation of natural language (NL) to SQL as an end-to-end machine translation problem (Sutskever et al (2014); Dong and Lapata (2016); Jia and Liang (2016)).
sent62: The approach is often called neural semantic parsing (Wang et al (2015)).
sent63: In other words, translating from NL to SQL can be formulated as a supervised machine learning problem on pairs of natural language and SQL queries.
sent64: In particular, machine translation can be modeled as a sequenceto-sequence problem where the input sequence is represented by the words (tokens) of the NL and the output sequence by the tokens of SQL.
sent65: The main goal is given an input sequence of tokens, predict the output sequence based on observed patterns in the past.
sent66: The main advantage of machine learning based approaches over traditional NLIDBs is that they support a richer linguistic variability in query expressions and thus users can formulate queries with greater flexibility.
sent67: However, one of the major challenges of supervised machine learning approaches is that they require a large training data set in order to achieve good accuracy on the translation task.
sent68: The most commonly used approach for sequenceto-sequence modeling is based on recurrent neural networks (RNNs, Elman (1990)) with an input encoder and an output decoder.
sent69: The encoder and decoder are implemented as bi-directional LSTMs (Long Short Term Memory) by Hochreiter and Schmidhuber (1997).
sent70: However, before an NL can be encoded, the tokens need to be represented as a vector that in turn can be processed by the neural network.
sent71: A widely used approach is to use word embeddings where the vocabulary of the NL is mapped to a high-dimensional vector (Pennington et al (2014)).
sent72: In order to improve the accuracy of the translation, attention models are often applied (Luong et al (2015)).
sent73: One of the currently most advanced neural machine translation systems was introduced by Iyer et al (2017).
sent74: Their approach uses an encoder-decoder model with global attention similar to Luong et al (2015) and a bi-directional LSTM network to encode the input tokens.
sent75: In order to improve the translation accuracy from NL to SQL compared to traditional machine translation approaches, the database schema is taken into account.
sent76: Moreover, external paraphrases are used to increase the linguistic variance of the training data.
sent77: In the first step of training the neural machine translation system, the process of generating training data is bootstrapped by manually-handcrafted templates for the NL and SQL query pairs.
sent78: In the next phase, the training set is extended by adding linguistic variations of the input questions and parts to the query are replaced with synonyms or paraphrases of the query.
sent79: The advantage of this approach is that it is query language independent and could in principle also be used to translate from NL to SPARQL.
sent80: However, the disadvantage is that a large, manually handcrafted a training set is necessary.
sent81: Zhong et al (2017) introduce a system called Seq2SQL.
sent82: Their approach uses a deep neural network architecture with reinforcement learning to translate from NL to SQL.
sent83: The authors released WikiSQL -a new data set based on Wikipedia consisting of 24,241 tables and 80,654 hand-annotated NL-SQL-pairs.
sent84: However, their approach was only demonstrated to work on simple single-table queries without joins.
sent85: SQLNet by Xu et al (2017) uses a more traditional machine translation approach without reinforcement learning.
sent86: However, even though SQLNet shows better accuracy than Seq2SQL, the experiments are also based on the WikiSQL data set and it is not clear how this approach would handle join queries against more realistic database settings with multiple tables.
sent87: Finally, Yavuz et al (2018) show further improvements over SQLNet by incorporating both the information about the database schema as well as the base data.
sent88: The paper in particular focuses on the generation of WHERE-clauses, which the authors identified as major problem of the relative low accuracy of SQL queries generated by Seq2SQL and SQLNet.
sent89: DBPal by Basik et al (2018) overcomes shortcomings of manually labeling large training data sets by synthetically generating a training set that only requires minimal annotations in the database.
sent90: Similar to Iyer et al (2017), DBPal uses the database schema and query templates to describe NL/SQL-pairs.
sent91: Moreover, inspired by Wang et al (2015), DBPal augments an initial set of NL queries using a paraphrasing approach based on a paraphrase dictionary.
sent92: The results show that on a single-table data set DPal performs better than the semantic parsing approach of Iyer et al (2017).
sent93: However, for a multi-table data set with join queries, DBPal performs worse.
sent94: The reason is that the limited training set does not seem to generalize well for join-queries.
sent95: The authors attributed the good performance of the system introduced by Iyer et al (2017) to overfitting.
sent96: Soru et al (2017) use neural machine translation approach similar to Iyer et al (2017).
sent97: However, the major difference is that they translate natural language to SPARQL rather than to SQL.
sent98: Moreover, they do not apply an attention mechanism.
sent99: In a subsequent white paper Hartmann et al (2018) present an approach to automatically generate a large set of training data consisting of 894,499 training examples based on set of 5000 natural language queries.
sent100: The approach is very similar to the approach used by DBPal.
sent101: In general, these new approaches show promising results, but they have either only been demonstrated to work for single-table data sets or require large amounts training data.
sent102: Hence, the practical usage in realistic database settings still needs to be shown.
sent103: Another interesting new trend is in the area of conversational systems such as Williams et al (2015); John et al (2017) that often apply neural machine translation techniques.
sent104: However, a detailed discussion on these systems is beyond the scope of this paper."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s75,"J, 2xS","for the evaluation of SODA (Blunschi et al (2012)) and the 10 sample questions of this paper. Therefore, we designed nine questions based on the operators of SQL and SPARQL: Joins, Filters (string, range, date or negation), Aggregations, Ordering, Union and Subqueries.

Furthermore, we added a question which is based on a concept (e.g., 'great movie'). Concepts are not part of SQL and SPARQL, but a common addition of NLIs. Table 1 shows those ten full sentence questions in English, which can be applied on the sample world (ordered roughly by difficulty).

The queries were designed in such a way that they cover a wide range of SQL-functionality (technical challenge) as well as linguistic variations (semantic challenge). We will now analyze each of these ten queries in more detail and describe the major challenges for the underlying system to solve them.

The first question (Q1) is a join over different tables (Person, Director, Directing and Movie) with an ISA-relationship between the tables Person and Director. Moreover, the query has a filter on the attribute Movie.Title, which has to be equal to 'Inglourious Basterds'. Therefore, the system faces three different challenges: (a) identify the bridge table Directing to link the tables Director and Movie, (b) identify the hierarchical structure (ISA-relationship) between Director and Person and (c) identify 'Inglourious Basterds' as a filter phrase for Movie.Title.

The second question (Q2) is based on a single table (Movie) with a range filter. The challenge for the NLIs is to translate 'higher than' into a comparison operator 'greater than'.

The third question (Q3) is a join over four tables (Person, Actor, Starring and Movie) and includes two filters: (a) a filter on the attribute Person.FirstName and Person.LastName and (b) a two-sided date range filter on the attribute Movie.ReleaseDate. The challenge in this query (compared to the previous ones) is the date range filter. The system needs to detect that 'from 2000 until 2010 ' refers to a range filter and that the numbers need to be translated into the dates 2000-01-01 and 2010-12-31.

The fourth question (Q4) is a join over two tables (Movie and Gross). In addition, an aggregation on the attribute Gross.Gross and grouping on the attribute Movie.id or ordering the result based on Gross.Gross is needed. For both approaches an aggregation to a single result (indicated by the keyword 'most') is requested.

The fifth question (Q5) is a join over two tables (Movie and Genre). The query can either be interpreted as 'movies that have both genres' (intersection) or 'movie with at least one of those genres ' (union). The expected answer is based on the union interpretation, which can be solved with two filters that are concatenated with an OR on the attribute Genre.Genre.

The sixth question (Q6) needs the definition of concepts. In the sample world the concept 'great movie' is defined as a movie with a rating greater or equal 8. If the system is capable of concepts, then it needs to detect the concept and translate it accordingly to the definition.

The seventh question (Q7) is a join over two tables (Movie and Genre) with an aggregation. The challenges are to (a) identify the grouping by the attribute Genre.Genre and (b) translate the token 'best' to a maximum aggregation on the attribute Movie.Rating.

The eighth question (Q8) is a join over two tables (Movie and Genre) with a negation on the attribute Movie.OriginalLang and a filter on the attribute Genre.Genre. The challenge in this question is to identify the negation 'non-Japanese'. Another possible input question with a negation over a larger movie database, would be 'All actors without an Oscar '. Here again, the challenge is to identify 'without' as a keyword for the negation.

The ninth question (Q9) is based on a single table (Movie) and includes a subquery. The challenge in this question is to divide it in two steps: first select the rating of the movie 'Sin City' and then use this SQL statement as a subquery to compare with the ranking of every other movie in the database.

The tenth question (Q10) is a join over two tables (Movie and Genre). One possible solution would include two not exist: the first one verifies for each movie that there exist no other genres as the genres of 'Sin City'. The second one verifies for each movie that it has no genre, which 'Sin City' does not have. For example, the movie 'Sin City' has the genre 'Thriller ', the movie 'Mission: Impossible' has the genres 'Thriller ' and 'Action'. The first not exist will check if 'Mission: Impossible' has the genre 'Thriller ' from 'Sin City' which is true. The second not exist checks if 'Sin City' has the genres 'Thriller ' and 'Action' (from 'Mission: Impossible'), which is false.

for the evaluation of SODA (Blunschi et al (2012)) and the 10 sample questions of this paper. Therefore, we designed nine questions based on the operators of SQL and SPARQL: Joins, Filters (string, range, date or negation), Aggregations, Ordering, Union and Subqueries.

Furthermore, we added a question which is based on a concept (e.g., 'great movie'). Concepts are not part of SQL and SPARQL, but a common addition of NLIs. Table 1 shows those ten full sentence questions in English, which can be applied on the sample world (ordered roughly by difficulty).

The queries were designed in such a way that they cover a wide range of SQL-functionality (technical challenge) as well as linguistic variations (semantic challenge). We will now analyze each of these ten queries in more detail and describe the major challenges for the underlying system to solve them.

The first question (Q1) is a join over different tables (Person, Director, Directing and Movie) with an ISA-relationship between the tables Person and Director. Moreover, the query has a filter on the attribute Movie.Title, which has to be equal to 'Inglourious Basterds'. Therefore, the system faces three different challenges: (a) identify the bridge table Directing to link the tables Director and Movie, (b) identify the hierarchical structure (ISA-relationship) between Director and Person and (c) identify 'Inglourious Basterds' as a filter phrase for Movie.Title.

The second question (Q2) is based on a single table (Movie) with a range filter. The challenge for the NLIs is to translate 'higher than' into a comparison operator 'greater than'.

The third question (Q3) is a join over four tables (Person, Actor, Starring and Movie) and includes two filters: (a) a filter on the attribute Person.FirstName and Person.LastName and (b) a two-sided date range filter on the attribute Movie.ReleaseDate. The challenge in this query (compared to the previous ones) is the date range filter. The system needs to detect that 'from 2000 until 2010 ' refers to a range filter and that the numbers need to be translated into the dates 2000-01-01 and 2010-12-31.

The fourth question (Q4) is a join over two tables (Movie and Gross). In addition, an aggregation on the attribute Gross.Gross and grouping on the attribute Movie.id or ordering the result based on Gross.Gross is needed. For both approaches an aggregation to a single result (indicated by the keyword 'most') is requested.

The fifth question (Q5) is a join over two tables (Movie and Genre). The query can either be interpreted as 'movies that have both genres' (intersection) or 'movie with at least one of those genres ' (union). The expected answer is based on the union interpretation, which can be solved with two filters that are concatenated with an OR on the attribute Genre.Genre.

The sixth question (Q6) needs the definition of concepts. In the sample world the concept 'great movie' is defined as a movie with a rating greater or equal 8. If the system is capable of concepts, then it needs to detect the concept and translate it accordingly to the definition.

The seventh question (Q7) is a join over two tables (Movie and Genre) with an aggregation. The challenges are to (a) identify the grouping by the attribute Genre.Genre and (b) translate the token 'best' to a maximum aggregation on the attribute Movie.Rating.

The eighth question (Q8) is a join over two tables (Movie and Genre) with a negation on the attribute Movie.OriginalLang and a filter on the attribute Genre.Genre. The challenge in this question is to identify the negation 'non-Japanese'. Another possible input question with a negation over a larger movie database, would be 'All actors without an Oscar '. Here again, the challenge is to identify 'without' as a keyword for the negation.

The ninth question (Q9) is based on a single table (Movie) and includes a subquery. The challenge in this question is to divide it in two steps: first select the rating of the movie 'Sin City' and then use this SQL statement as a subquery to compare with the ranking of every other movie in the database.

The tenth question (Q10) is a join over two tables (Movie and Genre). One possible solution would include two not exist: the first one verifies for each movie that there exist no other genres as the genres of 'Sin City'. The second one verifies for each movie that it has no genre, which 'Sin City' does not have. For example, the movie 'Sin City' has the genre 'Thriller ', the movie 'Mission: Impossible' has the genres 'Thriller ' and 'Action'. The first not exist will check if 'Mission: Impossible' has the genre 'Thriller ' from 'Sin City' which is true. The second not exist checks if 'Sin City' has the genres 'Thriller ' and 'Action' (from 'Mission: Impossible'), which is false.","[['b5'], [], [], [], [], [], [], [None], [], [], [], [], [], ['b5'], [], [], [], [], [], [], [None], [], [], [], [], []]","[['b5'], [], [], [], [], [], [], [None], [], [], [], [], [], ['b5'], [], [], [], [], [], [], [None], [], [], [], [], []]",4,"sent1: for the evaluation of SODA (Blunschi et al (2012)) and the 10 sample questions of this paper.
sent2: Therefore, we designed nine questions based on the operators of SQL and SPARQL: Joins, Filters (string, range, date or negation), Aggregations, Ordering, Union and Subqueries.
sent3: Furthermore, we added a question which is based on a concept (e.g., 'great movie').
sent4: Concepts are not part of SQL and SPARQL, but a common addition of NLIs.
sent5: Table 1 shows those ten full sentence questions in English, which can be applied on the sample world (ordered roughly by difficulty).
sent6: The queries were designed in such a way that they cover a wide range of SQL-functionality (technical challenge) as well as linguistic variations (semantic challenge).
sent7: We will now analyze each of these ten queries in more detail and describe the major challenges for the underlying system to solve them.
sent8: The first question (Q1) is a join over different tables (Person, Director, Directing and Movie) with an ISA-relationship between the tables Person and Director.
sent9: Moreover, the query has a filter on the attribute Movie.
sent10: Title, which has to be equal to 'Inglourious Basterds'.
sent11: Therefore, the system faces three different challenges: (a) identify the bridge table Directing to link the tables Director and Movie, (b) identify the hierarchical structure (ISA-relationship) between Director and Person and (c) identify 'Inglourious Basterds' as a filter phrase for Movie.
sent12: Title. The second question (Q2) is based on a single table (Movie) with a range filter.
sent13: The challenge for the NLIs is to translate 'higher than' into a comparison operator 'greater than'.
sent14: The third question (Q3) is a join over four tables (Person, Actor, Starring and Movie) and includes two filters: (a) a filter on the attribute Person.
sent15: FirstName and Person. LastName and (b) a two-sided date range filter on the attribute Movie.
sent16: ReleaseDate. The challenge in this query (compared to the previous ones) is the date range filter.
sent17: The system needs to detect that 'from 2000 until 2010 ' refers to a range filter and that the numbers need to be translated into the dates 2000-01-01 and 2010-12-31.
sent18: The fourth question (Q4) is a join over two tables (Movie and Gross).
sent19: In addition, an aggregation on the attribute Gross.
sent20: Gross and grouping on the attribute Movie.id or ordering the result based on Gross.
sent21: Gross is needed. For both approaches an aggregation to a single result (indicated by the keyword 'most') is requested.
sent22: The fifth question (Q5) is a join over two tables (Movie and Genre).
sent23: The query can either be interpreted as 'movies that have both genres' (intersection) or 'movie with at least one of those genres ' (union).
sent24: The expected answer is based on the union interpretation, which can be solved with two filters that are concatenated with an OR on the attribute Genre.
sent25: Genre. The sixth question (Q6) needs the definition of concepts.
sent26: In the sample world the concept 'great movie' is defined as a movie with a rating greater or equal 8.
sent27: If the system is capable of concepts, then it needs to detect the concept and translate it accordingly to the definition.
sent28: The seventh question (Q7) is a join over two tables (Movie and Genre) with an aggregation.
sent29: The challenges are to (a) identify the grouping by the attribute Genre.
sent30: Genre and (b) translate the token 'best' to a maximum aggregation on the attribute Movie.
sent31: Rating. The eighth question (Q8) is a join over two tables (Movie and Genre) with a negation on the attribute Movie.
sent32: OriginalLang and a filter on the attribute Genre.
sent33: Genre. The challenge in this question is to identify the negation 'non-Japanese'.
sent34: Another possible input question with a negation over a larger movie database, would be 'All actors without an Oscar '.
sent35: Here again, the challenge is to identify 'without' as a keyword for the negation.
sent36: The ninth question (Q9) is based on a single table (Movie) and includes a subquery.
sent37: The challenge in this question is to divide it in two steps: first select the rating of the movie 'Sin City' and then use this SQL statement as a subquery to compare with the ranking of every other movie in the database.
sent38: The tenth question (Q10) is a join over two tables (Movie and Genre).
sent39: One possible solution would include two not exist: the first one verifies for each movie that there exist no other genres as the genres of 'Sin City'.
sent40: The second one verifies for each movie that it has no genre, which 'Sin City' does not have.
sent41: For example, the movie 'Sin City' has the genre 'Thriller ', the movie 'Mission: Impossible' has the genres 'Thriller ' and 'Action'.
sent42: The first not exist will check if 'Mission: Impossible' has the genre 'Thriller ' from 'Sin City' which is true.
sent43: The second not exist checks if 'Sin City' has the genres 'Thriller ' and 'Action' (from 'Mission: Impossible'), which is false.
sent44: for the evaluation of SODA (Blunschi et al (2012)) and the 10 sample questions of this paper.
sent45: Therefore, we designed nine questions based on the operators of SQL and SPARQL: Joins, Filters (string, range, date or negation), Aggregations, Ordering, Union and Subqueries.
sent46: Furthermore, we added a question which is based on a concept (e.g., 'great movie').
sent47: Concepts are not part of SQL and SPARQL, but a common addition of NLIs.
sent48: Table 1 shows those ten full sentence questions in English, which can be applied on the sample world (ordered roughly by difficulty).
sent49: The queries were designed in such a way that they cover a wide range of SQL-functionality (technical challenge) as well as linguistic variations (semantic challenge).
sent50: We will now analyze each of these ten queries in more detail and describe the major challenges for the underlying system to solve them.
sent51: The first question (Q1) is a join over different tables (Person, Director, Directing and Movie) with an ISA-relationship between the tables Person and Director.
sent52: Moreover, the query has a filter on the attribute Movie.
sent53: Title, which has to be equal to 'Inglourious Basterds'.
sent54: Therefore, the system faces three different challenges: (a) identify the bridge table Directing to link the tables Director and Movie, (b) identify the hierarchical structure (ISA-relationship) between Director and Person and (c) identify 'Inglourious Basterds' as a filter phrase for Movie.
sent55: Title. The second question (Q2) is based on a single table (Movie) with a range filter.
sent56: The challenge for the NLIs is to translate 'higher than' into a comparison operator 'greater than'.
sent57: The third question (Q3) is a join over four tables (Person, Actor, Starring and Movie) and includes two filters: (a) a filter on the attribute Person.
sent58: FirstName and Person. LastName and (b) a two-sided date range filter on the attribute Movie.
sent59: ReleaseDate. The challenge in this query (compared to the previous ones) is the date range filter.
sent60: The system needs to detect that 'from 2000 until 2010 ' refers to a range filter and that the numbers need to be translated into the dates 2000-01-01 and 2010-12-31.
sent61: The fourth question (Q4) is a join over two tables (Movie and Gross).
sent62: In addition, an aggregation on the attribute Gross.
sent63: Gross and grouping on the attribute Movie.id or ordering the result based on Gross.
sent64: Gross is needed. For both approaches an aggregation to a single result (indicated by the keyword 'most') is requested.
sent65: The fifth question (Q5) is a join over two tables (Movie and Genre).
sent66: The query can either be interpreted as 'movies that have both genres' (intersection) or 'movie with at least one of those genres ' (union).
sent67: The expected answer is based on the union interpretation, which can be solved with two filters that are concatenated with an OR on the attribute Genre.
sent68: Genre. The sixth question (Q6) needs the definition of concepts.
sent69: In the sample world the concept 'great movie' is defined as a movie with a rating greater or equal 8.
sent70: If the system is capable of concepts, then it needs to detect the concept and translate it accordingly to the definition.
sent71: The seventh question (Q7) is a join over two tables (Movie and Genre) with an aggregation.
sent72: The challenges are to (a) identify the grouping by the attribute Genre.
sent73: Genre and (b) translate the token 'best' to a maximum aggregation on the attribute Movie.
sent74: Rating. The eighth question (Q8) is a join over two tables (Movie and Genre) with a negation on the attribute Movie.
sent75: OriginalLang and a filter on the attribute Genre.
sent76: Genre. The challenge in this question is to identify the negation 'non-Japanese'.
sent77: Another possible input question with a negation over a larger movie database, would be 'All actors without an Oscar '.
sent78: Here again, the challenge is to identify 'without' as a keyword for the negation.
sent79: The ninth question (Q9) is based on a single table (Movie) and includes a subquery.
sent80: The challenge in this question is to divide it in two steps: first select the rating of the movie 'Sin City' and then use this SQL statement as a subquery to compare with the ranking of every other movie in the database.
sent81: The tenth question (Q10) is a join over two tables (Movie and Genre).
sent82: One possible solution would include two not exist: the first one verifies for each movie that there exist no other genres as the genres of 'Sin City'.
sent83: The second one verifies for each movie that it has no genre, which 'Sin City' does not have.
sent84: For example, the movie 'Sin City' has the genre 'Thriller ', the movie 'Mission: Impossible' has the genres 'Thriller ' and 'Action'.
sent85: The first not exist will check if 'Mission: Impossible' has the genre 'Thriller ' from 'Sin City' which is true.
sent86: The second not exist checks if 'Sin City' has the genres 'Thriller ' and 'Action' (from 'Mission: Impossible'), which is false."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s79,Synonymy,"The difficulty of synonymy is that a simple lookup or matching is not enough. For example, the question 'All movies starring Brad Pitt from 2000 until 2010.' (Q3) could also be phrased as 'All movies playing Brad Pitt from 2000 until 2010.'. The answer should be the same, but because in the sample world no element is named 'playing', a lookup would not find an answer. Therefore, it is necessary that the system takes synonyms into account. A possible solution is the use of a translation dictionary. Usually such a dictionary is based on DBpedia (Lehmann et al, 2012) and/or WordNet (Miller, 1995).

The difficulty of synonymy is that a simple lookup or matching is not enough. For example, the question 'All movies starring Brad Pitt from 2000 until 2010.' (Q3) could also be phrased as 'All movies playing Brad Pitt from 2000 until 2010.'. The answer should be the same, but because in the sample world no element is named 'playing', a lookup would not find an answer. Therefore, it is necessary that the system takes synonyms into account. A possible solution is the use of a translation dictionary. Usually such a dictionary is based on DBpedia (Lehmann et al, 2012) and/or WordNet (Miller, 1995).","[['b39', 'b31'], ['b39', 'b31']]","[['b39', 'b31'], ['b39', 'b31']]",4,"sent1: The difficulty of synonymy is that a simple lookup or matching is not enough.
sent2: For example, the question 'All movies starring Brad Pitt from 2000 until 2010.'
sent3: (Q3) could also be phrased as 'All movies playing Brad Pitt from 2000 until 2010.'.
sent4: The answer should be the same, but because in the sample world no element is named 'playing', a lookup would not find an answer.
sent5: Therefore, it is necessary that the system takes synonyms into account.
sent6: A possible solution is the use of a translation dictionary.
sent7: Usually such a dictionary is based on DBpedia (Lehmann et al, 2012) and/or WordNet (Miller, 1995).
sent8: The difficulty of synonymy is that a simple lookup or matching is not enough.
sent9: For example, the question 'All movies starring Brad Pitt from 2000 until 2010.'
sent10: (Q3) could also be phrased as 'All movies playing Brad Pitt from 2000 until 2010.'.
sent11: The answer should be the same, but because in the sample world no element is named 'playing', a lookup would not find an answer.
sent12: Therefore, it is necessary that the system takes synonyms into account.
sent13: A possible solution is the use of a translation dictionary.
sent14: Usually such a dictionary is based on DBpedia (Lehmann et al, 2012) and/or WordNet (Miller, 1995)."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s85,Recently Developed NLIs,"In this section, we will focus on more recent NLIs starting from 2005. We will not discuss about older systems like BASEBALL (Green et al, 1961), LUNAR (Woods, 1973), RENDEZVOUS (Codd, 1974), LADDER (Sacerdoti, 1977), Chat-80 (Warren and Pereira, 1982), ASK (Thompson and Thompson, 1983) and JANUS (Weischedel, 1989), which are often quoted in the research field of NLIs. We will systematically analyze 24 recently developed systems in Sections 5.1 to 5.4 based on the sample world introduced in Section 2. The main goal is to highlight strengths and weaknesses of the different approaches based on a particular data model and particular questions to be able to directly compare the systems. The explanation is based on the information describing the systems found in the papers. Finally, in Section 6.1 we will evaluate the systems against the the sample questions and give an overall interpretation of the system.

There are different ways to classify NLIs. In this survey, we divide the NLIs into four main groups based on the technical approach they use:

In this section, we will focus on more recent NLIs starting from 2005. We will not discuss about older systems like BASEBALL (Green et al, 1961), LUNAR (Woods, 1973), RENDEZVOUS (Codd, 1974), LADDER (Sacerdoti, 1977), Chat-80 (Warren and Pereira, 1982), ASK (Thompson and Thompson, 1983) and JANUS (Weischedel, 1989), which are often quoted in the research field of NLIs. We will systematically analyze 24 recently developed systems in Sections 5.1 to 5.4 based on the sample world introduced in Section 2. The main goal is to highlight strengths and weaknesses of the different approaches based on a particular data model and particular questions to be able to directly compare the systems. The explanation is based on the information describing the systems found in the papers. Finally, in Section 6.1 we will evaluate the systems against the the sample questions and give an overall interpretation of the system.

There are different ways to classify NLIs. In this survey, we divide the NLIs into four main groups based on the technical approach they use:","[['b8', 'b52', 'b57', 'b60', 'b20', 'b58', 'b44'], [], ['b8', 'b52', 'b57', 'b60', 'b20', 'b58', 'b44'], []]","[['b8', 'b52', 'b57', 'b60', 'b20', 'b58', 'b44'], [], ['b8', 'b52', 'b57', 'b60', 'b20', 'b58', 'b44'], []]",14,"sent1: In this section, we will focus on more recent NLIs starting from 2005.
sent2: We will not discuss about older systems like BASEBALL (Green et al, 1961), LUNAR (Woods, 1973), RENDEZVOUS (Codd, 1974), LADDER (Sacerdoti, 1977), Chat-80 (Warren and Pereira, 1982), ASK (Thompson and Thompson, 1983) and JANUS (Weischedel, 1989), which are often quoted in the research field of NLIs.
sent3: We will systematically analyze 24 recently developed systems in Sections 5.1 to 5.4 based on the sample world introduced in Section 2.
sent4: The main goal is to highlight strengths and weaknesses of the different approaches based on a particular data model and particular questions to be able to directly compare the systems.
sent5: The explanation is based on the information describing the systems found in the papers.
sent6: Finally, in Section 6.1 we will evaluate the systems against the the sample questions and give an overall interpretation of the system.
sent7: There are different ways to classify NLIs.
sent8: In this survey, we divide the NLIs into four main groups based on the technical approach they use:In this section, we will focus on more recent NLIs starting from 2005.
sent9: We will not discuss about older systems like BASEBALL (Green et al, 1961), LUNAR (Woods, 1973), RENDEZVOUS (Codd, 1974), LADDER (Sacerdoti, 1977), Chat-80 (Warren and Pereira, 1982), ASK (Thompson and Thompson, 1983) and JANUS (Weischedel, 1989), which are often quoted in the research field of NLIs.
sent10: We will systematically analyze 24 recently developed systems in Sections 5.1 to 5.4 based on the sample world introduced in Section 2.
sent11: The main goal is to highlight strengths and weaknesses of the different approaches based on a particular data model and particular questions to be able to directly compare the systems.
sent12: The explanation is based on the information describing the systems found in the papers.
sent13: Finally, in Section 6.1 we will evaluate the systems against the the sample questions and give an overall interpretation of the system.
sent14: There are different ways to classify NLIs.
sent15: In this survey, we divide the NLIs into four main groups based on the technical approach they use:"
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s103,Querix,"Querix 5 (Kaufmann et al, 2006) allows users to enter questions in natural language to query an ontology. If the system identifies any ambiguities in the input question, it asks the user for clarification in a dialog. Querix uses a syntax tree to extract the sequence of words from the main word categories: noun (N), verb (V), preposition (P), wh-pronoun (Q, e.g. what, where, when, etc.) and conjunction (C). This sequence is called query skeleton. The query skeleton is used to enrich nouns and verbs and to identify subject-property-object patterns in the query.

In contrast to ATHENA, which uses a lot of different tools and technologies, Querix only uses the information of the query skeleton (parse tree) and the synonyms (for both the input question and the ontology) to translate the input question into SPARQL. For translating into SPARQL, Querix uses three components: query analyzer, matching center and query generator. The query analyzer handles two tasks: (1) It applies the Stanford Parser on the input question to generate a syntax tree, from which Querix extracts the query skeleton. For example, the query skeleton 'Q-V-N-P-N' is extracted from the input question (Q1) as 'Who (Q) is (V) the director (N) of (P) ""Inglourious Basterds"" (N) ? '. (2) It enriches all nouns and verbs with synonyms provided by Word-Net.

The matching center is the core component of Querix: (1) It tries to match the query skeleton with a small set of heuristic patterns. Those patterns are used to basically identify subject-property-object patterns in the input question. (2) It searches for matches between nouns and verbs of the input question with the resources in the ontology (including synonyms). (3) It tries to match the results of the two previous steps. The query generator then composes SPARQL queries from the joined triplets delivered by the last step of the matching center. If there are several different solutions with the highest cost score, Querix will consult the user by showing a menu from which the user can choose the intended meaning.

The simplicity of Querix is both a strength and a weakness: it is simple to use and completely portable, but this simplicity also reduces the number of questions that can be answered since they have to adhere to a predefined syntax.

Querix 5 (Kaufmann et al, 2006) allows users to enter questions in natural language to query an ontology. If the system identifies any ambiguities in the input question, it asks the user for clarification in a dialog. Querix uses a syntax tree to extract the sequence of words from the main word categories: noun (N), verb (V), preposition (P), wh-pronoun (Q, e.g. what, where, when, etc.) and conjunction (C). This sequence is called query skeleton. The query skeleton is used to enrich nouns and verbs and to identify subject-property-object patterns in the query.

In contrast to ATHENA, which uses a lot of different tools and technologies, Querix only uses the information of the query skeleton (parse tree) and the synonyms (for both the input question and the ontology) to translate the input question into SPARQL. For translating into SPARQL, Querix uses three components: query analyzer, matching center and query generator. The query analyzer handles two tasks: (1) It applies the Stanford Parser on the input question to generate a syntax tree, from which Querix extracts the query skeleton. For example, the query skeleton 'Q-V-N-P-N' is extracted from the input question (Q1) as 'Who (Q) is (V) the director (N) of (P) ""Inglourious Basterds"" (N) ? '. (2) It enriches all nouns and verbs with synonyms provided by Word-Net.

The matching center is the core component of Querix: (1) It tries to match the query skeleton with a small set of heuristic patterns. Those patterns are used to basically identify subject-property-object patterns in the input question. (2) It searches for matches between nouns and verbs of the input question with the resources in the ontology (including synonyms). (3) It tries to match the results of the two previous steps. The query generator then composes SPARQL queries from the joined triplets delivered by the last step of the matching center. If there are several different solutions with the highest cost score, Querix will consult the user by showing a menu from which the user can choose the intended meaning.

The simplicity of Querix is both a strength and a weakness: it is simple to use and completely portable, but this simplicity also reduces the number of questions that can be answered since they have to adhere to a predefined syntax.","[[None, 'b28'], [], [], [], [None, 'b28'], [], [], []]","[[None, 'b28'], [], [], [], [None, 'b28'], [], [], []]",4,"sent1: Querix 5 (Kaufmann et al, 2006) allows users to enter questions in natural language to query an ontology.
sent2: If the system identifies any ambiguities in the input question, it asks the user for clarification in a dialog.
sent3: Querix uses a syntax tree to extract the sequence of words from the main word categories: noun (N), verb (V), preposition (P), wh-pronoun (Q, e.g. what, where, when, etc.) and conjunction (C).
sent4: This sequence is called query skeleton.
sent5: The query skeleton is used to enrich nouns and verbs and to identify subject-property-object patterns in the query.
sent6: In contrast to ATHENA, which uses a lot of different tools and technologies, Querix only uses the information of the query skeleton (parse tree) and the synonyms (for both the input question and the ontology) to translate the input question into SPARQL.
sent7: For translating into SPARQL, Querix uses three components: query analyzer, matching center and query generator.
sent8: The query analyzer handles two tasks: (1)
sent9: It applies the Stanford Parser on the input question to generate a syntax tree, from which Querix extracts the query skeleton.
sent10: For example, the query skeleton 'Q-V-N-P-N' is extracted from the input question (Q1) as 'Who (Q) is (V) the director (N) of (P) ""Inglourious Basterds"" (N) ? '.
sent11: (2) It enriches all nouns and verbs with synonyms provided by Word-Net.
sent12: The matching center is the core component of Querix: (1)
sent13: It tries to match the query skeleton with a small set of heuristic patterns.
sent14: Those patterns are used to basically identify subject-property-object patterns in the input question.
sent15: (2) It searches for matches between nouns and verbs of the input question with the resources in the ontology (including synonyms).
sent16: (3) It tries to match the results of the two previous steps.
sent17: The query generator then composes SPARQL queries from the joined triplets delivered by the last step of the matching center.
sent18: If there are several different solutions with the highest cost score, Querix will consult the user by showing a menu from which the user can choose the intended meaning.
sent19: The simplicity of Querix is both a strength and a weakness: it is simple to use and completely portable, but this simplicity also reduces the number of questions that can be answered since they have to adhere to a predefined syntax.
sent20: Querix 5 (Kaufmann et al, 2006) allows users to enter questions in natural language to query an ontology.
sent21: If the system identifies any ambiguities in the input question, it asks the user for clarification in a dialog.
sent22: Querix uses a syntax tree to extract the sequence of words from the main word categories: noun (N), verb (V), preposition (P), wh-pronoun (Q, e.g. what, where, when, etc.) and conjunction (C).
sent23: This sequence is called query skeleton.
sent24: The query skeleton is used to enrich nouns and verbs and to identify subject-property-object patterns in the query.
sent25: In contrast to ATHENA, which uses a lot of different tools and technologies, Querix only uses the information of the query skeleton (parse tree) and the synonyms (for both the input question and the ontology) to translate the input question into SPARQL.
sent26: For translating into SPARQL, Querix uses three components: query analyzer, matching center and query generator.
sent27: The query analyzer handles two tasks: (1)
sent28: It applies the Stanford Parser on the input question to generate a syntax tree, from which Querix extracts the query skeleton.
sent29: For example, the query skeleton 'Q-V-N-P-N' is extracted from the input question (Q1) as 'Who (Q) is (V) the director (N) of (P) ""Inglourious Basterds"" (N) ? '.
sent30: (2) It enriches all nouns and verbs with synonyms provided by Word-Net.
sent31: The matching center is the core component of Querix: (1)
sent32: It tries to match the query skeleton with a small set of heuristic patterns.
sent33: Those patterns are used to basically identify subject-property-object patterns in the input question.
sent34: (2) It searches for matches between nouns and verbs of the input question with the resources in the ontology (including synonyms).
sent35: (3) It tries to match the results of the two previous steps.
sent36: The query generator then composes SPARQL queries from the joined triplets delivered by the last step of the matching center.
sent37: If there are several different solutions with the highest cost score, Querix will consult the user by showing a menu from which the user can choose the intended meaning.
sent38: The simplicity of Querix is both a strength and a weakness: it is simple to use and completely portable, but this simplicity also reduces the number of questions that can be answered since they have to adhere to a predefined syntax."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s91,SODA (Search Over DAta warehouse),"SODA (Blunschi et al, 2012) is a system that provides a keyword-based NLI for relational databases with some extensions in the direction of a pattern-based system. The base data consists of the relational database. The meta data can include multiple ontologies, which are handled like natural language patterns. For example, domain specific ontologies with concepts (like the concept 'great movie' in the sample world) or DBpedia to Fig. 7 Nodes in the meta data graph corresponding to the keywords 'director ' (red) and 'Inglourious Basterds' (green) found during the lookup step of SODA.

identify homonyms and synonyms. SODA uses both inverted indexes (base and meta data) as the basis for finding query matches in the data. The key innovation of SODA is that it provides the possibility to define meta data patterns which specify conceptual models. The concept 'good movie' could depend on various variables not only on the rating, but for example, also on the number of ratings. The users can then apply this concept to their input questions, for example, they could search for 'all great movie' (Q6) without having to specify what a great movie is.

Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for SODA could be: 'director Inglourious Basterds'.

SODA uses five steps to translate this keyword-based input question into a SQL query. The first step is the lookup: it checks the keywords against the inverted indexes over the database and provides all the nodes in the meta data graph where these keywords are found. For the input question Q1, this means that the keyword 'director ' can be found in the inverted index of the meta data, either the table name Director or to the attribute name Director.directorId and Directing.director-Id (Figure 7: red). The keyword 'Inglourious Basterds' is only found in the inverted index of the base data as a value of the attribute Movie.Title (Figure 7: green). This leads to three different solution sets for the next steps:

{Directing.directorId, Movie.Title}, {Director.directorId, Movie.Title} and {Director, Movie.Title}.

The second step is to assign a score to each solution of the lookup step. SODA uses a simple heuristic method, for example in-domain solutions receive a higher score. For the input question, the solution {Director, Movie.Title} receives the highest score, because the table name Director it is a full match and not only a fuzzy match like in directorId. Afterwards, only the best n solutions are provided to the next step.

The third step identifies which tables are used for each of the solutions provided by the previous step. Also, the relationships and inheritance structures between those tables are discovered in this step. For the best solution of the input question, the tables Director and Movie correspond to the different entry points. An entry point is a node in the meta data graph. The table Director is a child of the table Person (ISA-  Figure 7) and therefore this table is included.

The fourth step collects the filters. There are two types of filters which are collected. The first one are filters in the input question like 'Inglourious Basterds'. The second one are filter conditions that occur during traversing the meta data graph like the concept 'great movie'.

The fifth and last step generates a reasonable and executable SQL query from the information collected in the previous steps. A reasonable SQL query is a query which considers foreign keys and inheritance patterns in the schema. An executable SQL query is a query that can be executed on the underlying database.

The strengths of SODA are the use of meta data patterns and domain ontologies, which allow one to define concepts and include domain specific knowledge. In addition, the inclusion of external sources like DBpedia for homonyms and synonyms is beneficial for finding meaningful results. Furthermore, SODA is designed to evolve and thus improve over time based on user feedback.

The weaknesses of SODA are that it uses simple word recognition for comparison operators. For example, to retrieve all movies with a rating greater than 9, the input question needs to be written like 'rating > 9 ' (Q2). Moreover, SODA uses a very strict syntax for aggregation operators. For example, to retrieve the number of movies per year, the input question needs to be written like 'select count (movie) group by (year)'. These patterns are useful, but are not in natural language. Furthermore, there is no lemmatization, stemming or any other preprocessing of the input question which can lead to a problem with words that are used in plural. For example the input question 'all movies' would not detect the table Movie but the input question 'all movie' would display the expected result. Blunschi et al (2012) suggest extending SODA to handle temporal aspects of the data warehouse (e.g., bitemporal historization). They also pointed out that the GUI of SODA should be improved so that the users are engaged in selecting and ranking the different results. Furthermore, the user feedback provided by SODA is currently very basic and needs to be improved.

SODA (Blunschi et al, 2012) is a system that provides a keyword-based NLI for relational databases with some extensions in the direction of a pattern-based system. The base data consists of the relational database. The meta data can include multiple ontologies, which are handled like natural language patterns. For example, domain specific ontologies with concepts (like the concept 'great movie' in the sample world) or DBpedia to Fig. 7 Nodes in the meta data graph corresponding to the keywords 'director ' (red) and 'Inglourious Basterds' (green) found during the lookup step of SODA.

identify homonyms and synonyms. SODA uses both inverted indexes (base and meta data) as the basis for finding query matches in the data. The key innovation of SODA is that it provides the possibility to define meta data patterns which specify conceptual models. The concept 'good movie' could depend on various variables not only on the rating, but for example, also on the number of ratings. The users can then apply this concept to their input questions, for example, they could search for 'all great movie' (Q6) without having to specify what a great movie is.

Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for SODA could be: 'director Inglourious Basterds'.

SODA uses five steps to translate this keyword-based input question into a SQL query. The first step is the lookup: it checks the keywords against the inverted indexes over the database and provides all the nodes in the meta data graph where these keywords are found. For the input question Q1, this means that the keyword 'director ' can be found in the inverted index of the meta data, either the table name Director or to the attribute name Director.directorId and Directing.director-Id (Figure 7: red). The keyword 'Inglourious Basterds' is only found in the inverted index of the base data as a value of the attribute Movie.Title (Figure 7: green). This leads to three different solution sets for the next steps:

{Directing.directorId, Movie.Title}, {Director.directorId, Movie.Title} and {Director, Movie.Title}.

The second step is to assign a score to each solution of the lookup step. SODA uses a simple heuristic method, for example in-domain solutions receive a higher score. For the input question, the solution {Director, Movie.Title} receives the highest score, because the table name Director it is a full match and not only a fuzzy match like in directorId. Afterwards, only the best n solutions are provided to the next step.

The third step identifies which tables are used for each of the solutions provided by the previous step. Also, the relationships and inheritance structures between those tables are discovered in this step. For the best solution of the input question, the tables Director and Movie correspond to the different entry points. An entry point is a node in the meta data graph. The table Director is a child of the table Person (ISA-  Figure 7) and therefore this table is included.

The fourth step collects the filters. There are two types of filters which are collected. The first one are filters in the input question like 'Inglourious Basterds'. The second one are filter conditions that occur during traversing the meta data graph like the concept 'great movie'.

The fifth and last step generates a reasonable and executable SQL query from the information collected in the previous steps. A reasonable SQL query is a query which considers foreign keys and inheritance patterns in the schema. An executable SQL query is a query that can be executed on the underlying database.

The strengths of SODA are the use of meta data patterns and domain ontologies, which allow one to define concepts and include domain specific knowledge. In addition, the inclusion of external sources like DBpedia for homonyms and synonyms is beneficial for finding meaningful results. Furthermore, SODA is designed to evolve and thus improve over time based on user feedback.

The weaknesses of SODA are that it uses simple word recognition for comparison operators. For example, to retrieve all movies with a rating greater than 9, the input question needs to be written like 'rating > 9 ' (Q2). Moreover, SODA uses a very strict syntax for aggregation operators. For example, to retrieve the number of movies per year, the input question needs to be written like 'select count (movie) group by (year)'. These patterns are useful, but are not in natural language. Furthermore, there is no lemmatization, stemming or any other preprocessing of the input question which can lead to a problem with words that are used in plural. For example the input question 'all movies' would not detect the table Movie but the input question 'all movie' would display the expected result. Blunschi et al (2012) suggest extending SODA to handle temporal aspects of the data warehouse (e.g., bitemporal historization). They also pointed out that the GUI of SODA should be improved so that the users are engaged in selecting and ranking the different results. Furthermore, the user feedback provided by SODA is currently very basic and needs to be improved.","[[None, 'b5'], [], [], [], [], [], [], [], [], [], ['b5'], [None, 'b5'], [], [], [], [], [], [], [], [], [], ['b5']]","[[None, 'b5'], [], [], [], [], [], [], [], [], [], ['b5'], [None, 'b5'], [], [], [], [], [], [], [], [], [], ['b5']]",6,"sent1: SODA (Blunschi et al, 2012) is a system that provides a keyword-based NLI for relational databases with some extensions in the direction of a pattern-based system.
sent2: The base data consists of the relational database.
sent3: The meta data can include multiple ontologies, which are handled like natural language patterns.
sent4: For example, domain specific ontologies with concepts (like the concept 'great movie' in the sample world) or DBpedia to Fig. 7 Nodes in the meta data graph corresponding to the keywords 'director ' (red) and 'Inglourious Basterds' (green) found during the lookup step of SODA.
sent5: identify homonyms and synonyms.
sent6: SODA uses both inverted indexes (base and meta data) as the basis for finding query matches in the data.
sent7: The key innovation of SODA is that it provides the possibility to define meta data patterns which specify conceptual models.
sent8: The concept 'good movie' could depend on various variables not only on the rating, but for example, also on the number of ratings.
sent9: The users can then apply this concept to their input questions, for example, they could search for 'all great movie' (Q6) without having to specify what a great movie is.
sent10: Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for SODA could be: 'director Inglourious Basterds'.
sent11: SODA uses five steps to translate this keyword-based input question into a SQL query.
sent12: The first step is the lookup: it checks the keywords against the inverted indexes over the database and provides all the nodes in the meta data graph where these keywords are found.
sent13: For the input question Q1, this means that the keyword 'director ' can be found in the inverted index of the meta data, either the table name Director or to the attribute name Director.directorId and Directing.director-Id (Figure 7: red).
sent14: The keyword 'Inglourious Basterds' is only found in the inverted index of the base data as a value of the attribute Movie.
sent15: Title (Figure 7: green). This leads to three different solution sets for the next steps:{Directing.directorId, Movie.
sent16: Title}, {Director.directorId, Movie.
sent17: Title} and {Director, Movie. Title}.
sent18: The second step is to assign a score to each solution of the lookup step.
sent19: SODA uses a simple heuristic method, for example in-domain solutions receive a higher score.
sent20: For the input question, the solution {Director, Movie.Title} receives the highest score, because the table name Director it is a full match and not only a fuzzy match like in directorId.
sent21: Afterwards, only the best n solutions are provided to the next step.
sent22: The third step identifies which tables are used for each of the solutions provided by the previous step.
sent23: Also, the relationships and inheritance structures between those tables are discovered in this step.
sent24: For the best solution of the input question, the tables Director and Movie correspond to the different entry points.
sent25: An entry point is a node in the meta data graph.
sent26: The table Director is a child of the table Person (ISA-  Figure 7) and therefore this table is included.
sent27: The fourth step collects the filters.
sent28: There are two types of filters which are collected.
sent29: The first one are filters in the input question like 'Inglourious Basterds'.
sent30: The second one are filter conditions that occur during traversing the meta data graph like the concept 'great movie'.
sent31: The fifth and last step generates a reasonable and executable SQL query from the information collected in the previous steps.
sent32: A reasonable SQL query is a query which considers foreign keys and inheritance patterns in the schema.
sent33: An executable SQL query is a query that can be executed on the underlying database.
sent34: The strengths of SODA are the use of meta data patterns and domain ontologies, which allow one to define concepts and include domain specific knowledge.
sent35: In addition, the inclusion of external sources like DBpedia for homonyms and synonyms is beneficial for finding meaningful results.
sent36: Furthermore, SODA is designed to evolve and thus improve over time based on user feedback.
sent37: The weaknesses of SODA are that it uses simple word recognition for comparison operators.
sent38: For example, to retrieve all movies with a rating greater than 9, the input question needs to be written like 'rating > 9 ' (Q2).
sent39: Moreover, SODA uses a very strict syntax for aggregation operators.
sent40: For example, to retrieve the number of movies per year, the input question needs to be written like 'select count (movie) group by (year)'.
sent41: These patterns are useful, but are not in natural language.
sent42: Furthermore, there is no lemmatization, stemming or any other preprocessing of the input question which can lead to a problem with words that are used in plural.
sent43: For example the input question 'all movies' would not detect the table Movie but the input question 'all movie' would display the expected result.
sent44: Blunschi et al (2012) suggest extending SODA to handle temporal aspects of the data warehouse (e.g., bitemporal historization).
sent45: They also pointed out that the GUI of SODA should be improved so that the users are engaged in selecting and ranking the different results.
sent46: Furthermore, the user feedback provided by SODA is currently very basic and needs to be improved.
sent47: SODA (Blunschi et al, 2012) is a system that provides a keyword-based NLI for relational databases with some extensions in the direction of a pattern-based system.
sent48: The base data consists of the relational database.
sent49: The meta data can include multiple ontologies, which are handled like natural language patterns.
sent50: For example, domain specific ontologies with concepts (like the concept 'great movie' in the sample world) or DBpedia to Fig. 7 Nodes in the meta data graph corresponding to the keywords 'director ' (red) and 'Inglourious Basterds' (green) found during the lookup step of SODA.
sent51: identify homonyms and synonyms.
sent52: SODA uses both inverted indexes (base and meta data) as the basis for finding query matches in the data.
sent53: The key innovation of SODA is that it provides the possibility to define meta data patterns which specify conceptual models.
sent54: The concept 'good movie' could depend on various variables not only on the rating, but for example, also on the number of ratings.
sent55: The users can then apply this concept to their input questions, for example, they could search for 'all great movie' (Q6) without having to specify what a great movie is.
sent56: Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for SODA could be: 'director Inglourious Basterds'.
sent57: SODA uses five steps to translate this keyword-based input question into a SQL query.
sent58: The first step is the lookup: it checks the keywords against the inverted indexes over the database and provides all the nodes in the meta data graph where these keywords are found.
sent59: For the input question Q1, this means that the keyword 'director ' can be found in the inverted index of the meta data, either the table name Director or to the attribute name Director.directorId and Directing.director-Id (Figure 7: red).
sent60: The keyword 'Inglourious Basterds' is only found in the inverted index of the base data as a value of the attribute Movie.
sent61: Title (Figure 7: green). This leads to three different solution sets for the next steps:{Directing.directorId, Movie.
sent62: Title}, {Director.directorId, Movie.
sent63: Title} and {Director, Movie. Title}.
sent64: The second step is to assign a score to each solution of the lookup step.
sent65: SODA uses a simple heuristic method, for example in-domain solutions receive a higher score.
sent66: For the input question, the solution {Director, Movie.Title} receives the highest score, because the table name Director it is a full match and not only a fuzzy match like in directorId.
sent67: Afterwards, only the best n solutions are provided to the next step.
sent68: The third step identifies which tables are used for each of the solutions provided by the previous step.
sent69: Also, the relationships and inheritance structures between those tables are discovered in this step.
sent70: For the best solution of the input question, the tables Director and Movie correspond to the different entry points.
sent71: An entry point is a node in the meta data graph.
sent72: The table Director is a child of the table Person (ISA-  Figure 7) and therefore this table is included.
sent73: The fourth step collects the filters.
sent74: There are two types of filters which are collected.
sent75: The first one are filters in the input question like 'Inglourious Basterds'.
sent76: The second one are filter conditions that occur during traversing the meta data graph like the concept 'great movie'.
sent77: The fifth and last step generates a reasonable and executable SQL query from the information collected in the previous steps.
sent78: A reasonable SQL query is a query which considers foreign keys and inheritance patterns in the schema.
sent79: An executable SQL query is a query that can be executed on the underlying database.
sent80: The strengths of SODA are the use of meta data patterns and domain ontologies, which allow one to define concepts and include domain specific knowledge.
sent81: In addition, the inclusion of external sources like DBpedia for homonyms and synonyms is beneficial for finding meaningful results.
sent82: Furthermore, SODA is designed to evolve and thus improve over time based on user feedback.
sent83: The weaknesses of SODA are that it uses simple word recognition for comparison operators.
sent84: For example, to retrieve all movies with a rating greater than 9, the input question needs to be written like 'rating > 9 ' (Q2).
sent85: Moreover, SODA uses a very strict syntax for aggregation operators.
sent86: For example, to retrieve the number of movies per year, the input question needs to be written like 'select count (movie) group by (year)'.
sent87: These patterns are useful, but are not in natural language.
sent88: Furthermore, there is no lemmatization, stemming or any other preprocessing of the input question which can lead to a problem with words that are used in plural.
sent89: For example the input question 'all movies' would not detect the table Movie but the input question 'all movie' would display the expected result.
sent90: Blunschi et al (2012) suggest extending SODA to handle temporal aspects of the data warehouse (e.g., bitemporal historization).
sent91: They also pointed out that the GUI of SODA should be improved so that the users are engaged in selecting and ranking the different results.
sent92: Furthermore, the user feedback provided by SODA is currently very basic and needs to be improved."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s99,NLQ/A,"NLQ/A (Zheng et al, 2017) is an NLI to query a knowledge graph. The system is based on a new approach without NLP technologies like parsers or PoS taggers. The idea being that the errors made by these technologies are not worth the gain of information. For example, a parse tree helps for certain questions like subqueries (e.g., Q9), but if the parse tree is wrong, the system will fail to translate even simpler questions. Instead, NLQ/A lets the users resolve all ambiguity problems, also those which could be solved with PoS tagging or parse trees. To avoid needing too many interaction steps, NLQ/A provides an efficient greedy approach for the interaction process.

Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question could be: 'Who is the director of ""Inglourious Basterds""? '.

NLQ/A use four steps to answer the input question. The first step is to detect the phrases of the input question. In general, the phrases can be categorized into two types: independent and dependent phrases. Independent phrases are identified with a phrase dictionary. The dictionary consists of variables, aggregations, operators, modifiers and quantifier phrases. To detect dependent phrases, most stop words are removed (simplified input question). Some types of words like prepositions are still needed and therefore kept. Next 1:n-grams are generated. Phrases starting with prepositions are discarded. After stop word removal, the input question Q1 would become 'director of Inglourious Basterds'. If n is set to 2, the extracted phrases would be: {'director ', 'director of ', 'Inglourious', 'Inglourious Basterds', 'Basterds'}. Next, the phrases are extended according to a synonym dictionary. For example if there is a phrase 'starring' it would be extended with the phrase 'playing'. Those extended phrases are mapped to the knowledge graph based on the string similarity (edit distance). For one extended phrase there can be multiple candidate mappings. The next step takes the candidate mappings and tries to find the true meaning of the input question with the help of the users. To reduce the amount of interactions for the user, a phrase dependency graph (PDG) is proposed. The PDG consists of two parts: (PDG1) a graph where each node represents a phrase, two phrases are connected if they share at least one common token and (PDG2) a subgraph of the knowledge graph consisting of the candidates where each node represents a candidate, two nodes are connected if they are adjacent in the knowledge graph. The two parts are connected with edges, representing the mapping between phrases and candidates (see Figure 8).

In the third step, the users get involved to solve the ambiguity given in the PDG. In order to reduce the necessary user interactions, the NLI tries to find those edges which resolve the most ambiguities (similar to the idea of QUICK).

The last step takes the selected candidates and tries to connect them into one graph. The connected graph will include the answer to the question. Groups of already connected candidates in the PDG2 are called query fragments. In Figure 8, the candidates 'director-Of ' and 'Inglourious Basterds' are one query fragment. For each query fragment, the system tries to find the path with the highest similarity to the simplified input question. For the input question Q1, if the users select 'Director ' as candidate in step 3, the system would find the path as shown in Figure 9. 'Inglourious Basterds' is also a candidate, but not selected by the users because there is no ambiguity to solve.

The strengths of this NLI are the simplicity and the efficient user interaction process. The simplicity allows easy adaption on new knowledge graphs and together with the user interaction process it overcomes the difficulties of ambiguity.

The weakness of this system is that usually more than one user interaction is needed to resolve ambiguities, in the experiments the average number of interactions was three (Zheng et al, 2017).

NLQ/A (Zheng et al, 2017) is an NLI to query a knowledge graph. The system is based on a new approach without NLP technologies like parsers or PoS taggers. The idea being that the errors made by these technologies are not worth the gain of information. For example, a parse tree helps for certain questions like subqueries (e.g., Q9), but if the parse tree is wrong, the system will fail to translate even simpler questions. Instead, NLQ/A lets the users resolve all ambiguity problems, also those which could be solved with PoS tagging or parse trees. To avoid needing too many interaction steps, NLQ/A provides an efficient greedy approach for the interaction process.

Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question could be: 'Who is the director of ""Inglourious Basterds""? '.

NLQ/A use four steps to answer the input question. The first step is to detect the phrases of the input question. In general, the phrases can be categorized into two types: independent and dependent phrases. Independent phrases are identified with a phrase dictionary. The dictionary consists of variables, aggregations, operators, modifiers and quantifier phrases. To detect dependent phrases, most stop words are removed (simplified input question). Some types of words like prepositions are still needed and therefore kept. Next 1:n-grams are generated. Phrases starting with prepositions are discarded. After stop word removal, the input question Q1 would become 'director of Inglourious Basterds'. If n is set to 2, the extracted phrases would be: {'director ', 'director of ', 'Inglourious', 'Inglourious Basterds', 'Basterds'}. Next, the phrases are extended according to a synonym dictionary. For example if there is a phrase 'starring' it would be extended with the phrase 'playing'. Those extended phrases are mapped to the knowledge graph based on the string similarity (edit distance). For one extended phrase there can be multiple candidate mappings. The next step takes the candidate mappings and tries to find the true meaning of the input question with the help of the users. To reduce the amount of interactions for the user, a phrase dependency graph (PDG) is proposed. The PDG consists of two parts: (PDG1) a graph where each node represents a phrase, two phrases are connected if they share at least one common token and (PDG2) a subgraph of the knowledge graph consisting of the candidates where each node represents a candidate, two nodes are connected if they are adjacent in the knowledge graph. The two parts are connected with edges, representing the mapping between phrases and candidates (see Figure 8).

In the third step, the users get involved to solve the ambiguity given in the PDG. In order to reduce the necessary user interactions, the NLI tries to find those edges which resolve the most ambiguities (similar to the idea of QUICK).

The last step takes the selected candidates and tries to connect them into one graph. The connected graph will include the answer to the question. Groups of already connected candidates in the PDG2 are called query fragments. In Figure 8, the candidates 'director-Of ' and 'Inglourious Basterds' are one query fragment. For each query fragment, the system tries to find the path with the highest similarity to the simplified input question. For the input question Q1, if the users select 'Director ' as candidate in step 3, the system would find the path as shown in Figure 9. 'Inglourious Basterds' is also a candidate, but not selected by the users because there is no ambiguity to solve.

The strengths of this NLI are the simplicity and the efficient user interaction process. The simplicity allows easy adaption on new knowledge graphs and together with the user interaction process it overcomes the difficulties of ambiguity.

The weakness of this system is that usually more than one user interaction is needed to resolve ambiguities, in the experiments the average number of interactions was three (Zheng et al, 2017).","[['b65'], [], [], [], [], [], ['b65'], ['b65'], [], [], [], [], [], ['b65']]","[['b65'], [], [], [], [], [], ['b65'], ['b65'], [], [], [], [], [], ['b65']]",4,"sent1: NLQ/A (Zheng et al, 2017) is an NLI to query a knowledge graph.
sent2: The system is based on a new approach without NLP technologies like parsers or PoS taggers.
sent3: The idea being that the errors made by these technologies are not worth the gain of information.
sent4: For example, a parse tree helps for certain questions like subqueries (e.g., Q9), but if the parse tree is wrong, the system will fail to translate even simpler questions.
sent5: Instead, NLQ/A lets the users resolve all ambiguity problems, also those which could be solved with PoS tagging or parse trees.
sent6: To avoid needing too many interaction steps, NLQ/A provides an efficient greedy approach for the interaction process.
sent7: Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question could be: 'Who is the director of ""Inglourious Basterds""?
sent8: '. NLQ/A use four steps to answer the input question.
sent9: The first step is to detect the phrases of the input question.
sent10: In general, the phrases can be categorized into two types: independent and dependent phrases.
sent11: Independent phrases are identified with a phrase dictionary.
sent12: The dictionary consists of variables, aggregations, operators, modifiers and quantifier phrases.
sent13: To detect dependent phrases, most stop words are removed (simplified input question).
sent14: Some types of words like prepositions are still needed and therefore kept.
sent15: Next 1:n-grams are generated. Phrases starting with prepositions are discarded.
sent16: After stop word removal, the input question Q1 would become 'director of Inglourious Basterds'.
sent17: If n is set to 2, the extracted phrases would be: {'director ', 'director of ', 'Inglourious', 'Inglourious Basterds', 'Basterds'}.
sent18: Next, the phrases are extended according to a synonym dictionary.
sent19: For example if there is a phrase 'starring' it would be extended with the phrase 'playing'.
sent20: Those extended phrases are mapped to the knowledge graph based on the string similarity (edit distance).
sent21: For one extended phrase there can be multiple candidate mappings.
sent22: The next step takes the candidate mappings and tries to find the true meaning of the input question with the help of the users.
sent23: To reduce the amount of interactions for the user, a phrase dependency graph (PDG) is proposed.
sent24: The PDG consists of two parts: (PDG1) a graph where each node represents a phrase, two phrases are connected if they share at least one common token and (PDG2) a subgraph of the knowledge graph consisting of the candidates where each node represents a candidate, two nodes are connected if they are adjacent in the knowledge graph.
sent25: The two parts are connected with edges, representing the mapping between phrases and candidates (see Figure 8).
sent26: In the third step, the users get involved to solve the ambiguity given in the PDG.
sent27: In order to reduce the necessary user interactions, the NLI tries to find those edges which resolve the most ambiguities (similar to the idea of QUICK).
sent28: The last step takes the selected candidates and tries to connect them into one graph.
sent29: The connected graph will include the answer to the question.
sent30: Groups of already connected candidates in the PDG2 are called query fragments.
sent31: In Figure 8, the candidates 'director-Of ' and 'Inglourious Basterds' are one query fragment.
sent32: For each query fragment, the system tries to find the path with the highest similarity to the simplified input question.
sent33: For the input question Q1, if the users select 'Director ' as candidate in step 3, the system would find the path as shown in Figure 9.
sent34: 'Inglourious Basterds' is also a candidate, but not selected by the users because there is no ambiguity to solve.
sent35: The strengths of this NLI are the simplicity and the efficient user interaction process.
sent36: The simplicity allows easy adaption on new knowledge graphs and together with the user interaction process it overcomes the difficulties of ambiguity.
sent37: The weakness of this system is that usually more than one user interaction is needed to resolve ambiguities, in the experiments the average number of interactions was three (Zheng et al, 2017).
sent38: NLQ/A (Zheng et al, 2017) is an NLI to query a knowledge graph.
sent39: The system is based on a new approach without NLP technologies like parsers or PoS taggers.
sent40: The idea being that the errors made by these technologies are not worth the gain of information.
sent41: For example, a parse tree helps for certain questions like subqueries (e.g., Q9), but if the parse tree is wrong, the system will fail to translate even simpler questions.
sent42: Instead, NLQ/A lets the users resolve all ambiguity problems, also those which could be solved with PoS tagging or parse trees.
sent43: To avoid needing too many interaction steps, NLQ/A provides an efficient greedy approach for the interaction process.
sent44: Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question could be: 'Who is the director of ""Inglourious Basterds""?
sent45: '. NLQ/A use four steps to answer the input question.
sent46: The first step is to detect the phrases of the input question.
sent47: In general, the phrases can be categorized into two types: independent and dependent phrases.
sent48: Independent phrases are identified with a phrase dictionary.
sent49: The dictionary consists of variables, aggregations, operators, modifiers and quantifier phrases.
sent50: To detect dependent phrases, most stop words are removed (simplified input question).
sent51: Some types of words like prepositions are still needed and therefore kept.
sent52: Next 1:n-grams are generated. Phrases starting with prepositions are discarded.
sent53: After stop word removal, the input question Q1 would become 'director of Inglourious Basterds'.
sent54: If n is set to 2, the extracted phrases would be: {'director ', 'director of ', 'Inglourious', 'Inglourious Basterds', 'Basterds'}.
sent55: Next, the phrases are extended according to a synonym dictionary.
sent56: For example if there is a phrase 'starring' it would be extended with the phrase 'playing'.
sent57: Those extended phrases are mapped to the knowledge graph based on the string similarity (edit distance).
sent58: For one extended phrase there can be multiple candidate mappings.
sent59: The next step takes the candidate mappings and tries to find the true meaning of the input question with the help of the users.
sent60: To reduce the amount of interactions for the user, a phrase dependency graph (PDG) is proposed.
sent61: The PDG consists of two parts: (PDG1) a graph where each node represents a phrase, two phrases are connected if they share at least one common token and (PDG2) a subgraph of the knowledge graph consisting of the candidates where each node represents a candidate, two nodes are connected if they are adjacent in the knowledge graph.
sent62: The two parts are connected with edges, representing the mapping between phrases and candidates (see Figure 8).
sent63: In the third step, the users get involved to solve the ambiguity given in the PDG.
sent64: In order to reduce the necessary user interactions, the NLI tries to find those edges which resolve the most ambiguities (similar to the idea of QUICK).
sent65: The last step takes the selected candidates and tries to connect them into one graph.
sent66: The connected graph will include the answer to the question.
sent67: Groups of already connected candidates in the PDG2 are called query fragments.
sent68: In Figure 8, the candidates 'director-Of ' and 'Inglourious Basterds' are one query fragment.
sent69: For each query fragment, the system tries to find the path with the highest similarity to the simplified input question.
sent70: For the input question Q1, if the users select 'Director ' as candidate in step 3, the system would find the path as shown in Figure 9.
sent71: 'Inglourious Basterds' is also a candidate, but not selected by the users because there is no ambiguity to solve.
sent72: The strengths of this NLI are the simplicity and the efficient user interaction process.
sent73: The simplicity allows easy adaption on new knowledge graphs and together with the user interaction process it overcomes the difficulties of ambiguity.
sent74: The weakness of this system is that usually more than one user interaction is needed to resolve ambiguities, in the experiments the average number of interactions was three (Zheng et al, 2017)."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s35,Querix,"Querix 5 (Kaufmann et al, 2006) allows users to enter questions in natural language to query an ontology. If the system identifies any ambiguities in the input question, it asks the user for clarification in a dialog. Querix uses a syntax tree to extract the sequence of words from the main word categories: noun (N), verb (V), preposition (P), wh-pronoun (Q, e.g. what, where, when, etc.) and conjunction (C). This sequence is called query skeleton. The query skeleton is used to enrich nouns and verbs and to identify subject-property-object patterns in the query.

In contrast to ATHENA, which uses a lot of different tools and technologies, Querix only uses the information of the query skeleton (parse tree) and the synonyms (for both the input question and the ontology) to translate the input question into SPARQL. For translating into SPARQL, Querix uses three components: query analyzer, matching center and query generator. The query analyzer handles two tasks: (1) It applies the Stanford Parser on the input question to generate a syntax tree, from which Querix extracts the query skeleton. For example, the query skeleton 'Q-V-N-P-N' is extracted from the input question (Q1) as 'Who (Q) is (V) the director (N) of (P) ""Inglourious Basterds"" (N) ? '. (2) It enriches all nouns and verbs with synonyms provided by Word-Net.

The matching center is the core component of Querix: (1) It tries to match the query skeleton with a small set of heuristic patterns. Those patterns are used to basically identify subject-property-object patterns in the input question. (2) It searches for matches between nouns and verbs of the input question with the resources in the ontology (including synonyms). (3) It tries to match the results of the two previous steps. The query generator then composes SPARQL queries from the joined triplets delivered by the last step of the matching center. If there are several different solutions with the highest cost score, Querix will consult the user by showing a menu from which the user can choose the intended meaning.

The simplicity of Querix is both a strength and a weakness: it is simple to use and completely portable, but this simplicity also reduces the number of questions that can be answered since they have to adhere to a predefined syntax.

Querix 5 (Kaufmann et al, 2006) allows users to enter questions in natural language to query an ontology. If the system identifies any ambiguities in the input question, it asks the user for clarification in a dialog. Querix uses a syntax tree to extract the sequence of words from the main word categories: noun (N), verb (V), preposition (P), wh-pronoun (Q, e.g. what, where, when, etc.) and conjunction (C). This sequence is called query skeleton. The query skeleton is used to enrich nouns and verbs and to identify subject-property-object patterns in the query.

In contrast to ATHENA, which uses a lot of different tools and technologies, Querix only uses the information of the query skeleton (parse tree) and the synonyms (for both the input question and the ontology) to translate the input question into SPARQL. For translating into SPARQL, Querix uses three components: query analyzer, matching center and query generator. The query analyzer handles two tasks: (1) It applies the Stanford Parser on the input question to generate a syntax tree, from which Querix extracts the query skeleton. For example, the query skeleton 'Q-V-N-P-N' is extracted from the input question (Q1) as 'Who (Q) is (V) the director (N) of (P) ""Inglourious Basterds"" (N) ? '. (2) It enriches all nouns and verbs with synonyms provided by Word-Net.

The matching center is the core component of Querix: (1) It tries to match the query skeleton with a small set of heuristic patterns. Those patterns are used to basically identify subject-property-object patterns in the input question. (2) It searches for matches between nouns and verbs of the input question with the resources in the ontology (including synonyms). (3) It tries to match the results of the two previous steps. The query generator then composes SPARQL queries from the joined triplets delivered by the last step of the matching center. If there are several different solutions with the highest cost score, Querix will consult the user by showing a menu from which the user can choose the intended meaning.

The simplicity of Querix is both a strength and a weakness: it is simple to use and completely portable, but this simplicity also reduces the number of questions that can be answered since they have to adhere to a predefined syntax.","[[None, 'b28'], [], [], [], [None, 'b28'], [], [], []]","[[None, 'b28'], [], [], [], [None, 'b28'], [], [], []]",4,"sent1: Querix 5 (Kaufmann et al, 2006) allows users to enter questions in natural language to query an ontology.
sent2: If the system identifies any ambiguities in the input question, it asks the user for clarification in a dialog.
sent3: Querix uses a syntax tree to extract the sequence of words from the main word categories: noun (N), verb (V), preposition (P), wh-pronoun (Q, e.g. what, where, when, etc.) and conjunction (C).
sent4: This sequence is called query skeleton.
sent5: The query skeleton is used to enrich nouns and verbs and to identify subject-property-object patterns in the query.
sent6: In contrast to ATHENA, which uses a lot of different tools and technologies, Querix only uses the information of the query skeleton (parse tree) and the synonyms (for both the input question and the ontology) to translate the input question into SPARQL.
sent7: For translating into SPARQL, Querix uses three components: query analyzer, matching center and query generator.
sent8: The query analyzer handles two tasks: (1)
sent9: It applies the Stanford Parser on the input question to generate a syntax tree, from which Querix extracts the query skeleton.
sent10: For example, the query skeleton 'Q-V-N-P-N' is extracted from the input question (Q1) as 'Who (Q) is (V) the director (N) of (P) ""Inglourious Basterds"" (N) ? '.
sent11: (2) It enriches all nouns and verbs with synonyms provided by Word-Net.
sent12: The matching center is the core component of Querix: (1)
sent13: It tries to match the query skeleton with a small set of heuristic patterns.
sent14: Those patterns are used to basically identify subject-property-object patterns in the input question.
sent15: (2) It searches for matches between nouns and verbs of the input question with the resources in the ontology (including synonyms).
sent16: (3) It tries to match the results of the two previous steps.
sent17: The query generator then composes SPARQL queries from the joined triplets delivered by the last step of the matching center.
sent18: If there are several different solutions with the highest cost score, Querix will consult the user by showing a menu from which the user can choose the intended meaning.
sent19: The simplicity of Querix is both a strength and a weakness: it is simple to use and completely portable, but this simplicity also reduces the number of questions that can be answered since they have to adhere to a predefined syntax.
sent20: Querix 5 (Kaufmann et al, 2006) allows users to enter questions in natural language to query an ontology.
sent21: If the system identifies any ambiguities in the input question, it asks the user for clarification in a dialog.
sent22: Querix uses a syntax tree to extract the sequence of words from the main word categories: noun (N), verb (V), preposition (P), wh-pronoun (Q, e.g. what, where, when, etc.) and conjunction (C).
sent23: This sequence is called query skeleton.
sent24: The query skeleton is used to enrich nouns and verbs and to identify subject-property-object patterns in the query.
sent25: In contrast to ATHENA, which uses a lot of different tools and technologies, Querix only uses the information of the query skeleton (parse tree) and the synonyms (for both the input question and the ontology) to translate the input question into SPARQL.
sent26: For translating into SPARQL, Querix uses three components: query analyzer, matching center and query generator.
sent27: The query analyzer handles two tasks: (1)
sent28: It applies the Stanford Parser on the input question to generate a syntax tree, from which Querix extracts the query skeleton.
sent29: For example, the query skeleton 'Q-V-N-P-N' is extracted from the input question (Q1) as 'Who (Q) is (V) the director (N) of (P) ""Inglourious Basterds"" (N) ? '.
sent30: (2) It enriches all nouns and verbs with synonyms provided by Word-Net.
sent31: The matching center is the core component of Querix: (1)
sent32: It tries to match the query skeleton with a small set of heuristic patterns.
sent33: Those patterns are used to basically identify subject-property-object patterns in the input question.
sent34: (2) It searches for matches between nouns and verbs of the input question with the resources in the ontology (including synonyms).
sent35: (3) It tries to match the results of the two previous steps.
sent36: The query generator then composes SPARQL queries from the joined triplets delivered by the last step of the matching center.
sent37: If there are several different solutions with the highest cost score, Querix will consult the user by showing a menu from which the user can choose the intended meaning.
sent38: The simplicity of Querix is both a strength and a weakness: it is simple to use and completely portable, but this simplicity also reduces the number of questions that can be answered since they have to adhere to a predefined syntax."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s111,TR Discover,"TR Discover (Song et al, 2015) is a system providing an NLI which translates the input question in form of an English sentence (or sentence fragment) into SQL or SPARQL. It is either used for relational databases or ontologies, but does not need an ontology to work for relational databases. During the translation steps, TR Discover uses a First Order Logic (FOL) representation as an intermediate language. Furthermore, it provides auto-suggestions based on the user input. There are two types of suggestions: autocompletion and prediction.

TR Discover helps the users in formulating the question through an auto-suggestion feature. For example, assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1). When the users start typing 'p', TR Discover will not only suggest 'person' but also longer phrases like 'person directing' (autocomplete). After 'person directing' is selected (or typed), TR Discover will again suggest phrases, like 'movies' or even specific movies like 'Inglourious Basterds' (prediction). For input question Q1, the input could be 'person directing Inglourious Basterds'.

The suggestions are based upon the relationships and entities in the dataset and use the linguistic constraints encoded in a feature-based context-free grammar (FCFG). The grammar consists of grammar rules (G1-3) and lexical entries (L1-2). For the sample world (and the input question Q1), the following rules could be defined: The suggestions are computed based on the idea of left-corner parsing: given a query segment, it finds all grammar rules whose left corner on the right side matches the left side of the lexical entry of the query segment. Then, all leaf nodes (lexical entries) in the grammar that can be reached by using the adjacent element are found. For example, while typing 'person' (Q1), the lexical entries L1 and L2 are found and provided to the user.

TR Discover uses three steps to translate the English sentence or fragment of a sentence into a SQL or SPARQL query. The first step parses the input question into a FOL representation. The query parsing uses the FCFG. For the example input, the token 'person' Fig. 12 The parse tree for the FOL representation of the input question 'person directing Inglourious Basterds'. will be parsed by the lexical entry L1 and the token 'directing' will be parsed with the lexical entry L2. This leads to the FOL representation:

How exactly the phrase 'Inglourious Basterds' is matched to the base data and therefore can be used as part of the lexical entry L2 and how it is resolved, is not explained by Song et al (2015). If there are multiple possibilities to parse the input question, the first one is chosen.

The next step is to translate the generated FOL into a parse tree. The FOL parser takes a grammar and the FOL representation from the previous step, and generates a parse tree ( Figure 12) using ANTLER for implementation.

In the third step, an in-order traversal of the parse tree (provided by the previous step) is performed to translate it into an executable SQL or SPARQL query. While traversing the parse tree, the atomic logical conditions and connectors are put on a stack. After traversing, the constraints are popped from the stack to build the correct query constraints. The predicates are mapped to their corresponding attribute names (SQL) or ontology properties (SPARQL).

The strengths of TR Discover are the auto-suggestion and the possibility to translate natural language into different query languages such as SQL and SPARQL, because FOL is used as an intermediate language.

The weaknesses of TR Discover are that quantifiers (e.g., Q3: 'grossed most') cannot be used, synonyms are not properly handled, and negations only work for SPARQL. Song et al (2015) suggest extending TR Discover with a ranking system for multiple parses in the first step and to improve the synonym handling. Furthermore, they pointed out the possibility of applying user query logs to improve auto-suggestions.

TR Discover (Song et al, 2015) is a system providing an NLI which translates the input question in form of an English sentence (or sentence fragment) into SQL or SPARQL. It is either used for relational databases or ontologies, but does not need an ontology to work for relational databases. During the translation steps, TR Discover uses a First Order Logic (FOL) representation as an intermediate language. Furthermore, it provides auto-suggestions based on the user input. There are two types of suggestions: autocompletion and prediction.

TR Discover helps the users in formulating the question through an auto-suggestion feature. For example, assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1). When the users start typing 'p', TR Discover will not only suggest 'person' but also longer phrases like 'person directing' (autocomplete). After 'person directing' is selected (or typed), TR Discover will again suggest phrases, like 'movies' or even specific movies like 'Inglourious Basterds' (prediction). For input question Q1, the input could be 'person directing Inglourious Basterds'.

The suggestions are based upon the relationships and entities in the dataset and use the linguistic constraints encoded in a feature-based context-free grammar (FCFG). The grammar consists of grammar rules (G1-3) and lexical entries (L1-2). For the sample world (and the input question Q1), the following rules could be defined: The suggestions are computed based on the idea of left-corner parsing: given a query segment, it finds all grammar rules whose left corner on the right side matches the left side of the lexical entry of the query segment. Then, all leaf nodes (lexical entries) in the grammar that can be reached by using the adjacent element are found. For example, while typing 'person' (Q1), the lexical entries L1 and L2 are found and provided to the user.

TR Discover uses three steps to translate the English sentence or fragment of a sentence into a SQL or SPARQL query. The first step parses the input question into a FOL representation. The query parsing uses the FCFG. For the example input, the token 'person' Fig. 12 The parse tree for the FOL representation of the input question 'person directing Inglourious Basterds'. will be parsed by the lexical entry L1 and the token 'directing' will be parsed with the lexical entry L2. This leads to the FOL representation:

How exactly the phrase 'Inglourious Basterds' is matched to the base data and therefore can be used as part of the lexical entry L2 and how it is resolved, is not explained by Song et al (2015). If there are multiple possibilities to parse the input question, the first one is chosen.

The next step is to translate the generated FOL into a parse tree. The FOL parser takes a grammar and the FOL representation from the previous step, and generates a parse tree ( Figure 12) using ANTLER for implementation.

In the third step, an in-order traversal of the parse tree (provided by the previous step) is performed to translate it into an executable SQL or SPARQL query. While traversing the parse tree, the atomic logical conditions and connectors are put on a stack. After traversing, the constraints are popped from the stack to build the correct query constraints. The predicates are mapped to their corresponding attribute names (SQL) or ontology properties (SPARQL).

The strengths of TR Discover are the auto-suggestion and the possibility to translate natural language into different query languages such as SQL and SPARQL, because FOL is used as an intermediate language.

The weaknesses of TR Discover are that quantifiers (e.g., Q3: 'grossed most') cannot be used, synonyms are not properly handled, and negations only work for SPARQL. Song et al (2015) suggest extending TR Discover with a ranking system for multiple parses in the first step and to improve the synonym handling. Furthermore, they pointed out the possibility of applying user query logs to improve auto-suggestions.","[['b49'], [], [], [], ['b49'], [], [], [], ['b49'], ['b49'], [], [], [], ['b49'], [], [], [], ['b49']]","[['b49'], [], [], [], ['b49'], [], [], [], ['b49'], ['b49'], [], [], [], ['b49'], [], [], [], ['b49']]",6,"sent1: TR Discover (Song et al, 2015) is a system providing an NLI which translates the input question in form of an English sentence (or sentence fragment) into SQL or SPARQL.
sent2: It is either used for relational databases or ontologies, but does not need an ontology to work for relational databases.
sent3: During the translation steps, TR Discover uses a First Order Logic (FOL) representation as an intermediate language.
sent4: Furthermore, it provides auto-suggestions based on the user input.
sent5: There are two types of suggestions: autocompletion and prediction.
sent6: TR Discover helps the users in formulating the question through an auto-suggestion feature.
sent7: For example, assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1).
sent8: When the users start typing 'p', TR Discover will not only suggest 'person' but also longer phrases like 'person directing' (autocomplete).
sent9: After 'person directing' is selected (or typed), TR Discover will again suggest phrases, like 'movies' or even specific movies like 'Inglourious Basterds' (prediction).
sent10: For input question Q1, the input could be 'person directing Inglourious Basterds'.
sent11: The suggestions are based upon the relationships and entities in the dataset and use the linguistic constraints encoded in a feature-based context-free grammar (FCFG).
sent12: The grammar consists of grammar rules (G1-3) and lexical entries (L1-2).
sent13: For the sample world (and the input question Q1), the following rules could be defined: The suggestions are computed based on the idea of left-corner parsing: given a query segment, it finds all grammar rules whose left corner on the right side matches the left side of the lexical entry of the query segment.
sent14: Then, all leaf nodes (lexical entries) in the grammar that can be reached by using the adjacent element are found.
sent15: For example, while typing 'person' (Q1), the lexical entries L1 and L2 are found and provided to the user.
sent16: TR Discover uses three steps to translate the English sentence or fragment of a sentence into a SQL or SPARQL query.
sent17: The first step parses the input question into a FOL representation.
sent18: The query parsing uses the FCFG.
sent19: For the example input, the token 'person' Fig. 12 The parse tree for the FOL representation of the input question 'person directing Inglourious Basterds'. will be parsed by the lexical entry L1 and the token 'directing' will be parsed with the lexical entry L2.
sent20: This leads to the FOL representation:How exactly the phrase 'Inglourious Basterds' is matched to the base data and therefore can be used as part of the lexical entry L2 and how it is resolved, is not explained by Song et al (2015).
sent21: If there are multiple possibilities to parse the input question, the first one is chosen.
sent22: The next step is to translate the generated FOL into a parse tree.
sent23: The FOL parser takes a grammar and the FOL representation from the previous step, and generates a parse tree ( Figure 12) using ANTLER for implementation.
sent24: In the third step, an in-order traversal of the parse tree (provided by the previous step) is performed to translate it into an executable SQL or SPARQL query.
sent25: While traversing the parse tree, the atomic logical conditions and connectors are put on a stack.
sent26: After traversing, the constraints are popped from the stack to build the correct query constraints.
sent27: The predicates are mapped to their corresponding attribute names (SQL) or ontology properties (SPARQL).
sent28: The strengths of TR Discover are the auto-suggestion and the possibility to translate natural language into different query languages such as SQL and SPARQL, because FOL is used as an intermediate language.
sent29: The weaknesses of TR Discover are that quantifiers (e.g., Q3: 'grossed most') cannot be used, synonyms are not properly handled, and negations only work for SPARQL.
sent30: Song et al (2015) suggest extending TR Discover with a ranking system for multiple parses in the first step and to improve the synonym handling.
sent31: Furthermore, they pointed out the possibility of applying user query logs to improve auto-suggestions.
sent32: TR Discover (Song et al, 2015) is a system providing an NLI which translates the input question in form of an English sentence (or sentence fragment) into SQL or SPARQL.
sent33: It is either used for relational databases or ontologies, but does not need an ontology to work for relational databases.
sent34: During the translation steps, TR Discover uses a First Order Logic (FOL) representation as an intermediate language.
sent35: Furthermore, it provides auto-suggestions based on the user input.
sent36: There are two types of suggestions: autocompletion and prediction.
sent37: TR Discover helps the users in formulating the question through an auto-suggestion feature.
sent38: For example, assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1).
sent39: When the users start typing 'p', TR Discover will not only suggest 'person' but also longer phrases like 'person directing' (autocomplete).
sent40: After 'person directing' is selected (or typed), TR Discover will again suggest phrases, like 'movies' or even specific movies like 'Inglourious Basterds' (prediction).
sent41: For input question Q1, the input could be 'person directing Inglourious Basterds'.
sent42: The suggestions are based upon the relationships and entities in the dataset and use the linguistic constraints encoded in a feature-based context-free grammar (FCFG).
sent43: The grammar consists of grammar rules (G1-3) and lexical entries (L1-2).
sent44: For the sample world (and the input question Q1), the following rules could be defined: The suggestions are computed based on the idea of left-corner parsing: given a query segment, it finds all grammar rules whose left corner on the right side matches the left side of the lexical entry of the query segment.
sent45: Then, all leaf nodes (lexical entries) in the grammar that can be reached by using the adjacent element are found.
sent46: For example, while typing 'person' (Q1), the lexical entries L1 and L2 are found and provided to the user.
sent47: TR Discover uses three steps to translate the English sentence or fragment of a sentence into a SQL or SPARQL query.
sent48: The first step parses the input question into a FOL representation.
sent49: The query parsing uses the FCFG.
sent50: For the example input, the token 'person' Fig. 12 The parse tree for the FOL representation of the input question 'person directing Inglourious Basterds'. will be parsed by the lexical entry L1 and the token 'directing' will be parsed with the lexical entry L2.
sent51: This leads to the FOL representation:How exactly the phrase 'Inglourious Basterds' is matched to the base data and therefore can be used as part of the lexical entry L2 and how it is resolved, is not explained by Song et al (2015).
sent52: If there are multiple possibilities to parse the input question, the first one is chosen.
sent53: The next step is to translate the generated FOL into a parse tree.
sent54: The FOL parser takes a grammar and the FOL representation from the previous step, and generates a parse tree ( Figure 12) using ANTLER for implementation.
sent55: In the third step, an in-order traversal of the parse tree (provided by the previous step) is performed to translate it into an executable SQL or SPARQL query.
sent56: While traversing the parse tree, the atomic logical conditions and connectors are put on a stack.
sent57: After traversing, the constraints are popped from the stack to build the correct query constraints.
sent58: The predicates are mapped to their corresponding attribute names (SQL) or ontology properties (SPARQL).
sent59: The strengths of TR Discover are the auto-suggestion and the possibility to translate natural language into different query languages such as SQL and SPARQL, because FOL is used as an intermediate language.
sent60: The weaknesses of TR Discover are that quantifiers (e.g., Q3: 'grossed most') cannot be used, synonyms are not properly handled, and negations only work for SPARQL.
sent61: Song et al (2015) suggest extending TR Discover with a ranking system for multiple parses in the first step and to improve the synonym handling.
sent62: Furthermore, they pointed out the possibility of applying user query logs to improve auto-suggestions."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s113,SQUALL (Semantic Query and Update High-Level Language),"SQUALL (Ferré, 2014(Ferré, , 2011 is an NLI for searching and updating an RDF store. It uses the style of Montague grammars (context-free generative grammar) as an intermediate language (similar to TR Discover) to split the translation process in two parts: translating the natural language input question into a logical language and translating the logical language into a query language. Because of that, the second part is getting easier: the logical language and the query language share the same semantics and level of detail. The grammar of SQUALL consists of about 120 domain-independent rules.

The translation into the logical form is done in three steps. In the first step, the keywords are recognized (lookup step). The second step is a syntactic analysis based on a descending parser, which is fed with the grammar rules. Afterwards, the next step can generate the logical language based on the definition in the grammar. After the translation into the logical language, the translation in to the chosen formal language can be done.

The strength of SQUALL is that it is able to translate any type of input question, including aggregations, negations and subqueries. The weakness of SQUALL is that the users have to know the RDF vocabulary (e.g., classes and properties). For example, the input question Q1 needs to be formulated as 'Who is the director of Inglourious Basterds? '.

SQUALL (Ferré, 2014(Ferré, , 2011 is an NLI for searching and updating an RDF store. It uses the style of Montague grammars (context-free generative grammar) as an intermediate language (similar to TR Discover) to split the translation process in two parts: translating the natural language input question into a logical language and translating the logical language into a query language. Because of that, the second part is getting easier: the logical language and the query language share the same semantics and level of detail. The grammar of SQUALL consists of about 120 domain-independent rules.

The translation into the logical form is done in three steps. In the first step, the keywords are recognized (lookup step). The second step is a syntactic analysis based on a descending parser, which is fed with the grammar rules. Afterwards, the next step can generate the logical language based on the definition in the grammar. After the translation into the logical language, the translation in to the chosen formal language can be done.

The strength of SQUALL is that it is able to translate any type of input question, including aggregations, negations and subqueries. The weakness of SQUALL is that the users have to know the RDF vocabulary (e.g., classes and properties). For example, the input question Q1 needs to be formulated as 'Who is the director of Inglourious Basterds? '.","[['b16', 'b17'], [], [], ['b16', 'b17'], [], []]","[['b16', 'b17'], [], [], ['b16', 'b17'], [], []]",4,"sent1: SQUALL (Ferré, 2014(Ferré, , 2011 is an NLI for searching and updating an RDF store. It uses the style of Montague grammars (context-free generative grammar) as an intermediate language (similar to TR Discover) to split the translation process in two parts: translating the natural language input question into a logical language and translating the logical language into a query language.
sent2: Because of that, the second part is getting easier: the logical language and the query language share the same semantics and level of detail.
sent3: The grammar of SQUALL consists of about 120 domain-independent rules.
sent4: The translation into the logical form is done in three steps.
sent5: In the first step, the keywords are recognized (lookup step).
sent6: The second step is a syntactic analysis based on a descending parser, which is fed with the grammar rules.
sent7: Afterwards, the next step can generate the logical language based on the definition in the grammar.
sent8: After the translation into the logical language, the translation in to the chosen formal language can be done.
sent9: The strength of SQUALL is that it is able to translate any type of input question, including aggregations, negations and subqueries.
sent10: The weakness of SQUALL is that the users have to know the RDF vocabulary (e.g., classes and properties).
sent11: For example, the input question Q1 needs to be formulated as 'Who is the director of Inglourious Basterds? '.
sent12: SQUALL (Ferré, 2014(Ferré, , 2011 is an NLI for searching and updating an RDF store. It uses the style of Montague grammars (context-free generative grammar) as an intermediate language (similar to TR Discover) to split the translation process in two parts: translating the natural language input question into a logical language and translating the logical language into a query language.
sent13: Because of that, the second part is getting easier: the logical language and the query language share the same semantics and level of detail.
sent14: The grammar of SQUALL consists of about 120 domain-independent rules.
sent15: The translation into the logical form is done in three steps.
sent16: In the first step, the keywords are recognized (lookup step).
sent17: The second step is a syntactic analysis based on a descending parser, which is fed with the grammar rules.
sent18: Afterwards, the next step can generate the logical language based on the definition in the grammar.
sent19: After the translation into the logical language, the translation in to the chosen formal language can be done.
sent20: The strength of SQUALL is that it is able to translate any type of input question, including aggregations, negations and subqueries.
sent21: The weakness of SQUALL is that the users have to know the RDF vocabulary (e.g., classes and properties).
sent22: For example, the input question Q1 needs to be formulated as 'Who is the director of Inglourious Basterds? '."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s119,Evaluation of 24 recently developed NLIs,"In this section, we provide a systematic analysis of the major NLIs. We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity. Each system category, based on its technical approach, has its own strengths and weaknesses. There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.

We evaluate the systems based on what is reported in the papers. If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3. If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L). If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with . If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3. In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1). This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them. Therefore, they do not expect any complexer questions like Q4 or Q7. Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7). However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10). For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery. Trigger words are not sufficient to identify the range of each subquery.

Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR). This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified. Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.

Grammar-based systems offer the possibility to guide the users during the formulation of their questions. Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language. The huge disadvantage of grammar-based systems is that they need handcrafted rules. There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL). The general rules can be used for other domains and therefore increase the adaptability of the system. Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).

We will now analyze how well the different systems can handle the ten sample questions. The first question is a basic filter question and can be solved by all NLIs as shown in Table 3. The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others). Complex questions (e.g., aggregations) cannot be phrased with keywords only. Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences. In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords. Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible. They adapted their systems so that they can handle different forms of user input. Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution. This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors. Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms. This approach can be used to answer questions formulated as keywords or as complete sentences. Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR). The identification of possible subqueries is necessary to answer questions like Q9 and Q10.

Based on the sample questions, SQUALL, SPARK-LIS, NaLIR and ATHENA are the systems that per- 

In this section, we provide a systematic analysis of the major NLIs. We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity. Each system category, based on its technical approach, has its own strengths and weaknesses. There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.

We evaluate the systems based on what is reported in the papers. If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3. If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L). If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with . If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3. In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1). This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them. Therefore, they do not expect any complexer questions like Q4 or Q7. Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7). However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10). For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery. Trigger words are not sufficient to identify the range of each subquery.

Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR). This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified. Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.

Grammar-based systems offer the possibility to guide the users during the formulation of their questions. Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language. The huge disadvantage of grammar-based systems is that they need handcrafted rules. There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL). The general rules can be used for other domains and therefore increase the adaptability of the system. Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).

We will now analyze how well the different systems can handle the ten sample questions. The first question is a basic filter question and can be solved by all NLIs as shown in Table 3. The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others). Complex questions (e.g., aggregations) cannot be phrased with keywords only. Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences. In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords. Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible. They adapted their systems so that they can handle different forms of user input. Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution. This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors. Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms. This approach can be used to answer questions formulated as keywords or as complete sentences. Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR). The identification of possible subqueries is necessary to answer questions like Q9 and Q10.

Based on the sample questions, SQUALL, SPARK-LIS, NaLIR and ATHENA are the systems that per- ","[[], [], [], [], ['b53', 'b30'], [], [], [], [], [], ['b53', 'b30'], []]","[[], [], [], [], ['b53', 'b30'], [], [], [], [], [], ['b53', 'b30'], []]",4,"sent1: In this section, we provide a systematic analysis of the major NLIs.
sent2: We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity.
sent3: Each system category, based on its technical approach, has its own strengths and weaknesses.
sent4: There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.
sent5: We evaluate the systems based on what is reported in the papers.
sent6: If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3.
sent7: If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L).
sent8: If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with .
sent9: If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3.
sent10: In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1).
sent11: This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them.
sent12: Therefore, they do not expect any complexer questions like Q4 or Q7.
sent13: Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7).
sent14: However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10).
sent15: For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery.
sent16: Trigger words are not sufficient to identify the range of each subquery.
sent17: Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR).
sent18: This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified.
sent19: Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.
sent20: Grammar-based systems offer the possibility to guide the users during the formulation of their questions.
sent21: Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language.
sent22: The huge disadvantage of grammar-based systems is that they need handcrafted rules.
sent23: There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL).
sent24: The general rules can be used for other domains and therefore increase the adaptability of the system.
sent25: Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).
sent26: We will now analyze how well the different systems can handle the ten sample questions.
sent27: The first question is a basic filter question and can be solved by all NLIs as shown in Table 3.
sent28: The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others).
sent29: Complex questions (e.g., aggregations) cannot be phrased with keywords only.
sent30: Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences.
sent31: In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords.
sent32: Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible.
sent33: They adapted their systems so that they can handle different forms of user input.
sent34: Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution.
sent35: This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors.
sent36: Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms.
sent37: This approach can be used to answer questions formulated as keywords or as complete sentences.
sent38: Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR).
sent39: The identification of possible subqueries is necessary to answer questions like Q9 and Q10.
sent40: Based on the sample questions, SQUALL, SPARK-LIS, NaLIR and ATHENA are the systems that per-
sent41: In this section, we provide a systematic analysis of the major NLIs.
sent42: We categorized and analyzed the translation process of the 24 recently developed systems highlighted in Section 5 based on ten sample world questions with increasing complexity.
sent43: Each system category, based on its technical approach, has its own strengths and weaknesses.
sent44: There are also different aspects on which a system can focus (e.g., number of user interaction, ambiguity, efficiency, etc.), which we do not take into account in this paper.
sent45: We evaluate the systems based on what is reported in the papers.
sent46: If there is either an example of a similar question (e.g. 'Who is the president of the united states?' (Q1) or a clear statement written in the paper (e.g., 'we can identify aggregations' (Q7), we label those questions for the system with a checkmark () in Table  3.
sent47: If the question needs to be asked in a strict syntactical way (e.g., SODA needs the symbol '¿' instead of 'higher than') or the answer is partially correct (e.g., Q4 returns a ordered list of movies instead of only one), it is labeled with a triangle (L).
sent48: If there is a clear statement that something is not implemented (e.g. ATHENA does not support negations), we label it with .
sent49: If we were not able to conclude, if a system can or cannot answer a question based on the paper, we labeled it with a question mark in Table 3.
sent50: In general, as shown in Table 3, we can say that keyword-based NLIs are the least powerful and can only answer simple questions like string filters (Q1).
sent51: This limitation is based on the approach of these keyword-based systems: they expect just keywords (which are mostly filters) and the systems identify relationships between them.
sent52: Therefore, they do not expect any complexer questions like Q4 or Q7.
sent53: Pattern-based NLIs are an extension of keyword-based systems in such a way that they have a dictionary with trigger words to answer more complex questions like aggregations (Q7).
sent54: However, they cannot answer questions of higher difficulties, including subqueries (Q9/Q10).
sent55: For example, the difficulty with questions including subqueries is to identify which part of the input question belongs to which subquery.
sent56: Trigger words are not sufficient to identify the range of each subquery.
sent57: Parsing-based NLIs are able to answer these questions by using dependency or constituency trees (e.g., NaLIR).
sent58: This helps to identify and group the input question in such a way that the different parts of the subqueries can be identified.
sent59: Still, some of those systems struggle with the same problem as pattern-based NLIs: the systems are using trigger word dictionaries to identify certain types of questions like aggregations (e.g., ATHENA), which is not a general solution of the problem.
sent60: Grammar-based systems offer the possibility to guide the users during the formulation of their questions.
sent61: Dynamically applying the rules during typing allows the systems to ensure that the input question is always translatable into formal language.
sent62: The huge disadvantage of grammar-based systems is that they need handcrafted rules.
sent63: There are NLIs which use a set of general rules and domain-specific rules (e.g., SQUALL).
sent64: The general rules can be used for other domains and therefore increase the adaptability of the system.
sent65: Other systems try to extract the rules directly from the ontology and thereby reduce the number of handcrafted rules (e.g., Ginseng).
sent66: We will now analyze how well the different systems can handle the ten sample questions.
sent67: The first question is a basic filter question and can be solved by all NLIs as shown in Table 3.
sent68: The last three questions are the most difficult ones and can only be answered by a few systems (completely by SQUALL, SPARKLIS and partially by others).
sent69: Complex questions (e.g., aggregations) cannot be phrased with keywords only.
sent70: Therefore, the more complicated the users questions are, the more they will phrase them in grammatically correct sentences.
sent71: In contrast, simple questions (e.g., string filters like Q1) can be easily asked with keywords.
sent72: Both, Waltinger et al (2013) (USI Answer) and Lawrence and Riezler (2016) (NLmaps) are describing this phenomenon and that the users prefer to ask questions with keywords if possible.
sent73: They adapted their systems so that they can handle different forms of user input.
sent74: Because of that, Waltinger et al (2013) (USI Answer) point out that parse trees should only be used with caution.
sent75: This is similar to the approach of Zheng et al (2017) (NLQ/A), who remark that NLP technologies are not worth the risk, because wrong interpretations in the processing leads to errors.
sent76: Walter et al (2012) (BELA) propose a new approach of applying certain processing steps only if the question cannot be answered by using simpler mechanisms.
sent77: This approach can be used to answer questions formulated as keywords or as complete sentences.
sent78: Nevertheless, parse trees are useful to identify subqueries, but only in grammatically correct sentences (e.g., NaLIR).
sent79: The identification of possible subqueries is necessary to answer questions like Q9 and Q10.
sent80: Based on the sample questions, SQUALL, SPARK-LIS, NaLIR and ATHENA are the systems that per-"
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s121,Machine Learning Approaches for NLIs,"In current research, more and more systems include machine learning (ML) in their translation process (e.g., MEANS) or ranking (e.g., Aqqu).

KBQA (Cui et al, 2017) learns templates as a kind of question representation. It supports binary factoid questions as well as complex questions which are composed of binary factoid questions. OQA (Fader et al, 2014) approaches to leverage both curated and extracted KBs, by mining millions of rules from an unlabeled question corpus and across multiple KBs.

AMUSE (Hakimov et al, 2017) uses ML to determine the most probable meaning of a question and can handle different languages at the same time. Xser (Xu et al, 2014) divides the translation task into a KB-independent and a KB-related task. In the KBindependent task, they are developing a Directed Acyclic Graph parser to capture the structure of the query intentions and trained it on human labeled data.

A new promising avenue of research is to use deep learning techniques as the foundation for NLIDBs. The basic idea is to formulate the translation of natural language (NL) to SQL as an end-to-end machine translation problem (Sutskever et al (2014); Dong and Lapata (2016); Jia and Liang (2016)). The approach is often called neural semantic parsing (Wang et al (2015)). In other words, translating from NL to SQL can be formulated as a supervised machine learning problem on pairs of natural language and SQL queries. In particular, machine translation can be modeled as a sequenceto-sequence problem where the input sequence is represented by the words (tokens) of the NL and the output sequence by the tokens of SQL. The main goal is given an input sequence of tokens, predict the output sequence based on observed patterns in the past.

The main advantage of machine learning based approaches over traditional NLIDBs is that they support a richer linguistic variability in query expressions and thus users can formulate queries with greater flexibility. However, one of the major challenges of supervised machine learning approaches is that they require a large training data set in order to achieve good accuracy on the translation task.

The most commonly used approach for sequenceto-sequence modeling is based on recurrent neural networks (RNNs, Elman (1990)) with an input encoder and an output decoder. The encoder and decoder are implemented as bi-directional LSTMs (Long Short Term Memory) by Hochreiter and Schmidhuber (1997). However, before an NL can be encoded, the tokens need to be represented as a vector that in turn can be processed by the neural network. A widely used approach is to use word embeddings where the vocabulary of the NL is mapped to a high-dimensional vector (Pennington et al (2014)). In order to improve the accuracy of the translation, attention models are often applied (Luong et al (2015)).

One of the currently most advanced neural machine translation systems was introduced by Iyer et al (2017). Their approach uses an encoder-decoder model with global attention similar to Luong et al (2015) and a bi-directional LSTM network to encode the input tokens. In order to improve the translation accuracy from NL to SQL compared to traditional machine translation approaches, the database schema is taken into account. Moreover, external paraphrases are used to increase the linguistic variance of the training data. In the first step of training the neural machine translation system, the process of generating training data is bootstrapped by manually-handcrafted templates for the NL and SQL query pairs. In the next phase, the training set is extended by adding linguistic variations of the input questions and parts to the query are replaced with synonyms or paraphrases of the query. The advantage of this approach is that it is query language independent and could in principle also be used to translate from NL to SPARQL. However, the disadvantage is that a large, manually handcrafted a training set is necessary.

Zhong et al (2017) introduce a system called Seq2SQL. Their approach uses a deep neural network architecture with reinforcement learning to translate from NL to SQL. The authors released WikiSQL -a new data set based on Wikipedia consisting of 24,241 tables and 80,654 hand-annotated NL-SQL-pairs. However, their approach was only demonstrated to work on simple single-table queries without joins. SQLNet by Xu et al (2017) uses a more traditional machine translation approach without reinforcement learning. However, even though SQLNet shows better accuracy than Seq2SQL, the experiments are also based on the WikiSQL data set and it is not clear how this approach would handle join queries against more realistic database settings with multiple tables. Finally, Yavuz et al (2018) show further improvements over SQLNet by incorporating both the information about the database schema as well as the base data. The paper in particular focuses on the generation of WHERE-clauses, which the authors identified as major problem of the relative low accuracy of SQL queries generated by Seq2SQL and SQLNet.

DBPal by Basik et al (2018) overcomes shortcomings of manually labeling large training data sets by synthetically generating a training set that only requires minimal annotations in the database. Similar to Iyer et al (2017), DBPal uses the database schema and query templates to describe NL/SQL-pairs. Moreover, inspired by Wang et al (2015), DBPal augments an initial set of NL queries using a paraphrasing approach based on a paraphrase dictionary. The results show that on a single-table data set DPal performs better than the semantic parsing approach of Iyer et al (2017). However, for a multi-table data set with join queries, DBPal performs worse. The reason is that the limited training set does not seem to generalize well for join-queries. The authors attributed the good performance of the system introduced by Iyer et al (2017) to overfitting. Soru et al (2017) use neural machine translation approach similar to Iyer et al (2017). However, the major difference is that they translate natural language to SPARQL rather than to SQL. Moreover, they do not apply an attention mechanism. In a subsequent white paper Hartmann et al (2018) present an approach to automatically generate a large set of training data consisting of 894,499 training examples based on set of 5000 natural language queries. The approach is very similar to the approach used by DBPal.

In general, these new approaches show promising results, but they have either only been demonstrated to work for single-table data sets or require large amounts training data. Hence, the practical usage in realistic database settings still needs to be shown.

Another interesting new trend is in the area of conversational systems such as Williams et al (2015); John et al (2017) that often apply neural machine translation techniques. However, a detailed discussion on these systems is beyond the scope of this paper.

In current research, more and more systems include machine learning (ML) in their translation process (e.g., MEANS) or ranking (e.g., Aqqu).

KBQA (Cui et al, 2017) learns templates as a kind of question representation. It supports binary factoid questions as well as complex questions which are composed of binary factoid questions. OQA (Fader et al, 2014) approaches to leverage both curated and extracted KBs, by mining millions of rules from an unlabeled question corpus and across multiple KBs.

AMUSE (Hakimov et al, 2017) uses ML to determine the most probable meaning of a question and can handle different languages at the same time. Xser (Xu et al, 2014) divides the translation task into a KB-independent and a KB-related task. In the KBindependent task, they are developing a Directed Acyclic Graph parser to capture the structure of the query intentions and trained it on human labeled data.

A new promising avenue of research is to use deep learning techniques as the foundation for NLIDBs. The basic idea is to formulate the translation of natural language (NL) to SQL as an end-to-end machine translation problem (Sutskever et al (2014); Dong and Lapata (2016); Jia and Liang (2016)). The approach is often called neural semantic parsing (Wang et al (2015)). In other words, translating from NL to SQL can be formulated as a supervised machine learning problem on pairs of natural language and SQL queries. In particular, machine translation can be modeled as a sequenceto-sequence problem where the input sequence is represented by the words (tokens) of the NL and the output sequence by the tokens of SQL. The main goal is given an input sequence of tokens, predict the output sequence based on observed patterns in the past.

The main advantage of machine learning based approaches over traditional NLIDBs is that they support a richer linguistic variability in query expressions and thus users can formulate queries with greater flexibility. However, one of the major challenges of supervised machine learning approaches is that they require a large training data set in order to achieve good accuracy on the translation task.

The most commonly used approach for sequenceto-sequence modeling is based on recurrent neural networks (RNNs, Elman (1990)) with an input encoder and an output decoder. The encoder and decoder are implemented as bi-directional LSTMs (Long Short Term Memory) by Hochreiter and Schmidhuber (1997). However, before an NL can be encoded, the tokens need to be represented as a vector that in turn can be processed by the neural network. A widely used approach is to use word embeddings where the vocabulary of the NL is mapped to a high-dimensional vector (Pennington et al (2014)). In order to improve the accuracy of the translation, attention models are often applied (Luong et al (2015)).

One of the currently most advanced neural machine translation systems was introduced by Iyer et al (2017). Their approach uses an encoder-decoder model with global attention similar to Luong et al (2015) and a bi-directional LSTM network to encode the input tokens. In order to improve the translation accuracy from NL to SQL compared to traditional machine translation approaches, the database schema is taken into account. Moreover, external paraphrases are used to increase the linguistic variance of the training data. In the first step of training the neural machine translation system, the process of generating training data is bootstrapped by manually-handcrafted templates for the NL and SQL query pairs. In the next phase, the training set is extended by adding linguistic variations of the input questions and parts to the query are replaced with synonyms or paraphrases of the query. The advantage of this approach is that it is query language independent and could in principle also be used to translate from NL to SPARQL. However, the disadvantage is that a large, manually handcrafted a training set is necessary.

Zhong et al (2017) introduce a system called Seq2SQL. Their approach uses a deep neural network architecture with reinforcement learning to translate from NL to SQL. The authors released WikiSQL -a new data set based on Wikipedia consisting of 24,241 tables and 80,654 hand-annotated NL-SQL-pairs. However, their approach was only demonstrated to work on simple single-table queries without joins. SQLNet by Xu et al (2017) uses a more traditional machine translation approach without reinforcement learning. However, even though SQLNet shows better accuracy than Seq2SQL, the experiments are also based on the WikiSQL data set and it is not clear how this approach would handle join queries against more realistic database settings with multiple tables. Finally, Yavuz et al (2018) show further improvements over SQLNet by incorporating both the information about the database schema as well as the base data. The paper in particular focuses on the generation of WHERE-clauses, which the authors identified as major problem of the relative low accuracy of SQL queries generated by Seq2SQL and SQLNet.

DBPal by Basik et al (2018) overcomes shortcomings of manually labeling large training data sets by synthetically generating a training set that only requires minimal annotations in the database. Similar to Iyer et al (2017), DBPal uses the database schema and query templates to describe NL/SQL-pairs. Moreover, inspired by Wang et al (2015), DBPal augments an initial set of NL queries using a paraphrasing approach based on a paraphrase dictionary. The results show that on a single-table data set DPal performs better than the semantic parsing approach of Iyer et al (2017). However, for a multi-table data set with join queries, DBPal performs worse. The reason is that the limited training set does not seem to generalize well for join-queries. The authors attributed the good performance of the system introduced by Iyer et al (2017) to overfitting. Soru et al (2017) use neural machine translation approach similar to Iyer et al (2017). However, the major difference is that they translate natural language to SPARQL rather than to SQL. Moreover, they do not apply an attention mechanism. In a subsequent white paper Hartmann et al (2018) present an approach to automatically generate a large set of training data consisting of 894,499 training examples based on set of 5000 natural language queries. The approach is very similar to the approach used by DBPal.

In general, these new approaches show promising results, but they have either only been demonstrated to work for single-table data sets or require large amounts training data. Hence, the practical usage in realistic database settings still needs to be shown.

Another interesting new trend is in the area of conversational systems such as Williams et al (2015); John et al (2017) that often apply neural machine translation techniques. However, a detailed discussion on these systems is beyond the scope of this paper.","[[], ['b9', 'b15'], ['b61', 'b22'], ['b56', 'b12', 'b26'], [], ['b14', 'b36', 'b42', 'b24'], ['b36'], ['b62', 'b63'], ['b50', 'b56', 'b25', 'b23', 'b0'], [], ['b27', 'b59'], [], ['b9', 'b15'], ['b61', 'b22'], ['b56', 'b12', 'b26'], [], ['b14', 'b36', 'b42', 'b24'], ['b36'], ['b62', 'b63'], ['b50', 'b56', 'b25', 'b23', 'b0'], [], ['b27', 'b59']]","[[], ['b9', 'b15'], ['b61', 'b22'], ['b56', 'b12', 'b26'], [], ['b14', 'b36', 'b42', 'b24'], ['b36'], ['b62', 'b63'], ['b50', 'b56', 'b25', 'b23', 'b0'], [], ['b27', 'b59'], [], ['b9', 'b15'], ['b61', 'b22'], ['b56', 'b12', 'b26'], [], ['b14', 'b36', 'b42', 'b24'], ['b36'], ['b62', 'b63'], ['b50', 'b56', 'b25', 'b23', 'b0'], [], ['b27', 'b59']]",42,"sent1: In current research, more and more systems include machine learning (ML) in their translation process (e.g., MEANS) or ranking (e.g., Aqqu).
sent2: KBQA (Cui et al, 2017) learns templates as a kind of question representation.
sent3: It supports binary factoid questions as well as complex questions which are composed of binary factoid questions.
sent4: OQA (Fader et al, 2014) approaches to leverage both curated and extracted KBs, by mining millions of rules from an unlabeled question corpus and across multiple KBs.
sent5: AMUSE (Hakimov et al, 2017) uses ML to determine the most probable meaning of a question and can handle different languages at the same time.
sent6: Xser (Xu et al, 2014) divides the translation task into a KB-independent and a KB-related task.
sent7: In the KBindependent task, they are developing a Directed Acyclic Graph parser to capture the structure of the query intentions and trained it on human labeled data.
sent8: A new promising avenue of research is to use deep learning techniques as the foundation for NLIDBs.
sent9: The basic idea is to formulate the translation of natural language (NL) to SQL as an end-to-end machine translation problem (Sutskever et al (2014); Dong and Lapata (2016); Jia and Liang (2016)).
sent10: The approach is often called neural semantic parsing (Wang et al (2015)).
sent11: In other words, translating from NL to SQL can be formulated as a supervised machine learning problem on pairs of natural language and SQL queries.
sent12: In particular, machine translation can be modeled as a sequenceto-sequence problem where the input sequence is represented by the words (tokens) of the NL and the output sequence by the tokens of SQL.
sent13: The main goal is given an input sequence of tokens, predict the output sequence based on observed patterns in the past.
sent14: The main advantage of machine learning based approaches over traditional NLIDBs is that they support a richer linguistic variability in query expressions and thus users can formulate queries with greater flexibility.
sent15: However, one of the major challenges of supervised machine learning approaches is that they require a large training data set in order to achieve good accuracy on the translation task.
sent16: The most commonly used approach for sequenceto-sequence modeling is based on recurrent neural networks (RNNs, Elman (1990)) with an input encoder and an output decoder.
sent17: The encoder and decoder are implemented as bi-directional LSTMs (Long Short Term Memory) by Hochreiter and Schmidhuber (1997).
sent18: However, before an NL can be encoded, the tokens need to be represented as a vector that in turn can be processed by the neural network.
sent19: A widely used approach is to use word embeddings where the vocabulary of the NL is mapped to a high-dimensional vector (Pennington et al (2014)).
sent20: In order to improve the accuracy of the translation, attention models are often applied (Luong et al (2015)).
sent21: One of the currently most advanced neural machine translation systems was introduced by Iyer et al (2017).
sent22: Their approach uses an encoder-decoder model with global attention similar to Luong et al (2015) and a bi-directional LSTM network to encode the input tokens.
sent23: In order to improve the translation accuracy from NL to SQL compared to traditional machine translation approaches, the database schema is taken into account.
sent24: Moreover, external paraphrases are used to increase the linguistic variance of the training data.
sent25: In the first step of training the neural machine translation system, the process of generating training data is bootstrapped by manually-handcrafted templates for the NL and SQL query pairs.
sent26: In the next phase, the training set is extended by adding linguistic variations of the input questions and parts to the query are replaced with synonyms or paraphrases of the query.
sent27: The advantage of this approach is that it is query language independent and could in principle also be used to translate from NL to SPARQL.
sent28: However, the disadvantage is that a large, manually handcrafted a training set is necessary.
sent29: Zhong et al (2017) introduce a system called Seq2SQL.
sent30: Their approach uses a deep neural network architecture with reinforcement learning to translate from NL to SQL.
sent31: The authors released WikiSQL -a new data set based on Wikipedia consisting of 24,241 tables and 80,654 hand-annotated NL-SQL-pairs.
sent32: However, their approach was only demonstrated to work on simple single-table queries without joins.
sent33: SQLNet by Xu et al (2017) uses a more traditional machine translation approach without reinforcement learning.
sent34: However, even though SQLNet shows better accuracy than Seq2SQL, the experiments are also based on the WikiSQL data set and it is not clear how this approach would handle join queries against more realistic database settings with multiple tables.
sent35: Finally, Yavuz et al (2018) show further improvements over SQLNet by incorporating both the information about the database schema as well as the base data.
sent36: The paper in particular focuses on the generation of WHERE-clauses, which the authors identified as major problem of the relative low accuracy of SQL queries generated by Seq2SQL and SQLNet.
sent37: DBPal by Basik et al (2018) overcomes shortcomings of manually labeling large training data sets by synthetically generating a training set that only requires minimal annotations in the database.
sent38: Similar to Iyer et al (2017), DBPal uses the database schema and query templates to describe NL/SQL-pairs.
sent39: Moreover, inspired by Wang et al (2015), DBPal augments an initial set of NL queries using a paraphrasing approach based on a paraphrase dictionary.
sent40: The results show that on a single-table data set DPal performs better than the semantic parsing approach of Iyer et al (2017).
sent41: However, for a multi-table data set with join queries, DBPal performs worse.
sent42: The reason is that the limited training set does not seem to generalize well for join-queries.
sent43: The authors attributed the good performance of the system introduced by Iyer et al (2017) to overfitting.
sent44: Soru et al (2017) use neural machine translation approach similar to Iyer et al (2017).
sent45: However, the major difference is that they translate natural language to SPARQL rather than to SQL.
sent46: Moreover, they do not apply an attention mechanism.
sent47: In a subsequent white paper Hartmann et al (2018) present an approach to automatically generate a large set of training data consisting of 894,499 training examples based on set of 5000 natural language queries.
sent48: The approach is very similar to the approach used by DBPal.
sent49: In general, these new approaches show promising results, but they have either only been demonstrated to work for single-table data sets or require large amounts training data.
sent50: Hence, the practical usage in realistic database settings still needs to be shown.
sent51: Another interesting new trend is in the area of conversational systems such as Williams et al (2015); John et al (2017) that often apply neural machine translation techniques.
sent52: However, a detailed discussion on these systems is beyond the scope of this paper.
sent53: In current research, more and more systems include machine learning (ML) in their translation process (e.g., MEANS) or ranking (e.g., Aqqu).
sent54: KBQA (Cui et al, 2017) learns templates as a kind of question representation.
sent55: It supports binary factoid questions as well as complex questions which are composed of binary factoid questions.
sent56: OQA (Fader et al, 2014) approaches to leverage both curated and extracted KBs, by mining millions of rules from an unlabeled question corpus and across multiple KBs.
sent57: AMUSE (Hakimov et al, 2017) uses ML to determine the most probable meaning of a question and can handle different languages at the same time.
sent58: Xser (Xu et al, 2014) divides the translation task into a KB-independent and a KB-related task.
sent59: In the KBindependent task, they are developing a Directed Acyclic Graph parser to capture the structure of the query intentions and trained it on human labeled data.
sent60: A new promising avenue of research is to use deep learning techniques as the foundation for NLIDBs.
sent61: The basic idea is to formulate the translation of natural language (NL) to SQL as an end-to-end machine translation problem (Sutskever et al (2014); Dong and Lapata (2016); Jia and Liang (2016)).
sent62: The approach is often called neural semantic parsing (Wang et al (2015)).
sent63: In other words, translating from NL to SQL can be formulated as a supervised machine learning problem on pairs of natural language and SQL queries.
sent64: In particular, machine translation can be modeled as a sequenceto-sequence problem where the input sequence is represented by the words (tokens) of the NL and the output sequence by the tokens of SQL.
sent65: The main goal is given an input sequence of tokens, predict the output sequence based on observed patterns in the past.
sent66: The main advantage of machine learning based approaches over traditional NLIDBs is that they support a richer linguistic variability in query expressions and thus users can formulate queries with greater flexibility.
sent67: However, one of the major challenges of supervised machine learning approaches is that they require a large training data set in order to achieve good accuracy on the translation task.
sent68: The most commonly used approach for sequenceto-sequence modeling is based on recurrent neural networks (RNNs, Elman (1990)) with an input encoder and an output decoder.
sent69: The encoder and decoder are implemented as bi-directional LSTMs (Long Short Term Memory) by Hochreiter and Schmidhuber (1997).
sent70: However, before an NL can be encoded, the tokens need to be represented as a vector that in turn can be processed by the neural network.
sent71: A widely used approach is to use word embeddings where the vocabulary of the NL is mapped to a high-dimensional vector (Pennington et al (2014)).
sent72: In order to improve the accuracy of the translation, attention models are often applied (Luong et al (2015)).
sent73: One of the currently most advanced neural machine translation systems was introduced by Iyer et al (2017).
sent74: Their approach uses an encoder-decoder model with global attention similar to Luong et al (2015) and a bi-directional LSTM network to encode the input tokens.
sent75: In order to improve the translation accuracy from NL to SQL compared to traditional machine translation approaches, the database schema is taken into account.
sent76: Moreover, external paraphrases are used to increase the linguistic variance of the training data.
sent77: In the first step of training the neural machine translation system, the process of generating training data is bootstrapped by manually-handcrafted templates for the NL and SQL query pairs.
sent78: In the next phase, the training set is extended by adding linguistic variations of the input questions and parts to the query are replaced with synonyms or paraphrases of the query.
sent79: The advantage of this approach is that it is query language independent and could in principle also be used to translate from NL to SPARQL.
sent80: However, the disadvantage is that a large, manually handcrafted a training set is necessary.
sent81: Zhong et al (2017) introduce a system called Seq2SQL.
sent82: Their approach uses a deep neural network architecture with reinforcement learning to translate from NL to SQL.
sent83: The authors released WikiSQL -a new data set based on Wikipedia consisting of 24,241 tables and 80,654 hand-annotated NL-SQL-pairs.
sent84: However, their approach was only demonstrated to work on simple single-table queries without joins.
sent85: SQLNet by Xu et al (2017) uses a more traditional machine translation approach without reinforcement learning.
sent86: However, even though SQLNet shows better accuracy than Seq2SQL, the experiments are also based on the WikiSQL data set and it is not clear how this approach would handle join queries against more realistic database settings with multiple tables.
sent87: Finally, Yavuz et al (2018) show further improvements over SQLNet by incorporating both the information about the database schema as well as the base data.
sent88: The paper in particular focuses on the generation of WHERE-clauses, which the authors identified as major problem of the relative low accuracy of SQL queries generated by Seq2SQL and SQLNet.
sent89: DBPal by Basik et al (2018) overcomes shortcomings of manually labeling large training data sets by synthetically generating a training set that only requires minimal annotations in the database.
sent90: Similar to Iyer et al (2017), DBPal uses the database schema and query templates to describe NL/SQL-pairs.
sent91: Moreover, inspired by Wang et al (2015), DBPal augments an initial set of NL queries using a paraphrasing approach based on a paraphrase dictionary.
sent92: The results show that on a single-table data set DPal performs better than the semantic parsing approach of Iyer et al (2017).
sent93: However, for a multi-table data set with join queries, DBPal performs worse.
sent94: The reason is that the limited training set does not seem to generalize well for join-queries.
sent95: The authors attributed the good performance of the system introduced by Iyer et al (2017) to overfitting.
sent96: Soru et al (2017) use neural machine translation approach similar to Iyer et al (2017).
sent97: However, the major difference is that they translate natural language to SPARQL rather than to SQL.
sent98: Moreover, they do not apply an attention mechanism.
sent99: In a subsequent white paper Hartmann et al (2018) present an approach to automatically generate a large set of training data consisting of 894,499 training examples based on set of 5000 natural language queries.
sent100: The approach is very similar to the approach used by DBPal.
sent101: In general, these new approaches show promising results, but they have either only been demonstrated to work for single-table data sets or require large amounts training data.
sent102: Hence, the practical usage in realistic database settings still needs to be shown.
sent103: Another interesting new trend is in the area of conversational systems such as Williams et al (2015); John et al (2017) that often apply neural machine translation techniques.
sent104: However, a detailed discussion on these systems is beyond the scope of this paper."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s90,Keyword-based systems,"The core of keyword-based NLIs is their lookup step. In this step, the system tries to match the given keywords against an inverted index of the base and meta data. To identify keywords in the input question, some systems are using stop word removal (e.g., NLP-Reduce (Kaufmann et al, 2007)), others are expecting only keywords from the users as input (e.g., SODA (Blunschi et al, 2012)).

Most questions are easily formulated with keywords. However, there are some cases where keywords are not enough to express the intention of the users. For example, for the question 'What was the best movie of each genre? ' (Q7) the ""keyword-only version"" would be something like 'best movie genre', which is more likely to be interpreted as 'the genre of the best movie'. If the users would write the question like 'best movie by genre', a keyword-based NLI would try to lookup the token 'by' in the base and meta data or classify 'by' as a stop word and ignore it.

In the following, we will summarize seven keywordbased NLI. We decided to describe SODA (Blunschi et al, 2012) -as the first system -in depth, because it can solve the most of our sample input questions in this category (see Section 2.2). SODA is an NLI that expects only keywords from the user and can handle aggregations by using specific non-natural language templates. Afterwards, the other systems are summarized and we highlight the difference between them to SODA and each other.

The core of keyword-based NLIs is their lookup step. In this step, the system tries to match the given keywords against an inverted index of the base and meta data. To identify keywords in the input question, some systems are using stop word removal (e.g., NLP-Reduce (Kaufmann et al, 2007)), others are expecting only keywords from the users as input (e.g., SODA (Blunschi et al, 2012)).

Most questions are easily formulated with keywords. However, there are some cases where keywords are not enough to express the intention of the users. For example, for the question 'What was the best movie of each genre? ' (Q7) the ""keyword-only version"" would be something like 'best movie genre', which is more likely to be interpreted as 'the genre of the best movie'. If the users would write the question like 'best movie by genre', a keyword-based NLI would try to lookup the token 'by' in the base and meta data or classify 'by' as a stop word and ignore it.

In the following, we will summarize seven keywordbased NLI. We decided to describe SODA (Blunschi et al, 2012) -as the first system -in depth, because it can solve the most of our sample input questions in this category (see Section 2.2). SODA is an NLI that expects only keywords from the user and can handle aggregations by using specific non-natural language templates. Afterwards, the other systems are summarized and we highlight the difference between them to SODA and each other.","[['b29', 'b5'], [], ['b5'], ['b29', 'b5'], [], ['b5']]","[['b29', 'b5'], [], ['b5'], ['b29', 'b5'], [], ['b5']]",6,"sent1: The core of keyword-based NLIs is their lookup step.
sent2: In this step, the system tries to match the given keywords against an inverted index of the base and meta data.
sent3: To identify keywords in the input question, some systems are using stop word removal (e.g., NLP-Reduce (Kaufmann et al, 2007)), others are expecting only keywords from the users as input (e.g., SODA (Blunschi et al, 2012)).
sent4: Most questions are easily formulated with keywords.
sent5: However, there are some cases where keywords are not enough to express the intention of the users.
sent6: For example, for the question 'What was the best movie of each genre? '
sent7: (Q7) the ""keyword-only version"" would be something like 'best movie genre', which is more likely to be interpreted as 'the genre of the best movie'.
sent8: If the users would write the question like 'best movie by genre', a keyword-based NLI would try to lookup the token 'by' in the base and meta data or classify 'by' as a stop word and ignore it.
sent9: In the following, we will summarize seven keywordbased NLI.
sent10: We decided to describe SODA (Blunschi et al, 2012)
sent11: -as the first system -in depth, because it can solve the most of our sample input questions in this category (see Section 2.2).
sent12: SODA is an NLI that expects only keywords from the user and can handle aggregations by using specific non-natural language templates.
sent13: Afterwards, the other systems are summarized and we highlight the difference between them to SODA and each other.
sent14: The core of keyword-based NLIs is their lookup step.
sent15: In this step, the system tries to match the given keywords against an inverted index of the base and meta data.
sent16: To identify keywords in the input question, some systems are using stop word removal (e.g., NLP-Reduce (Kaufmann et al, 2007)), others are expecting only keywords from the users as input (e.g., SODA (Blunschi et al, 2012)).
sent17: Most questions are easily formulated with keywords.
sent18: However, there are some cases where keywords are not enough to express the intention of the users.
sent19: For example, for the question 'What was the best movie of each genre? '
sent20: (Q7) the ""keyword-only version"" would be something like 'best movie genre', which is more likely to be interpreted as 'the genre of the best movie'.
sent21: If the users would write the question like 'best movie by genre', a keyword-based NLI would try to lookup the token 'by' in the base and meta data or classify 'by' as a stop word and ignore it.
sent22: In the following, we will summarize seven keywordbased NLI.
sent23: We decided to describe SODA (Blunschi et al, 2012)
sent24: -as the first system -in depth, because it can solve the most of our sample input questions in this category (see Section 2.2).
sent25: SODA is an NLI that expects only keywords from the user and can handle aggregations by using specific non-natural language templates.
sent26: Afterwards, the other systems are summarized and we highlight the difference between them to SODA and each other."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s34,ATHENA,"ATHENA (Saha et al, 2016) is an ontology-driven NLI for relational databases, which handles full sentences in English as the input question. For ATHENA, ontologydriven means that it is based on the information of a given ontology and needs mapping between an ontology and a relational database. A set of synonyms can be associated with each ontology element. During the translation of an input question into a SQL query, ATHENA uses an intermediate query language before subsequently translating it into SQL.

Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for ATHENA could be: 'Who is the director of ""Inglourious Basterds""? '.

ATHENA uses four steps to translate a full sentence input question into a SQL query. In the first step, the ontology evidence annotator is used, which maps the input to a set of ontology elements. There are five types of possible matches: a. meta data: Finding a match in the inverted index for the meta data (and the associated set of synonyms). Longer matches over the input question are preferred if there are multiple matches. b. translation index : The translation index is an extension of the inverted index over the base data, which is enriched with variations for person and company names. For example, for the person name 'Brad Pitt' there would also be an entry 'B. Pitt'. c. time range expressions: Finding all time ranges like 'from 2000 until 2010 ' (Q5) with the TIMEX annotator. Those time ranges are then matched to the ontology properties with the corresponding data type. d. numeric expressions: Finding all tokens that include numeric quantities with the Stanford Numeric Expressions annotator. The numeric quantities can be either in the form of numbers (e.g., 9) or in text form (e.g., nine). Those numeric expressions are then matched to the ontology properties with the corresponding data type. e. dependencies: Annotating dependencies between tokens in the input question. For example, in the input question Q1 there is a dependency between the tokens 'director ' and 'Inglourious Basterds' indicated by the token 'of '. For the input question Q1 the meta data annotation will detect three different matches for 'director ', namely the table name Director and the attribute name Director.directorId and Directing.director-Id (Figure 10: red). The translation index will find a match for the bi-gram 'Inglourious Basterds', corresponding to the attribute Movie.Title (Figure 10: green).

The next step generates a ranked list of interpretations. An interpretation is a set of ontology elements provided by the previous step. If n ontology elements exist for one token, there will also be n different interpretations, one for each ontology element. For the given input question there are three different interpretations possible: {Directing.directorId, Movie.Title}, {Director.directorId, Movie.Title} and {Director, Movie.Title}. Each interpretation is represented by a set of interpretation trees. An interpretation tree (iTree) is a subtree of the ontology. Each iTree must satisfy: a. evidence cover : All tokens, which were annotated in the previous step, need to be covered. b. weak connectedness: All concepts need to be at least weakly connected through an undirected path and each property must be connected to its corresponding concept. For the first interpretation this means that Director and Movie need to be connected, for example, via the relation Directing. The attribute Title needs to be connected with the corresponding concept (in this case the table) Movie. c. inheritance constraint: No ontology element is allowed to inherit from its child concepts. For example, the ontology element Person is not allowed to inherit Role of Actor. The other direction is allowed, such that Actor inherits FirstName and LastName from Person. d. relationship constraint: All relationships given in the input question are included, not depending on the direction of the path. For example, the tree tokens 'movies', 'starring' and 'Brad Pitt' (Q5) imply a relationship constraint between the ontology element Movie, Starring and Person. Those three ontology elements need to be connected. Accordingly, in this example the ontology element Actor needs to be included. For the purpose of ranking the different interpretations, ATHENA generates one single iTree. It can consist of a union of multiple iTrees or a single iTree. Figure 11 shows a possible iTree for the interpretation {Director, Movie.Title}, which is extended with the ontology element Directing and Movie. After this step, for each interpretation only one iTree is left.

The third step uses the ranked list of interpretations to generate an intermediate query in the Ontology Query Language (OQL). OQL was specifically developed for ATHENA to be an intermediate language between the input question and SQL and is able to express queries that include aggregations, unions and single nested subqueries. The structure of an OQL query is similar to SQL and is generated as follows: a. from clause: Specifies all concepts found in the ontology along with their aliases. The aliases are needed, for example, if a concept occurs multiple times. For example, the input question 'Show me all drama and comedy movies.' (Q4) would point to Genre in the ontology twice: once for the token 'drama' and once for 'comedy'. Therefore, two aliases are needed to distinguish between them. b. group by clause: The group by clause is triggered by the word 'by' and only tokens annotated with meta data in step 1.a are considered. For example, the input question 'What was the best movie by genre? ' (modified Q7). To identify the dependencies between dependent and dependee (illustrated by the 'by'), the Stanford Dependency Parser is used. c. select clause: There are two possible types: aggregation and display properties. The aggregation properties depend on the group by clause. The default aggregation function is sum. For the (modified) input question Q7, ATHENA would detect a group by clause because the 'by genre' needs an aggregation function. Assuming ATHENA can translate 'best movie' to mean 'best ranked movie', it would apply the aggregation function max on Movie.Rating.

If there are no aggregations, ATHENA uses the tokens annotated with meta data as display properties, which are shown to the user. d. order by clause: Properties used in the order by clause are indicated by tokens like 'least', 'most', 'ordered by', 'top' and others. For example, the input question 'Which movie has grossed most? ' (Q3) would trigger an order by clause for Movie.Gross because of the trigger word 'most'. e. where clause: Tokens annotated with the translation index, time range or numerical expression are used in the where clause to filter the result (e.g., the tokens 'Inglourious Basterds'). If the filter is applied on an aggregation, a having clause is generated instead of the where clause.

The final step translates the OQL query into a SQL query, where each attribute and join condition is either concrete or virtual. Concrete means that a direct mapping between ontology and the database (e.g., 'director ') exist. Virtual implies that a (complex) relationship between the ontology elements and the database (e.g., 'great movie') exists. Furthermore, the result of the best ranked interpretation is directly displayed, but the users will see the other interpretations as well. All the top n interpretations that ATHENA has found are translated back into full sentences in English for the users, so that the users can choose the best fitting one.

The strengths of ATHENA are the ontology as an abstraction of the relational database and the natural language explanation for each interpretation of the input question. The translation index contains not only synonyms but also semantic variants for certain types of values like persons and company names. Furthermore, ATHENA can handle single level nesting in the input question. An example input question could be 'All movies with rating higher than the rating of ""Sin City"".' (Q9).

One of the weaknesses of ATHENA is that neither negation (Q8) nor multiple elements in the group by clause (e.g., 'What was the best movie by year and genre? ') are supported. Saha et al (2016) suggest extending ATHENA to handle more than single level nesting. Furthermore, they suggest enabling a possibility to answer follow-up questions using the context of the previous questions.

ATHENA (Saha et al, 2016) is an ontology-driven NLI for relational databases, which handles full sentences in English as the input question. For ATHENA, ontologydriven means that it is based on the information of a given ontology and needs mapping between an ontology and a relational database. A set of synonyms can be associated with each ontology element. During the translation of an input question into a SQL query, ATHENA uses an intermediate query language before subsequently translating it into SQL.

Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for ATHENA could be: 'Who is the director of ""Inglourious Basterds""? '.

ATHENA uses four steps to translate a full sentence input question into a SQL query. In the first step, the ontology evidence annotator is used, which maps the input to a set of ontology elements. There are five types of possible matches: a. meta data: Finding a match in the inverted index for the meta data (and the associated set of synonyms). Longer matches over the input question are preferred if there are multiple matches. b. translation index : The translation index is an extension of the inverted index over the base data, which is enriched with variations for person and company names. For example, for the person name 'Brad Pitt' there would also be an entry 'B. Pitt'. c. time range expressions: Finding all time ranges like 'from 2000 until 2010 ' (Q5) with the TIMEX annotator. Those time ranges are then matched to the ontology properties with the corresponding data type. d. numeric expressions: Finding all tokens that include numeric quantities with the Stanford Numeric Expressions annotator. The numeric quantities can be either in the form of numbers (e.g., 9) or in text form (e.g., nine). Those numeric expressions are then matched to the ontology properties with the corresponding data type. e. dependencies: Annotating dependencies between tokens in the input question. For example, in the input question Q1 there is a dependency between the tokens 'director ' and 'Inglourious Basterds' indicated by the token 'of '. For the input question Q1 the meta data annotation will detect three different matches for 'director ', namely the table name Director and the attribute name Director.directorId and Directing.director-Id (Figure 10: red). The translation index will find a match for the bi-gram 'Inglourious Basterds', corresponding to the attribute Movie.Title (Figure 10: green).

The next step generates a ranked list of interpretations. An interpretation is a set of ontology elements provided by the previous step. If n ontology elements exist for one token, there will also be n different interpretations, one for each ontology element. For the given input question there are three different interpretations possible: {Directing.directorId, Movie.Title}, {Director.directorId, Movie.Title} and {Director, Movie.Title}. Each interpretation is represented by a set of interpretation trees. An interpretation tree (iTree) is a subtree of the ontology. Each iTree must satisfy: a. evidence cover : All tokens, which were annotated in the previous step, need to be covered. b. weak connectedness: All concepts need to be at least weakly connected through an undirected path and each property must be connected to its corresponding concept. For the first interpretation this means that Director and Movie need to be connected, for example, via the relation Directing. The attribute Title needs to be connected with the corresponding concept (in this case the table) Movie. c. inheritance constraint: No ontology element is allowed to inherit from its child concepts. For example, the ontology element Person is not allowed to inherit Role of Actor. The other direction is allowed, such that Actor inherits FirstName and LastName from Person. d. relationship constraint: All relationships given in the input question are included, not depending on the direction of the path. For example, the tree tokens 'movies', 'starring' and 'Brad Pitt' (Q5) imply a relationship constraint between the ontology element Movie, Starring and Person. Those three ontology elements need to be connected. Accordingly, in this example the ontology element Actor needs to be included. For the purpose of ranking the different interpretations, ATHENA generates one single iTree. It can consist of a union of multiple iTrees or a single iTree. Figure 11 shows a possible iTree for the interpretation {Director, Movie.Title}, which is extended with the ontology element Directing and Movie. After this step, for each interpretation only one iTree is left.

The third step uses the ranked list of interpretations to generate an intermediate query in the Ontology Query Language (OQL). OQL was specifically developed for ATHENA to be an intermediate language between the input question and SQL and is able to express queries that include aggregations, unions and single nested subqueries. The structure of an OQL query is similar to SQL and is generated as follows: a. from clause: Specifies all concepts found in the ontology along with their aliases. The aliases are needed, for example, if a concept occurs multiple times. For example, the input question 'Show me all drama and comedy movies.' (Q4) would point to Genre in the ontology twice: once for the token 'drama' and once for 'comedy'. Therefore, two aliases are needed to distinguish between them. b. group by clause: The group by clause is triggered by the word 'by' and only tokens annotated with meta data in step 1.a are considered. For example, the input question 'What was the best movie by genre? ' (modified Q7). To identify the dependencies between dependent and dependee (illustrated by the 'by'), the Stanford Dependency Parser is used. c. select clause: There are two possible types: aggregation and display properties. The aggregation properties depend on the group by clause. The default aggregation function is sum. For the (modified) input question Q7, ATHENA would detect a group by clause because the 'by genre' needs an aggregation function. Assuming ATHENA can translate 'best movie' to mean 'best ranked movie', it would apply the aggregation function max on Movie.Rating.

If there are no aggregations, ATHENA uses the tokens annotated with meta data as display properties, which are shown to the user. d. order by clause: Properties used in the order by clause are indicated by tokens like 'least', 'most', 'ordered by', 'top' and others. For example, the input question 'Which movie has grossed most? ' (Q3) would trigger an order by clause for Movie.Gross because of the trigger word 'most'. e. where clause: Tokens annotated with the translation index, time range or numerical expression are used in the where clause to filter the result (e.g., the tokens 'Inglourious Basterds'). If the filter is applied on an aggregation, a having clause is generated instead of the where clause.

The final step translates the OQL query into a SQL query, where each attribute and join condition is either concrete or virtual. Concrete means that a direct mapping between ontology and the database (e.g., 'director ') exist. Virtual implies that a (complex) relationship between the ontology elements and the database (e.g., 'great movie') exists. Furthermore, the result of the best ranked interpretation is directly displayed, but the users will see the other interpretations as well. All the top n interpretations that ATHENA has found are translated back into full sentences in English for the users, so that the users can choose the best fitting one.

The strengths of ATHENA are the ontology as an abstraction of the relational database and the natural language explanation for each interpretation of the input question. The translation index contains not only synonyms but also semantic variants for certain types of values like persons and company names. Furthermore, ATHENA can handle single level nesting in the input question. An example input question could be 'All movies with rating higher than the rating of ""Sin City"".' (Q9).

One of the weaknesses of ATHENA is that neither negation (Q8) nor multiple elements in the group by clause (e.g., 'What was the best movie by year and genre? ') are supported. Saha et al (2016) suggest extending ATHENA to handle more than single level nesting. Furthermore, they suggest enabling a possibility to answer follow-up questions using the context of the previous questions.","[['b45'], [], [], [], [], [], [], [], ['b45'], ['b45'], [], [], [], [], [], [], [], ['b45']]","[['b45'], [], [], [], [], [], [], [], ['b45'], ['b45'], [], [], [], [], [], [], [], ['b45']]",4,"sent1: ATHENA (Saha et al, 2016) is an ontology-driven NLI for relational databases, which handles full sentences in English as the input question.
sent2: For ATHENA, ontologydriven means that it is based on the information of a given ontology and needs mapping between an ontology and a relational database.
sent3: A set of synonyms can be associated with each ontology element.
sent4: During the translation of an input question into a SQL query, ATHENA uses an intermediate query language before subsequently translating it into SQL.
sent5: Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for ATHENA could be: 'Who is the director of ""Inglourious Basterds""?
sent6: '. ATHENA uses four steps to translate a full sentence input question into a SQL query.
sent7: In the first step, the ontology evidence annotator is used, which maps the input to a set of ontology elements.
sent8: There are five types of possible matches: a.
sent9: meta data: Finding a match in the inverted index for the meta data (and the associated set of synonyms).
sent10: Longer matches over the input question are preferred if there are multiple matches.
sent11: b. translation index : The translation index is an extension of the inverted index over the base data, which is enriched with variations for person and company names.
sent12: For example, for the person name 'Brad Pitt' there would also be an entry 'B. Pitt'.
sent13: c. time range expressions: Finding all time ranges like 'from 2000 until 2010 ' (Q5) with the TIMEX annotator.
sent14: Those time ranges are then matched to the ontology properties with the corresponding data type.
sent15: d. numeric expressions: Finding all tokens that include numeric quantities with the Stanford Numeric Expressions annotator.
sent16: The numeric quantities can be either in the form of numbers (e.g., 9) or in text form (e.g., nine).
sent17: Those numeric expressions are then matched to the ontology properties with the corresponding data type.
sent18: e. dependencies: Annotating dependencies between tokens in the input question.
sent19: For example, in the input question Q1 there is a dependency between the tokens 'director ' and 'Inglourious Basterds' indicated by the token 'of '.
sent20: For the input question Q1 the meta data annotation will detect three different matches for 'director ', namely the table name Director and the attribute name Director.directorId and Directing.director-Id (Figure 10: red).
sent21: The translation index will find a match for the bi-gram 'Inglourious Basterds', corresponding to the attribute Movie.
sent22: Title (Figure 10: green). The next step generates a ranked list of interpretations.
sent23: An interpretation is a set of ontology elements provided by the previous step.
sent24: If n ontology elements exist for one token, there will also be n different interpretations, one for each ontology element.
sent25: For the given input question there are three different interpretations possible: {Directing.directorId, Movie.
sent26: Title}, {Director.directorId, Movie.
sent27: Title} and {Director, Movie. Title}.
sent28: Each interpretation is represented by a set of interpretation trees.
sent29: An interpretation tree (iTree) is a subtree of the ontology.
sent30: Each iTree must satisfy: a. evidence cover : All tokens, which were annotated in the previous step, need to be covered.
sent31: b. weak connectedness: All concepts need to be at least weakly connected through an undirected path and each property must be connected to its corresponding concept.
sent32: For the first interpretation this means that Director and Movie need to be connected, for example, via the relation Directing.
sent33: The attribute Title needs to be connected with the corresponding concept (in this case the table) Movie. c. inheritance constraint: No ontology element is allowed to inherit from its child concepts.
sent34: For example, the ontology element Person is not allowed to inherit Role of Actor.
sent35: The other direction is allowed, such that Actor inherits FirstName and LastName from Person.
sent36: d. relationship constraint: All relationships given in the input question are included, not depending on the direction of the path.
sent37: For example, the tree tokens 'movies', 'starring' and 'Brad Pitt' (Q5) imply a relationship constraint between the ontology element Movie, Starring and Person.
sent38: Those three ontology elements need to be connected.
sent39: Accordingly, in this example the ontology element Actor needs to be included.
sent40: For the purpose of ranking the different interpretations, ATHENA generates one single iTree.
sent41: It can consist of a union of multiple iTrees or a single iTree.
sent42: Figure 11 shows a possible iTree for the interpretation {Director, Movie.
sent43: Title}, which is extended with the ontology element Directing and Movie.
sent44: After this step, for each interpretation only one iTree is left.
sent45: The third step uses the ranked list of interpretations to generate an intermediate query in the Ontology Query Language (OQL).
sent46: OQL was specifically developed for ATHENA to be an intermediate language between the input question and SQL and is able to express queries that include aggregations, unions and single nested subqueries.
sent47: The structure of an OQL query is similar to SQL and is generated as follows: a.
sent48: from clause: Specifies all concepts found in the ontology along with their aliases.
sent49: The aliases are needed, for example, if a concept occurs multiple times.
sent50: For example, the input question 'Show me all drama and comedy movies.'
sent51: (Q4) would point to Genre in the ontology twice: once for the token 'drama' and once for 'comedy'.
sent52: Therefore, two aliases are needed to distinguish between them.
sent53: b. group by clause: The group by clause is triggered by the word 'by' and only tokens annotated with meta data in step 1.a are considered.
sent54: For example, the input question 'What was the best movie by genre? '
sent55: (modified Q7). To identify the dependencies between dependent and dependee (illustrated by the 'by'), the Stanford Dependency Parser is used.
sent56: c. select clause: There are two possible types: aggregation and display properties.
sent57: The aggregation properties depend on the group by clause.
sent58: The default aggregation function is sum.
sent59: For the (modified) input question Q7, ATHENA would detect a group by clause because the 'by genre' needs an aggregation function.
sent60: Assuming ATHENA can translate 'best movie' to mean 'best ranked movie', it would apply the aggregation function max on Movie.
sent61: Rating. If there are no aggregations, ATHENA uses the tokens annotated with meta data as display properties, which are shown to the user.
sent62: d. order by clause: Properties used in the order by clause are indicated by tokens like 'least', 'most', 'ordered by', 'top' and others.
sent63: For example, the input question 'Which movie has grossed most? ' (Q3) would trigger an order by clause for Movie.
sent64: Gross because of the trigger word 'most'.
sent65: e. where clause: Tokens annotated with the translation index, time range or numerical expression are used in the where clause to filter the result (e.g., the tokens 'Inglourious Basterds').
sent66: If the filter is applied on an aggregation, a having clause is generated instead of the where clause.
sent67: The final step translates the OQL query into a SQL query, where each attribute and join condition is either concrete or virtual.
sent68: Concrete means that a direct mapping between ontology and the database (e.g., 'director ') exist.
sent69: Virtual implies that a (complex) relationship between the ontology elements and the database (e.g., 'great movie') exists.
sent70: Furthermore, the result of the best ranked interpretation is directly displayed, but the users will see the other interpretations as well.
sent71: All the top n interpretations that ATHENA has found are translated back into full sentences in English for the users, so that the users can choose the best fitting one.
sent72: The strengths of ATHENA are the ontology as an abstraction of the relational database and the natural language explanation for each interpretation of the input question.
sent73: The translation index contains not only synonyms but also semantic variants for certain types of values like persons and company names.
sent74: Furthermore, ATHENA can handle single level nesting in the input question.
sent75: An example input question could be 'All movies with rating higher than the rating of ""Sin City"".'
sent76: (Q9).One of the weaknesses of ATHENA is that neither negation (Q8) nor multiple elements in the group by clause (e.g., 'What was the best movie by year and genre? ') are supported.
sent77: Saha et al (2016) suggest extending ATHENA to handle more than single level nesting.
sent78: Furthermore, they suggest enabling a possibility to answer follow-up questions using the context of the previous questions.
sent79: ATHENA (Saha et al, 2016) is an ontology-driven NLI for relational databases, which handles full sentences in English as the input question.
sent80: For ATHENA, ontologydriven means that it is based on the information of a given ontology and needs mapping between an ontology and a relational database.
sent81: A set of synonyms can be associated with each ontology element.
sent82: During the translation of an input question into a SQL query, ATHENA uses an intermediate query language before subsequently translating it into SQL.
sent83: Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for ATHENA could be: 'Who is the director of ""Inglourious Basterds""?
sent84: '. ATHENA uses four steps to translate a full sentence input question into a SQL query.
sent85: In the first step, the ontology evidence annotator is used, which maps the input to a set of ontology elements.
sent86: There are five types of possible matches: a.
sent87: meta data: Finding a match in the inverted index for the meta data (and the associated set of synonyms).
sent88: Longer matches over the input question are preferred if there are multiple matches.
sent89: b. translation index : The translation index is an extension of the inverted index over the base data, which is enriched with variations for person and company names.
sent90: For example, for the person name 'Brad Pitt' there would also be an entry 'B. Pitt'.
sent91: c. time range expressions: Finding all time ranges like 'from 2000 until 2010 ' (Q5) with the TIMEX annotator.
sent92: Those time ranges are then matched to the ontology properties with the corresponding data type.
sent93: d. numeric expressions: Finding all tokens that include numeric quantities with the Stanford Numeric Expressions annotator.
sent94: The numeric quantities can be either in the form of numbers (e.g., 9) or in text form (e.g., nine).
sent95: Those numeric expressions are then matched to the ontology properties with the corresponding data type.
sent96: e. dependencies: Annotating dependencies between tokens in the input question.
sent97: For example, in the input question Q1 there is a dependency between the tokens 'director ' and 'Inglourious Basterds' indicated by the token 'of '.
sent98: For the input question Q1 the meta data annotation will detect three different matches for 'director ', namely the table name Director and the attribute name Director.directorId and Directing.director-Id (Figure 10: red).
sent99: The translation index will find a match for the bi-gram 'Inglourious Basterds', corresponding to the attribute Movie.
sent100: Title (Figure 10: green). The next step generates a ranked list of interpretations.
sent101: An interpretation is a set of ontology elements provided by the previous step.
sent102: If n ontology elements exist for one token, there will also be n different interpretations, one for each ontology element.
sent103: For the given input question there are three different interpretations possible: {Directing.directorId, Movie.
sent104: Title}, {Director.directorId, Movie.
sent105: Title} and {Director, Movie. Title}.
sent106: Each interpretation is represented by a set of interpretation trees.
sent107: An interpretation tree (iTree) is a subtree of the ontology.
sent108: Each iTree must satisfy: a. evidence cover : All tokens, which were annotated in the previous step, need to be covered.
sent109: b. weak connectedness: All concepts need to be at least weakly connected through an undirected path and each property must be connected to its corresponding concept.
sent110: For the first interpretation this means that Director and Movie need to be connected, for example, via the relation Directing.
sent111: The attribute Title needs to be connected with the corresponding concept (in this case the table) Movie. c. inheritance constraint: No ontology element is allowed to inherit from its child concepts.
sent112: For example, the ontology element Person is not allowed to inherit Role of Actor.
sent113: The other direction is allowed, such that Actor inherits FirstName and LastName from Person.
sent114: d. relationship constraint: All relationships given in the input question are included, not depending on the direction of the path.
sent115: For example, the tree tokens 'movies', 'starring' and 'Brad Pitt' (Q5) imply a relationship constraint between the ontology element Movie, Starring and Person.
sent116: Those three ontology elements need to be connected.
sent117: Accordingly, in this example the ontology element Actor needs to be included.
sent118: For the purpose of ranking the different interpretations, ATHENA generates one single iTree.
sent119: It can consist of a union of multiple iTrees or a single iTree.
sent120: Figure 11 shows a possible iTree for the interpretation {Director, Movie.
sent121: Title}, which is extended with the ontology element Directing and Movie.
sent122: After this step, for each interpretation only one iTree is left.
sent123: The third step uses the ranked list of interpretations to generate an intermediate query in the Ontology Query Language (OQL).
sent124: OQL was specifically developed for ATHENA to be an intermediate language between the input question and SQL and is able to express queries that include aggregations, unions and single nested subqueries.
sent125: The structure of an OQL query is similar to SQL and is generated as follows: a.
sent126: from clause: Specifies all concepts found in the ontology along with their aliases.
sent127: The aliases are needed, for example, if a concept occurs multiple times.
sent128: For example, the input question 'Show me all drama and comedy movies.'
sent129: (Q4) would point to Genre in the ontology twice: once for the token 'drama' and once for 'comedy'.
sent130: Therefore, two aliases are needed to distinguish between them.
sent131: b. group by clause: The group by clause is triggered by the word 'by' and only tokens annotated with meta data in step 1.a are considered.
sent132: For example, the input question 'What was the best movie by genre? '
sent133: (modified Q7). To identify the dependencies between dependent and dependee (illustrated by the 'by'), the Stanford Dependency Parser is used.
sent134: c. select clause: There are two possible types: aggregation and display properties.
sent135: The aggregation properties depend on the group by clause.
sent136: The default aggregation function is sum.
sent137: For the (modified) input question Q7, ATHENA would detect a group by clause because the 'by genre' needs an aggregation function.
sent138: Assuming ATHENA can translate 'best movie' to mean 'best ranked movie', it would apply the aggregation function max on Movie.
sent139: Rating. If there are no aggregations, ATHENA uses the tokens annotated with meta data as display properties, which are shown to the user.
sent140: d. order by clause: Properties used in the order by clause are indicated by tokens like 'least', 'most', 'ordered by', 'top' and others.
sent141: For example, the input question 'Which movie has grossed most? ' (Q3) would trigger an order by clause for Movie.
sent142: Gross because of the trigger word 'most'.
sent143: e. where clause: Tokens annotated with the translation index, time range or numerical expression are used in the where clause to filter the result (e.g., the tokens 'Inglourious Basterds').
sent144: If the filter is applied on an aggregation, a having clause is generated instead of the where clause.
sent145: The final step translates the OQL query into a SQL query, where each attribute and join condition is either concrete or virtual.
sent146: Concrete means that a direct mapping between ontology and the database (e.g., 'director ') exist.
sent147: Virtual implies that a (complex) relationship between the ontology elements and the database (e.g., 'great movie') exists.
sent148: Furthermore, the result of the best ranked interpretation is directly displayed, but the users will see the other interpretations as well.
sent149: All the top n interpretations that ATHENA has found are translated back into full sentences in English for the users, so that the users can choose the best fitting one.
sent150: The strengths of ATHENA are the ontology as an abstraction of the relational database and the natural language explanation for each interpretation of the input question.
sent151: The translation index contains not only synonyms but also semantic variants for certain types of values like persons and company names.
sent152: Furthermore, ATHENA can handle single level nesting in the input question.
sent153: An example input question could be 'All movies with rating higher than the rating of ""Sin City"".'
sent154: (Q9).One of the weaknesses of ATHENA is that neither negation (Q8) nor multiple elements in the group by clause (e.g., 'What was the best movie by year and genre? ') are supported.
sent155: Saha et al (2016) suggest extending ATHENA to handle more than single level nesting.
sent156: Furthermore, they suggest enabling a possibility to answer follow-up questions using the context of the previous questions."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s102,ATHENA,"ATHENA (Saha et al, 2016) is an ontology-driven NLI for relational databases, which handles full sentences in English as the input question. For ATHENA, ontologydriven means that it is based on the information of a given ontology and needs mapping between an ontology and a relational database. A set of synonyms can be associated with each ontology element. During the translation of an input question into a SQL query, ATHENA uses an intermediate query language before subsequently translating it into SQL.

Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for ATHENA could be: 'Who is the director of ""Inglourious Basterds""? '.

ATHENA uses four steps to translate a full sentence input question into a SQL query. In the first step, the ontology evidence annotator is used, which maps the input to a set of ontology elements. There are five types of possible matches: a. meta data: Finding a match in the inverted index for the meta data (and the associated set of synonyms). Longer matches over the input question are preferred if there are multiple matches. b. translation index : The translation index is an extension of the inverted index over the base data, which is enriched with variations for person and company names. For example, for the person name 'Brad Pitt' there would also be an entry 'B. Pitt'. c. time range expressions: Finding all time ranges like 'from 2000 until 2010 ' (Q5) with the TIMEX annotator. Those time ranges are then matched to the ontology properties with the corresponding data type. d. numeric expressions: Finding all tokens that include numeric quantities with the Stanford Numeric Expressions annotator. The numeric quantities can be either in the form of numbers (e.g., 9) or in text form (e.g., nine). Those numeric expressions are then matched to the ontology properties with the corresponding data type. e. dependencies: Annotating dependencies between tokens in the input question. For example, in the input question Q1 there is a dependency between the tokens 'director ' and 'Inglourious Basterds' indicated by the token 'of '. For the input question Q1 the meta data annotation will detect three different matches for 'director ', namely the table name Director and the attribute name Director.directorId and Directing.director-Id (Figure 10: red). The translation index will find a match for the bi-gram 'Inglourious Basterds', corresponding to the attribute Movie.Title (Figure 10: green).

The next step generates a ranked list of interpretations. An interpretation is a set of ontology elements provided by the previous step. If n ontology elements exist for one token, there will also be n different interpretations, one for each ontology element. For the given input question there are three different interpretations possible: {Directing.directorId, Movie.Title}, {Director.directorId, Movie.Title} and {Director, Movie.Title}. Each interpretation is represented by a set of interpretation trees. An interpretation tree (iTree) is a subtree of the ontology. Each iTree must satisfy: a. evidence cover : All tokens, which were annotated in the previous step, need to be covered. b. weak connectedness: All concepts need to be at least weakly connected through an undirected path and each property must be connected to its corresponding concept. For the first interpretation this means that Director and Movie need to be connected, for example, via the relation Directing. The attribute Title needs to be connected with the corresponding concept (in this case the table) Movie. c. inheritance constraint: No ontology element is allowed to inherit from its child concepts. For example, the ontology element Person is not allowed to inherit Role of Actor. The other direction is allowed, such that Actor inherits FirstName and LastName from Person. d. relationship constraint: All relationships given in the input question are included, not depending on the direction of the path. For example, the tree tokens 'movies', 'starring' and 'Brad Pitt' (Q5) imply a relationship constraint between the ontology element Movie, Starring and Person. Those three ontology elements need to be connected. Accordingly, in this example the ontology element Actor needs to be included. For the purpose of ranking the different interpretations, ATHENA generates one single iTree. It can consist of a union of multiple iTrees or a single iTree. Figure 11 shows a possible iTree for the interpretation {Director, Movie.Title}, which is extended with the ontology element Directing and Movie. After this step, for each interpretation only one iTree is left.

The third step uses the ranked list of interpretations to generate an intermediate query in the Ontology Query Language (OQL). OQL was specifically developed for ATHENA to be an intermediate language between the input question and SQL and is able to express queries that include aggregations, unions and single nested subqueries. The structure of an OQL query is similar to SQL and is generated as follows: a. from clause: Specifies all concepts found in the ontology along with their aliases. The aliases are needed, for example, if a concept occurs multiple times. For example, the input question 'Show me all drama and comedy movies.' (Q4) would point to Genre in the ontology twice: once for the token 'drama' and once for 'comedy'. Therefore, two aliases are needed to distinguish between them. b. group by clause: The group by clause is triggered by the word 'by' and only tokens annotated with meta data in step 1.a are considered. For example, the input question 'What was the best movie by genre? ' (modified Q7). To identify the dependencies between dependent and dependee (illustrated by the 'by'), the Stanford Dependency Parser is used. c. select clause: There are two possible types: aggregation and display properties. The aggregation properties depend on the group by clause. The default aggregation function is sum. For the (modified) input question Q7, ATHENA would detect a group by clause because the 'by genre' needs an aggregation function. Assuming ATHENA can translate 'best movie' to mean 'best ranked movie', it would apply the aggregation function max on Movie.Rating.

If there are no aggregations, ATHENA uses the tokens annotated with meta data as display properties, which are shown to the user. d. order by clause: Properties used in the order by clause are indicated by tokens like 'least', 'most', 'ordered by', 'top' and others. For example, the input question 'Which movie has grossed most? ' (Q3) would trigger an order by clause for Movie.Gross because of the trigger word 'most'. e. where clause: Tokens annotated with the translation index, time range or numerical expression are used in the where clause to filter the result (e.g., the tokens 'Inglourious Basterds'). If the filter is applied on an aggregation, a having clause is generated instead of the where clause.

The final step translates the OQL query into a SQL query, where each attribute and join condition is either concrete or virtual. Concrete means that a direct mapping between ontology and the database (e.g., 'director ') exist. Virtual implies that a (complex) relationship between the ontology elements and the database (e.g., 'great movie') exists. Furthermore, the result of the best ranked interpretation is directly displayed, but the users will see the other interpretations as well. All the top n interpretations that ATHENA has found are translated back into full sentences in English for the users, so that the users can choose the best fitting one.

The strengths of ATHENA are the ontology as an abstraction of the relational database and the natural language explanation for each interpretation of the input question. The translation index contains not only synonyms but also semantic variants for certain types of values like persons and company names. Furthermore, ATHENA can handle single level nesting in the input question. An example input question could be 'All movies with rating higher than the rating of ""Sin City"".' (Q9).

One of the weaknesses of ATHENA is that neither negation (Q8) nor multiple elements in the group by clause (e.g., 'What was the best movie by year and genre? ') are supported. Saha et al (2016) suggest extending ATHENA to handle more than single level nesting. Furthermore, they suggest enabling a possibility to answer follow-up questions using the context of the previous questions.

ATHENA (Saha et al, 2016) is an ontology-driven NLI for relational databases, which handles full sentences in English as the input question. For ATHENA, ontologydriven means that it is based on the information of a given ontology and needs mapping between an ontology and a relational database. A set of synonyms can be associated with each ontology element. During the translation of an input question into a SQL query, ATHENA uses an intermediate query language before subsequently translating it into SQL.

Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for ATHENA could be: 'Who is the director of ""Inglourious Basterds""? '.

ATHENA uses four steps to translate a full sentence input question into a SQL query. In the first step, the ontology evidence annotator is used, which maps the input to a set of ontology elements. There are five types of possible matches: a. meta data: Finding a match in the inverted index for the meta data (and the associated set of synonyms). Longer matches over the input question are preferred if there are multiple matches. b. translation index : The translation index is an extension of the inverted index over the base data, which is enriched with variations for person and company names. For example, for the person name 'Brad Pitt' there would also be an entry 'B. Pitt'. c. time range expressions: Finding all time ranges like 'from 2000 until 2010 ' (Q5) with the TIMEX annotator. Those time ranges are then matched to the ontology properties with the corresponding data type. d. numeric expressions: Finding all tokens that include numeric quantities with the Stanford Numeric Expressions annotator. The numeric quantities can be either in the form of numbers (e.g., 9) or in text form (e.g., nine). Those numeric expressions are then matched to the ontology properties with the corresponding data type. e. dependencies: Annotating dependencies between tokens in the input question. For example, in the input question Q1 there is a dependency between the tokens 'director ' and 'Inglourious Basterds' indicated by the token 'of '. For the input question Q1 the meta data annotation will detect three different matches for 'director ', namely the table name Director and the attribute name Director.directorId and Directing.director-Id (Figure 10: red). The translation index will find a match for the bi-gram 'Inglourious Basterds', corresponding to the attribute Movie.Title (Figure 10: green).

The next step generates a ranked list of interpretations. An interpretation is a set of ontology elements provided by the previous step. If n ontology elements exist for one token, there will also be n different interpretations, one for each ontology element. For the given input question there are three different interpretations possible: {Directing.directorId, Movie.Title}, {Director.directorId, Movie.Title} and {Director, Movie.Title}. Each interpretation is represented by a set of interpretation trees. An interpretation tree (iTree) is a subtree of the ontology. Each iTree must satisfy: a. evidence cover : All tokens, which were annotated in the previous step, need to be covered. b. weak connectedness: All concepts need to be at least weakly connected through an undirected path and each property must be connected to its corresponding concept. For the first interpretation this means that Director and Movie need to be connected, for example, via the relation Directing. The attribute Title needs to be connected with the corresponding concept (in this case the table) Movie. c. inheritance constraint: No ontology element is allowed to inherit from its child concepts. For example, the ontology element Person is not allowed to inherit Role of Actor. The other direction is allowed, such that Actor inherits FirstName and LastName from Person. d. relationship constraint: All relationships given in the input question are included, not depending on the direction of the path. For example, the tree tokens 'movies', 'starring' and 'Brad Pitt' (Q5) imply a relationship constraint between the ontology element Movie, Starring and Person. Those three ontology elements need to be connected. Accordingly, in this example the ontology element Actor needs to be included. For the purpose of ranking the different interpretations, ATHENA generates one single iTree. It can consist of a union of multiple iTrees or a single iTree. Figure 11 shows a possible iTree for the interpretation {Director, Movie.Title}, which is extended with the ontology element Directing and Movie. After this step, for each interpretation only one iTree is left.

The third step uses the ranked list of interpretations to generate an intermediate query in the Ontology Query Language (OQL). OQL was specifically developed for ATHENA to be an intermediate language between the input question and SQL and is able to express queries that include aggregations, unions and single nested subqueries. The structure of an OQL query is similar to SQL and is generated as follows: a. from clause: Specifies all concepts found in the ontology along with their aliases. The aliases are needed, for example, if a concept occurs multiple times. For example, the input question 'Show me all drama and comedy movies.' (Q4) would point to Genre in the ontology twice: once for the token 'drama' and once for 'comedy'. Therefore, two aliases are needed to distinguish between them. b. group by clause: The group by clause is triggered by the word 'by' and only tokens annotated with meta data in step 1.a are considered. For example, the input question 'What was the best movie by genre? ' (modified Q7). To identify the dependencies between dependent and dependee (illustrated by the 'by'), the Stanford Dependency Parser is used. c. select clause: There are two possible types: aggregation and display properties. The aggregation properties depend on the group by clause. The default aggregation function is sum. For the (modified) input question Q7, ATHENA would detect a group by clause because the 'by genre' needs an aggregation function. Assuming ATHENA can translate 'best movie' to mean 'best ranked movie', it would apply the aggregation function max on Movie.Rating.

If there are no aggregations, ATHENA uses the tokens annotated with meta data as display properties, which are shown to the user. d. order by clause: Properties used in the order by clause are indicated by tokens like 'least', 'most', 'ordered by', 'top' and others. For example, the input question 'Which movie has grossed most? ' (Q3) would trigger an order by clause for Movie.Gross because of the trigger word 'most'. e. where clause: Tokens annotated with the translation index, time range or numerical expression are used in the where clause to filter the result (e.g., the tokens 'Inglourious Basterds'). If the filter is applied on an aggregation, a having clause is generated instead of the where clause.

The final step translates the OQL query into a SQL query, where each attribute and join condition is either concrete or virtual. Concrete means that a direct mapping between ontology and the database (e.g., 'director ') exist. Virtual implies that a (complex) relationship between the ontology elements and the database (e.g., 'great movie') exists. Furthermore, the result of the best ranked interpretation is directly displayed, but the users will see the other interpretations as well. All the top n interpretations that ATHENA has found are translated back into full sentences in English for the users, so that the users can choose the best fitting one.

The strengths of ATHENA are the ontology as an abstraction of the relational database and the natural language explanation for each interpretation of the input question. The translation index contains not only synonyms but also semantic variants for certain types of values like persons and company names. Furthermore, ATHENA can handle single level nesting in the input question. An example input question could be 'All movies with rating higher than the rating of ""Sin City"".' (Q9).

One of the weaknesses of ATHENA is that neither negation (Q8) nor multiple elements in the group by clause (e.g., 'What was the best movie by year and genre? ') are supported. Saha et al (2016) suggest extending ATHENA to handle more than single level nesting. Furthermore, they suggest enabling a possibility to answer follow-up questions using the context of the previous questions.","[['b45'], [], [], [], [], [], [], [], ['b45'], ['b45'], [], [], [], [], [], [], [], ['b45']]","[['b45'], [], [], [], [], [], [], [], ['b45'], ['b45'], [], [], [], [], [], [], [], ['b45']]",4,"sent1: ATHENA (Saha et al, 2016) is an ontology-driven NLI for relational databases, which handles full sentences in English as the input question.
sent2: For ATHENA, ontologydriven means that it is based on the information of a given ontology and needs mapping between an ontology and a relational database.
sent3: A set of synonyms can be associated with each ontology element.
sent4: During the translation of an input question into a SQL query, ATHENA uses an intermediate query language before subsequently translating it into SQL.
sent5: Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for ATHENA could be: 'Who is the director of ""Inglourious Basterds""?
sent6: '. ATHENA uses four steps to translate a full sentence input question into a SQL query.
sent7: In the first step, the ontology evidence annotator is used, which maps the input to a set of ontology elements.
sent8: There are five types of possible matches: a.
sent9: meta data: Finding a match in the inverted index for the meta data (and the associated set of synonyms).
sent10: Longer matches over the input question are preferred if there are multiple matches.
sent11: b. translation index : The translation index is an extension of the inverted index over the base data, which is enriched with variations for person and company names.
sent12: For example, for the person name 'Brad Pitt' there would also be an entry 'B. Pitt'.
sent13: c. time range expressions: Finding all time ranges like 'from 2000 until 2010 ' (Q5) with the TIMEX annotator.
sent14: Those time ranges are then matched to the ontology properties with the corresponding data type.
sent15: d. numeric expressions: Finding all tokens that include numeric quantities with the Stanford Numeric Expressions annotator.
sent16: The numeric quantities can be either in the form of numbers (e.g., 9) or in text form (e.g., nine).
sent17: Those numeric expressions are then matched to the ontology properties with the corresponding data type.
sent18: e. dependencies: Annotating dependencies between tokens in the input question.
sent19: For example, in the input question Q1 there is a dependency between the tokens 'director ' and 'Inglourious Basterds' indicated by the token 'of '.
sent20: For the input question Q1 the meta data annotation will detect three different matches for 'director ', namely the table name Director and the attribute name Director.directorId and Directing.director-Id (Figure 10: red).
sent21: The translation index will find a match for the bi-gram 'Inglourious Basterds', corresponding to the attribute Movie.
sent22: Title (Figure 10: green). The next step generates a ranked list of interpretations.
sent23: An interpretation is a set of ontology elements provided by the previous step.
sent24: If n ontology elements exist for one token, there will also be n different interpretations, one for each ontology element.
sent25: For the given input question there are three different interpretations possible: {Directing.directorId, Movie.
sent26: Title}, {Director.directorId, Movie.
sent27: Title} and {Director, Movie. Title}.
sent28: Each interpretation is represented by a set of interpretation trees.
sent29: An interpretation tree (iTree) is a subtree of the ontology.
sent30: Each iTree must satisfy: a. evidence cover : All tokens, which were annotated in the previous step, need to be covered.
sent31: b. weak connectedness: All concepts need to be at least weakly connected through an undirected path and each property must be connected to its corresponding concept.
sent32: For the first interpretation this means that Director and Movie need to be connected, for example, via the relation Directing.
sent33: The attribute Title needs to be connected with the corresponding concept (in this case the table) Movie. c. inheritance constraint: No ontology element is allowed to inherit from its child concepts.
sent34: For example, the ontology element Person is not allowed to inherit Role of Actor.
sent35: The other direction is allowed, such that Actor inherits FirstName and LastName from Person.
sent36: d. relationship constraint: All relationships given in the input question are included, not depending on the direction of the path.
sent37: For example, the tree tokens 'movies', 'starring' and 'Brad Pitt' (Q5) imply a relationship constraint between the ontology element Movie, Starring and Person.
sent38: Those three ontology elements need to be connected.
sent39: Accordingly, in this example the ontology element Actor needs to be included.
sent40: For the purpose of ranking the different interpretations, ATHENA generates one single iTree.
sent41: It can consist of a union of multiple iTrees or a single iTree.
sent42: Figure 11 shows a possible iTree for the interpretation {Director, Movie.
sent43: Title}, which is extended with the ontology element Directing and Movie.
sent44: After this step, for each interpretation only one iTree is left.
sent45: The third step uses the ranked list of interpretations to generate an intermediate query in the Ontology Query Language (OQL).
sent46: OQL was specifically developed for ATHENA to be an intermediate language between the input question and SQL and is able to express queries that include aggregations, unions and single nested subqueries.
sent47: The structure of an OQL query is similar to SQL and is generated as follows: a.
sent48: from clause: Specifies all concepts found in the ontology along with their aliases.
sent49: The aliases are needed, for example, if a concept occurs multiple times.
sent50: For example, the input question 'Show me all drama and comedy movies.'
sent51: (Q4) would point to Genre in the ontology twice: once for the token 'drama' and once for 'comedy'.
sent52: Therefore, two aliases are needed to distinguish between them.
sent53: b. group by clause: The group by clause is triggered by the word 'by' and only tokens annotated with meta data in step 1.a are considered.
sent54: For example, the input question 'What was the best movie by genre? '
sent55: (modified Q7). To identify the dependencies between dependent and dependee (illustrated by the 'by'), the Stanford Dependency Parser is used.
sent56: c. select clause: There are two possible types: aggregation and display properties.
sent57: The aggregation properties depend on the group by clause.
sent58: The default aggregation function is sum.
sent59: For the (modified) input question Q7, ATHENA would detect a group by clause because the 'by genre' needs an aggregation function.
sent60: Assuming ATHENA can translate 'best movie' to mean 'best ranked movie', it would apply the aggregation function max on Movie.
sent61: Rating. If there are no aggregations, ATHENA uses the tokens annotated with meta data as display properties, which are shown to the user.
sent62: d. order by clause: Properties used in the order by clause are indicated by tokens like 'least', 'most', 'ordered by', 'top' and others.
sent63: For example, the input question 'Which movie has grossed most? ' (Q3) would trigger an order by clause for Movie.
sent64: Gross because of the trigger word 'most'.
sent65: e. where clause: Tokens annotated with the translation index, time range or numerical expression are used in the where clause to filter the result (e.g., the tokens 'Inglourious Basterds').
sent66: If the filter is applied on an aggregation, a having clause is generated instead of the where clause.
sent67: The final step translates the OQL query into a SQL query, where each attribute and join condition is either concrete or virtual.
sent68: Concrete means that a direct mapping between ontology and the database (e.g., 'director ') exist.
sent69: Virtual implies that a (complex) relationship between the ontology elements and the database (e.g., 'great movie') exists.
sent70: Furthermore, the result of the best ranked interpretation is directly displayed, but the users will see the other interpretations as well.
sent71: All the top n interpretations that ATHENA has found are translated back into full sentences in English for the users, so that the users can choose the best fitting one.
sent72: The strengths of ATHENA are the ontology as an abstraction of the relational database and the natural language explanation for each interpretation of the input question.
sent73: The translation index contains not only synonyms but also semantic variants for certain types of values like persons and company names.
sent74: Furthermore, ATHENA can handle single level nesting in the input question.
sent75: An example input question could be 'All movies with rating higher than the rating of ""Sin City"".'
sent76: (Q9).One of the weaknesses of ATHENA is that neither negation (Q8) nor multiple elements in the group by clause (e.g., 'What was the best movie by year and genre? ') are supported.
sent77: Saha et al (2016) suggest extending ATHENA to handle more than single level nesting.
sent78: Furthermore, they suggest enabling a possibility to answer follow-up questions using the context of the previous questions.
sent79: ATHENA (Saha et al, 2016) is an ontology-driven NLI for relational databases, which handles full sentences in English as the input question.
sent80: For ATHENA, ontologydriven means that it is based on the information of a given ontology and needs mapping between an ontology and a relational database.
sent81: A set of synonyms can be associated with each ontology element.
sent82: During the translation of an input question into a SQL query, ATHENA uses an intermediate query language before subsequently translating it into SQL.
sent83: Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for ATHENA could be: 'Who is the director of ""Inglourious Basterds""?
sent84: '. ATHENA uses four steps to translate a full sentence input question into a SQL query.
sent85: In the first step, the ontology evidence annotator is used, which maps the input to a set of ontology elements.
sent86: There are five types of possible matches: a.
sent87: meta data: Finding a match in the inverted index for the meta data (and the associated set of synonyms).
sent88: Longer matches over the input question are preferred if there are multiple matches.
sent89: b. translation index : The translation index is an extension of the inverted index over the base data, which is enriched with variations for person and company names.
sent90: For example, for the person name 'Brad Pitt' there would also be an entry 'B. Pitt'.
sent91: c. time range expressions: Finding all time ranges like 'from 2000 until 2010 ' (Q5) with the TIMEX annotator.
sent92: Those time ranges are then matched to the ontology properties with the corresponding data type.
sent93: d. numeric expressions: Finding all tokens that include numeric quantities with the Stanford Numeric Expressions annotator.
sent94: The numeric quantities can be either in the form of numbers (e.g., 9) or in text form (e.g., nine).
sent95: Those numeric expressions are then matched to the ontology properties with the corresponding data type.
sent96: e. dependencies: Annotating dependencies between tokens in the input question.
sent97: For example, in the input question Q1 there is a dependency between the tokens 'director ' and 'Inglourious Basterds' indicated by the token 'of '.
sent98: For the input question Q1 the meta data annotation will detect three different matches for 'director ', namely the table name Director and the attribute name Director.directorId and Directing.director-Id (Figure 10: red).
sent99: The translation index will find a match for the bi-gram 'Inglourious Basterds', corresponding to the attribute Movie.
sent100: Title (Figure 10: green). The next step generates a ranked list of interpretations.
sent101: An interpretation is a set of ontology elements provided by the previous step.
sent102: If n ontology elements exist for one token, there will also be n different interpretations, one for each ontology element.
sent103: For the given input question there are three different interpretations possible: {Directing.directorId, Movie.
sent104: Title}, {Director.directorId, Movie.
sent105: Title} and {Director, Movie. Title}.
sent106: Each interpretation is represented by a set of interpretation trees.
sent107: An interpretation tree (iTree) is a subtree of the ontology.
sent108: Each iTree must satisfy: a. evidence cover : All tokens, which were annotated in the previous step, need to be covered.
sent109: b. weak connectedness: All concepts need to be at least weakly connected through an undirected path and each property must be connected to its corresponding concept.
sent110: For the first interpretation this means that Director and Movie need to be connected, for example, via the relation Directing.
sent111: The attribute Title needs to be connected with the corresponding concept (in this case the table) Movie. c. inheritance constraint: No ontology element is allowed to inherit from its child concepts.
sent112: For example, the ontology element Person is not allowed to inherit Role of Actor.
sent113: The other direction is allowed, such that Actor inherits FirstName and LastName from Person.
sent114: d. relationship constraint: All relationships given in the input question are included, not depending on the direction of the path.
sent115: For example, the tree tokens 'movies', 'starring' and 'Brad Pitt' (Q5) imply a relationship constraint between the ontology element Movie, Starring and Person.
sent116: Those three ontology elements need to be connected.
sent117: Accordingly, in this example the ontology element Actor needs to be included.
sent118: For the purpose of ranking the different interpretations, ATHENA generates one single iTree.
sent119: It can consist of a union of multiple iTrees or a single iTree.
sent120: Figure 11 shows a possible iTree for the interpretation {Director, Movie.
sent121: Title}, which is extended with the ontology element Directing and Movie.
sent122: After this step, for each interpretation only one iTree is left.
sent123: The third step uses the ranked list of interpretations to generate an intermediate query in the Ontology Query Language (OQL).
sent124: OQL was specifically developed for ATHENA to be an intermediate language between the input question and SQL and is able to express queries that include aggregations, unions and single nested subqueries.
sent125: The structure of an OQL query is similar to SQL and is generated as follows: a.
sent126: from clause: Specifies all concepts found in the ontology along with their aliases.
sent127: The aliases are needed, for example, if a concept occurs multiple times.
sent128: For example, the input question 'Show me all drama and comedy movies.'
sent129: (Q4) would point to Genre in the ontology twice: once for the token 'drama' and once for 'comedy'.
sent130: Therefore, two aliases are needed to distinguish between them.
sent131: b. group by clause: The group by clause is triggered by the word 'by' and only tokens annotated with meta data in step 1.a are considered.
sent132: For example, the input question 'What was the best movie by genre? '
sent133: (modified Q7). To identify the dependencies between dependent and dependee (illustrated by the 'by'), the Stanford Dependency Parser is used.
sent134: c. select clause: There are two possible types: aggregation and display properties.
sent135: The aggregation properties depend on the group by clause.
sent136: The default aggregation function is sum.
sent137: For the (modified) input question Q7, ATHENA would detect a group by clause because the 'by genre' needs an aggregation function.
sent138: Assuming ATHENA can translate 'best movie' to mean 'best ranked movie', it would apply the aggregation function max on Movie.
sent139: Rating. If there are no aggregations, ATHENA uses the tokens annotated with meta data as display properties, which are shown to the user.
sent140: d. order by clause: Properties used in the order by clause are indicated by tokens like 'least', 'most', 'ordered by', 'top' and others.
sent141: For example, the input question 'Which movie has grossed most? ' (Q3) would trigger an order by clause for Movie.
sent142: Gross because of the trigger word 'most'.
sent143: e. where clause: Tokens annotated with the translation index, time range or numerical expression are used in the where clause to filter the result (e.g., the tokens 'Inglourious Basterds').
sent144: If the filter is applied on an aggregation, a having clause is generated instead of the where clause.
sent145: The final step translates the OQL query into a SQL query, where each attribute and join condition is either concrete or virtual.
sent146: Concrete means that a direct mapping between ontology and the database (e.g., 'director ') exist.
sent147: Virtual implies that a (complex) relationship between the ontology elements and the database (e.g., 'great movie') exists.
sent148: Furthermore, the result of the best ranked interpretation is directly displayed, but the users will see the other interpretations as well.
sent149: All the top n interpretations that ATHENA has found are translated back into full sentences in English for the users, so that the users can choose the best fitting one.
sent150: The strengths of ATHENA are the ontology as an abstraction of the relational database and the natural language explanation for each interpretation of the input question.
sent151: The translation index contains not only synonyms but also semantic variants for certain types of values like persons and company names.
sent152: Furthermore, ATHENA can handle single level nesting in the input question.
sent153: An example input question could be 'All movies with rating higher than the rating of ""Sin City"".'
sent154: (Q9).One of the weaknesses of ATHENA is that neither negation (Q8) nor multiple elements in the group by clause (e.g., 'What was the best movie by year and genre? ') are supported.
sent155: Saha et al (2016) suggest extending ATHENA to handle more than single level nesting.
sent156: Furthermore, they suggest enabling a possibility to answer follow-up questions using the context of the previous questions."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s23,SODA (Search Over DAta warehouse),"SODA (Blunschi et al, 2012) is a system that provides a keyword-based NLI for relational databases with some extensions in the direction of a pattern-based system. The base data consists of the relational database. The meta data can include multiple ontologies, which are handled like natural language patterns. For example, domain specific ontologies with concepts (like the concept 'great movie' in the sample world) or DBpedia to Fig. 7 Nodes in the meta data graph corresponding to the keywords 'director ' (red) and 'Inglourious Basterds' (green) found during the lookup step of SODA.

identify homonyms and synonyms. SODA uses both inverted indexes (base and meta data) as the basis for finding query matches in the data. The key innovation of SODA is that it provides the possibility to define meta data patterns which specify conceptual models. The concept 'good movie' could depend on various variables not only on the rating, but for example, also on the number of ratings. The users can then apply this concept to their input questions, for example, they could search for 'all great movie' (Q6) without having to specify what a great movie is.

Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for SODA could be: 'director Inglourious Basterds'.

SODA uses five steps to translate this keyword-based input question into a SQL query. The first step is the lookup: it checks the keywords against the inverted indexes over the database and provides all the nodes in the meta data graph where these keywords are found. For the input question Q1, this means that the keyword 'director ' can be found in the inverted index of the meta data, either the table name Director or to the attribute name Director.directorId and Directing.director-Id (Figure 7: red). The keyword 'Inglourious Basterds' is only found in the inverted index of the base data as a value of the attribute Movie.Title (Figure 7: green). This leads to three different solution sets for the next steps:

{Directing.directorId, Movie.Title}, {Director.directorId, Movie.Title} and {Director, Movie.Title}.

The second step is to assign a score to each solution of the lookup step. SODA uses a simple heuristic method, for example in-domain solutions receive a higher score. For the input question, the solution {Director, Movie.Title} receives the highest score, because the table name Director it is a full match and not only a fuzzy match like in directorId. Afterwards, only the best n solutions are provided to the next step.

The third step identifies which tables are used for each of the solutions provided by the previous step. Also, the relationships and inheritance structures between those tables are discovered in this step. For the best solution of the input question, the tables Director and Movie correspond to the different entry points. An entry point is a node in the meta data graph. The table Director is a child of the table Person (ISA-  Figure 7) and therefore this table is included.

The fourth step collects the filters. There are two types of filters which are collected. The first one are filters in the input question like 'Inglourious Basterds'. The second one are filter conditions that occur during traversing the meta data graph like the concept 'great movie'.

The fifth and last step generates a reasonable and executable SQL query from the information collected in the previous steps. A reasonable SQL query is a query which considers foreign keys and inheritance patterns in the schema. An executable SQL query is a query that can be executed on the underlying database.

The strengths of SODA are the use of meta data patterns and domain ontologies, which allow one to define concepts and include domain specific knowledge. In addition, the inclusion of external sources like DBpedia for homonyms and synonyms is beneficial for finding meaningful results. Furthermore, SODA is designed to evolve and thus improve over time based on user feedback.

The weaknesses of SODA are that it uses simple word recognition for comparison operators. For example, to retrieve all movies with a rating greater than 9, the input question needs to be written like 'rating > 9 ' (Q2). Moreover, SODA uses a very strict syntax for aggregation operators. For example, to retrieve the number of movies per year, the input question needs to be written like 'select count (movie) group by (year)'. These patterns are useful, but are not in natural language. Furthermore, there is no lemmatization, stemming or any other preprocessing of the input question which can lead to a problem with words that are used in plural. For example the input question 'all movies' would not detect the table Movie but the input question 'all movie' would display the expected result. Blunschi et al (2012) suggest extending SODA to handle temporal aspects of the data warehouse (e.g., bitemporal historization). They also pointed out that the GUI of SODA should be improved so that the users are engaged in selecting and ranking the different results. Furthermore, the user feedback provided by SODA is currently very basic and needs to be improved.

SODA (Blunschi et al, 2012) is a system that provides a keyword-based NLI for relational databases with some extensions in the direction of a pattern-based system. The base data consists of the relational database. The meta data can include multiple ontologies, which are handled like natural language patterns. For example, domain specific ontologies with concepts (like the concept 'great movie' in the sample world) or DBpedia to Fig. 7 Nodes in the meta data graph corresponding to the keywords 'director ' (red) and 'Inglourious Basterds' (green) found during the lookup step of SODA.

identify homonyms and synonyms. SODA uses both inverted indexes (base and meta data) as the basis for finding query matches in the data. The key innovation of SODA is that it provides the possibility to define meta data patterns which specify conceptual models. The concept 'good movie' could depend on various variables not only on the rating, but for example, also on the number of ratings. The users can then apply this concept to their input questions, for example, they could search for 'all great movie' (Q6) without having to specify what a great movie is.

Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for SODA could be: 'director Inglourious Basterds'.

SODA uses five steps to translate this keyword-based input question into a SQL query. The first step is the lookup: it checks the keywords against the inverted indexes over the database and provides all the nodes in the meta data graph where these keywords are found. For the input question Q1, this means that the keyword 'director ' can be found in the inverted index of the meta data, either the table name Director or to the attribute name Director.directorId and Directing.director-Id (Figure 7: red). The keyword 'Inglourious Basterds' is only found in the inverted index of the base data as a value of the attribute Movie.Title (Figure 7: green). This leads to three different solution sets for the next steps:

{Directing.directorId, Movie.Title}, {Director.directorId, Movie.Title} and {Director, Movie.Title}.

The second step is to assign a score to each solution of the lookup step. SODA uses a simple heuristic method, for example in-domain solutions receive a higher score. For the input question, the solution {Director, Movie.Title} receives the highest score, because the table name Director it is a full match and not only a fuzzy match like in directorId. Afterwards, only the best n solutions are provided to the next step.

The third step identifies which tables are used for each of the solutions provided by the previous step. Also, the relationships and inheritance structures between those tables are discovered in this step. For the best solution of the input question, the tables Director and Movie correspond to the different entry points. An entry point is a node in the meta data graph. The table Director is a child of the table Person (ISA-  Figure 7) and therefore this table is included.

The fourth step collects the filters. There are two types of filters which are collected. The first one are filters in the input question like 'Inglourious Basterds'. The second one are filter conditions that occur during traversing the meta data graph like the concept 'great movie'.

The fifth and last step generates a reasonable and executable SQL query from the information collected in the previous steps. A reasonable SQL query is a query which considers foreign keys and inheritance patterns in the schema. An executable SQL query is a query that can be executed on the underlying database.

The strengths of SODA are the use of meta data patterns and domain ontologies, which allow one to define concepts and include domain specific knowledge. In addition, the inclusion of external sources like DBpedia for homonyms and synonyms is beneficial for finding meaningful results. Furthermore, SODA is designed to evolve and thus improve over time based on user feedback.

The weaknesses of SODA are that it uses simple word recognition for comparison operators. For example, to retrieve all movies with a rating greater than 9, the input question needs to be written like 'rating > 9 ' (Q2). Moreover, SODA uses a very strict syntax for aggregation operators. For example, to retrieve the number of movies per year, the input question needs to be written like 'select count (movie) group by (year)'. These patterns are useful, but are not in natural language. Furthermore, there is no lemmatization, stemming or any other preprocessing of the input question which can lead to a problem with words that are used in plural. For example the input question 'all movies' would not detect the table Movie but the input question 'all movie' would display the expected result. Blunschi et al (2012) suggest extending SODA to handle temporal aspects of the data warehouse (e.g., bitemporal historization). They also pointed out that the GUI of SODA should be improved so that the users are engaged in selecting and ranking the different results. Furthermore, the user feedback provided by SODA is currently very basic and needs to be improved.","[[None, 'b5'], [], [], [], [], [], [], [], [], [], ['b5'], [None, 'b5'], [], [], [], [], [], [], [], [], [], ['b5']]","[[None, 'b5'], [], [], [], [], [], [], [], [], [], ['b5'], [None, 'b5'], [], [], [], [], [], [], [], [], [], ['b5']]",6,"sent1: SODA (Blunschi et al, 2012) is a system that provides a keyword-based NLI for relational databases with some extensions in the direction of a pattern-based system.
sent2: The base data consists of the relational database.
sent3: The meta data can include multiple ontologies, which are handled like natural language patterns.
sent4: For example, domain specific ontologies with concepts (like the concept 'great movie' in the sample world) or DBpedia to Fig. 7 Nodes in the meta data graph corresponding to the keywords 'director ' (red) and 'Inglourious Basterds' (green) found during the lookup step of SODA.
sent5: identify homonyms and synonyms.
sent6: SODA uses both inverted indexes (base and meta data) as the basis for finding query matches in the data.
sent7: The key innovation of SODA is that it provides the possibility to define meta data patterns which specify conceptual models.
sent8: The concept 'good movie' could depend on various variables not only on the rating, but for example, also on the number of ratings.
sent9: The users can then apply this concept to their input questions, for example, they could search for 'all great movie' (Q6) without having to specify what a great movie is.
sent10: Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for SODA could be: 'director Inglourious Basterds'.
sent11: SODA uses five steps to translate this keyword-based input question into a SQL query.
sent12: The first step is the lookup: it checks the keywords against the inverted indexes over the database and provides all the nodes in the meta data graph where these keywords are found.
sent13: For the input question Q1, this means that the keyword 'director ' can be found in the inverted index of the meta data, either the table name Director or to the attribute name Director.directorId and Directing.director-Id (Figure 7: red).
sent14: The keyword 'Inglourious Basterds' is only found in the inverted index of the base data as a value of the attribute Movie.
sent15: Title (Figure 7: green). This leads to three different solution sets for the next steps:{Directing.directorId, Movie.
sent16: Title}, {Director.directorId, Movie.
sent17: Title} and {Director, Movie. Title}.
sent18: The second step is to assign a score to each solution of the lookup step.
sent19: SODA uses a simple heuristic method, for example in-domain solutions receive a higher score.
sent20: For the input question, the solution {Director, Movie.Title} receives the highest score, because the table name Director it is a full match and not only a fuzzy match like in directorId.
sent21: Afterwards, only the best n solutions are provided to the next step.
sent22: The third step identifies which tables are used for each of the solutions provided by the previous step.
sent23: Also, the relationships and inheritance structures between those tables are discovered in this step.
sent24: For the best solution of the input question, the tables Director and Movie correspond to the different entry points.
sent25: An entry point is a node in the meta data graph.
sent26: The table Director is a child of the table Person (ISA-  Figure 7) and therefore this table is included.
sent27: The fourth step collects the filters.
sent28: There are two types of filters which are collected.
sent29: The first one are filters in the input question like 'Inglourious Basterds'.
sent30: The second one are filter conditions that occur during traversing the meta data graph like the concept 'great movie'.
sent31: The fifth and last step generates a reasonable and executable SQL query from the information collected in the previous steps.
sent32: A reasonable SQL query is a query which considers foreign keys and inheritance patterns in the schema.
sent33: An executable SQL query is a query that can be executed on the underlying database.
sent34: The strengths of SODA are the use of meta data patterns and domain ontologies, which allow one to define concepts and include domain specific knowledge.
sent35: In addition, the inclusion of external sources like DBpedia for homonyms and synonyms is beneficial for finding meaningful results.
sent36: Furthermore, SODA is designed to evolve and thus improve over time based on user feedback.
sent37: The weaknesses of SODA are that it uses simple word recognition for comparison operators.
sent38: For example, to retrieve all movies with a rating greater than 9, the input question needs to be written like 'rating > 9 ' (Q2).
sent39: Moreover, SODA uses a very strict syntax for aggregation operators.
sent40: For example, to retrieve the number of movies per year, the input question needs to be written like 'select count (movie) group by (year)'.
sent41: These patterns are useful, but are not in natural language.
sent42: Furthermore, there is no lemmatization, stemming or any other preprocessing of the input question which can lead to a problem with words that are used in plural.
sent43: For example the input question 'all movies' would not detect the table Movie but the input question 'all movie' would display the expected result.
sent44: Blunschi et al (2012) suggest extending SODA to handle temporal aspects of the data warehouse (e.g., bitemporal historization).
sent45: They also pointed out that the GUI of SODA should be improved so that the users are engaged in selecting and ranking the different results.
sent46: Furthermore, the user feedback provided by SODA is currently very basic and needs to be improved.
sent47: SODA (Blunschi et al, 2012) is a system that provides a keyword-based NLI for relational databases with some extensions in the direction of a pattern-based system.
sent48: The base data consists of the relational database.
sent49: The meta data can include multiple ontologies, which are handled like natural language patterns.
sent50: For example, domain specific ontologies with concepts (like the concept 'great movie' in the sample world) or DBpedia to Fig. 7 Nodes in the meta data graph corresponding to the keywords 'director ' (red) and 'Inglourious Basterds' (green) found during the lookup step of SODA.
sent51: identify homonyms and synonyms.
sent52: SODA uses both inverted indexes (base and meta data) as the basis for finding query matches in the data.
sent53: The key innovation of SODA is that it provides the possibility to define meta data patterns which specify conceptual models.
sent54: The concept 'good movie' could depend on various variables not only on the rating, but for example, also on the number of ratings.
sent55: The users can then apply this concept to their input questions, for example, they could search for 'all great movie' (Q6) without having to specify what a great movie is.
sent56: Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question for SODA could be: 'director Inglourious Basterds'.
sent57: SODA uses five steps to translate this keyword-based input question into a SQL query.
sent58: The first step is the lookup: it checks the keywords against the inverted indexes over the database and provides all the nodes in the meta data graph where these keywords are found.
sent59: For the input question Q1, this means that the keyword 'director ' can be found in the inverted index of the meta data, either the table name Director or to the attribute name Director.directorId and Directing.director-Id (Figure 7: red).
sent60: The keyword 'Inglourious Basterds' is only found in the inverted index of the base data as a value of the attribute Movie.
sent61: Title (Figure 7: green). This leads to three different solution sets for the next steps:{Directing.directorId, Movie.
sent62: Title}, {Director.directorId, Movie.
sent63: Title} and {Director, Movie. Title}.
sent64: The second step is to assign a score to each solution of the lookup step.
sent65: SODA uses a simple heuristic method, for example in-domain solutions receive a higher score.
sent66: For the input question, the solution {Director, Movie.Title} receives the highest score, because the table name Director it is a full match and not only a fuzzy match like in directorId.
sent67: Afterwards, only the best n solutions are provided to the next step.
sent68: The third step identifies which tables are used for each of the solutions provided by the previous step.
sent69: Also, the relationships and inheritance structures between those tables are discovered in this step.
sent70: For the best solution of the input question, the tables Director and Movie correspond to the different entry points.
sent71: An entry point is a node in the meta data graph.
sent72: The table Director is a child of the table Person (ISA-  Figure 7) and therefore this table is included.
sent73: The fourth step collects the filters.
sent74: There are two types of filters which are collected.
sent75: The first one are filters in the input question like 'Inglourious Basterds'.
sent76: The second one are filter conditions that occur during traversing the meta data graph like the concept 'great movie'.
sent77: The fifth and last step generates a reasonable and executable SQL query from the information collected in the previous steps.
sent78: A reasonable SQL query is a query which considers foreign keys and inheritance patterns in the schema.
sent79: An executable SQL query is a query that can be executed on the underlying database.
sent80: The strengths of SODA are the use of meta data patterns and domain ontologies, which allow one to define concepts and include domain specific knowledge.
sent81: In addition, the inclusion of external sources like DBpedia for homonyms and synonyms is beneficial for finding meaningful results.
sent82: Furthermore, SODA is designed to evolve and thus improve over time based on user feedback.
sent83: The weaknesses of SODA are that it uses simple word recognition for comparison operators.
sent84: For example, to retrieve all movies with a rating greater than 9, the input question needs to be written like 'rating > 9 ' (Q2).
sent85: Moreover, SODA uses a very strict syntax for aggregation operators.
sent86: For example, to retrieve the number of movies per year, the input question needs to be written like 'select count (movie) group by (year)'.
sent87: These patterns are useful, but are not in natural language.
sent88: Furthermore, there is no lemmatization, stemming or any other preprocessing of the input question which can lead to a problem with words that are used in plural.
sent89: For example the input question 'all movies' would not detect the table Movie but the input question 'all movie' would display the expected result.
sent90: Blunschi et al (2012) suggest extending SODA to handle temporal aspects of the data warehouse (e.g., bitemporal historization).
sent91: They also pointed out that the GUI of SODA should be improved so that the users are engaged in selecting and ranking the different results.
sent92: Furthermore, the user feedback provided by SODA is currently very basic and needs to be improved."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s22,Keyword-based systems,"The core of keyword-based NLIs is their lookup step. In this step, the system tries to match the given keywords against an inverted index of the base and meta data. To identify keywords in the input question, some systems are using stop word removal (e.g., NLP-Reduce (Kaufmann et al, 2007)), others are expecting only keywords from the users as input (e.g., SODA (Blunschi et al, 2012)).

Most questions are easily formulated with keywords. However, there are some cases where keywords are not enough to express the intention of the users. For example, for the question 'What was the best movie of each genre? ' (Q7) the ""keyword-only version"" would be something like 'best movie genre', which is more likely to be interpreted as 'the genre of the best movie'. If the users would write the question like 'best movie by genre', a keyword-based NLI would try to lookup the token 'by' in the base and meta data or classify 'by' as a stop word and ignore it.

In the following, we will summarize seven keywordbased NLI. We decided to describe SODA (Blunschi et al, 2012) -as the first system -in depth, because it can solve the most of our sample input questions in this category (see Section 2.2). SODA is an NLI that expects only keywords from the user and can handle aggregations by using specific non-natural language templates. Afterwards, the other systems are summarized and we highlight the difference between them to SODA and each other.

The core of keyword-based NLIs is their lookup step. In this step, the system tries to match the given keywords against an inverted index of the base and meta data. To identify keywords in the input question, some systems are using stop word removal (e.g., NLP-Reduce (Kaufmann et al, 2007)), others are expecting only keywords from the users as input (e.g., SODA (Blunschi et al, 2012)).

Most questions are easily formulated with keywords. However, there are some cases where keywords are not enough to express the intention of the users. For example, for the question 'What was the best movie of each genre? ' (Q7) the ""keyword-only version"" would be something like 'best movie genre', which is more likely to be interpreted as 'the genre of the best movie'. If the users would write the question like 'best movie by genre', a keyword-based NLI would try to lookup the token 'by' in the base and meta data or classify 'by' as a stop word and ignore it.

In the following, we will summarize seven keywordbased NLI. We decided to describe SODA (Blunschi et al, 2012) -as the first system -in depth, because it can solve the most of our sample input questions in this category (see Section 2.2). SODA is an NLI that expects only keywords from the user and can handle aggregations by using specific non-natural language templates. Afterwards, the other systems are summarized and we highlight the difference between them to SODA and each other.","[['b29', 'b5'], [], ['b5'], ['b29', 'b5'], [], ['b5']]","[['b29', 'b5'], [], ['b5'], ['b29', 'b5'], [], ['b5']]",6,"sent1: The core of keyword-based NLIs is their lookup step.
sent2: In this step, the system tries to match the given keywords against an inverted index of the base and meta data.
sent3: To identify keywords in the input question, some systems are using stop word removal (e.g., NLP-Reduce (Kaufmann et al, 2007)), others are expecting only keywords from the users as input (e.g., SODA (Blunschi et al, 2012)).
sent4: Most questions are easily formulated with keywords.
sent5: However, there are some cases where keywords are not enough to express the intention of the users.
sent6: For example, for the question 'What was the best movie of each genre? '
sent7: (Q7) the ""keyword-only version"" would be something like 'best movie genre', which is more likely to be interpreted as 'the genre of the best movie'.
sent8: If the users would write the question like 'best movie by genre', a keyword-based NLI would try to lookup the token 'by' in the base and meta data or classify 'by' as a stop word and ignore it.
sent9: In the following, we will summarize seven keywordbased NLI.
sent10: We decided to describe SODA (Blunschi et al, 2012)
sent11: -as the first system -in depth, because it can solve the most of our sample input questions in this category (see Section 2.2).
sent12: SODA is an NLI that expects only keywords from the user and can handle aggregations by using specific non-natural language templates.
sent13: Afterwards, the other systems are summarized and we highlight the difference between them to SODA and each other.
sent14: The core of keyword-based NLIs is their lookup step.
sent15: In this step, the system tries to match the given keywords against an inverted index of the base and meta data.
sent16: To identify keywords in the input question, some systems are using stop word removal (e.g., NLP-Reduce (Kaufmann et al, 2007)), others are expecting only keywords from the users as input (e.g., SODA (Blunschi et al, 2012)).
sent17: Most questions are easily formulated with keywords.
sent18: However, there are some cases where keywords are not enough to express the intention of the users.
sent19: For example, for the question 'What was the best movie of each genre? '
sent20: (Q7) the ""keyword-only version"" would be something like 'best movie genre', which is more likely to be interpreted as 'the genre of the best movie'.
sent21: If the users would write the question like 'best movie by genre', a keyword-based NLI would try to lookup the token 'by' in the base and meta data or classify 'by' as a stop word and ignore it.
sent22: In the following, we will summarize seven keywordbased NLI.
sent23: We decided to describe SODA (Blunschi et al, 2012)
sent24: -as the first system -in depth, because it can solve the most of our sample input questions in this category (see Section 2.2).
sent25: SODA is an NLI that expects only keywords from the user and can handle aggregations by using specific non-natural language templates.
sent26: Afterwards, the other systems are summarized and we highlight the difference between them to SODA and each other."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s17,Recently Developed NLIs,"In this section, we will focus on more recent NLIs starting from 2005. We will not discuss about older systems like BASEBALL (Green et al, 1961), LUNAR (Woods, 1973), RENDEZVOUS (Codd, 1974), LADDER (Sacerdoti, 1977), Chat-80 (Warren and Pereira, 1982), ASK (Thompson and Thompson, 1983) and JANUS (Weischedel, 1989), which are often quoted in the research field of NLIs. We will systematically analyze 24 recently developed systems in Sections 5.1 to 5.4 based on the sample world introduced in Section 2. The main goal is to highlight strengths and weaknesses of the different approaches based on a particular data model and particular questions to be able to directly compare the systems. The explanation is based on the information describing the systems found in the papers. Finally, in Section 6.1 we will evaluate the systems against the the sample questions and give an overall interpretation of the system.

There are different ways to classify NLIs. In this survey, we divide the NLIs into four main groups based on the technical approach they use:

In this section, we will focus on more recent NLIs starting from 2005. We will not discuss about older systems like BASEBALL (Green et al, 1961), LUNAR (Woods, 1973), RENDEZVOUS (Codd, 1974), LADDER (Sacerdoti, 1977), Chat-80 (Warren and Pereira, 1982), ASK (Thompson and Thompson, 1983) and JANUS (Weischedel, 1989), which are often quoted in the research field of NLIs. We will systematically analyze 24 recently developed systems in Sections 5.1 to 5.4 based on the sample world introduced in Section 2. The main goal is to highlight strengths and weaknesses of the different approaches based on a particular data model and particular questions to be able to directly compare the systems. The explanation is based on the information describing the systems found in the papers. Finally, in Section 6.1 we will evaluate the systems against the the sample questions and give an overall interpretation of the system.

There are different ways to classify NLIs. In this survey, we divide the NLIs into four main groups based on the technical approach they use:","[['b8', 'b52', 'b57', 'b60', 'b20', 'b58', 'b44'], [], ['b8', 'b52', 'b57', 'b60', 'b20', 'b58', 'b44'], []]","[['b8', 'b52', 'b57', 'b60', 'b20', 'b58', 'b44'], [], ['b8', 'b52', 'b57', 'b60', 'b20', 'b58', 'b44'], []]",14,"sent1: In this section, we will focus on more recent NLIs starting from 2005.
sent2: We will not discuss about older systems like BASEBALL (Green et al, 1961), LUNAR (Woods, 1973), RENDEZVOUS (Codd, 1974), LADDER (Sacerdoti, 1977), Chat-80 (Warren and Pereira, 1982), ASK (Thompson and Thompson, 1983) and JANUS (Weischedel, 1989), which are often quoted in the research field of NLIs.
sent3: We will systematically analyze 24 recently developed systems in Sections 5.1 to 5.4 based on the sample world introduced in Section 2.
sent4: The main goal is to highlight strengths and weaknesses of the different approaches based on a particular data model and particular questions to be able to directly compare the systems.
sent5: The explanation is based on the information describing the systems found in the papers.
sent6: Finally, in Section 6.1 we will evaluate the systems against the the sample questions and give an overall interpretation of the system.
sent7: There are different ways to classify NLIs.
sent8: In this survey, we divide the NLIs into four main groups based on the technical approach they use:In this section, we will focus on more recent NLIs starting from 2005.
sent9: We will not discuss about older systems like BASEBALL (Green et al, 1961), LUNAR (Woods, 1973), RENDEZVOUS (Codd, 1974), LADDER (Sacerdoti, 1977), Chat-80 (Warren and Pereira, 1982), ASK (Thompson and Thompson, 1983) and JANUS (Weischedel, 1989), which are often quoted in the research field of NLIs.
sent10: We will systematically analyze 24 recently developed systems in Sections 5.1 to 5.4 based on the sample world introduced in Section 2.
sent11: The main goal is to highlight strengths and weaknesses of the different approaches based on a particular data model and particular questions to be able to directly compare the systems.
sent12: The explanation is based on the information describing the systems found in the papers.
sent13: Finally, in Section 6.1 we will evaluate the systems against the the sample questions and give an overall interpretation of the system.
sent14: There are different ways to classify NLIs.
sent15: In this survey, we divide the NLIs into four main groups based on the technical approach they use:"
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s11,Synonymy,"The difficulty of synonymy is that a simple lookup or matching is not enough. For example, the question 'All movies starring Brad Pitt from 2000 until 2010.' (Q3) could also be phrased as 'All movies playing Brad Pitt from 2000 until 2010.'. The answer should be the same, but because in the sample world no element is named 'playing', a lookup would not find an answer. Therefore, it is necessary that the system takes synonyms into account. A possible solution is the use of a translation dictionary. Usually such a dictionary is based on DBpedia (Lehmann et al, 2012) and/or WordNet (Miller, 1995).

The difficulty of synonymy is that a simple lookup or matching is not enough. For example, the question 'All movies starring Brad Pitt from 2000 until 2010.' (Q3) could also be phrased as 'All movies playing Brad Pitt from 2000 until 2010.'. The answer should be the same, but because in the sample world no element is named 'playing', a lookup would not find an answer. Therefore, it is necessary that the system takes synonyms into account. A possible solution is the use of a translation dictionary. Usually such a dictionary is based on DBpedia (Lehmann et al, 2012) and/or WordNet (Miller, 1995).","[['b39', 'b31'], ['b39', 'b31']]","[['b39', 'b31'], ['b39', 'b31']]",4,"sent1: The difficulty of synonymy is that a simple lookup or matching is not enough.
sent2: For example, the question 'All movies starring Brad Pitt from 2000 until 2010.'
sent3: (Q3) could also be phrased as 'All movies playing Brad Pitt from 2000 until 2010.'.
sent4: The answer should be the same, but because in the sample world no element is named 'playing', a lookup would not find an answer.
sent5: Therefore, it is necessary that the system takes synonyms into account.
sent6: A possible solution is the use of a translation dictionary.
sent7: Usually such a dictionary is based on DBpedia (Lehmann et al, 2012) and/or WordNet (Miller, 1995).
sent8: The difficulty of synonymy is that a simple lookup or matching is not enough.
sent9: For example, the question 'All movies starring Brad Pitt from 2000 until 2010.'
sent10: (Q3) could also be phrased as 'All movies playing Brad Pitt from 2000 until 2010.'.
sent11: The answer should be the same, but because in the sample world no element is named 'playing', a lookup would not find an answer.
sent12: Therefore, it is necessary that the system takes synonyms into account.
sent13: A possible solution is the use of a translation dictionary.
sent14: Usually such a dictionary is based on DBpedia (Lehmann et al, 2012) and/or WordNet (Miller, 1995)."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s7,"J, 2xS","for the evaluation of SODA (Blunschi et al (2012)) and the 10 sample questions of this paper. Therefore, we designed nine questions based on the operators of SQL and SPARQL: Joins, Filters (string, range, date or negation), Aggregations, Ordering, Union and Subqueries.

Furthermore, we added a question which is based on a concept (e.g., 'great movie'). Concepts are not part of SQL and SPARQL, but a common addition of NLIs. Table 1 shows those ten full sentence questions in English, which can be applied on the sample world (ordered roughly by difficulty).

The queries were designed in such a way that they cover a wide range of SQL-functionality (technical challenge) as well as linguistic variations (semantic challenge). We will now analyze each of these ten queries in more detail and describe the major challenges for the underlying system to solve them.

The first question (Q1) is a join over different tables (Person, Director, Directing and Movie) with an ISA-relationship between the tables Person and Director. Moreover, the query has a filter on the attribute Movie.Title, which has to be equal to 'Inglourious Basterds'. Therefore, the system faces three different challenges: (a) identify the bridge table Directing to link the tables Director and Movie, (b) identify the hierarchical structure (ISA-relationship) between Director and Person and (c) identify 'Inglourious Basterds' as a filter phrase for Movie.Title.

The second question (Q2) is based on a single table (Movie) with a range filter. The challenge for the NLIs is to translate 'higher than' into a comparison operator 'greater than'.

The third question (Q3) is a join over four tables (Person, Actor, Starring and Movie) and includes two filters: (a) a filter on the attribute Person.FirstName and Person.LastName and (b) a two-sided date range filter on the attribute Movie.ReleaseDate. The challenge in this query (compared to the previous ones) is the date range filter. The system needs to detect that 'from 2000 until 2010 ' refers to a range filter and that the numbers need to be translated into the dates 2000-01-01 and 2010-12-31.

The fourth question (Q4) is a join over two tables (Movie and Gross). In addition, an aggregation on the attribute Gross.Gross and grouping on the attribute Movie.id or ordering the result based on Gross.Gross is needed. For both approaches an aggregation to a single result (indicated by the keyword 'most') is requested.

The fifth question (Q5) is a join over two tables (Movie and Genre). The query can either be interpreted as 'movies that have both genres' (intersection) or 'movie with at least one of those genres ' (union). The expected answer is based on the union interpretation, which can be solved with two filters that are concatenated with an OR on the attribute Genre.Genre.

The sixth question (Q6) needs the definition of concepts. In the sample world the concept 'great movie' is defined as a movie with a rating greater or equal 8. If the system is capable of concepts, then it needs to detect the concept and translate it accordingly to the definition.

The seventh question (Q7) is a join over two tables (Movie and Genre) with an aggregation. The challenges are to (a) identify the grouping by the attribute Genre.Genre and (b) translate the token 'best' to a maximum aggregation on the attribute Movie.Rating.

The eighth question (Q8) is a join over two tables (Movie and Genre) with a negation on the attribute Movie.OriginalLang and a filter on the attribute Genre.Genre. The challenge in this question is to identify the negation 'non-Japanese'. Another possible input question with a negation over a larger movie database, would be 'All actors without an Oscar '. Here again, the challenge is to identify 'without' as a keyword for the negation.

The ninth question (Q9) is based on a single table (Movie) and includes a subquery. The challenge in this question is to divide it in two steps: first select the rating of the movie 'Sin City' and then use this SQL statement as a subquery to compare with the ranking of every other movie in the database.

The tenth question (Q10) is a join over two tables (Movie and Genre). One possible solution would include two not exist: the first one verifies for each movie that there exist no other genres as the genres of 'Sin City'. The second one verifies for each movie that it has no genre, which 'Sin City' does not have. For example, the movie 'Sin City' has the genre 'Thriller ', the movie 'Mission: Impossible' has the genres 'Thriller ' and 'Action'. The first not exist will check if 'Mission: Impossible' has the genre 'Thriller ' from 'Sin City' which is true. The second not exist checks if 'Sin City' has the genres 'Thriller ' and 'Action' (from 'Mission: Impossible'), which is false.

for the evaluation of SODA (Blunschi et al (2012)) and the 10 sample questions of this paper. Therefore, we designed nine questions based on the operators of SQL and SPARQL: Joins, Filters (string, range, date or negation), Aggregations, Ordering, Union and Subqueries.

Furthermore, we added a question which is based on a concept (e.g., 'great movie'). Concepts are not part of SQL and SPARQL, but a common addition of NLIs. Table 1 shows those ten full sentence questions in English, which can be applied on the sample world (ordered roughly by difficulty).

The queries were designed in such a way that they cover a wide range of SQL-functionality (technical challenge) as well as linguistic variations (semantic challenge). We will now analyze each of these ten queries in more detail and describe the major challenges for the underlying system to solve them.

The first question (Q1) is a join over different tables (Person, Director, Directing and Movie) with an ISA-relationship between the tables Person and Director. Moreover, the query has a filter on the attribute Movie.Title, which has to be equal to 'Inglourious Basterds'. Therefore, the system faces three different challenges: (a) identify the bridge table Directing to link the tables Director and Movie, (b) identify the hierarchical structure (ISA-relationship) between Director and Person and (c) identify 'Inglourious Basterds' as a filter phrase for Movie.Title.

The second question (Q2) is based on a single table (Movie) with a range filter. The challenge for the NLIs is to translate 'higher than' into a comparison operator 'greater than'.

The third question (Q3) is a join over four tables (Person, Actor, Starring and Movie) and includes two filters: (a) a filter on the attribute Person.FirstName and Person.LastName and (b) a two-sided date range filter on the attribute Movie.ReleaseDate. The challenge in this query (compared to the previous ones) is the date range filter. The system needs to detect that 'from 2000 until 2010 ' refers to a range filter and that the numbers need to be translated into the dates 2000-01-01 and 2010-12-31.

The fourth question (Q4) is a join over two tables (Movie and Gross). In addition, an aggregation on the attribute Gross.Gross and grouping on the attribute Movie.id or ordering the result based on Gross.Gross is needed. For both approaches an aggregation to a single result (indicated by the keyword 'most') is requested.

The fifth question (Q5) is a join over two tables (Movie and Genre). The query can either be interpreted as 'movies that have both genres' (intersection) or 'movie with at least one of those genres ' (union). The expected answer is based on the union interpretation, which can be solved with two filters that are concatenated with an OR on the attribute Genre.Genre.

The sixth question (Q6) needs the definition of concepts. In the sample world the concept 'great movie' is defined as a movie with a rating greater or equal 8. If the system is capable of concepts, then it needs to detect the concept and translate it accordingly to the definition.

The seventh question (Q7) is a join over two tables (Movie and Genre) with an aggregation. The challenges are to (a) identify the grouping by the attribute Genre.Genre and (b) translate the token 'best' to a maximum aggregation on the attribute Movie.Rating.

The eighth question (Q8) is a join over two tables (Movie and Genre) with a negation on the attribute Movie.OriginalLang and a filter on the attribute Genre.Genre. The challenge in this question is to identify the negation 'non-Japanese'. Another possible input question with a negation over a larger movie database, would be 'All actors without an Oscar '. Here again, the challenge is to identify 'without' as a keyword for the negation.

The ninth question (Q9) is based on a single table (Movie) and includes a subquery. The challenge in this question is to divide it in two steps: first select the rating of the movie 'Sin City' and then use this SQL statement as a subquery to compare with the ranking of every other movie in the database.

The tenth question (Q10) is a join over two tables (Movie and Genre). One possible solution would include two not exist: the first one verifies for each movie that there exist no other genres as the genres of 'Sin City'. The second one verifies for each movie that it has no genre, which 'Sin City' does not have. For example, the movie 'Sin City' has the genre 'Thriller ', the movie 'Mission: Impossible' has the genres 'Thriller ' and 'Action'. The first not exist will check if 'Mission: Impossible' has the genre 'Thriller ' from 'Sin City' which is true. The second not exist checks if 'Sin City' has the genres 'Thriller ' and 'Action' (from 'Mission: Impossible'), which is false.","[['b5'], [], [], [], [], [], [], [None], [], [], [], [], [], ['b5'], [], [], [], [], [], [], [None], [], [], [], [], []]","[['b5'], [], [], [], [], [], [], [None], [], [], [], [], [], ['b5'], [], [], [], [], [], [], [None], [], [], [], [], []]",4,"sent1: for the evaluation of SODA (Blunschi et al (2012)) and the 10 sample questions of this paper.
sent2: Therefore, we designed nine questions based on the operators of SQL and SPARQL: Joins, Filters (string, range, date or negation), Aggregations, Ordering, Union and Subqueries.
sent3: Furthermore, we added a question which is based on a concept (e.g., 'great movie').
sent4: Concepts are not part of SQL and SPARQL, but a common addition of NLIs.
sent5: Table 1 shows those ten full sentence questions in English, which can be applied on the sample world (ordered roughly by difficulty).
sent6: The queries were designed in such a way that they cover a wide range of SQL-functionality (technical challenge) as well as linguistic variations (semantic challenge).
sent7: We will now analyze each of these ten queries in more detail and describe the major challenges for the underlying system to solve them.
sent8: The first question (Q1) is a join over different tables (Person, Director, Directing and Movie) with an ISA-relationship between the tables Person and Director.
sent9: Moreover, the query has a filter on the attribute Movie.
sent10: Title, which has to be equal to 'Inglourious Basterds'.
sent11: Therefore, the system faces three different challenges: (a) identify the bridge table Directing to link the tables Director and Movie, (b) identify the hierarchical structure (ISA-relationship) between Director and Person and (c) identify 'Inglourious Basterds' as a filter phrase for Movie.
sent12: Title. The second question (Q2) is based on a single table (Movie) with a range filter.
sent13: The challenge for the NLIs is to translate 'higher than' into a comparison operator 'greater than'.
sent14: The third question (Q3) is a join over four tables (Person, Actor, Starring and Movie) and includes two filters: (a) a filter on the attribute Person.
sent15: FirstName and Person. LastName and (b) a two-sided date range filter on the attribute Movie.
sent16: ReleaseDate. The challenge in this query (compared to the previous ones) is the date range filter.
sent17: The system needs to detect that 'from 2000 until 2010 ' refers to a range filter and that the numbers need to be translated into the dates 2000-01-01 and 2010-12-31.
sent18: The fourth question (Q4) is a join over two tables (Movie and Gross).
sent19: In addition, an aggregation on the attribute Gross.
sent20: Gross and grouping on the attribute Movie.id or ordering the result based on Gross.
sent21: Gross is needed. For both approaches an aggregation to a single result (indicated by the keyword 'most') is requested.
sent22: The fifth question (Q5) is a join over two tables (Movie and Genre).
sent23: The query can either be interpreted as 'movies that have both genres' (intersection) or 'movie with at least one of those genres ' (union).
sent24: The expected answer is based on the union interpretation, which can be solved with two filters that are concatenated with an OR on the attribute Genre.
sent25: Genre. The sixth question (Q6) needs the definition of concepts.
sent26: In the sample world the concept 'great movie' is defined as a movie with a rating greater or equal 8.
sent27: If the system is capable of concepts, then it needs to detect the concept and translate it accordingly to the definition.
sent28: The seventh question (Q7) is a join over two tables (Movie and Genre) with an aggregation.
sent29: The challenges are to (a) identify the grouping by the attribute Genre.
sent30: Genre and (b) translate the token 'best' to a maximum aggregation on the attribute Movie.
sent31: Rating. The eighth question (Q8) is a join over two tables (Movie and Genre) with a negation on the attribute Movie.
sent32: OriginalLang and a filter on the attribute Genre.
sent33: Genre. The challenge in this question is to identify the negation 'non-Japanese'.
sent34: Another possible input question with a negation over a larger movie database, would be 'All actors without an Oscar '.
sent35: Here again, the challenge is to identify 'without' as a keyword for the negation.
sent36: The ninth question (Q9) is based on a single table (Movie) and includes a subquery.
sent37: The challenge in this question is to divide it in two steps: first select the rating of the movie 'Sin City' and then use this SQL statement as a subquery to compare with the ranking of every other movie in the database.
sent38: The tenth question (Q10) is a join over two tables (Movie and Genre).
sent39: One possible solution would include two not exist: the first one verifies for each movie that there exist no other genres as the genres of 'Sin City'.
sent40: The second one verifies for each movie that it has no genre, which 'Sin City' does not have.
sent41: For example, the movie 'Sin City' has the genre 'Thriller ', the movie 'Mission: Impossible' has the genres 'Thriller ' and 'Action'.
sent42: The first not exist will check if 'Mission: Impossible' has the genre 'Thriller ' from 'Sin City' which is true.
sent43: The second not exist checks if 'Sin City' has the genres 'Thriller ' and 'Action' (from 'Mission: Impossible'), which is false.
sent44: for the evaluation of SODA (Blunschi et al (2012)) and the 10 sample questions of this paper.
sent45: Therefore, we designed nine questions based on the operators of SQL and SPARQL: Joins, Filters (string, range, date or negation), Aggregations, Ordering, Union and Subqueries.
sent46: Furthermore, we added a question which is based on a concept (e.g., 'great movie').
sent47: Concepts are not part of SQL and SPARQL, but a common addition of NLIs.
sent48: Table 1 shows those ten full sentence questions in English, which can be applied on the sample world (ordered roughly by difficulty).
sent49: The queries were designed in such a way that they cover a wide range of SQL-functionality (technical challenge) as well as linguistic variations (semantic challenge).
sent50: We will now analyze each of these ten queries in more detail and describe the major challenges for the underlying system to solve them.
sent51: The first question (Q1) is a join over different tables (Person, Director, Directing and Movie) with an ISA-relationship between the tables Person and Director.
sent52: Moreover, the query has a filter on the attribute Movie.
sent53: Title, which has to be equal to 'Inglourious Basterds'.
sent54: Therefore, the system faces three different challenges: (a) identify the bridge table Directing to link the tables Director and Movie, (b) identify the hierarchical structure (ISA-relationship) between Director and Person and (c) identify 'Inglourious Basterds' as a filter phrase for Movie.
sent55: Title. The second question (Q2) is based on a single table (Movie) with a range filter.
sent56: The challenge for the NLIs is to translate 'higher than' into a comparison operator 'greater than'.
sent57: The third question (Q3) is a join over four tables (Person, Actor, Starring and Movie) and includes two filters: (a) a filter on the attribute Person.
sent58: FirstName and Person. LastName and (b) a two-sided date range filter on the attribute Movie.
sent59: ReleaseDate. The challenge in this query (compared to the previous ones) is the date range filter.
sent60: The system needs to detect that 'from 2000 until 2010 ' refers to a range filter and that the numbers need to be translated into the dates 2000-01-01 and 2010-12-31.
sent61: The fourth question (Q4) is a join over two tables (Movie and Gross).
sent62: In addition, an aggregation on the attribute Gross.
sent63: Gross and grouping on the attribute Movie.id or ordering the result based on Gross.
sent64: Gross is needed. For both approaches an aggregation to a single result (indicated by the keyword 'most') is requested.
sent65: The fifth question (Q5) is a join over two tables (Movie and Genre).
sent66: The query can either be interpreted as 'movies that have both genres' (intersection) or 'movie with at least one of those genres ' (union).
sent67: The expected answer is based on the union interpretation, which can be solved with two filters that are concatenated with an OR on the attribute Genre.
sent68: Genre. The sixth question (Q6) needs the definition of concepts.
sent69: In the sample world the concept 'great movie' is defined as a movie with a rating greater or equal 8.
sent70: If the system is capable of concepts, then it needs to detect the concept and translate it accordingly to the definition.
sent71: The seventh question (Q7) is a join over two tables (Movie and Genre) with an aggregation.
sent72: The challenges are to (a) identify the grouping by the attribute Genre.
sent73: Genre and (b) translate the token 'best' to a maximum aggregation on the attribute Movie.
sent74: Rating. The eighth question (Q8) is a join over two tables (Movie and Genre) with a negation on the attribute Movie.
sent75: OriginalLang and a filter on the attribute Genre.
sent76: Genre. The challenge in this question is to identify the negation 'non-Japanese'.
sent77: Another possible input question with a negation over a larger movie database, would be 'All actors without an Oscar '.
sent78: Here again, the challenge is to identify 'without' as a keyword for the negation.
sent79: The ninth question (Q9) is based on a single table (Movie) and includes a subquery.
sent80: The challenge in this question is to divide it in two steps: first select the rating of the movie 'Sin City' and then use this SQL statement as a subquery to compare with the ranking of every other movie in the database.
sent81: The tenth question (Q10) is a join over two tables (Movie and Genre).
sent82: One possible solution would include two not exist: the first one verifies for each movie that there exist no other genres as the genres of 'Sin City'.
sent83: The second one verifies for each movie that it has no genre, which 'Sin City' does not have.
sent84: For example, the movie 'Sin City' has the genre 'Thriller ', the movie 'Mission: Impossible' has the genres 'Thriller ' and 'Action'.
sent85: The first not exist will check if 'Mission: Impossible' has the genre 'Thriller ' from 'Sin City' which is true.
sent86: The second not exist checks if 'Sin City' has the genres 'Thriller ' and 'Action' (from 'Mission: Impossible'), which is false."
195316636,A Comparative Survey of Recent Natural Language Interfaces for Databases,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/c4e3955219e8b008e4a14fbd0a1aac17cdba568d,s31,NLQ/A,"NLQ/A (Zheng et al, 2017) is an NLI to query a knowledge graph. The system is based on a new approach without NLP technologies like parsers or PoS taggers. The idea being that the errors made by these technologies are not worth the gain of information. For example, a parse tree helps for certain questions like subqueries (e.g., Q9), but if the parse tree is wrong, the system will fail to translate even simpler questions. Instead, NLQ/A lets the users resolve all ambiguity problems, also those which could be solved with PoS tagging or parse trees. To avoid needing too many interaction steps, NLQ/A provides an efficient greedy approach for the interaction process.

Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question could be: 'Who is the director of ""Inglourious Basterds""? '.

NLQ/A use four steps to answer the input question. The first step is to detect the phrases of the input question. In general, the phrases can be categorized into two types: independent and dependent phrases. Independent phrases are identified with a phrase dictionary. The dictionary consists of variables, aggregations, operators, modifiers and quantifier phrases. To detect dependent phrases, most stop words are removed (simplified input question). Some types of words like prepositions are still needed and therefore kept. Next 1:n-grams are generated. Phrases starting with prepositions are discarded. After stop word removal, the input question Q1 would become 'director of Inglourious Basterds'. If n is set to 2, the extracted phrases would be: {'director ', 'director of ', 'Inglourious', 'Inglourious Basterds', 'Basterds'}. Next, the phrases are extended according to a synonym dictionary. For example if there is a phrase 'starring' it would be extended with the phrase 'playing'. Those extended phrases are mapped to the knowledge graph based on the string similarity (edit distance). For one extended phrase there can be multiple candidate mappings. The next step takes the candidate mappings and tries to find the true meaning of the input question with the help of the users. To reduce the amount of interactions for the user, a phrase dependency graph (PDG) is proposed. The PDG consists of two parts: (PDG1) a graph where each node represents a phrase, two phrases are connected if they share at least one common token and (PDG2) a subgraph of the knowledge graph consisting of the candidates where each node represents a candidate, two nodes are connected if they are adjacent in the knowledge graph. The two parts are connected with edges, representing the mapping between phrases and candidates (see Figure 8).

In the third step, the users get involved to solve the ambiguity given in the PDG. In order to reduce the necessary user interactions, the NLI tries to find those edges which resolve the most ambiguities (similar to the idea of QUICK).

The last step takes the selected candidates and tries to connect them into one graph. The connected graph will include the answer to the question. Groups of already connected candidates in the PDG2 are called query fragments. In Figure 8, the candidates 'director-Of ' and 'Inglourious Basterds' are one query fragment. For each query fragment, the system tries to find the path with the highest similarity to the simplified input question. For the input question Q1, if the users select 'Director ' as candidate in step 3, the system would find the path as shown in Figure 9. 'Inglourious Basterds' is also a candidate, but not selected by the users because there is no ambiguity to solve.

The strengths of this NLI are the simplicity and the efficient user interaction process. The simplicity allows easy adaption on new knowledge graphs and together with the user interaction process it overcomes the difficulties of ambiguity.

The weakness of this system is that usually more than one user interaction is needed to resolve ambiguities, in the experiments the average number of interactions was three (Zheng et al, 2017).

NLQ/A (Zheng et al, 2017) is an NLI to query a knowledge graph. The system is based on a new approach without NLP technologies like parsers or PoS taggers. The idea being that the errors made by these technologies are not worth the gain of information. For example, a parse tree helps for certain questions like subqueries (e.g., Q9), but if the parse tree is wrong, the system will fail to translate even simpler questions. Instead, NLQ/A lets the users resolve all ambiguity problems, also those which could be solved with PoS tagging or parse trees. To avoid needing too many interaction steps, NLQ/A provides an efficient greedy approach for the interaction process.

Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question could be: 'Who is the director of ""Inglourious Basterds""? '.

NLQ/A use four steps to answer the input question. The first step is to detect the phrases of the input question. In general, the phrases can be categorized into two types: independent and dependent phrases. Independent phrases are identified with a phrase dictionary. The dictionary consists of variables, aggregations, operators, modifiers and quantifier phrases. To detect dependent phrases, most stop words are removed (simplified input question). Some types of words like prepositions are still needed and therefore kept. Next 1:n-grams are generated. Phrases starting with prepositions are discarded. After stop word removal, the input question Q1 would become 'director of Inglourious Basterds'. If n is set to 2, the extracted phrases would be: {'director ', 'director of ', 'Inglourious', 'Inglourious Basterds', 'Basterds'}. Next, the phrases are extended according to a synonym dictionary. For example if there is a phrase 'starring' it would be extended with the phrase 'playing'. Those extended phrases are mapped to the knowledge graph based on the string similarity (edit distance). For one extended phrase there can be multiple candidate mappings. The next step takes the candidate mappings and tries to find the true meaning of the input question with the help of the users. To reduce the amount of interactions for the user, a phrase dependency graph (PDG) is proposed. The PDG consists of two parts: (PDG1) a graph where each node represents a phrase, two phrases are connected if they share at least one common token and (PDG2) a subgraph of the knowledge graph consisting of the candidates where each node represents a candidate, two nodes are connected if they are adjacent in the knowledge graph. The two parts are connected with edges, representing the mapping between phrases and candidates (see Figure 8).

In the third step, the users get involved to solve the ambiguity given in the PDG. In order to reduce the necessary user interactions, the NLI tries to find those edges which resolve the most ambiguities (similar to the idea of QUICK).

The last step takes the selected candidates and tries to connect them into one graph. The connected graph will include the answer to the question. Groups of already connected candidates in the PDG2 are called query fragments. In Figure 8, the candidates 'director-Of ' and 'Inglourious Basterds' are one query fragment. For each query fragment, the system tries to find the path with the highest similarity to the simplified input question. For the input question Q1, if the users select 'Director ' as candidate in step 3, the system would find the path as shown in Figure 9. 'Inglourious Basterds' is also a candidate, but not selected by the users because there is no ambiguity to solve.

The strengths of this NLI are the simplicity and the efficient user interaction process. The simplicity allows easy adaption on new knowledge graphs and together with the user interaction process it overcomes the difficulties of ambiguity.

The weakness of this system is that usually more than one user interaction is needed to resolve ambiguities, in the experiments the average number of interactions was three (Zheng et al, 2017).","[['b65'], [], [], [], [], [], ['b65'], ['b65'], [], [], [], [], [], ['b65']]","[['b65'], [], [], [], [], [], ['b65'], ['b65'], [], [], [], [], [], ['b65']]",4,"sent1: NLQ/A (Zheng et al, 2017) is an NLI to query a knowledge graph.
sent2: The system is based on a new approach without NLP technologies like parsers or PoS taggers.
sent3: The idea being that the errors made by these technologies are not worth the gain of information.
sent4: For example, a parse tree helps for certain questions like subqueries (e.g., Q9), but if the parse tree is wrong, the system will fail to translate even simpler questions.
sent5: Instead, NLQ/A lets the users resolve all ambiguity problems, also those which could be solved with PoS tagging or parse trees.
sent6: To avoid needing too many interaction steps, NLQ/A provides an efficient greedy approach for the interaction process.
sent7: Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question could be: 'Who is the director of ""Inglourious Basterds""?
sent8: '. NLQ/A use four steps to answer the input question.
sent9: The first step is to detect the phrases of the input question.
sent10: In general, the phrases can be categorized into two types: independent and dependent phrases.
sent11: Independent phrases are identified with a phrase dictionary.
sent12: The dictionary consists of variables, aggregations, operators, modifiers and quantifier phrases.
sent13: To detect dependent phrases, most stop words are removed (simplified input question).
sent14: Some types of words like prepositions are still needed and therefore kept.
sent15: Next 1:n-grams are generated. Phrases starting with prepositions are discarded.
sent16: After stop word removal, the input question Q1 would become 'director of Inglourious Basterds'.
sent17: If n is set to 2, the extracted phrases would be: {'director ', 'director of ', 'Inglourious', 'Inglourious Basterds', 'Basterds'}.
sent18: Next, the phrases are extended according to a synonym dictionary.
sent19: For example if there is a phrase 'starring' it would be extended with the phrase 'playing'.
sent20: Those extended phrases are mapped to the knowledge graph based on the string similarity (edit distance).
sent21: For one extended phrase there can be multiple candidate mappings.
sent22: The next step takes the candidate mappings and tries to find the true meaning of the input question with the help of the users.
sent23: To reduce the amount of interactions for the user, a phrase dependency graph (PDG) is proposed.
sent24: The PDG consists of two parts: (PDG1) a graph where each node represents a phrase, two phrases are connected if they share at least one common token and (PDG2) a subgraph of the knowledge graph consisting of the candidates where each node represents a candidate, two nodes are connected if they are adjacent in the knowledge graph.
sent25: The two parts are connected with edges, representing the mapping between phrases and candidates (see Figure 8).
sent26: In the third step, the users get involved to solve the ambiguity given in the PDG.
sent27: In order to reduce the necessary user interactions, the NLI tries to find those edges which resolve the most ambiguities (similar to the idea of QUICK).
sent28: The last step takes the selected candidates and tries to connect them into one graph.
sent29: The connected graph will include the answer to the question.
sent30: Groups of already connected candidates in the PDG2 are called query fragments.
sent31: In Figure 8, the candidates 'director-Of ' and 'Inglourious Basterds' are one query fragment.
sent32: For each query fragment, the system tries to find the path with the highest similarity to the simplified input question.
sent33: For the input question Q1, if the users select 'Director ' as candidate in step 3, the system would find the path as shown in Figure 9.
sent34: 'Inglourious Basterds' is also a candidate, but not selected by the users because there is no ambiguity to solve.
sent35: The strengths of this NLI are the simplicity and the efficient user interaction process.
sent36: The simplicity allows easy adaption on new knowledge graphs and together with the user interaction process it overcomes the difficulties of ambiguity.
sent37: The weakness of this system is that usually more than one user interaction is needed to resolve ambiguities, in the experiments the average number of interactions was three (Zheng et al, 2017).
sent38: NLQ/A (Zheng et al, 2017) is an NLI to query a knowledge graph.
sent39: The system is based on a new approach without NLP technologies like parsers or PoS taggers.
sent40: The idea being that the errors made by these technologies are not worth the gain of information.
sent41: For example, a parse tree helps for certain questions like subqueries (e.g., Q9), but if the parse tree is wrong, the system will fail to translate even simpler questions.
sent42: Instead, NLQ/A lets the users resolve all ambiguity problems, also those which could be solved with PoS tagging or parse trees.
sent43: To avoid needing too many interaction steps, NLQ/A provides an efficient greedy approach for the interaction process.
sent44: Assuming the users want to know the director of the movie 'Inglourious Basterds' (Q1), the input question could be: 'Who is the director of ""Inglourious Basterds""?
sent45: '. NLQ/A use four steps to answer the input question.
sent46: The first step is to detect the phrases of the input question.
sent47: In general, the phrases can be categorized into two types: independent and dependent phrases.
sent48: Independent phrases are identified with a phrase dictionary.
sent49: The dictionary consists of variables, aggregations, operators, modifiers and quantifier phrases.
sent50: To detect dependent phrases, most stop words are removed (simplified input question).
sent51: Some types of words like prepositions are still needed and therefore kept.
sent52: Next 1:n-grams are generated. Phrases starting with prepositions are discarded.
sent53: After stop word removal, the input question Q1 would become 'director of Inglourious Basterds'.
sent54: If n is set to 2, the extracted phrases would be: {'director ', 'director of ', 'Inglourious', 'Inglourious Basterds', 'Basterds'}.
sent55: Next, the phrases are extended according to a synonym dictionary.
sent56: For example if there is a phrase 'starring' it would be extended with the phrase 'playing'.
sent57: Those extended phrases are mapped to the knowledge graph based on the string similarity (edit distance).
sent58: For one extended phrase there can be multiple candidate mappings.
sent59: The next step takes the candidate mappings and tries to find the true meaning of the input question with the help of the users.
sent60: To reduce the amount of interactions for the user, a phrase dependency graph (PDG) is proposed.
sent61: The PDG consists of two parts: (PDG1) a graph where each node represents a phrase, two phrases are connected if they share at least one common token and (PDG2) a subgraph of the knowledge graph consisting of the candidates where each node represents a candidate, two nodes are connected if they are adjacent in the knowledge graph.
sent62: The two parts are connected with edges, representing the mapping between phrases and candidates (see Figure 8).
sent63: In the third step, the users get involved to solve the ambiguity given in the PDG.
sent64: In order to reduce the necessary user interactions, the NLI tries to find those edges which resolve the most ambiguities (similar to the idea of QUICK).
sent65: The last step takes the selected candidates and tries to connect them into one graph.
sent66: The connected graph will include the answer to the question.
sent67: Groups of already connected candidates in the PDG2 are called query fragments.
sent68: In Figure 8, the candidates 'director-Of ' and 'Inglourious Basterds' are one query fragment.
sent69: For each query fragment, the system tries to find the path with the highest similarity to the simplified input question.
sent70: For the input question Q1, if the users select 'Director ' as candidate in step 3, the system would find the path as shown in Figure 9.
sent71: 'Inglourious Basterds' is also a candidate, but not selected by the users because there is no ambiguity to solve.
sent72: The strengths of this NLI are the simplicity and the efficient user interaction process.
sent73: The simplicity allows easy adaption on new knowledge graphs and together with the user interaction process it overcomes the difficulties of ambiguity.
sent74: The weakness of this system is that usually more than one user interaction is needed to resolve ambiguities, in the experiments the average number of interactions was three (Zheng et al, 2017)."
195345457,Emotionally-Aware Chatbots: A Survey,Computer Science,https://www.semanticscholar.org/paper/c131665638feb8c11f936989ffc6187317593b41,s0,HISTORY OF EMOTIONALLY-AWARE CHATBOT,"The early development of chatbot was inspired by Turing test in 1950 [57]. Eliza was the first publicly known chatbot, built by using simple hand-crafted script [58]. Parry [8] was another chatbot which successfully passed the Turing test. Similar to Eliza, Parry still uses a rule-based approach but with a better understanding, including the mental model that can stimulate emotion. Therefore, Parry is the first chatbot which involving emotion in its development. Also, worth to be mentioned is ALICE (Artificial Linguistic Internet Computer Entity), a customizable chatbot by using Artificial Intelligence Markup Language (AIML). Therefore, ALICE still also use a rule-based approach by executing a pattern-matcher recursively to obtain the response. Then in May 2014, Microsoft introduced XiaoIce [66], an empathetic social chatbot which is able to recognize users' emotional needs. XiaoIce can provide an engaging interpersonal communication by giving encouragement or other affective messages, so that can hold human attention during communication.

Nowadays, most of chatbots technologies were built by using neural-based approach. Emotional Chatting Machine (ECM) [65] was the first works which exploiting deep learning approach in building a large-scale emotionally-aware conversational bot. Then several studies were proposed to deal with this research area by introducing emotion embedding representation [2,9,50] or modeling as reinforcement learning problem [23,55]. Most of these studies used encoder-decoder architecture, specifically sequence to sequence (seq2seq) learning. Some works also tried to introduce a new dataset in order to have a better gold standard and improve system performance. [45] introduce EMPATHETICDIA-LOGUES dataset, a novel dataset containing 25k conversations include emotional contexts information to facilitate training and evaluating the textual conversational system. Then, work from [18] produce a dataset containing 1.5 million Twitter conversation, gathered by using Twitter API from customer care account of 62 brands across several industries. This dataset was used to build tone-aware customer care chatbot. Finally, [27] tried to enhance SEMAINE corpus [29] by using crowdsourcing scenario to obtain a human judgement for deciding which response that elicits positive emotion. Their dataset was used to develop a chatbot which captures human emotional states and elicits positive emotion during the conversation.","[['b7', 'b57', 'b56', 'b65'], ['b64', 'b8', 'b49', 'b17', 'b22', 'b28', 'b26', 'b54', 'b1', 'b44']]","[['b7', 'b57', 'b56', 'b65'], ['b64', 'b8', 'b49', 'b17', 'b22', 'b28', 'b26', 'b54', 'b1', 'b44']]",14,"sent1: The early development of chatbot was inspired by Turing test in 1950 [57].
sent2: Eliza was the first publicly known chatbot, built by using simple hand-crafted script [58].
sent3: Parry [8] was another chatbot which successfully passed the Turing test.
sent4: Similar to Eliza, Parry still uses a rule-based approach but with a better understanding, including the mental model that can stimulate emotion.
sent5: Therefore, Parry is the first chatbot which involving emotion in its development.
sent6: Also, worth to be mentioned is ALICE (Artificial Linguistic Internet Computer Entity), a customizable chatbot by using Artificial Intelligence Markup Language (AIML).
sent7: Therefore, ALICE still also use a rule-based approach by executing a pattern-matcher recursively to obtain the response.
sent8: Then in May 2014, Microsoft introduced XiaoIce [66], an empathetic social chatbot which is able to recognize users' emotional needs.
sent9: XiaoIce can provide an engaging interpersonal communication by giving encouragement or other affective messages, so that can hold human attention during communication.
sent10: Nowadays, most of chatbots technologies were built by using neural-based approach.
sent11: Emotional Chatting Machine (ECM)
sent12: [65] was the first works which exploiting deep learning approach in building a large-scale emotionally-aware conversational bot.
sent13: Then several studies were proposed to deal with this research area by introducing emotion embedding representation [2,9,50] or modeling as reinforcement learning problem [23,55].
sent14: Most of these studies used encoder-decoder architecture, specifically sequence to sequence (seq2seq) learning.
sent15: Some works also tried to introduce a new dataset in order to have a better gold standard and improve system performance.
sent16: [45] introduce EMPATHETICDIA-LOGUES dataset, a novel dataset containing 25k conversations include emotional contexts information to facilitate training and evaluating the textual conversational system.
sent17: Then, work from [18] produce a dataset containing 1.5 million Twitter conversation, gathered by using Twitter API from customer care account of 62 brands across several industries.
sent18: This dataset was used to build tone-aware customer care chatbot.
sent19: Finally, [27] tried to enhance SEMAINE corpus [29] by using crowdsourcing scenario to obtain a human judgement for deciding which response that elicits positive emotion.
sent20: Their dataset was used to develop a chatbot which captures human emotional states and elicits positive emotion during the conversation."
195345457,Emotionally-Aware Chatbots: A Survey,Computer Science,https://www.semanticscholar.org/paper/c131665638feb8c11f936989ffc6187317593b41,s1,BUILDING EMOTIONALLY-AWARE CHATBOT (EAC),"As we mentioned before that emotion is an essential aspect of building humanize chatbot. The rise of the emotionally-aware chatbot is started by Parry [8] in early 1975. Now, most of EAC development exploits neural-based model. In this section, we will try to review previous works which focus on EAC development. Table 1 summarizes this information includes the objective and exploited approach of each work. In early development, EAC is designed by using a rule-based approach. However, in recent years mostly EAC exploit neural-based approach. Studies in EAC development become a hot topic start from 2017, noted by the first shared task in Emotion Generation Challenge on NLPCC 2017 [20]. Based on Table 1 this research line continues to gain massive attention from scholars in the latest years. Based on Table 1, we can see that most of all recent EAC was built by using encoder-decoder architecture with sequence-to-sequence learning. These seq2seq learning models maximize the likelihood of response and are prepared to incorporate rich data to generate an appropriate answer. Basic seq2seq architecture structured of two recurrent neural networks (RNNs), one as an encoder processing the input and one as a decoder generating the response. long short term memory (LSTM) or gated recurrent unit (GRU) was the most dominant variant of RNNs which used to learn the conversational dataset in these models. Some studies also tried to model this task as a reinforcement learning task, in order to get more generic responses and let the chatbot able to achieve successful long-term conversation. Attention mechanism was also introduced in this report 6 . This mechanism will allow the decoder to focus only on some important parts in the input at every decoding step.

Another vital part of building EAC is emotion classifier to detect emotion contained in the text to produce a more meaningful response. Emotion detection is a well-established task in natural language processing research area. This task was promoted in two latest series of SemEval-2018 (Task 1) and SemEval-2019 (Task 3). Some tasks were focusing on classifying utterance into several categories of emotion [34]. However, there is also a task which trying to predict the emotion intensities contained in the text [33]. In the early development of emotion classifier, most of the studies proposed to use traditional machine-learning approach. However, the neural-based approach is able to gain better performance, which leads more scholars to exploit it to deal with this task. In chatbot, the system will generate several responses based on several emotion categories. Then the system will respond with the most appropriate emotion based on emotion detected on posted utterance by emotion classifier. Based on Table 1, studies have different emotion categories based on their focus and objective in building chatbots.","[['b7', 'b5', 'b19'], ['b33', 'b32']]","[['b7', 'b5', 'b19'], ['b33', 'b32']]",5,"sent1: As we mentioned before that emotion is an essential aspect of building humanize chatbot.
sent2: The rise of the emotionally-aware chatbot is started by Parry [8] in early 1975.
sent3: Now, most of EAC development exploits neural-based model.
sent4: In this section, we will try to review previous works which focus on EAC development.
sent5: Table 1 summarizes this information includes the objective and exploited approach of each work.
sent6: In early development, EAC is designed by using a rule-based approach.
sent7: However, in recent years mostly EAC exploit neural-based approach.
sent8: Studies in EAC development become a hot topic start from 2017, noted by the first shared task in Emotion Generation Challenge on NLPCC 2017 [20].
sent9: Based on Table 1 this research line continues to gain massive attention from scholars in the latest years.
sent10: Based on Table 1, we can see that most of all recent EAC was built by using encoder-decoder architecture with sequence-to-sequence learning.
sent11: These seq2seq learning models maximize the likelihood of response and are prepared to incorporate rich data to generate an appropriate answer.
sent12: Basic seq2seq architecture structured of two recurrent neural networks (RNNs), one as an encoder processing the input and one as a decoder generating the response.
sent13: long short term memory (LSTM) or gated recurrent unit (GRU) was the most dominant variant of RNNs which used to learn the conversational dataset in these models.
sent14: Some studies also tried to model this task as a reinforcement learning task, in order to get more generic responses and let the chatbot able to achieve successful long-term conversation.
sent15: Attention mechanism was also introduced in this report 6 .
sent16: This mechanism will allow the decoder to focus only on some important parts in the input at every decoding step.
sent17: Another vital part of building EAC is emotion classifier to detect emotion contained in the text to produce a more meaningful response.
sent18: Emotion detection is a well-established task in natural language processing research area.
sent19: This task was promoted in two latest series of SemEval-2018 (Task 1) and SemEval-2019 (Task 3).
sent20: Some tasks were focusing on classifying utterance into several categories of emotion [34].
sent21: However, there is also a task which trying to predict the emotion intensities contained in the text [33].
sent22: In the early development of emotion classifier, most of the studies proposed to use traditional machine-learning approach.
sent23: However, the neural-based approach is able to gain better performance, which leads more scholars to exploit it to deal with this task.
sent24: In chatbot, the system will generate several responses based on several emotion categories.
sent25: Then the system will respond with the most appropriate emotion based on emotion detected on posted utterance by emotion classifier.
sent26: Based on Table 1, studies have different emotion categories based on their focus and objective in building chatbots."
195345457,Emotionally-Aware Chatbots: A Survey,Computer Science,https://www.semanticscholar.org/paper/c131665638feb8c11f936989ffc6187317593b41,s2,Authors,"Year Focus Approach Kenneth Mark Colby [8] 1975

Designing chatbot behave like paranoid person

Rule-based approach with capability to simulate emotion. Zhou et. al. [66] 2014 Building empathetic chatbot for social interaction

Seq2seq learning with GRU-RNN model which takes into account intelligent quotient (IQ) and emotion quotien (EQ). Zhou et. al. [65] 2018 Building emotion chatting machine (ECM) for large scale conversation generation Seq2seq learning with GRU which incorporates emotion detection to capture implicit internal emotion states. Colombo et. al. [9] 2019 Developing affect-driven dialogue system which generates emotional responses in a controlled manner Seq2seq with GRU which incorporates emotion classifier, also affective re-ranking in the last step to produce the response. Li et. al. [23] 2019 Building conversational system emotional by using editing constraints to generate more meaningful and customizable emotional replies Encoder-decoder architecture with reinforcement learning approach by using asynchronous decoder which uses keyword predictor to predict the topic and emotion editor to produce emotional-embedded response. Zhang et. al. [62] 2017 Building emotional conversation systems which produces several responses for every emotion category

Multi-task seq2seq model with GRU which utilize bidirectional long short term memory (Bi-LSTMs) as emotion classifier.

Lubis et. al. [27] 2019 Building a fully data driven chatoriented dialogue system that can dynamically mimic affective human interactions

Proposing a seq2seq response generator, includes emotion encoder which is trained jointly with the entire network to encode and maintain the emotional context throughout the dialogue, but focusing only on positive emotion Ashgar et. al. [2] 2018 Building open-domain neural dialogue models by augmenting them with affective intelligence.

Encoder-decoder architecture with LSTM model using cognitively engineered affective word embedding, and also affectively diverse beam search for decoding Sun et. al. [55] 2018 Developing conversation generation which addresses the emotional factor by changing the model's input.

Encoder-decoder framework based on LSTM using three inputs, a sequence without an emotional category, a sequence with an emotional category for the input sentence, and a sequence with an emotional category for the output responses. Hu et. al. [18] 2018 Building a tone-aware chatbot for customer care on social media.

Encoder-decoder architecture with LSTM, which the decoder is modified to handle meta information by add an embedding vector as tone indicator to produce several tone responses including passionate, empathetic, and neutral. Sun et. al. [55] 2018 Building emotional human machine conversation chatbot that be able to understands the user's emotions, and consider the user's emotions then give a satisfactory response.

Model this problem as reinforcement learning task by proposing a new neural model based on Seq2Seq model which uses generative adversarial network (GAN) called EM-SeqGAN.

Shantala et. al. [50] 2018 Building neural dialogue system which addresses emotional aspect.

Seq2Seq model with LSTM by using a novel emotion embedding. Zhong et. al. [64] 2018

Building affect-rich open domain human-machine conversation system.

Extends Seq2Seq model and adopts VAD (Valence, Arousal and Dominance) affective notations. Proposed model also considers the effect of negators and intensifiers via a novel affective attention mechanism. Catania et. al. [6] 2019 Building a modular framework to facilitate and accelerate the realization and the maintenance of intelligent Conversational Agents with both rational and emotional capabilities.

The model consists of several modules including intent predictor, sentiment analysis, emotion analysis, topic analysis, and profiling module. ","[['b7'], [], ['b65'], ['b64', 'b8', 'b61', 'b22'], [], ['b26'], ['b1'], ['b54'], ['b17'], ['b54'], [], ['b49'], ['b63'], [], ['b5'], []]","[['b7'], [], ['b65'], ['b64', 'b8', 'b61', 'b22'], [], ['b26'], ['b1'], ['b54'], ['b17'], ['b54'], [], ['b49'], ['b63'], [], ['b5'], []]",14,"sent1: Year Focus Approach Kenneth Mark Colby [8] 1975Designing chatbot behave like paranoid personRule-based approach with capability to simulate emotion.
sent2: Zhou et. al. [66] 2014 Building empathetic chatbot for social interactionSeq2seq learning with GRU-RNN model which takes into account intelligent quotient (IQ) and emotion quotien (EQ).
sent3: Zhou et. al. [65] 2018 Building emotion chatting machine (ECM) for large scale conversation generation Seq2seq learning with GRU which incorporates emotion detection to capture implicit internal emotion states.
sent4: Colombo et. al. [9] 2019 Developing affect-driven dialogue system which generates emotional responses in a controlled manner Seq2seq with GRU which incorporates emotion classifier, also affective re-ranking in the last step to produce the response.
sent5: Li et. al. [23] 2019 Building conversational system emotional by using editing constraints to generate more meaningful and customizable emotional replies Encoder-decoder architecture with reinforcement learning approach by using asynchronous decoder which uses keyword predictor to predict the topic and emotion editor to produce emotional-embedded response.
sent6: Zhang et. al. [62] 2017 Building emotional conversation systems which produces several responses for every emotion categoryMulti-task seq2seq model with GRU which utilize bidirectional long short term memory (Bi-LSTMs) as emotion classifier.
sent7: Lubis et. al. [27] 2019 Building a fully data driven chatoriented dialogue system that can dynamically mimic affective human interactionsProposing a seq2seq response generator, includes emotion encoder which is trained jointly with the entire network to encode and maintain the emotional context throughout the dialogue, but focusing only on positive emotion Ashgar et.
sent8: al. [2] 2018 Building open-domain neural dialogue models by augmenting them with affective intelligence.
sent9: Encoder-decoder architecture with LSTM model using cognitively engineered affective word embedding, and also affectively diverse beam search for decoding Sun et.
sent10: al. [55] 2018 Developing conversation generation which addresses the emotional factor by changing the model's input.Encoder-decoder framework based on LSTM using three inputs, a sequence without an emotional category, a sequence with an emotional category for the input sentence, and a sequence with an emotional category for the output responses.
sent11: Hu et. al. [18] 2018 Building a tone-aware chatbot for customer care on social media.
sent12: Encoder-decoder architecture with LSTM, which the decoder is modified to handle meta information by add an embedding vector as tone indicator to produce several tone responses including passionate, empathetic, and neutral.
sent13: Sun et. al. [55] 2018 Building emotional human machine conversation chatbot that be able to understands the user's emotions, and consider the user's emotions then give a satisfactory response.
sent14: Model this problem as reinforcement learning task by proposing a new neural model based on Seq2Seq model which uses generative adversarial network (GAN) called EM-SeqGAN.Shantala et.
sent15: al. [50] 2018 Building neural dialogue system which addresses emotional aspect.
sent16: Seq2Seq model with LSTM by using a novel emotion embedding.
sent17: Zhong et. al. [64] 2018 Building affect-rich open domain human-machine conversation system.
sent18: Extends Seq2Seq model and adopts VAD (Valence, Arousal and Dominance) affective notations.
sent19: Proposed model also considers the effect of negators and intensifiers via a novel affective attention mechanism.
sent20: Catania et. al. [6] 2019 Building a modular framework to facilitate and accelerate the realization and the maintenance of intelligent Conversational Agents with both rational and emotional capabilities.
sent21: The model consists of several modules including intent predictor, sentiment analysis, emotion analysis, topic analysis, and profiling module."
195345457,Emotionally-Aware Chatbots: A Survey,Computer Science,https://www.semanticscholar.org/paper/c131665638feb8c11f936989ffc6187317593b41,s10,Automatic Evaluation.,"In automatic evaluation, some studies focus on evaluating the system at emotion level [23,65]. Therefore, some common metrics such as precision, recall, and accuracy are used to measure system performance, compared to the gold label. This evaluation is similar to emotion classification tasks such as previous SemEval 2018 [34] and SemEval 2019 13 . Other studies also proposed to use perplexity to evaluate the model at the content level (to determine whether the content is relevant and grammatical) [23,26,45]. This evaluation metric is widely used to evaluate dialogue-based systems which rely on probabilistic approach [48]. Another work by [45] used BLEU to evaluate the machine response and compare against the gold response (the actual response), although using BLEU to measure conversation generation task is not recommended by [25] due to its low correlation with human judgment.","[['b64', 'b22', 'b25', 'b47', 'b33', 'b44', 'b12', 'b24']]","[['b64', 'b22', 'b25', 'b47', 'b33', 'b44', 'b12', 'b24']]",8,"sent1: In automatic evaluation, some studies focus on evaluating the system at emotion level [23,65].
sent2: Therefore, some common metrics such as precision, recall, and accuracy are used to measure system performance, compared to the gold label.
sent3: This evaluation is similar to emotion classification tasks such as previous SemEval 2018 [34] and SemEval 2019 13 .
sent4: Other studies also proposed to use perplexity to evaluate the model at the content level (to determine whether the content is relevant and grammatical) [23,26,45].
sent5: This evaluation metric is widely used to evaluate dialogue-based systems which rely on probabilistic approach [48].
sent6: Another work by [45] used BLEU to evaluate the machine response and compare against the gold response (the actual response), although using BLEU to measure conversation generation task is not recommended by [25] due to its low correlation with human judgment."
195345457,Emotionally-Aware Chatbots: A Survey,Computer Science,https://www.semanticscholar.org/paper/c131665638feb8c11f936989ffc6187317593b41,s14,Authors,"Name Descroption Pennebaker et. al. [39] Linguistic Inquiry and Word Count (LIWC)

LIWC is a computer application that offers an efficient instrument for word-by-word study of the emotional, cognitive, and structural elements in English. LIWC also provide a dictionary which has 4500 words distributed into 64 different emotional categories including positive and negative. Mohammad et. al. [35] Emolex Emolex was built by crowdsourcing scenario. Emolex contains 14.182 words associated with eight primary emotion based on [40] : joy, sadness, anger, fear, trust, surprise, disgust, and anticipation. Poria et. al. [42] EmoSenticNet EmoSenticNet(EmoSN) is an enriched version of SenticNet [5]. Work by [42] added emotion label by mapping WordNet-Affect label to the SenticNet concepts. As a WordNet-Affect label, EmoSenticNet uses Six Ekman's basic emotion : anger, disgust, fear, happiness, sadness, and surprise. The whole list of this lexica contains 13.189 emotion labeled words. Whissell et. al. [60] Dictionary of Affect in Language (DAL)

DAL was developed by [60] and composed of 8742 English words. These words were labeled by three scores represent three emotion dimension : Pleasantness (the degree of pleasure), Activation (the degree of human response under some emotional condition), and Imagery (the degree of how easy the given words to be formed a mental picture). Bradley et. al. [4] Affective Norms for English Words (ANEW)

This lexicon consists of 1,034 English words rated with ratings based on the Valence-Arousal-Dominance (VAD) model [37]. de Albornoz et. al. [11] SentiSense SentiSense attaches emotional meanings to concepts from WordNet database. It is composed by a list of 5,496 words words labeled with emotional labels consisted of 14 emotional categories, which refer to a merge of Arnold, Plutchik and Parrot models. Strapparava et. al. [54] WordNet-Affect WordNet-Affect is a lexical resource created based on WordNet which contains information about the emotions that the words convey. This lexicon is organized in six basic emotions: anger, disgust, fear, joy, sadness, surprise and, contains 2,874 synsets and 4,787 words Saif M. Mohammad [32] NRC VAD The NRC VAD Lexicon is a list of English words and their valence, arousal, and dominance scores ranged between 0 (low) and 1 (high). This lexicon contains 20,000 terms and was built by crowdsourcing scenario. Staiano and Guerini [53] DepecheMood DepecheMood is an emotion lexicon that crowdsource emotions through rappler website 11 . This lexicon contains 37,000 entries rated by eight emotion categories (happy, sad, angry, afraid, annoyed, inspired, amused, and don't care). Badaro et.al. [3] EmoWordNet EmoWordNet is created by expanding an existing emotion lexicon, DepecheMood [53], by leveraging semantic knowledge from English WordNet. By alligning DepecheMood and WordNet, EmoWordNet consists of 67K terms, almost 1.8 times the size of DepecheMood. This lexicon has same emotion label as DepecheMood i.e., happy, sad, angry, afraid, annoyed, inspired, amused, and don't care. Table 3: Summarization of the available affective resources for emotion classification task.

[51] provides a long history of chatbots technology development. They also described several uses of chatbots' in some practical domains such as tools entertainment, tools to learn and practice language, information retrieval tools, and assistance for e-commerce of other business activities. Then, [12] reviewed the development of chatbots from rudimentary model to more advanced intelligent system. They summarized several techniques used to develop chatbots from early development until recent years. Recently, [21] provide a more systematic shape to review some previous works on chatbots' development. They classify chatbots into two main categories based on goals, including task-oriented chatbots and nontask oriented chatbot. They also classify chatbot based on its development technique into three main categories, including rule-based, retrieval-based, and generative-based approach. Furthermore, they also summarized the detailed technique on these three main approaches.","[['b38'], ['b4', 'b41', 'b59', 'b34', 'b39'], ['b3', 'b59'], ['b52', 'b10', 'b53', 'b31', 'b36', 'b2'], ['b20', 'b11']]","[['b38'], ['b4', 'b41', 'b59', 'b34', 'b39'], ['b3', 'b59'], ['b52', 'b10', 'b53', 'b31', 'b36', 'b2'], ['b20', 'b11']]",16,"sent1: Name Descroption Pennebaker et.
sent2: al. [39] Linguistic Inquiry and Word Count (LIWC)LIWC is a computer application that offers an efficient instrument for word-by-word study of the emotional, cognitive, and structural elements in English.
sent3: LIWC also provide a dictionary which has 4500 words distributed into 64 different emotional categories including positive and negative.
sent4: Mohammad et. al. [35] Emolex Emolex was built by crowdsourcing scenario.
sent5: Emolex contains 14.182 words associated with eight primary emotion based on [40] : joy, sadness, anger, fear, trust, surprise, disgust, and anticipation.
sent6: Poria et. al. [42] EmoSenticNet
sent7: EmoSenticNet(EmoSN) is an enriched version of SenticNet [5].
sent8: Work by [42] added emotion label by mapping WordNet-Affect label to the SenticNet concepts.
sent9: As a WordNet-Affect label, EmoSenticNet uses Six Ekman's basic emotion : anger, disgust, fear, happiness, sadness, and surprise.
sent10: The whole list of this lexica contains 13.189 emotion labeled words.
sent11: Whissell et. al. [60] Dictionary of Affect in Language (DAL)DAL was developed by [60] and composed of 8742 English words.
sent12: These words were labeled by three scores represent three emotion dimension : Pleasantness (the degree of pleasure), Activation (the degree of human response under some emotional condition), and Imagery (the degree of how easy the given words to be formed a mental picture).
sent13: Bradley et. al. [4] Affective Norms for English Words (ANEW)This lexicon consists of 1,034 English words rated with ratings based on the Valence-Arousal-Dominance (VAD) model [37]. de Albornoz et.
sent14: al. [11] SentiSense SentiSense attaches emotional meanings to concepts from WordNet database.
sent15: It is composed by a list of 5,496 words words labeled with emotional labels consisted of 14 emotional categories, which refer to a merge of Arnold, Plutchik and Parrot models.
sent16: Strapparava et. al. [54] WordNet-Affect WordNet-Affect is a lexical resource created based on WordNet which contains information about the emotions that the words convey.
sent17: This lexicon is organized in six basic emotions: anger, disgust, fear, joy, sadness, surprise and, contains 2,874 synsets and 4,787 words Saif M. Mohammad [32] NRC VAD The NRC VAD Lexicon is a list of English words and their valence, arousal, and dominance scores ranged between 0 (low) and 1 (high).
sent18: This lexicon contains 20,000 terms and was built by crowdsourcing scenario.
sent19: Staiano and Guerini [53] DepecheMood DepecheMood is an emotion lexicon that crowdsource emotions through rappler website 11 .
sent20: This lexicon contains 37,000 entries rated by eight emotion categories (happy, sad, angry, afraid, annoyed, inspired, amused, and don't care).
sent21: Badaro et.al. [3] EmoWordNet EmoWordNet is created by expanding an existing emotion lexicon, DepecheMood [53], by leveraging semantic knowledge from English WordNet.
sent22: By alligning DepecheMood and WordNet, EmoWordNet consists of 67K terms, almost 1.8 times the size of DepecheMood.
sent23: This lexicon has same emotion label as DepecheMood i.e., happy, sad, angry, afraid, annoyed, inspired, amused, and don't care.
sent24: Table 3: Summarization of the available affective resources for emotion classification task.
sent25: [51] provides a long history of chatbots technology development.
sent26: They also described several uses of chatbots' in some practical domains such as tools entertainment, tools to learn and practice language, information retrieval tools, and assistance for e-commerce of other business activities.
sent27: Then, [12] reviewed the development of chatbots from rudimentary model to more advanced intelligent system.
sent28: They summarized several techniques used to develop chatbots from early development until recent years.
sent29: Recently, [21] provide a more systematic shape to review some previous works on chatbots' development.
sent30: They classify chatbots into two main categories based on goals, including task-oriented chatbots and nontask oriented chatbot.
sent31: They also classify chatbot based on its development technique into three main categories, including rule-based, retrieval-based, and generative-based approach.
sent32: Furthermore, they also summarized the detailed technique on these three main approaches."
195345457,Emotionally-Aware Chatbots: A Survey,Computer Science,https://www.semanticscholar.org/paper/c131665638feb8c11f936989ffc6187317593b41,s12,Authors,"Year Language Source Description Zhou et. al. [65] 2018 Chinese Weibo This dataset used STC dataset [49], and performed automatic annotation by using the best performing classifier trained on NLPCC 2013 8 and NLPCC 2014 9 datasets. This dataset contains 217,905 conversations where each response annotated by 6 emotion labels. Rashkin et. al. [45] 2018 English ParlAI Platform This dataset is built by using crowd-sourcing scenario using ParlAI which involves 810 different participants. This dataset contains 24,850 conversations/prompts where each conversation annotated by 32 emotion labels. Huang et. al. [20] 2017 Chinese Weibo Similar to [65], the training data is automatically annotated by using trained emotion classifier. This dataset contains more than 1 million conversations as training and 200 conversation which was manually annotated as test set, where each response labeled by 5 emotion labels. Hu et. al. [18] 2018 English Twitter This dataset gathered from Twitter based on customer care conversation in 62 brands across different industries. 500 conversation was chosen randomly and annotated by 8 major tones using CrowdFlower 10 platform Lubis et. al. [26] 2018 English Crowd source This dataset was built by enhancing SEMAINE dataset [29], adding a label on instance which elicit positive emotions (8 labels including alert, excited, elated, happy, content, serene, relaxed, calm). The final dataset contains 2,349 conversations. Li et. al. [24] 2017 English Online Website

This dataset (called ""DailyDialog"") is crawled from various websites which serve for English learner to practice English dialog in daily life. DailyDialog contains 13,118 dialogs, and was manually annotated with communication intention and emotion information. They use six primary basic emotion (Anger, Disgust, Fear, Happiness, Sadness, Surprise) to label the conversations. Huang et. al. [19] 2018 English Facebook Message

This dataset was collected from private conversation on Facebook messenger application. 8,818 messages were manually annotated by using seven emotion labels including joy, anger, sadness, anticipation, tired, fear and neutral. Table 2: Summarization of dataset available for emotionally-aware chatbot.

criteria, content (scale 0,1,2) and emotion (scale 0,1). Content is focused on measuring whether the response is natural acceptable and could plausible produced by a human. This metric measurement is already adopted and recommended by researchers and conversation challenging tasks, as proposed in [49]. Meanwhile, emotion is defined as whether the emotion expression contained in the response agrees with the given gold emotion category. Similarly, [23] used four annotators to score the response based on consistency, logic and emotion. Consistency measures the fluency and grammatical aspect of the response. Logic measures the degree whether the post and response logically match. Emotion measures the response, whether it contains the appropriate emotion. All of these aspects were measured by three scales 0, 1, and 2. Meanwhile, [26] proposed naturalness and emotion impact as criteria to evaluate the chatbots' response. Naturalness evaluates whether the response is intelligible, logically follows the context of the conversation, and acceptable as a human response, while emotion impact measures whether the response elicits a positive emotional or triggers an emotionally-positive dialogue, since their study focus only on positive emotion. Another study by [45] uses crowdsourcing to gather human judgement based on three aspects of performance including empathy/sympathy -did the responses show understanding of the feelings of the person talking about their experience?; relevance -did the responses seem appropriate to the conversation? Were they on-topic?; and fluency -could you understand the responses? Did the language seem accurate?. All of these aspects recorded with three different response, i.e., (1: not at all, 3: somewhat, 5: very much) from around 100 different annotators. After getting all of the human judgement with different criteria, some of these studies used a t-test to get the statistical significance [23,26], while some other used inter-annotator agreement measurement such as Fleiss Kappa [45,65]. Based on these evaluations, they can compare their system performance with baseline or any other state of the art systems.","[['b64', 'b17', 'b28', 'b25', 'b48', 'b23', 'b44', 'b19'], ['b18'], [], ['b64', 'b22', 'b25', 'b48', 'b44']]","[['b64', 'b17', 'b28', 'b25', 'b48', 'b23', 'b44', 'b19'], ['b18'], [], ['b64', 'b22', 'b25', 'b48', 'b44']]",14,"sent1: Year Language Source Description Zhou et.
sent2: al. [65] 2018 Chinese Weibo This dataset used STC dataset [49], and performed automatic annotation by using the best performing classifier trained on NLPCC 2013 8 and NLPCC 2014 9 datasets.
sent3: This dataset contains 217,905 conversations where each response annotated by 6 emotion labels.
sent4: Rashkin et. al. [45] 2018 English ParlAI Platform
sent5: This dataset is built by using crowd-sourcing scenario using ParlAI which involves 810 different participants.
sent6: This dataset contains 24,850 conversations/prompts where each conversation annotated by 32 emotion labels.
sent7: Huang et. al. [20] 2017 Chinese Weibo Similar to [65], the training data is automatically annotated by using trained emotion classifier.
sent8: This dataset contains more than 1 million conversations as training and 200 conversation which was manually annotated as test set, where each response labeled by 5 emotion labels.
sent9: Hu et. al. [18] 2018 English Twitter
sent10: This dataset gathered from Twitter based on customer care conversation in 62 brands across different industries.
sent11: 500 conversation was chosen randomly and annotated by 8 major tones using CrowdFlower 10 platform Lubis et.
sent12: al. [26] 2018 English Crowd source This dataset was built by enhancing SEMAINE dataset [29], adding a label on instance which elicit positive emotions (8 labels including alert, excited, elated, happy, content, serene, relaxed, calm).
sent13: The final dataset contains 2,349 conversations.
sent14: Li et. al. [24] 2017 English Online WebsiteThis dataset (called ""DailyDialog"") is crawled from various websites which serve for English learner to practice English dialog in daily life.
sent15: DailyDialog contains 13,118 dialogs, and was manually annotated with communication intention and emotion information.
sent16: They use six primary basic emotion (Anger, Disgust, Fear, Happiness, Sadness, Surprise) to label the conversations.
sent17: Huang et. al. [19] 2018 English Facebook MessageThis dataset was collected from private conversation on Facebook messenger application.
sent18: 8,818 messages were manually annotated by using seven emotion labels including joy, anger, sadness, anticipation, tired, fear and neutral.
sent19: Table 2: Summarization of dataset available for emotionally-aware chatbot.
sent20: criteria, content (scale 0,1,2) and emotion (scale 0,1).
sent21: Content is focused on measuring whether the response is natural acceptable and could plausible produced by a human.
sent22: This metric measurement is already adopted and recommended by researchers and conversation challenging tasks, as proposed in [49].
sent23: Meanwhile, emotion is defined as whether the emotion expression contained in the response agrees with the given gold emotion category.
sent24: Similarly, [23] used four annotators to score the response based on consistency, logic and emotion.
sent25: Consistency measures the fluency and grammatical aspect of the response.
sent26: Logic measures the degree whether the post and response logically match.
sent27: Emotion measures the response, whether it contains the appropriate emotion.
sent28: All of these aspects were measured by three scales 0, 1, and 2.
sent29: Meanwhile, [26] proposed naturalness and emotion impact as criteria to evaluate the chatbots' response.
sent30: Naturalness evaluates whether the response is intelligible, logically follows the context of the conversation, and acceptable as a human response, while emotion impact measures whether the response elicits a positive emotional or triggers an emotionally-positive dialogue, since their study focus only on positive emotion.
sent31: Another study by [45] uses crowdsourcing to gather human judgement based on three aspects of performance including empathy/sympathy -did the responses show understanding of the feelings of the person talking about their experience?; relevance -did
sent32: the responses seem appropriate to the conversation?
sent33: Were they on-topic?; and fluency -could you understand the responses?
sent34: Did the language seem accurate?.
sent35: All of these aspects recorded with three different response, i.e., (1: not at all, 3: somewhat, 5: very much) from around 100 different annotators.
sent36: After getting all of the human judgement with different criteria, some of these studies used a t-test to get the statistical significance [23,26], while some other used inter-annotator agreement measurement such as Fleiss Kappa [45,65].
sent37: Based on these evaluations, they can compare their system performance with baseline or any other state of the art systems."
201070768,Enhancing the Demand for Labour survey by including skills from online job advertisements using model-assisted calibration,"Mathematics, Computer Science, Economics",https://www.semanticscholar.org/paper/b1826f1b35347fb1c190f9296b3c2fb7139b9b72,s5,Data integration approach,"Enhancing probability survey with online data (i.e. non-probability sample) may be achieved by data integration.  

The goal of data integration is to estimate some quantity (e.g. mean, total) of target variables Y present only in online data. Elliott and Valliant (2017) summarised possible approaches that consider pseudo-randomization (i.e calibration) or model-based approach. In addition, Kim and Wang (2018) consider mass imputation and double robust estimation that take into account propensity score weighting.

In the paper we consider pseudo-randomization approach in which pseudo-weights from nonprobability sample are calibrated to estimated totals T X or estimated total of Y based on approach introduced by Wu and Sitter (2001) and further developed for non-probability samples by Chen (2016). Detailed description is presented in the sections below.","[[], ['b18', 'b24'], [None, 'b46']]","[[], ['b18', 'b24'], [None, 'b46']]",4,"sent1: Enhancing probability survey with online data (i.e. non-probability sample) may be achieved by data integration.
sent2: The goal of data integration is to estimate some quantity (e.g. mean, total) of target variables Y present only in online data.
sent3: Elliott and Valliant (2017) summarised possible approaches that consider pseudo-randomization (i.e calibration) or model-based approach.
sent4: In addition, Kim and Wang (2018) consider mass imputation and double robust estimation that take into account propensity score weighting.
sent5: In the paper we consider pseudo-randomization approach in which pseudo-weights from nonprobability sample are calibrated to estimated totals T X or estimated total of Y based on approach introduced by Wu and Sitter (2001) and further developed for non-probability samples by Chen (2016).
sent6: Detailed description is presented in the sections below."
201070768,Enhancing the Demand for Labour survey by including skills from online job advertisements using model-assisted calibration,"Mathematics, Computer Science, Economics",https://www.semanticscholar.org/paper/b1826f1b35347fb1c190f9296b3c2fb7139b9b72,s6,Traditional calibration,"Calibration was proposed by Deville and Särndal (1992) and is a method of searching for so called calibrated weights by minimizing the distance measure between the sampling weights and the new weights, which satisfy certain calibration constraints. As a consequence, when the new weights are applied to the auxiliary variables in the sample, they reproduce the known population totals of the auxiliary variables exactly. It is also important that the new weights should be as close as possible to sampling weights in the sense of the selected distance measure (Särndal and Lundström, 2005).

Following the notation in Chen et al. (2019), let us define the online (non-probability) sample as s A,t of size n A,t where t = 1, ..., T denotes the wave. For simplicity, we drop subscript t. This sample contains variables of interest Y k , where k = 1, ..., K. Further, let d A n A ×1 be a vector of pseudo-weights that are typically set to N/n A for all units i ∈ s A,t , where N is the size of the target population. In this approach we assume simple random sampling design for sample s A .

Let D A be a diagonal matrix of pseudo-design weights and w n A ×1 be calibrated weights that minimize an expected distance measure with respect to the design of A

under the constraint:

where T X is a row vector of estimated population totals (e.g. from the reference, external probability sample) of sample calibration variables X and g(w i , d A i ) is a differentiable function with respect to w i , strictly convex on an interval containing d A i and g(d A i , d A i ) = 0. The commonly used generalized regression (GREG) estimator uses the

For this distance measure:

The estimate of the population mean of outcome y k assuming that we have k target variables is based on calibrated weights:

The calibrated weights defined do not rely on any outcome variable. Thus the same set of weights can be applied to all variables in the survey.

In the case when only estimates of totals T X are known, Dever and Valliant (2010) introduced estimated control calibration. In this framework, we replace T X in (3) with T X , which results in

and thus the estimated mean is given by

Following Chen et al. (2019) we denote this estimator as ECGREG (Estimated control GREG) to distinguish it from GREG with known population totals.","[['b40', 'b17'], ['b8'], [], [], [], [], [], [], [], [], ['b8']]","[['b40', 'b17'], ['b8'], [], [], [], [], [], [], [], [], ['b8']]",4,"sent1: Calibration was proposed by Deville and Särndal (1992) and is a method of searching for so called calibrated weights by minimizing the distance measure between the sampling weights and the new weights, which satisfy certain calibration constraints.
sent2: As a consequence, when the new weights are applied to the auxiliary variables in the sample, they reproduce the known population totals of the auxiliary variables exactly.
sent3: It is also important that the new weights should be as close as possible to sampling weights in the sense of the selected distance measure (Särndal and Lundström, 2005).
sent4: Following the notation in Chen et al. (2019), let us define the online (non-probability) sample as s A,t of size n A,t where t = 1, ..., T denotes the wave.
sent5: For simplicity, we drop subscript t.
sent6: This sample contains variables of interest Y k , where k = 1, ..., K. Further, let d A n
sent7: A ×1 be a vector of pseudo-weights that are typically set to N/n A for all units i ∈ s A,t , where N is the size of the target population.
sent8: In this approach we assume simple random sampling design for sample s A .
sent9: Let D A be a diagonal matrix of pseudo-design weights and w n A ×1 be calibrated weights that minimize an expected distance measure with respect to the design of Aunder the constraint:where T X is a row vector of estimated population totals (e.g. from the reference, external probability sample) of sample calibration variables X and g(w i , d A i ) is a differentiable function with respect to w i , strictly convex on an interval containing d A i and g(d A i , d A i ) = 0.
sent10: The commonly used generalized regression (GREG) estimator uses theFor this distance measure:The estimate of the population mean of outcome y k assuming that we have k target variables is based on calibrated weights:The calibrated weights defined do not rely on any outcome variable.
sent11: Thus the same set of weights can be applied to all variables in the survey.
sent12: In the case when only estimates of totals T X are known, Dever and Valliant (2010) introduced estimated control calibration.
sent13: In this framework, we replace T X in (3) with T X , which results inand thus the estimated mean is given byFollowing Chen et al. (2019) we denote this estimator as ECGREG (Estimated control GREG) to distinguish it from GREG with known population totals."
212633493,Deep Neural Networks for Automatic Speech Processing: A Survey from Large Corpora to Limited Data,"Computer Science, Mathematics, Linguistics, Engineering",https://www.semanticscholar.org/paper/7a307b379eb714df1e38fe9a80b7ed94c0fbd64e,s21,5) Graph neural network,"The use of Graph Neural Network (GNN) is used by V. Garcia and J. Bruna introduce the use of Graph Neural Network (GNN) in their few-shot framework [33]. This framework is designed to be used with multiple episodes they called tasks. In this framework, one model is used over a complete graph G. G = (V, E) where every node corresponds to an example. GNN for few-shot learning consists in applying Graph Convolutions Layers over the graph G.

Initial vertices construction to guess the ground truth of a queryx i from the query set Q:

, h(y 1 )), . . . , (Enc(x s ), h(y s )), (Enc(x 1 ), u), . . . , (Enc(x r ), u)

where Enc is an embedding extraction function (a Neural Network or any classic feature extraction technique), h the one-hot encoding function and u = K −1 1 K an uniform distribution for examples with unknown labels (the unsupervised ones fromx and/or from the query set Q).

From now the vertices at each layer l (with 0 being the initial vertices) will be denoted:

where n = s + r + 1 and V (l) ∈ R n * d l .

Every layers in GNN are computed as follows:

with A (l) being the adjacency operators constructed from V (l) and Gc being the graph convolution.

a) The adjacency operators construction: The adjacency operator us a set:

withÃ (l) being the adjacency matrix of V (l) .

For every (i, j) ∈ E (recall we have complete graphs), we compute the values of the adjacency matrix such as:

where:

with f being a multi-layer perceptron with its parameter denoted θ f .Ã (l) is then normalized using the softmax function over each line. b) Graph convolution: The graph convolution requires the construction of the adjacency operators set and is computed as follows:

with B being an adjacency operator from A, θ (k) B,l ∈ R d l−1 ,d l learnable parameters and ρ being a point wise linearity (usually leaky ReLU).

c) Training the model: The output of the resulting GNN model is a mapping of the vertices to a K-simplex that give the probability ofx i being in class k. V. Garcia and J. Bruna used the cross-entropy to learn the model other all examples in the query set Q [33]. Hence, the GNN few-shot framework consists in learning θ f and θ 1,l . . . θ card(A),l parameters over all episodes. d) Few-shot GNN on audio: This framework was used by [34] on 5-way audio classification problems. The 5 ways episodes are randomly selected from the initial dataset: Au-dioSet [35] for creating the 5-ways training episodes and [36] data to create the 5-ways test episodes.

S. Zhang et al. compare the use of per class attention (or intra-class) and global attention which gave the best results [34]. They applied it for each layer. Their experiments were done for 1-shot, 5-shots and 10-shots with the respective accuracy of 69.4%±0.66, 78.3%±0.46 and 83.6%±0.98. Such results really motivate us in the path of few-shot learning for speech signals. Nevertheless, this framework does not allow the use of many classes and shots per episode which increase the number of nodes and thus the computations in forward time. Hence, it is not suited for large vocabulary problems.","[['b33'], [], [], [], [], [], [], [], [], [], [], [], [], [], ['b34', 'b35', 'b36', 'b33'], ['b34']]","[['b33'], [], [], [], [], [], [], [], [], [], [], [], [], [], ['b34', 'b35', 'b36', 'b33'], ['b34']]",6,"sent1: The use of Graph Neural Network (GNN) is used by V. Garcia and J. Bruna introduce the use of Graph Neural Network (GNN) in their few-shot framework [33].
sent2: This framework is designed to be used with multiple episodes they called tasks.
sent3: In this framework, one model is used over a complete graph G. G = (V, E) where every node corresponds to an example.
sent4: GNN for few-shot learning consists in applying Graph Convolutions Layers over the graph
sent5: G.Initial vertices construction to guess the ground truth of a queryx i from the query set Q:, h(y 1 )), . . . , (Enc(x s ), h(y s )), (Enc(x 1 ), u), . . . , (Enc(x r ), u)where Enc is an embedding extraction function (a Neural Network or any classic feature extraction technique), h the one-hot encoding function and u = K −1 1 K an uniform distribution for examples with unknown labels (the unsupervised ones fromx and/or from the query set Q).
sent6: From now the vertices at each layer l (with 0 being the initial vertices) will be denoted:where n = s + r + 1 and V (l) ∈ R n * d l .
sent7: Every layers in GNN are computed as follows:with A (l) being the adjacency operators constructed from V (l) and Gc being the graph convolution.
sent8: a) The adjacency operators construction: The adjacency operator us a set:withÃ (l) being the adjacency matrix of V (l) .
sent9: For every (i, j) ∈ E (recall we have complete graphs), we compute the values of the adjacency matrix such as:where:with f being a multi-layer perceptron with its parameter denoted θ f .Ã (l) is then normalized using the softmax function over each line.
sent10: b) Graph convolution: The graph convolution requires the construction of the adjacency operators set and is computed as follows:with B being an adjacency operator from A, θ (k) B,l ∈ R d l−1 ,d l learnable parameters and ρ being a point wise linearity (usually leaky ReLU).c)
sent11: Training the model: The output of the resulting GNN model is a mapping of the vertices to a K-simplex that give the probability ofx i being in class k. V. Garcia and J. Bruna used the cross-entropy to learn the model other all examples in the query set Q [33].
sent12: Hence, the GNN few-shot framework consists in learning θ f and θ
sent13: 1,l . . . θ card(A),l parameters over all episodes.
sent14: d) Few-shot GNN on audio: This framework was used by [34] on 5-way audio classification problems.
sent15: The 5 ways episodes are randomly selected from the initial dataset: Au-dioSet [35] for creating the 5-ways training episodes and [36] data to create the 5-ways test episodes.
sent16: S. Zhang et al. compare the use of per class attention (or intra-class) and global attention which gave the best results [34].
sent17: They applied it for each layer.
sent18: Their experiments were done for 1-shot, 5-shots and 10-shots with the respective accuracy of 69.4%±0.66, 78.3%±0.46 and 83.6%±0.98.
sent19: Such results really motivate us in the path of few-shot learning for speech signals.
sent20: Nevertheless, this framework does not allow the use of many classes and shots per episode which increase the number of nodes and thus the computations in forward time.
sent21: Hence, it is not suited for large vocabulary problems."
212633493,Deep Neural Networks for Automatic Speech Processing: A Survey from Large Corpora to Limited Data,"Computer Science, Mathematics, Linguistics, Engineering",https://www.semanticscholar.org/paper/7a307b379eb714df1e38fe9a80b7ed94c0fbd64e,s5,A. Data augmentation,"The first way to leverage the lack of data is to artificially augment the number of data. To do so, classic approach consists for example in adding noise or deformation. Such as in [12]. They obtain near SOTA on Librispeech (1000 hours from [3]) with an end-to-end models. Nevertheless, they obtain SOTA results on SwitchBoard (300 hours from [13]) with a WER of 6.8%/14.1% on the Switchboard/CallHome portion using shallow fusion and their data augmentation. But theses are handcrafted augmentations and some of them require additional audios (like adding noise).

Some other approaches use generative models to have new samples such as in [14], [15]. A. Chatziagapi et al. used conditional Generative Adversarial Networks (GAN) to generate new samples [14]. Conditioned GAN are GAN where we can control the mode of the generated samples. Doing so, they balanced their initial dataset and obtain better results. Y. Jiao et al. used Deep Convolutional GANs to generate dysarthric speech and improve their results [15].","[['b11', 'b2', 'b12'], ['b14', 'b13']]","[['b11', 'b2', 'b12'], ['b14', 'b13']]",5,"sent1: The first way to leverage the lack of data is to artificially augment the number of data.
sent2: To do so, classic approach consists for example in adding noise or deformation.
sent3: Such as in [12]. They obtain near SOTA on Librispeech (1000 hours from [3]) with an end-to-end models.
sent4: Nevertheless, they obtain SOTA results on SwitchBoard (300 hours from [13]) with a WER of 6.8%/14.1% on the Switchboard/CallHome portion using shallow fusion and their data augmentation.
sent5: But theses are handcrafted augmentations and some of them require additional audios (like adding noise).
sent6: Some other approaches use generative models to have new samples such as in [14], [15]. A. Chatziagapi et al. used conditional Generative Adversarial Networks (GAN) to generate new samples [14].
sent7: Conditioned GAN are GAN where we can control the mode of the generated samples.
sent8: Doing so, they balanced their initial dataset and obtain better results.
sent9: Y. Jiao et al. used Deep Convolutional GANs to generate dysarthric speech and improve their results [15]."
212633493,Deep Neural Networks for Automatic Speech Processing: A Survey from Large Corpora to Limited Data,"Computer Science, Mathematics, Linguistics, Engineering",https://www.semanticscholar.org/paper/7a307b379eb714df1e38fe9a80b7ed94c0fbd64e,s8,D. Multi-task approach,"Multi-task models can be viewed as an extension of the Encoder-Decoder architecture where you have a decoder per task with a shared encoder (like in Figure 1). Then those tasks are trained conjointly with classic feed-forward algorithms. The goal of a multi-task learning is to have an encoder outputting sufficient information for every task. Doing so, it can potentially improve the performances of each task compared to mono task architectures. It is a way to have a more representative encoder given the same amount of data.

In emotion recognition, [22] got SOTA results over a modified version of the IEMOCAP database to have a fourclass problem. Those emotions are: angry, happy, neutral and sad. Y. Li et al. used an end-to-end multi-task system with only supervised tasks: gender identification and emotion identification [22]. The resulting model achieve an overall accuracy for the emotion task (which is the main target) of 81.6% and an average accuracy of each emotion category of 82.8%. Using such approach allows them to achieve balanced results over unbalanced data.

Nevertheless, using only supervised tasks requires multiple ground-truth for the targeted dataset. S. Pascual   a combination of self-supervised tasks combined with unsupervised tasks to tackle this problem and used the resulting encoder for transfer learning [23]. They recently improved this work in [24] where they use more tasks, a recurrent unit on top of the encoder and denoising mechanisms using multiple data augmentation on their system.","[[], ['b21'], [None, 'b23', 'b22']]","[[], ['b21'], [None, 'b23', 'b22']]",4,"sent1: Multi-task models can be viewed as an extension of the Encoder-Decoder architecture where you have a decoder per task with a shared encoder (like in Figure 1).
sent2: Then those tasks are trained conjointly with classic feed-forward algorithms.
sent3: The goal of a multi-task learning is to have an encoder outputting sufficient information for every task.
sent4: Doing so, it can potentially improve the performances of each task compared to mono task architectures.
sent5: It is a way to have a more representative encoder given the same amount of data.
sent6: In emotion recognition, [22] got SOTA results over a modified version of the IEMOCAP database to have a fourclass problem.
sent7: Those emotions are: angry, happy, neutral and sad.
sent8: Y. Li et al. used an end-to-end multi-task system with only supervised tasks: gender identification and emotion identification [22].
sent9: The resulting model achieve an overall accuracy for the emotion task (which is the main target) of 81.6% and an average accuracy of each emotion category of 82.8%.
sent10: Using such approach allows them to achieve balanced results over unbalanced data.
sent11: Nevertheless, using only supervised tasks requires multiple ground-truth for the targeted dataset.
sent12: S. Pascual   a combination of self-supervised tasks combined with unsupervised tasks to tackle this problem and used the resulting encoder for transfer learning [23].
sent13: They recently improved this work in [24] where they use more tasks, a recurrent unit on top of the encoder and denoising mechanisms using multiple data augmentation on their system."
212633493,Deep Neural Networks for Automatic Speech Processing: A Survey from Large Corpora to Limited Data,"Computer Science, Mathematics, Linguistics, Engineering",https://www.semanticscholar.org/paper/7a307b379eb714df1e38fe9a80b7ed94c0fbd64e,s9,E. Transfer Learning,"Transfer learning techniques consist of using a pre-trained model and transfer its knowledge to solve a related problem/task. Usually we use the encoding part of the pre-trained model to initialize the model for the new problem/task.

Contrastive Predictive Coding (CPC from [25]) is an architecture to learn unsupervised audio representation using a 2-level architecture combined with a self-supervised loss. They achieved good results by transferring the obtained model for speaker identification and phone classification (on LibriSpeech dataset) compared to MFCC features. This work inspired [23]. They developed an unsupervised multi-task model (with certain losses being self-supervised) to obtain better encoders for transfer learning. They applied it on multiple tasks and obtain decent results on speaker identification (using VTCK), emotion recognition (using INTERFACE) and ASR (using TIMIT).

The benefit of pre-trained network for transfer learning decrease as the target task diverges from the original task of the pre-trained network [26]. To tackle this, [25], [23] attempt to have generic tasks with their unsupervised approach, and they obtained promising results. Also, the benefit of transfer learning decrease when the dissimilarity between the datasets increase [26]. This problem can discourage the use of transfer learning for some pathological speech. Whereas, Dysarthric and Accented Speech seems similar to speech in librispeech dataset according to [27]. Where they successfully used transfer learning to improve their results over a 36.7 hours dataset.

Nevertheless, [8] showed that acoustic characteristics of unimpaired and impaired speech are very different. In the case of having few data such problems can be critical. It is why looking into few-shot techniques could be helpful.","[[], ['b22', 'b24'], ['b25', 'b27', 'b22', 'b24'], ['b7']]","[[], ['b22', 'b24'], ['b25', 'b27', 'b22', 'b24'], ['b7']]",7,"sent1: Transfer learning techniques consist of using a pre-trained model and transfer its knowledge to solve a related problem/task.
sent2: Usually we use the encoding part of the pre-trained model to initialize the model for the new problem/task.
sent3: Contrastive Predictive Coding (CPC from [25]) is an architecture to learn unsupervised audio representation using a 2-level architecture combined with a self-supervised loss.
sent4: They achieved good results by transferring the obtained model for speaker identification and phone classification (on LibriSpeech dataset) compared to MFCC features.
sent5: This work inspired [23]. They developed an unsupervised multi-task model (with certain losses being self-supervised) to obtain better encoders for transfer learning.
sent6: They applied it on multiple tasks and obtain decent results on speaker identification (using VTCK), emotion recognition (using INTERFACE) and ASR (using TIMIT).
sent7: The benefit of pre-trained network for transfer learning decrease as the target task diverges from the original task of the pre-trained network [26].
sent8: To tackle this, [25], [23] attempt to have generic tasks with their unsupervised approach, and they obtained promising results.
sent9: Also, the benefit of transfer learning decrease when the dissimilarity between the datasets increase [26].
sent10: This problem can discourage the use of transfer learning for some pathological speech.
sent11: Whereas, Dysarthric and Accented Speech seems similar to speech in librispeech dataset according to [27].
sent12: Where they successfully used transfer learning to improve their results over a 36.7 hours dataset.
sent13: Nevertheless, [8] showed that acoustic characteristics of unimpaired and impaired speech are very different.
sent14: In the case of having few data such problems can be critical.
sent15: It is why looking into few-shot techniques could be helpful."
212633493,Deep Neural Networks for Automatic Speech Processing: A Survey from Large Corpora to Limited Data,"Computer Science, Mathematics, Linguistics, Engineering",https://www.semanticscholar.org/paper/7a307b379eb714df1e38fe9a80b7ed94c0fbd64e,s15,2) Matching Network,"Matching Networks from [30] is a few-shot framework designed to be trained on multiple episodes. This framework is composed of one model ϕ. This model is trained over a set of training episodes (with typically 5 to 25 ways). This model evaluates new examples given the support set S like in the Siamese framework:

ϕ(x, S) :→ŷ (8) In matching learning, ϕ is as follows:

with, a being the attention kernel.

In [30] this attention kernel is as follows:

where c is the cosine distance, f and g are embedding functions.

O. Vinyals et al. used a recurrent architecture to modulate the representation of f using the support set S [30]. The goal is to have f following the same type of representation of g. To do this, g function is as follows:

where − → h i and ← − h i represent a bi-LSTM output over g ′ (x i ) which is a DNN.

f function is as follows:

with, attLST M being an LSTM with a fixed number of recurrences to do (here m), g(S) represents the application of g to each x i from the S set. f ′ is a DNN with the same architecture as g ′ , but not necessarily share the parameter values. Hence, training this framework consists in the maximization of the log likelihood of ϕ given the parameters of g and f . Figure 3 illustrates forward time of the Matching Network model. For forward time on new samples g(S) can be pre-calculated to gain computation time. Nevertheless, as for Siamese networks, Matching networks have the same disadvantages when q and/or K become too high. Furthermore, adding new classes to a trained Matching Network model is not as easy as for Siamese Network models. Indeed, it requires retraining the Matching Network model to add an element to the support set. Whereas, Matching learning showed better results than the Siamese framework on image datasets from [30] experiments. It is why it should be investigated in speech processing to see if it is still the case. ","[['b30'], ['b7'], [], ['b30'], [], ['b30'], [], [], ['b30']]","[['b30'], ['b7'], [], ['b30'], [], ['b30'], [], [], ['b30']]",5,"sent1: Matching Networks from [30] is a few-shot framework designed to be trained on multiple episodes.
sent2: This framework is composed of one model ϕ.
sent3: This model is trained over a set of training episodes (with typically 5 to 25 ways).
sent4: This model evaluates new examples given the support set S like in the Siamese framework:ϕ(x, S) :
sent5: →ŷ (8) In matching learning, ϕ is as follows:with, a being the attention kernel.
sent6: In [30] this attention kernel is as follows:where c is the cosine distance, f and g are embedding functions.
sent7: O. Vinyals et al. used a recurrent architecture to modulate the representation of f using the support set S [30].
sent8: The goal is to have f following the same type of representation of g.
sent9: To do this, g function is as follows:where − → h i and ← − h i represent a bi-LSTM output over g ′ (x i ) which is a DNN.
sent10: f function is as follows:with, attLST M being an LSTM with a fixed number of recurrences to do (here m), g(S) represents the application of g to each x i from the S set.
sent11: f ′ is a DNN with the same architecture as g ′ , but not necessarily share the parameter values.
sent12: Hence, training this framework consists in the maximization of the log likelihood of ϕ given the parameters of g and f .
sent13: Figure 3 illustrates forward time of the Matching Network model.
sent14: For forward time on new samples g(S) can be pre-calculated to gain computation time.
sent15: Nevertheless, as for Siamese networks, Matching networks have the same disadvantages when q and/or K become too high.
sent16: Furthermore, adding new classes to a trained Matching Network model is not as easy as for Siamese Network models.
sent17: Indeed, it requires retraining the Matching Network model to add an element to the support set.
sent18: Whereas, Matching learning showed better results than the Siamese framework on image datasets from [30] experiments.
sent19: It is why it should be investigated in speech processing to see if it is still the case."
212633633,"第 ** 卷 第 * 期 中文信息学报 Overview of the CCKS 2019 Knowledge Graph Evaluation Track: Entity, Rela- tion, Event and QA",Computer Science,https://www.semanticscholar.org/paper/81374b14e36bd993f20d92b9fca0a822c47e382a,s0,3 典型模型与系统,"自从 BERT [12] 发布后， 它很快超过 ELMo [21] [22] 和 GloVe [23] 

正如前面提到的那样， 预测结果中是否有 NA 关系不会影响 % 值的计算，并且评测数据中各关 系的分类也是不平衡的。这就导致，如果一个关 系抽取系统对其中几个大类的预测有非常好的性 能时，在整体数据上的性能也同样会很好。在测 试集数据中， ""现夫"" 、 ""现妻"" 、 ""生父"" 、 ""生母"" 、 ""儿子"" 、 ""恋人"" 、 ""老师""这七种关系占据了大 多数。 在 Sent-Track 中， 2300 个标准答案中有 2000 个属于这七类；在 Bag-Track 中，740 个标准答案  ","[['b23', 'b24', 'b12', 'b22'], []]","[['b23', 'b24', 'b12', 'b22'], []]",4,"sent1: 自从 BERT [12] 发布后， 它很快超过 ELMo [21] [22] 和 GloVe [23] 正如前面提到的那样， 预测结果中是否有 NA 关系不会影响 % 值的计算，并且评测数据中各关 系的分类也是不平衡的。这就导致，如果一个关 系抽取系统对其中几个大类的预测有非常好的性 能时，在整体数据上的性能也同样会很好。在测 试集数据中， ""现夫"" 、 ""现妻"" 、 ""生父"" 、 ""生母"" 、 ""儿子"" 、 ""恋人"" 、 ""老师""这七种关系占据了大 多数。 在 Sent-Track 中， 2300 个标准答案中有 2000 个属于这七类；在 Bag-Track 中，740 个标准答案"
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s17,Topic 6: Host Immune Response to 19-nCoV,"This topic was discussed in about 6.36% (1837/28,904) of the publications (eg, [48][49][50][51][52]). Authors who had the highest number of publications related to this topic were Alessandro Sette (n=7), Stanley Perlman (n=6), Nima Rezaei (n=6), Irfan Rahman (n=5), and Akiko Iwasaki (n=6). The top 5 journals and preprint servers in terms of publishing articles related to this topic were bioRxiv (n=199), Medical Hypotheses (n=50), the Journal of Medical Virology (n=49), Frontiers in Immunology (n=22), and the British Journal of Haematology (n=19). The earliest article related to this topic was published on January 2, 2020. From that date until week 14, there was a slight increase in the number of weekly publications before it increased markedly, peaking in week 25 (n=155) (Multimedia Appendix 1). The mean number of weekly publications in this cluster was 62.8 (SD 54.6).","[['b50', 'b49', 'b47', 'b51', 'b48']]","[['b50', 'b49', 'b47', 'b51', 'b48']]",5,"sent1: This topic was discussed in about 6.36% (1837/28,904) of the publications (eg, [48][49][50][51][52]).
sent2: Authors who had the highest number of publications related to this topic were Alessandro Sette (n=7), Stanley Perlman (n=6), Nima Rezaei (n=6), Irfan Rahman (n=5), and Akiko Iwasaki (n=6).
sent3: The top 5 journals and preprint servers in terms of publishing articles related to this topic were bioRxiv (n=199), Medical Hypotheses (n=50), the Journal of Medical Virology (n=49), Frontiers in Immunology (n=22), and the British Journal of Haematology (n=19).
sent4: The earliest article related to this topic was published on January 2, 2020.
sent5: From that date until week 14, there was a slight increase in the number of weekly publications before it increased markedly, peaking in week 25 (n=155) (Multimedia Appendix 1).
sent6: The mean number of weekly publications in this cluster was 62.8 (SD 54.6)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s15,Topic 4: Epidemic Models for COVID-19 Spread,"A total of 10.25% (2964/28,904) of the included publications were related to this topic (eg, [39][40][41][42][43]). The 5 most prominent authors in this cluster were Gerardo Chowell (n=22), Benjamin J Cowling (n=18), Kenji Mizumoto (n=14), Shi Zhao (n=13), and Rosalind M Eggo (n=13). The most common journals and preprint servers where the articles related to this topic were published included Chaos, Solitons & Fractals (n=73), medRxiv (n=66), the International Journal of Infectious Diseases (n=36), Zhonghua liuxingbingxue zazhi (n=30), and bioRxiv (n=26). The first paper related to this topic was published on the January 19, 2020. Although there was a sharp increase in the number of weekly publications between weeks 12 and 15, the trend was almost stable from week 15 to week 22. Then, a rapid decline in the number of weekly publications was noticed (Multimedia Appendix 1). The mean number of weekly publications in this cluster was 101.6 (SD 68.2).","[['b41', 'b42', 'b40', 'b38', 'b39']]","[['b41', 'b42', 'b40', 'b38', 'b39']]",5,"sent1: A total of 10.25% (2964/28,904) of the included publications were related to this topic (eg, [39][40][41][42][43]).
sent2: The 5 most prominent authors in this cluster were Gerardo Chowell (n=22), Benjamin J Cowling (n=18), Kenji Mizumoto (n=14), Shi Zhao (n=13), and Rosalind M Eggo (n=13).
sent3: The most common journals and preprint servers where the articles related to this topic were published included Chaos, Solitons & Fractals (n=73), medRxiv (n=66), the International Journal of Infectious Diseases (n=36), Zhonghua liuxingbingxue zazhi (n=30), and bioRxiv (n=26).
sent4: The first paper related to this topic was published on the January 19, 2020.
sent5: Although there was a sharp increase in the number of weekly publications between weeks 12 and 15, the trend was almost stable from week 15 to week 22.
sent6: Then, a rapid decline in the number of weekly publications was noticed (Multimedia Appendix 1).
sent7: The mean number of weekly publications in this cluster was 101.6 (SD 68.2)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s13,Topic 2: Clinical Care Practices During the COVID-19 Pandemic,"A total of 17.71% (5118/28,904) of all included publications were mainly about clinical care practices for non-COVID-19 patients during the COVID-19 pandemic (eg, [29][30][31][32][33]). The following authors published the highest number of publications related to this topic: Karthik Rajasekaran (n=14), Francesco Esperto (n=12), Raju Vaishya (n=9), Namrata Sharma (n=8), and Santosh G Honavar (n=8). The top 5 journals publishing articles related to this topic were Otolaryngology-Head and Neck Surgery (n=115), the Journal of the European Academy of Dermatology and Venereology (n=45), Cureus Journal of Medical Science (n=41), Anaesthesia (n=40), and World Neurosurgery (n=35). In this cluster, the first article was published on January 3, 2020. There was a considerable rise in the number of weekly publications from week 12 until it reached a peak in week 23 (n=479); this was followed by a sharp decrease (Multimedia Appendix 1). The mean number of weekly publications in this cluster was 175.2 (SD 159.6).","[['b29', 'b28', 'b31', 'b30', 'b32']]","[['b29', 'b28', 'b31', 'b30', 'b32']]",5,"sent1: A total of 17.71% (5118/28,904) of all included publications were mainly about clinical care practices for non-COVID-19 patients during the COVID-19 pandemic (eg, [29][30][31][32][33]).
sent2: The following authors published the highest number of publications related to this topic: Karthik Rajasekaran (n=14), Francesco Esperto (n=12), Raju Vaishya (n=9), Namrata Sharma (n=8), and Santosh G Honavar (n=8).
sent3: The top 5 journals publishing articles related to this topic were Otolaryngology-Head and Neck Surgery (n=115), the Journal of the European Academy of Dermatology and Venereology (n=45), Cureus Journal of Medical Science (n=41), Anaesthesia (n=40), and World Neurosurgery (n=35).
sent4: In this cluster, the first article was published on January 3, 2020.
sent5: There was a considerable rise in the number of weekly publications from week 12 until it reached a peak in week 23 (n=479); this was followed by a sharp decrease (Multimedia Appendix 1).
sent6: The mean number of weekly publications in this cluster was 175.2 (SD 159.6)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s18,Topic 7: Diagnosis of COVID-19 Using Polymerase Chain Reaction,"Using polymerase chain reaction (PCR) for diagnosing COVID-19 was a key topic discussed in 5.54% (1602/28,904) of the publications (eg, [53][54][55][56][57]). The most common authors writing about this topic were Alexander L Greninger (n=11), Kwok-Yung Yuen (n=10), Jasper Fuk-Woo Chan (n=9), Kelvin Kai-Wang To (n=9), and Cyril Chik-Yan Yip (n=9). The top 5 journals and preprint servers that published the highest number of studies related to this topic were bioRxiv (n=136), the Journal of Medical Virology (n=53), the Journal of Clinical Virology (n=40), medRxiv (n=31), and Clinical Infectious Diseases (n=30). The earliest study in this cluster was published at the beginning of week 3. The highest number of weekly publications was 119 in weeks 22 and 23 (Multimedia Appendix 1). The mean number of weekly publications related to this topic was 54.8 (SD 45.6).","[['b52', 'b56', 'b55', 'b54', 'b53']]","[['b52', 'b56', 'b55', 'b54', 'b53']]",5,"sent1: Using polymerase chain reaction (PCR) for diagnosing COVID-19 was a key topic discussed in 5.54% (1602/28,904) of the publications (eg, [53][54][55][56][57]).
sent2: The most common authors writing about this topic were Alexander L Greninger (n=11), Kwok-Yung Yuen (n=10), Jasper Fuk-Woo Chan (n=9), Kelvin Kai-Wang To (n=9), and Cyril Chik-Yan Yip (n=9).
sent3: The top 5 journals and preprint servers that published the highest number of studies related to this topic were bioRxiv (n=136), the Journal of Medical Virology (n=53), the Journal of Clinical Virology (n=40), medRxiv (n=31), and Clinical Infectious Diseases (n=30).
sent4: The earliest study in this cluster was published at the beginning of week 3.
sent5: The highest number of weekly publications was 119 in weeks 22 and 23 (Multimedia Appendix 1).
sent6: The mean number of weekly publications related to this topic was 54.8 (SD 45.6)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s12,Topic 1: Public Health Response,"This topic was addressed by 18.66% (5393/28,904) of the publications. The publications in this cluster mainly discussed how public health authorities in various countries responded to the COVID-19 pandemic (eg, [24][25][26][27][28]). The top 5 authors in terms of the highest number of publications related to this topic were Claudine McCarthy (n=8), Valerie A Canady (n=6), Alison Knopf (n=6), Alimuddin Zumla (n=6), and Nima Rezaei (n=6). The top 5 journals and preprint servers hosting the highest number of publishing articles related to this topic were the International Journal of Environmental Research and Public Health (n=82), Science of the Total Environment (n=80), New Scientist (n=56), Journal of Medical Virology (n=53), and bioRxiv (n=50). The first paper related to this topic was published on January 10, 2020. The number of publications in each week increased significantly until it reached a peak in week 23 (n=434); it then decreased noticeably (Multimedia Appendix 1). The mean number of weekly publications in this cluster was 183.6 (SD 151.5).","[['b27', 'b25', 'b26', 'b23', 'b24']]","[['b27', 'b25', 'b26', 'b23', 'b24']]",5,"sent1: This topic was addressed by 18.66% (5393/28,904) of the publications.
sent2: The publications in this cluster mainly discussed how public health authorities in various countries responded to the COVID-19 pandemic (eg, [24][25][26][27][28]).
sent3: The top 5 authors in terms of the highest number of publications related to this topic were Claudine McCarthy (n=8), Valerie A Canady (n=6), Alison Knopf (n=6), Alimuddin Zumla (n=6), and Nima Rezaei (n=6).
sent4: The top 5 journals and preprint servers hosting the highest number of publishing articles related to this topic were the International Journal of Environmental Research and Public Health (n=82), Science of the Total Environment (n=80), New Scientist (n=56), Journal of Medical Virology (n=53), and bioRxiv (n=50).
sent5: The first paper related to this topic was published on January 10, 2020.
sent6: The number of publications in each week increased significantly until it reached a peak in week 23 (n=434); it then decreased noticeably (Multimedia Appendix 1).
sent7: The mean number of weekly publications in this cluster was 183.6 (SD 151.5)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s2,Research Problem and Aim,"Manually conducting a comprehensive review of the thousands of COVID-19-related publications is a daunting and time-consuming task. Artificial intelligence (AI) methods can play a pivotal role in rapidly surveying the enormous number of publications and extracting critical insights from them. Therefore, in March 2020, the White House strongly recommended researchers to exploit AI methods in COVID-19 research [8].

Several studies have used AI methods to conduct a bibliometric analysis of research on the COVID-19 pandemic [6][7][8][10][11][12]. However, we identified the following research gaps in these studies. First, publications analyzed in most studies were dated, approximately to the first three months after the onset of the COVID-19 outbreak; thus, numerous studies published afterward were not analyzed [7,[10][11][12][13][14][15][16]. Second, several studies analyzed publications related to all types of coronaviruses instead of focusing on COVID-19 [7,[10][11][12]17,18]; hence, the results related to COVID-19 were aggregated with those related to other coronaviruses. Third, several studies included only a few (ranging from 38 to 1482) of the large number of publications related to COVID-19 available in the search period [7,10,[12][13][14][15][16]19]. Fourth, most studies did not examine the topics that previous studied had addressed, instead they assessed only the metadata of those studies (eg, countries, authors, number of citations, and published journals) [13,14,[16][17][18][19]. Fifth, topic identification among various studies was conducted using manual screening instead of AI methods [13][14][15]. To fill the abovementioned gaps, this study aims to conduct an extensive bibliometric analysis to provide a comprehensive overview of the existing COVID-19 literature.","[['b7'], ['b15', 'b14', 'b10', 'b16', 'b17', 'b5', 'b7', 'b9', 'b18', 'b11', 'b13', 'b12', 'b6']]","[['b7'], ['b15', 'b14', 'b10', 'b16', 'b17', 'b5', 'b7', 'b9', 'b18', 'b11', 'b13', 'b12', 'b6']]",14,"sent1: Manually conducting a comprehensive review of the thousands of COVID-19-related publications is a daunting and time-consuming task.
sent2: Artificial intelligence (AI) methods can play a pivotal role in rapidly surveying the enormous number of publications and extracting critical insights from them.
sent3: Therefore, in March 2020, the White House strongly recommended researchers to exploit AI methods in COVID-19 research [8].
sent4: Several studies have used AI methods to conduct a bibliometric analysis of research on the COVID-19 pandemic [6][7][8][10][11][12].
sent5: However, we identified the following research gaps in these studies.
sent6: First, publications analyzed in most studies were dated, approximately to the first three months after the onset of the COVID-19 outbreak; thus, numerous studies published afterward were not analyzed [7,[10][11][12][13][14][15][16].
sent7: Second, several studies analyzed publications related to all types of coronaviruses instead of focusing on COVID-19
sent8: [7,[10][11][12]17,18] ; hence, the results related to COVID-19 were aggregated with those related to other coronaviruses.
sent9: Third, several studies included only a few (ranging from 38 to 1482) of the large number of publications related to COVID-19 available in the search period [7,10,[12][13][14][15][16]19].
sent10: Fourth, most studies did not examine the topics that previous studied had addressed, instead they assessed only the metadata of those studies (eg, countries, authors, number of citations, and published journals) [13,14,[16][17][18][19].
sent11: Fifth, topic identification among various studies was conducted using manual screening instead of AI methods [13][14][15].
sent12: To fill the abovementioned gaps, this study aims to conduct an extensive bibliometric analysis to provide a comprehensive overview of the existing COVID-19 literature."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s1,Background,"In December 2019, Wuhan city in China registered several cases of an unknown disease characterized by pneumonia, dry cough, fatigue, and fever [1]. The investigations revealed that a novel coronavirus (2019-nCoV) was the causative agent of the disease, which was subsequently named COVID-19 [1]. Since then, COVID-19 has spread around the globe, leading the World Health Organization to classify it as a pandemic [2]. This highly contagious pathogen has affected almost every aspect of our daily lives, such as education, traveling, business, transportation, sports, and health care [3]. Most importantly, the COVID-19 pandemic has claimed more than 775,000 lives as of August 19, 2020 [4]. To curb the impact of COVID-19, authorities need to implement effective public health measures related to COVID-19 surveillance, diagnostics, vaccines, treatments, and research [5].

Given the novelty and, consequently, the lack of knowledge about the disease, research can play a crucial role in the fight against the COVID-19 pandemic. Scientists have rapidly mobilized to manage and slowdown the growth of the pandemic. The scientific literature in this domain area has exponentially increased [6,7]. By the end of May 2020, Aristovnik et al [6] and Doanvo et al [8] retrieved 10,344 and 18,412 COVID-19-related publications written in the English language, respectively, from the Scopus database and the COVID-19 Open Research Dataset . In addition, as of July 13, 2020, more than 1711 clinical trials were registered in different clinical trial registries (eg, NCT, EUCTR, and ISRCTN) [9].

It is very important to have a comprehensive overview of the current state of the literature on COVID-19 for several reasons, namely: (1) to organize and coordinate the literature; (2) to explore research topics addressed; (3) to prioritize research needs or gaps; (4) to understand the evolution of the literature; (5) to recognize the leading researchers, institutes, and countries in this area; and (6) to explore connections between research topics and areas.","[['b4', 'b3', 'b1', 'b2', 'b0'], ['b8', 'b5', 'b6', 'b7'], ['b3', 'b1', 'b5', 'b4']]","[['b4', 'b3', 'b1', 'b2', 'b0'], ['b8', 'b5', 'b6', 'b7'], ['b3', 'b1', 'b5', 'b4']]",13,"sent1: In December 2019, Wuhan city in China registered several cases of an unknown disease characterized by pneumonia, dry cough, fatigue, and fever [1].
sent2: The investigations revealed that a novel coronavirus (2019-nCoV) was the causative agent of the disease, which was subsequently named COVID-19 [1].
sent3: Since then, COVID-19 has spread around the globe, leading the World Health Organization to classify it as a pandemic [2].
sent4: This highly contagious pathogen has affected almost every aspect of our daily lives, such as education, traveling, business, transportation, sports, and health care [3].
sent5: Most importantly, the COVID-19 pandemic has claimed more than 775,000 lives as of August 19, 2020 [4].
sent6: To curb the impact of COVID-19, authorities need to implement effective public health measures related to COVID-19 surveillance, diagnostics, vaccines, treatments, and research [5].
sent7: Given the novelty and, consequently, the lack of knowledge about the disease, research can play a crucial role in the fight against the COVID-19 pandemic.
sent8: Scientists have rapidly mobilized to manage and slowdown the growth of the pandemic.
sent9: The scientific literature in this domain area has exponentially increased [6,7].
sent10: By the end of May 2020, Aristovnik et al [6] and Doanvo et al [8] retrieved 10,344 and 18,412 COVID-19-related publications written in the English language, respectively, from the Scopus database and the COVID-19 Open Research Dataset .
sent11: In addition, as of July 13, 2020, more than 1711 clinical trials were registered in different clinical trial registries (eg, NCT, EUCTR, and ISRCTN) [9].
sent12: It is very important to have a comprehensive overview of the current state of the literature on COVID-19 for several reasons, namely: (1) to organize and coordinate the literature; (2) to explore research topics addressed; (3) to prioritize research needs or gaps; (4) to understand the evolution of the literature; (5) to recognize the leading researchers, institutes, and countries in this area; and (6) to explore connections between research topics and areas."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s14,Topic 3: Clinical Characteristics and Risk Factors of COVID-19,"This topic was discussed in 11.46% (3313/28,904) of the publications (eg, [34][35][36][37][38]). The top 5 authors who published the highest number of publications in this cluster were Lei Liu (n=25), Lanjuan Li (n=13), Jifang Sheng (n=10), Giuseppe Lippi (n=9), and Nanshan Zhong (n=9). The top 5 journals and preprint servers publishing articles related to this topic were the Journal of Medical Virology (n=109), medRxiv (n=57), Cureus Journal of Medical Science (n=35), Clinical Infectious Diseases (n=33), and the International Journal of Infectious Diseases (n=32). The first article related to this topic was published on January 17, 2020. The mean number of weekly publications in this cluster was 113.7 (SD 98.9), and the highest number of weekly publications was 266 in week 20 (Multimedia Appendix 1).","[['b37', 'b35', 'b36', 'b34', 'b33']]","[['b37', 'b35', 'b36', 'b34', 'b33']]",5,"sent1: This topic was discussed in 11.46% (3313/28,904) of the publications (eg, [34][35][36][37][38]).
sent2: The top 5 authors who published the highest number of publications in this cluster were Lei Liu (n=25), Lanjuan Li (n=13), Jifang Sheng (n=10), Giuseppe Lippi (n=9), and Nanshan Zhong (n=9).
sent3: The top 5 journals and preprint servers publishing articles related to this topic were the Journal of Medical Virology (n=109), medRxiv (n=57), Cureus Journal of Medical Science (n=35), Clinical Infectious Diseases (n=33), and the International Journal of Infectious Diseases (n=32).
sent4: The first article related to this topic was published on January 17, 2020.
sent5: The mean number of weekly publications in this cluster was 113.7 (SD 98.9), and the highest number of weekly publications was 266 in week 20 (Multimedia Appendix 1)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s19,Topic 8: Mental Health and Disorders During the COVID-19 Pandemic,"This topic is about COVID-19-related mental health and disorders, which was explored by 3.17% (915/28,904) of the publications (eg, [58][59][60][61][62]). The top 5 authors in terms of number of publications related to this topic were Valerie A Canady (n=15), Mark D Griffiths (n=8), Stephen X Zhang (n=6), Zhilei Shang (n=5), and Modesto Leite Rolim Neto (n=5). The top 5 journals publishing studies related to this topic were Psychological Trauma: Theory, Research, Practice, and Policy (n=101); Psychiatry Research (n=48); the International Journal of Environmental Research and Public Health (n=37); the Journal of Affective Disorders (n=23); and Mental Health Weekly (n=23). In this cluster, the first article was published at the beginning of week 8. There was a considerable rise in the number of weekly publications from week 14 until a peak was reached in week 23 (n=94); this was followed by a steep decline (Multimedia Appendix 1). The mean number of weekly publications in this cluster was 31.1 (SD 30.9).","[['b59', 'b57', 'b60', 'b61', 'b58']]","[['b59', 'b57', 'b60', 'b61', 'b58']]",5,"sent1: This topic is about COVID-19-related mental health and disorders, which was explored by 3.17% (915/28,904) of the publications (eg, [58][59][60][61][62]).
sent2: The top 5 authors in terms of number of publications related to this topic were Valerie A Canady (n=15), Mark D Griffiths (n=8), Stephen X Zhang (n=6), Zhilei Shang (n=5), and Modesto Leite Rolim Neto (n=5).
sent3: The top 5 journals publishing studies related to this topic were Psychological Trauma: Theory, Research, Practice, and Policy (n=101); Psychiatry Research (n=48); the International Journal of Environmental Research and Public Health (n=37); the Journal of Affective Disorders (n=23); and Mental Health Weekly (n=23).
sent4: In this cluster, the first article was published at the beginning of week 8.
sent5: There was a considerable rise in the number of weekly publications from week 14 until a peak was reached in week 23 (n=94); this was followed by a steep decline (Multimedia Appendix 1).
sent6: The mean number of weekly publications in this cluster was 31.1 (SD 30.9)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s16,Topic 5: Therapies and Vaccines for COVID-19,"In all, 6.38% (1845/28,904) of the publications were about the development and repurposing of therapies and vaccines for COVID-19 (eg, [44][45][46][47]). The following authors published the highest number of articles related to this topic: Wei Zhang (n=13), Xiuna Yang (n=9), Haitao Yang (n=9), Zihe Rao (n=9), and Yao Zhao (n=8). The journals and preprint servers publishing the highest number of studies in this cluster were bioRxiv (n=174), the Journal of Biomolecular Structure and Dynamics (n=74), Trials (n=49), the Journal of Medical Virology (n=20), and Clinical Pharmacology & Therapeutics (n=14). In this cluster, the first article was published on January 6, 2020. The number of weekly publications increased dramatically from week 14 until a peak was reached in week 22 (n=144); thereafter, it decreased noticeably (Multimedia Appendix 1). The mean number of weekly publications in this cluster was 62.9 (SD 52.3).","[['b45', 'b43', 'b46', 'b44']]","[['b45', 'b43', 'b46', 'b44']]",4,"sent1: In all, 6.38% (1845/28,904) of the publications were about the development and repurposing of therapies and vaccines for COVID-19 (eg, [44][45][46][47]).
sent2: The following authors published the highest number of articles related to this topic: Wei Zhang (n=13), Xiuna Yang (n=9), Haitao Yang (n=9), Zihe Rao (n=9), and Yao Zhao (n=8).
sent3: The journals and preprint servers publishing the highest number of studies in this cluster were bioRxiv (n=174), the Journal of Biomolecular Structure and Dynamics (n=74), Trials (n=49), the Journal of Medical Virology (n=20), and Clinical Pharmacology & Therapeutics (n=14).
sent4: In this cluster, the first article was published on January 6, 2020.
sent5: The number of weekly publications increased dramatically from week 14 until a peak was reached in week 22 (n=144); thereafter, it decreased noticeably (Multimedia Appendix 1).
sent6: The mean number of weekly publications in this cluster was 62.9 (SD 52.3)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s21,Topic 10: Social Distancing Measures,"A total of 3% (868/28,904) of the articles discussed the topic of social distancing measures used to fight against the COVID-19 pandemic (eg, [68][69][70][71][72]). Authors who had the highest number of publications related to this topic were Lei Zhang (n=7), Adam J Kucharski (n=6), Amy Gimma (n=5), Gerardo Chowell (n=5), and Petra Klepac (4). The top 5 journals and preprint servers in terms of publishing articles related to this topic were medRxiv (n=28); Chaos, Solitons & Fractals (n=6); Morbidity and Mortality Weekly Report (n=5); Science (n=5); and Disaster Medicine and Public Health Preparedness (n=5). The earliest article related to this topic was published in week 7. There was a dramatic rise in the number of weekly publications between week 12 and week 19; thereafter, the trend was unstable from week 20 to week 29 (Multimedia Appendix 1). The mean number of weekly publications in this cluster was 29.8 (SD 26.8).","[['b67', 'b68', 'b70', 'b71', 'b69']]","[['b67', 'b68', 'b70', 'b71', 'b69']]",5,"sent1: A total of 3% (868/28,904) of the articles discussed the topic of social distancing measures used to fight against the COVID-19 pandemic (eg, [68][69][70][71][72]).
sent2: Authors who had the highest number of publications related to this topic were Lei Zhang (n=7), Adam J Kucharski (n=6), Amy Gimma (n=5), Gerardo Chowell (n=5), and Petra Klepac (4).
sent3: The top 5 journals and preprint servers in terms of publishing articles related to this topic were medRxiv (n=28); Chaos, Solitons & Fractals (n=6); Morbidity and Mortality Weekly Report (n=5); Science (n=5); and Disaster Medicine and Public Health Preparedness (n=5).
sent4: The earliest article related to this topic was published in week 7.
sent5: There was a dramatic rise in the number of weekly publications between week 12 and week 19; thereafter, the trend was unstable from week 20 to week 29 (Multimedia Appendix 1).
sent6: The mean number of weekly publications in this cluster was 29.8 (SD 26.8)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s22,Topic 11: Virus Genomics,"Around 2.82% (816/28,904) of the publications were about genome sequences of 2019-nCoV (eg, [73][74][75][76][77]). The most common authors writing about this topic were Massimo Ciccozzi (n=11), Andrew Rambaut (n=11), Marta Giovanetti (n=10), Silvia Angeletti (n=10), and Domenico Benvenuto (n=9). The top 5 journals and preprint servers that published the highest number of studies in this cluster were bioRxiv (n=295); the Journal of Medical Virology (n=32); Microbiology Resource Announcements (n=14); Viruses (n=13); and Infection, Genetics and Evolution (n=12). The earliest study in this cluster was published at the beginning of week 4. The mean number of weekly publications in this cluster was 27.7 (SD 18.2). The highest number of weekly publications was 56 in week 24 (Multimedia Appendix 1). ","[['b74', 'b77', 'b76', 'b73', 'b72']]","[['b74', 'b77', 'b76', 'b73', 'b72']]",5,"sent1: Around 2.82% (816/28,904) of the publications were about genome sequences of 2019-nCoV (eg, [73][74][75][76][77]).
sent2: The most common authors writing about this topic were Massimo Ciccozzi (n=11), Andrew Rambaut (n=11), Marta Giovanetti (n=10), Silvia Angeletti (n=10), and Domenico Benvenuto (n=9).
sent3: The top 5 journals and preprint servers that published the highest number of studies in this cluster were bioRxiv (n=295); the Journal of Medical Virology (n=32); Microbiology Resource Announcements (n=14); Viruses (n=13); and Infection, Genetics and Evolution (n=12).
sent4: The earliest study in this cluster was published at the beginning of week 4.
sent5: The mean number of weekly publications in this cluster was 27.7 (SD 18.2).
sent6: The highest number of weekly publications was 56 in week 24 (Multimedia Appendix 1)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s24,Topic 13: Host Cell Entry,"Host cell entry for 19-nCoV (via angiotensin-converting enzyme 2) was a key topic discussed in 2.02% (584/28,904) of the reviewed publications (eg, [83][84][85][86][87]). The 5 most prominent authors in this cluster were Serpil Erzurum (n=4), Giuseppe Lippi (n=4), Daniel Batlle (n=4), Hong Gao (n=4), and Claudio Cavallini (n=3). The most common journals and preprint servers in this cluster were bioRxiv (n=117), the Journal of Medical Virology (n=11), Medical Hypotheses (n=10), European Respiratory Journal (n=8), and medRxiv (n=7). The first article related to this topic was published in the mid of week 4. The number of weekly publications was almost stable between weeks 4 and 13. Thereafter, a sharp increase was noticed between weeks 14 and 16, but it was not stable from then until week 29 (Multimedia Appendix 1). The number of publications in week 20 was the highest (n=50). The mean number of weekly publications in this cluster was 19.9 (SD 16.6).","[['b85', 'b84', 'b83', 'b87', 'b86']]","[['b85', 'b84', 'b83', 'b87', 'b86']]",5,"sent1: Host cell entry for 19-nCoV (via angiotensin-converting enzyme 2) was a key topic discussed in 2.02% (584/28,904) of the reviewed publications (eg, [83][84][85][86][87]).
sent2: The 5 most prominent authors in this cluster were Serpil Erzurum (n=4), Giuseppe Lippi (n=4), Daniel Batlle (n=4), Hong Gao (n=4), and Claudio Cavallini (n=3).
sent3: The most common journals and preprint servers in this cluster were bioRxiv (n=117), the Journal of Medical Virology (n=11), Medical Hypotheses (n=10), European Respiratory Journal (n=8), and medRxiv (n=7).
sent4: The first article related to this topic was published in the mid of week 4.
sent5: The number of weekly publications was almost stable between weeks 4 and 13.
sent6: Thereafter, a sharp increase was noticed between weeks 14 and 16, but it was not stable from then until week 29 (Multimedia Appendix 1).
sent7: The number of publications in week 20 was the highest (n=50).
sent8: The mean number of weekly publications in this cluster was 19.9 (SD 16.6)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s25,Topic 14: Patients With Cancer During the COVID-19 Pandemic,"A total of 1.53% (441/28,904) of the included publications were about patients with cancer during the COVID-19 pandemic (eg, [88][89][90][91]). The following authors published the highest number of articles related to this topic: Solange Peters (n=6), Umberto Ricardi (n=5), Conghua Xie (n=5), Giuseppe Curigliano (n=5), and Alessio Cortellini (n=4). The top 5 journals in terms of publishing articles related to this topic were Head & Neck (n=16), ecancermedicalscience (n=15), Radiotherapy and Oncology (n=10), Advances in Radiation Oncology (n=8), and Cancer Discovery (n=8). From week 1 to 7, only 1 study related to this topic was published. The highest number of weekly publications was in week 22 (n=49), but since then, the number of weekly publications declined steeply (Multimedia Appendix 1). The mean number of weekly publications related to this topic was 15.2 (SD 14.4).","[['b88', 'b90', 'b89', 'b91']]","[['b88', 'b90', 'b89', 'b91']]",4,"sent1: A total of 1.53% (441/28,904) of the included publications were about patients with cancer during the COVID-19 pandemic (eg, [88][89][90][91]).
sent2: The following authors published the highest number of articles related to this topic: Solange Peters (n=6), Umberto Ricardi (n=5), Conghua Xie (n=5), Giuseppe Curigliano (n=5), and Alessio Cortellini (n=4).
sent3: The top 5 journals in terms of publishing articles related to this topic were Head & Neck (n=16), ecancermedicalscience (n=15), Radiotherapy and Oncology (n=10), Advances in Radiation Oncology (n=8), and Cancer Discovery (n=8).
sent4: From week 1 to 7, only 1 study related to this topic was published.
sent5: The highest number of weekly publications was in week 22 (n=49), but since then, the number of weekly publications declined steeply (Multimedia Appendix 1).
sent6: The mean number of weekly publications related to this topic was 15.2 (SD 14.4)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s26,Topic 15: Detection of 2019-nCoV Antibodies,"Detection of antibodies against 2019-nCoV using serological assays was a topic discussed in 1.42% (411/28,904) of all publications (eg, [92][93][94][95][96]). The top 5 authors writing about this topic were Florian Krammer (n=6), Jing Wang (n=6), Yong Zhang (n=5), Juan Chen (n=5), and Viviana Simon (n=5). The top 5 journals and preprint servers that published the highest number of studies in this cluster were bioRxiv (n=30), medRxiv (n=20), the Journal of Medical Virology (n=18), the Journal of Clinical Virology (n=14), and the Journal of Clinical Microbiology (n=7). Only 1 study in this cluster was published in the first 6 weeks. There was a dramatic increase in the number of weekly publications between weeks 16 and 21. Although the number of weekly publications slightly decreased from week 22 until week 26, it increased rapidly until reaching the peak in weeks 28 and 29 (n=36) (Multimedia Appendix 1). The mean number of weekly publications in this cluster was 14.1 (SD 12.8).","[['b95', 'b96', 'b93', 'b92', 'b94']]","[['b95', 'b96', 'b93', 'b92', 'b94']]",5,"sent1: Detection of antibodies against 2019-nCoV using serological assays was a topic discussed in 1.42% (411/28,904) of all publications (eg, [92][93][94][95][96]).
sent2: The top 5 authors writing about this topic were Florian Krammer (n=6), Jing Wang (n=6), Yong Zhang (n=5), Juan Chen (n=5), and Viviana Simon (n=5).
sent3: The top 5 journals and preprint servers that published the highest number of studies in this cluster were bioRxiv (n=30), medRxiv (n=20), the Journal of Medical Virology (n=18), the Journal of Clinical Virology (n=14), and the Journal of Clinical Microbiology (n=7).
sent4: Only 1 study in this cluster was published in the first 6 weeks.
sent5: There was a dramatic increase in the number of weekly publications between weeks 16 and 21.
sent6: Although the number of weekly publications slightly decreased from week 22 until week 26, it increased rapidly until reaching the peak in weeks 28 and 29 (n=36) (Multimedia Appendix 1).
sent7: The mean number of weekly publications in this cluster was 14.1 (SD 12.8)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s27,Topic 16: Personal Protective Equipment,"Around 1.21% (350/28,904) of the publications focused on personal protective equipment in the COVID-19 era (eg, [97][98][99][100]). Authors who published the most in this cluster were Holly Seale (n=3), Keith K Wannomae (n=3), Lei Liao (n=3), Wang Xiao (n=3), and Steven Chu (n=3). The highest numbers of studies were published in the following journals and preprint servers: the American Journal of Infection Control (n=8), the Journal of Hospital Infection (n=7), Anaesthesia (n=6), the Journal of the European Academy of Dermatology and Venereology (n=6), ACS Nano (n=5), and medRxiv (n=5). No articles in this cluster were published before week 9. The mean number of weekly publications in this cluster was 11.9 (SD 11.6). The highest number of weekly publications was 34 in week 23 (Multimedia Appendix 1).","[['b99', 'b97', 'b98', 'b100']]","[['b99', 'b97', 'b98', 'b100']]",4,"sent1: Around 1.21% (350/28,904) of the publications focused on personal protective equipment in the COVID-19 era (eg, [97][98][99][100]).
sent2: Authors who published the most in this cluster were Holly Seale (n=3), Keith K Wannomae (n=3), Lei Liao (n=3), Wang Xiao (n=3), and Steven Chu (n=3).
sent3: The highest numbers of studies were published in the following journals and preprint servers: the American Journal of Infection Control (n=8), the Journal of Hospital Infection (n=7), Anaesthesia (n=6), the Journal of the European Academy of Dermatology and Venereology (n=6), ACS Nano (n=5), and medRxiv (n=5).
sent4: No articles in this cluster were published before week 9.
sent5: The mean number of weekly publications in this cluster was 11.9 (SD 11.6).
sent6: The highest number of weekly publications was 34 in week 23 (Multimedia Appendix 1)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s28,Topic 17: Diabetes Mellitus and COVID-19,"Health care management, clinical characteristics, and risk factors for mortality of COVID-19 patients with diabetes was discussed in 1.16% (336/28,904) of the included articles (eg, [101][102][103][104]. Technology & Therapeutics (n=6). Only 4 articles related to this topic were published between weeks 1 and 12. However, there was a substantial increase in the number of weekly publications from week 17 until the peak was reached in week 20 (n=35); this was followed by a slight decrease (Multimedia Appendix 1). The mean number of weekly publications in this cluster was 11.4 (SD 11.6).","[['b102', 'b104', 'b101', 'b103']]","[['b102', 'b104', 'b101', 'b103']]",4,"sent1: Health care management, clinical characteristics, and risk factors for mortality of COVID-19 patients with diabetes was discussed in 1.16% (336/28,904) of the included articles (eg, [101][102][103][104]. Technology & Therapeutics (n=6).
sent2: Only 4 articles related to this topic were published between weeks 1 and 12.
sent3: However, there was a substantial increase in the number of weekly publications from week 17 until the peak was reached in week 20 (n=35); this was followed by a slight decrease (Multimedia Appendix 1).
sent4: The mean number of weekly publications in this cluster was 11.4 (SD 11.6)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s29,Topic 18: Pregnancy and Childbirth During the COVID-19 Pandemic,"About 1.08% (312/28,904) of the publications focused on numerous aspects of pregnancy and childbirth during the COVID-19 pandemic (eg, [105][106][107][108][109]). The most common authors writing about this topic were Ling Feng (n=7), Jiafu Li (n=6), Olivier Picone (n=5), Dunjin Chen (n=5), and Guoqiang Sun (n=5). The top 5 journals in terms of publishing articles in this cluster were the International Journal of Gynaecology and Obstetrics (n=18), The Journal of Maternal-Fetal & Neonatal Medicine (n=16), the American Journal of Obstetrics and Gynecology (n=20), Obstetrics and Gynecology (n=10), and the American Journal of Perinatology (n=9). The earliest article in this cluster was published on February 10, 2020. The mean number of weekly publications related to this topic was 10.8 (SD 8.9), and the highest number of weekly publications was 27 in week 21 (Multimedia Appendix 1).","[['b105', 'b107', 'b109', 'b106', 'b108']]","[['b105', 'b107', 'b109', 'b106', 'b108']]",5,"sent1: About 1.08% (312/28,904) of the publications focused on numerous aspects of pregnancy and childbirth during the COVID-19 pandemic (eg, [105][106][107][108][109]).
sent2: The most common authors writing about this topic were Ling Feng (n=7), Jiafu Li (n=6), Olivier Picone (n=5), Dunjin Chen (n=5), and Guoqiang Sun (n=5).
sent3: The top 5 journals in terms of publishing articles in this cluster were the International Journal of Gynaecology and Obstetrics (n=18), The Journal of Maternal-Fetal & Neonatal Medicine (n=16), the American Journal of Obstetrics and Gynecology (n=20), Obstetrics and Gynecology (n=10), and the American Journal of Perinatology (n=9).
sent4: The earliest article in this cluster was published on February 10, 2020.
sent5: The mean number of weekly publications related to this topic was 10.8 (SD 8.9), and the highest number of weekly publications was 27 in week 21 (Multimedia Appendix 1)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s30,Topic 19: Organ Transplantation During the COVID-19 Pandemic,"Organ transplantation in the era of COVID-19 was a key topic in 0.76% (219/28,904) of the included articles (eg, [110][111][112][113] ","[['b111', 'b110', 'b113', 'b112']]","[['b111', 'b110', 'b113', 'b112']]",4,"sent1: Organ transplantation in the era of COVID-19 was a key topic in 0.76% (219/28,904) of the included articles (eg, [110][111][112][113]"
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s20,Topic 9: Diagnosis of COVID-19 Based on Chest Imaging,"Diagnosis of COVID-19 based on chest imaging (eg, x-ray and computed tomography) was the main topic in 3.02% (874/28,904) of the included publications (eg, [63][64][65][66][67]). The 5 most prominent authors in this topic were Liming Xia (n=14), Michael Chung (n=12), Hongjun Li (n=11), Dinggang Shen (n=11), and Fuhua Yan (n=11). The top 5 journals in this cluster were European Radiology (n=38), Radiology (n=21), the American Journal of Roentgenology (n=17), Radiology: Cardiothoracic Imaging (n=16), and Clinical Nuclear Medicine (n=16). The first article about this topic was published on February 3, 2020. The mean number of weekly publications in this cluster was 29.9 (SD 21.4), and the highest number of weekly publications was 60 in week 18 (Multimedia Appendix 1).","[['b64', 'b62', 'b66', 'b63', 'b65']]","[['b64', 'b62', 'b66', 'b63', 'b65']]",5,"sent1: Diagnosis of COVID-19 based on chest imaging (eg, x-ray and computed tomography) was the main topic in 3.02% (874/28,904) of the included publications (eg, [63][64][65][66][67]).
sent2: The 5 most prominent authors in this topic were Liming Xia (n=14), Michael Chung (n=12), Hongjun Li (n=11), Dinggang Shen (n=11), and Fuhua Yan (n=11).
sent3: The top 5 journals in this cluster were European Radiology (n=38), Radiology (n=21), the American Journal of Roentgenology (n=17), Radiology: Cardiothoracic Imaging (n=16), and Clinical Nuclear Medicine (n=16).
sent4: The first article about this topic was published on February 3, 2020.
sent5: The mean number of weekly publications in this cluster was 29.9 (SD 21.4), and the highest number of weekly publications was 60 in week 18 (Multimedia Appendix 1)."
229535897,A Comprehensive Overview of the COVID-19 Literature: Machine Learning-Based Bibliometric Analysis,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/1e782ad8be1903c27de50beac2c46af68035c07e,s32,Principal Findings,"We found that 5.92% (1714/28,904) of the included published articles were hosted on preprint servers (bioRxiv or medRxiv). Although these servers are not the only preprint servers available in the academic publishing landscape (many journals publish articles online before they go into print, and we have also observed a rise of purely online journals), they are indicative of the pace with which new knowledge is made available by the international research community. Since such preprint servers do not undergo formal peer reviewing and are, thus, not regarded publications in the traditional academic sense, many researchers are using this device to make findings available and to solicit feedback from the international community before undergoing formal peer-reviewing by journals-a process that takes at least 2 months to get the submitted paper published.

Among the peer-reviewed journals, the Journal of Medical Virology has published the highest number of COVID-19-related articles (n=468). Aristovnik et al and Hossain also listed the Journal of Medical Virology in the top-5 journals publishing COVID-19-related articles [6,14]. The Journal of Medical Virology clearly stands out, as it has published more than twice the number of papers compared to the second-ranked journal-the International Journal of Environmental Research and Public Health (n=223). Aristovnik et al [6] listed the International Journal of Environmental Research and Public Health among the 10 top-ranked journals based on COVID-19-related research articles [6]. The source normalized impact per paper (SNIP), in the year 2019, was 0.780 for the Journal of Medical Virology [114] and 1.248 for the International Journal of Environmental Research and Public Health [115], and the average time from the submission to the first decision was about 6 weeks [116] and 3 weeks [117], respectively. We believe the speed of the reviewing process of these journals may have motivated the authors to submit their work to these journals.

Considering the study methods, we found that the highest number of studies (n=1515) were surveys, followed by reviews (systematic review, scoping review, or meta-analyses), as shown in Table 3. As the number of research studies on COVID-19 is rapidly increasing, review articles are of utmost importance to summarize the ongoing effort and progress to combat against COVID-19. We found case-control studies to be the lowest represented study design (n=62 only). We speculate that the lack of available data was the main reason for the scarcity of this type of research study. Interestingly, 362 randomized control trials in 7 months indicate the enormous effort made by the scientific community to combat this pandemic.

Furthermore, we grouped the 19 topics addressed in the included studies into six thematic areas (summarized in Table 6). The dominant thematic clusters were ""Clinical aspects"" (29.17%) and ""Epidemiology"" (28.91%). The ""Clinical aspects"" theme covers multiple aspects of the clinical practices for patient care and risk factors related to COVID-19. It consists of two topics (ie, ""clinical care practices for patients during the COVID-19 pandemic"" and ""clinical characteristics and risk factors of . Interestingly, the ""Epidemiology"" theme also comprises only two topics (ie, ""Public health response"" and ""Epidemic models for COVID-19 spread""), further underscoring the dominance of these topics.  (16) The third most prominent theme ""Therapeutics"" (21.03%) comprises five topics, making it the most diverse theme; the topics in this theme range from ""host cell entry"" to drug discovery-related terms such as ""Protein structures of 2019-nCoV"" and ""Virus genomics,"" as well as ""Therapies and vaccines for COVID-19."" This theme highlights the initiatives of the scientific community to discover drugs and vaccines and understand the underlying virus-host mechanism to pave the way for effective therapeutic solutions for COVID-19. Considering the severity of COVID-19, we believe there still may be a lack of publications in this theme, despite comprising slightly more than 20% of all articles. We believe that, as clinical practices and public health responses mature, this theme will receive more research articles in the near future.

Almost 10% of articles form the ""Diagnostics"" theme. This theme focuses on the diagnosis of COVID-19 based on PCR, radiological images, or antibodies. PCR is among the most accurate technologies to diagnose COVID-19 [114], which explains the numerous relevant publications. Due to the advancement of deep learning techniques, radiological image-based diagnosis is becoming more effective and has the potential to save time in clinical environments [115]. As a result, we observed a large number of publications on radiological image-based analyses, which is captured under the topic ""Diagnosis of COVID-19 based on chest imaging."" Antibodies, developed in hosts combatting the novel coronavirus, can be considered a detection mechanism that may play an important role complementary to PCR testing [116]. It can be very effective for the diagnosis of patients with asymptomatic COVID-19 or negative RT-PCR results [117]. We also noticed many publications on antibody responses against COVID-19, which are covered by the topic ""Detection of 2019-nCoV antibodies"".

The interplay between COVID-19 and related medical conditions is captured by the ""Related conditions"" theme. This theme not only comprises articles discussing related conditions caused by COVID-19 but also other conditions that may elevate the COVID-19 risk for the patients with those conditions. About 8% of all articles fall into this theme, covering topics such as mental disorder, diabetes, cancer, pregnancy, childbirth complications, and organ transplantation.

Only slightly more than 4% of all articles fall into the ""Prevention"" theme. This may be surprising, since prevention is of utmost importance while vaccines and treatments are still under development. However, we believe that this theme is not covered by more studies due to the recent wide acceptance and effectiveness of social distancing and personal protective equipment. Consequently, we expect the percentage of articles grouped in this theme to further reduce in the future. Further insights into the research landscape and the shift in themes over time is summarized in Multimedia Appendix 2.

We noticed that biomedical informatics had a crucial role in several topics. For instance, clinical decision support systems were used in many studies to diagnose COVID-19 based on chest imaging. Telemedicine was also used in multiple studies to provide the required health care support for the patients during the COVID-19 pandemic. Further, mobile applications, including contact tracing apps, were one of the main social distancing measures described in these studies. AI-based models were used in multiple studies to predict protein structures of 2019-nCoV to understand the underlying mechanism of drug-target interaction. Many studies proposed novel AI-based models to discover COVID-19 drugs and vaccines and repurpose existing drugs approved by the Food and Drug Administration as a part of the treatment plan for COVID-19.","[[], ['b115', 'b5', 'b114', 'b13'], [], ['b15'], ['b116', 'b115', 'b114', None], [], [], []]","[[], ['b115', 'b5', 'b114', 'b13'], [], ['b15'], ['b116', 'b115', 'b114', None], [], [], []]",9,"sent1: We found that 5.92% (1714/28,904) of the included published articles were hosted on preprint servers (bioRxiv or medRxiv).
sent2: Although these servers are not the only preprint servers available in the academic publishing landscape (many journals publish articles online before they go into print, and we have also observed a rise of purely online journals), they are indicative of the pace with which new knowledge is made available by the international research community.
sent3: Since such preprint servers do not undergo formal peer reviewing and are, thus, not regarded publications in the traditional academic sense, many researchers are using this device to make findings available and to solicit feedback from the international community before undergoing formal peer-reviewing by journals-a process that takes at least 2 months to get the submitted paper published.
sent4: Among the peer-reviewed journals, the Journal of Medical Virology has published the highest number of COVID-19-related articles (n=468).
sent5: Aristovnik et al and Hossain also listed the Journal of Medical Virology in the top-5 journals publishing COVID-19-related articles [6,14].
sent6: The Journal of Medical Virology clearly stands out, as it has published more than twice the number of papers compared to the second-ranked journal-the International Journal of Environmental Research and Public Health (n=223).
sent7: Aristovnik et al [6] listed the International Journal of Environmental Research and Public Health among the 10 top-ranked journals based on COVID-19-related research articles [6].
sent8: The source normalized impact per paper (SNIP), in the year 2019, was 0.780 for the Journal of Medical Virology [114] and 1.248 for the International Journal of Environmental Research and Public Health [115], and the average time from the submission to the first decision was about 6 weeks [116] and 3 weeks [117], respectively.
sent9: We believe the speed of the reviewing process of these journals may have motivated the authors to submit their work to these journals.
sent10: Considering the study methods, we found that the highest number of studies (n=1515) were surveys, followed by reviews (systematic review, scoping review, or meta-analyses), as shown in Table 3.
sent11: As the number of research studies on COVID-19 is rapidly increasing, review articles are of utmost importance to summarize the ongoing effort and progress to combat against COVID-19.
sent12: We found case-control studies to be the lowest represented study design (n=62 only).
sent13: We speculate that the lack of available data was the main reason for the scarcity of this type of research study.
sent14: Interestingly, 362 randomized control trials in 7 months indicate the enormous effort made by the scientific community to combat this pandemic.
sent15: Furthermore, we grouped the 19 topics addressed in the included studies into six thematic areas (summarized in Table 6).
sent16: The dominant thematic clusters were ""Clinical aspects"" (29.17%) and ""Epidemiology"" (28.91%).
sent17: The ""Clinical aspects"" theme covers multiple aspects of the clinical practices for patient care and risk factors related to COVID-19.
sent18: It consists of two topics (ie, ""clinical care practices for patients during the COVID-19 pandemic"" and ""clinical characteristics and risk factors of . Interestingly, the ""Epidemiology"" theme also comprises only two topics (ie, ""Public health response"" and ""Epidemic models for COVID-19 spread""), further underscoring the dominance of these topics.
sent19: (16) The third most prominent theme ""Therapeutics"" (21.03%) comprises five topics, making it the most diverse theme; the topics in this theme range from ""host cell entry"" to drug discovery-related terms such as ""Protein structures of 2019-nCoV"" and ""Virus genomics,"" as well as ""Therapies and vaccines for COVID-19.""
sent20: This theme highlights the initiatives of the scientific community to discover drugs and vaccines and understand the underlying virus-host mechanism to pave the way for effective therapeutic solutions for COVID-19.
sent21: Considering the severity of COVID-19, we believe there still may be a lack of publications in this theme, despite comprising slightly more than 20% of all articles.
sent22: We believe that, as clinical practices and public health responses mature, this theme will receive more research articles in the near future.
sent23: Almost 10% of articles form the ""Diagnostics"" theme.
sent24: This theme focuses on the diagnosis of COVID-19 based on PCR, radiological images, or antibodies.
sent25: PCR is among the most accurate technologies to diagnose COVID-19 [114], which explains the numerous relevant publications.
sent26: Due to the advancement of deep learning techniques, radiological image-based diagnosis is becoming more effective and has the potential to save time in clinical environments [115].
sent27: As a result, we observed a large number of publications on radiological image-based analyses, which is captured under the topic ""Diagnosis of COVID-19 based on chest imaging.""
sent28: Antibodies, developed in hosts combatting the novel coronavirus, can be considered a detection mechanism that may play an important role complementary to PCR testing [116].
sent29: It can be very effective for the diagnosis of patients with asymptomatic COVID-19 or negative RT-PCR results [117].
sent30: We also noticed many publications on antibody responses against COVID-19, which are covered by the topic ""Detection of 2019-nCoV antibodies"".
sent31: The interplay between COVID-19 and related medical conditions is captured by the ""Related conditions"" theme.
sent32: This theme not only comprises articles discussing related conditions caused by COVID-19 but also other conditions that may elevate the COVID-19 risk for the patients with those conditions.
sent33: About 8% of all articles fall into this theme, covering topics such as mental disorder, diabetes, cancer, pregnancy, childbirth complications, and organ transplantation.
sent34: Only slightly more than 4% of all articles fall into the ""Prevention"" theme.
sent35: This may be surprising, since prevention is of utmost importance while vaccines and treatments are still under development.
sent36: However, we believe that this theme is not covered by more studies due to the recent wide acceptance and effectiveness of social distancing and personal protective equipment.
sent37: Consequently, we expect the percentage of articles grouped in this theme to further reduce in the future.
sent38: Further insights into the research landscape and the shift in themes over time is summarized in Multimedia Appendix 2.
sent39: We noticed that biomedical informatics had a crucial role in several topics.
sent40: For instance, clinical decision support systems were used in many studies to diagnose COVID-19 based on chest imaging.
sent41: Telemedicine was also used in multiple studies to provide the required health care support for the patients during the COVID-19 pandemic.
sent42: Further, mobile applications, including contact tracing apps, were one of the main social distancing measures described in these studies.
sent43: AI-based models were used in multiple studies to predict protein structures of 2019-nCoV to understand the underlying mechanism of drug-target interaction.
sent44: Many studies proposed novel AI-based models to discover COVID-19 drugs and vaccines and repurpose existing drugs approved by the Food and Drug Administration as a part of the treatment plan for COVID-19."
231603122,Persuasive Natural Language Generation -A Literature Review,"Computer Science, Business, Linguistics",https://www.semanticscholar.org/paper/e9ac35f68ef4290d74551bafdfb09a4ea0983940,s2,Method,"This paper's methodology follows a framework proposed by vom Brocke et al. (2009) which is based on a screening of the review literature itself and especially highlights the need for comprehensively documenting the process of literature search in such an article (Duerr et al. 2016). The framework is structured into the following five steps: (1) definition of review scope, (2) conceptualization of topic, (3) literature search, (4) literature analysis and synthesis, and (5) research agenda. Each of the steps will be briefly explained, when it will be addressed in the course of this work.

The first step is the definition of the review scope of this literature review. It is summarized in Figure 1 (categories applicable to this review on Persuasion and NLG research are highlighted) which is based on the taxonomy adapted by vom Brocke et al. (2009).  Brocke et al. 2009) This literature review focuses on outcomes of applied research in the domains of Persuasion and Natural Language Generation. The goal is to integrate findings with respect to five categories concluded from the business problem of persuading individuals through textual exchange (DeMers 2016). These categories were chosen as they address the psychological and technical aspects of persuasion and are a prerequisite to creating a persuasive NLG artificial intelligence. We selected this field because the artificial generation of persuasion through NLG has, as this literature review reveals, not been addressed in seminal articles following our structured research approach. Persuasion is already commonly studied in psychology (Quirk et al. 1985, Marwell & Schmitt 1967. Also, numerous studies in natural language processing focus on identifying and classifying persuasion in an automated way (Li et al. 2020, Rocklage et al. 2018, Iyer & Sycara 2019). This paper is organized along a conceptual structure. We did not take a particular perspective to provide a neutral representation of the results. As an audience of this review specialized scholars having an interest in the field of persuasion and the artificial generation of it were chosen. For coverage, our literature review can be categorized as representative as our research has been limited to certain journals, but does not consider the totality of the literature. The second step is conceptualization of the topic .

It addresses the point that an author of a review article must begin with a topic in need of review, a broad conception of what is known about the topic, and potential areas where new knowledge may be needed. In the following, we conceptualize persuasion, and embed it into a business context. Furthermore, we introduce related theories, and conclude a categorization for the successive literature review.

In persuasion, the persuader induces a particular kind of mental state in the persuadee , e.g., through threats, but unlike an expression of sentiment, persuasion intends a change in the mental state of the persuadee (Iyer & Sycara 2019). Contemporary psychology and communication science (Rocklage et al. 2018, Park et al. 2015, Hunter et al. 2019 require the persuader to be acting intentionally, that is, the persuasive act. In the context of NLG, we usually refer to messages generated or augmented by an artificial intelligence, if we use the term persuasive act.

In his seminal work ' On Rhetoric ', Aristotle introduced his well-known ontology for persuasive acts. Accordingly, persuasion depends on multiple facets: emotions ( pathos ), logical structure of the argument ( logos ), the context ( cairos ), and on the speaker ( ethos ) (Schiappa & Nordin 2013).

Likewise, contemporary business literature conceptualizes persuasive acts through ""principles of persuasion"" (DeMers 2016). The author concludes that six interventions help at achieving persuasiveness. The first is being confident and remaining confident during the entirety of an appeal. Next, the introduction of logical argumentation fosters persuasiveness, since individuals are more inclined to be persuaded by logic. Additionally, making an appeal seem beneficial to the other party, by demonstrating the value of an appeal, choosing words carefully (i.e., selecting from a vocabulary that may be more persuasive), and using flattery (i.e., finding appropriate compliments) are recommended. Lastly, DeMers (2016) reveals that being patient and persistent (i.e., not to greatly alter one's approach) strengthens a persuader's persuasiveness. Next, we embed the presented ""principles of persuasion"" into related theories on persuasion (Cameron 2009).

Festinger's Theory of Cognitive Dissonance (1957) focuses on the relationships among cognitive elements, which include beliefs, opinions, attitudes, or knowledge (O'Keefe 1990). This relates most to Aristotle's cairos , and the need to create benevolence for the persuadee. Evaluating the 'business principles', this theory resonates best with what DeMers (2016) defines as 'making [the cognition] appealing to the other party'. However cognitions, and thus, a persuadee's perceived benevolence, can be dissonant, consonant, or irrelevant to each other. If a persuadee is presented with a sufficiently vast cognitive inconsistency, then they will perceive psychological discomfort, leading to an attempt to restore their cognitive balance through a reduction or elimination of the inconsistency (Stiff 1994, Harmon-Jones 2002. The magnitude of dissonance determines one's motivation to reduce it (Stiff 1994, Festinger 1957. Approaches towards reducing dissonance are: changing terms to make them more consonant, adding additional consonant percipience, or altering the magnitude of the percipience (Harmon-Jones 2002, Stiff 1994, O'Keefe 1990.

In 'principles of persuasion', DeMers (2016) contends that appropriate flattering and the usage of so-called high value words contribute to persuasive acts in business contexts (cf. Aristotle's ethos ). Accordingly, Language Expectancy Theory (LET) identifies written or spoken language as a rule-based system through which persuadees develop expectations and preferences towards ''appropriate'' linguistic usage of words in varying situations (Burgoon & Miller 1985). Such expectations are frequently consistent with sociological and cultural norms, while preferences tend to relate to societal standards and cultural values (Burgoon & Miller 1985, Burgoon et al. 2002. Positive expectations that facilitate a persuasive act are, for instance, if a persuader stylizes a behavior that is perceived as more preferred than expected by the persuadee. In contrast, negative ones are inhibiting persuasion, e.g., when the persuader makes use of language that is considered to be socially unacceptable (Burgoon & Miller 1985, Burgoon et al. 2002.

Next, the 'principles of persuasion' confer that an argument based on logic is persuasive (DeMers 2016). What Aristotle terms logos is consistent with the theory of probabilistic models. Probabilistic models (McGuire 1981, Wyer 1970) are based on the rules of formal logic and probability, and predict beliefs regarding the conclusion of reasoning. These predictions are based on mathematical probability, and as such this theory is consistent with what Aristotle defines as logos . An exemplary belief syllogism (McGuire 1981) is composed of two premises that lead to a logical conclusion. The theorists (McGuire 1981, Wyer 1970 explain that believing in the premises leads to the expectation that the identified conclusion will follow. However, rather than solely thinking in all-or-nothing scenarios, beliefs can be ascertained through subjective probabilities: one's judgment of the probability that each of the premises is true (McGuire 1981, Wyer 1970. Furthermore, if a message evokes a perceptual change in the truth of the premise, or additional premises are supplemented, following this theory, a change in perceiving the conclusion is expected.

Last, Balance Theory focuses on the triadic relationship involving two individuals (e.g., persuader and persuadee), the persuadee's attitude toward the persuader (Aristotle's pathos ), and their attitudes toward an attitudinal object (Heider 1958). The resulting triad can be balanced or unbalanced: This triad is in balance if all three relationships are positive, or one is positive and two are negative. If all three relationships are negative, or one is negative and two are positive, an unbalanced triad results, often motivating one to alter one of the three relationships (Heider 1958). Building on this theory, we aim to identify relational determinants that relate to improving the relationship between the persuader and the persuadee towards the attitudinal object (Heider 1958). In the business framework, we relate those determinants towards the pattern of 'trustworthiness', that represent the persuadee's attitude towards the persuader. In his persuasion attempt, the persuader wants the persuadee to have a positive attitude.","[['b24', 'b75'], ['b64', 'b43', 'b55', 'b51', 'b66', 'b75'], [], ['b43', 'b66', 'b41', 'b62'], ['b68'], ['b12'], [None, 'b71', 'b34', 'b60'], [None, 'b11'], ['b83', 'b56'], ['b35']]","[['b24', 'b75'], ['b64', 'b43', 'b55', 'b51', 'b66', 'b75'], [], ['b43', 'b66', 'b41', 'b62'], ['b68'], ['b12'], [None, 'b71', 'b34', 'b60'], [None, 'b11'], ['b83', 'b56'], ['b35']]",23,"sent1: This paper's methodology follows a framework proposed by vom Brocke et al. (2009) which is based on a screening of the review literature itself and especially highlights the need for comprehensively documenting the process of literature search in such an article (Duerr et al. 2016).
sent2: The framework is structured into the following five steps: (1) definition of review scope, (2) conceptualization of topic, (3) literature search, (4) literature analysis and synthesis, and (5) research agenda.
sent3: Each of the steps will be briefly explained, when it will be addressed in the course of this work.
sent4: The first step is the definition of the review scope of this literature review.
sent5: It is summarized in Figure 1 (categories applicable to this review on Persuasion and NLG research are highlighted) which is based on the taxonomy adapted by vom Brocke et al. (2009).
sent6: Brocke et al. 2009) This literature review focuses on outcomes of applied research in the domains of Persuasion and Natural Language Generation.
sent7: The goal is to integrate findings with respect to five categories concluded from the business problem of persuading individuals through textual exchange (DeMers 2016).
sent8: These categories were chosen as they address the psychological and technical aspects of persuasion and are a prerequisite to creating a persuasive NLG artificial intelligence.
sent9: We selected this field because the artificial generation of persuasion through NLG has, as this literature review reveals, not been addressed in seminal articles following our structured research approach.
sent10: Persuasion is already commonly studied in psychology (Quirk et al. 1985, Marwell & Schmitt 1967. Also, numerous studies in natural language processing focus on identifying and classifying persuasion in an automated way (Li et al. 2020, Rocklage et al. 2018, Iyer & Sycara 2019).
sent11: This paper is organized along a conceptual structure.
sent12: We did not take a particular perspective to provide a neutral representation of the results.
sent13: As an audience of this review specialized scholars having an interest in the field of persuasion and the artificial generation of it were chosen.
sent14: For coverage, our literature review can be categorized as representative as our research has been limited to certain journals, but does not consider the totality of the literature.
sent15: The second step is conceptualization of the topic .
sent16: It addresses the point that an author of a review article must begin with a topic in need of review, a broad conception of what is known about the topic, and potential areas where new knowledge may be needed.
sent17: In the following, we conceptualize persuasion, and embed it into a business context.
sent18: Furthermore, we introduce related theories, and conclude a categorization for the successive literature review.
sent19: In persuasion, the persuader induces a particular kind of mental state in the persuadee , e.g., through threats, but unlike an expression of sentiment, persuasion intends a change in the mental state of the persuadee (Iyer & Sycara 2019).
sent20: Contemporary psychology and communication science (Rocklage et al. 2018, Park et al. 2015, Hunter et al. 2019 require the persuader to be acting intentionally, that is, the persuasive act.
sent21: In the context of NLG, we usually refer to messages generated or augmented by an artificial intelligence, if we use the term persuasive act.
sent22: In his seminal work ' On Rhetoric ', Aristotle introduced his well-known ontology for persuasive acts.
sent23: Accordingly, persuasion depends on multiple facets: emotions ( pathos ), logical structure of the argument ( logos ), the context ( cairos ), and on the speaker ( ethos ) (Schiappa & Nordin 2013).
sent24: Likewise, contemporary business literature conceptualizes persuasive acts through ""principles of persuasion"" (DeMers 2016).
sent25: The author concludes that six interventions help at achieving persuasiveness.
sent26: The first is being confident and remaining confident during the entirety of an appeal.
sent27: Next, the introduction of logical argumentation fosters persuasiveness, since individuals are more inclined to be persuaded by logic.
sent28: Additionally, making an appeal seem beneficial to the other party, by demonstrating the value of an appeal, choosing words carefully (i.e., selecting from a vocabulary that may be more persuasive), and using flattery (i.e., finding appropriate compliments) are recommended.
sent29: Lastly, DeMers (2016) reveals that being patient and persistent (i.e., not to greatly alter one's approach) strengthens a persuader's persuasiveness.
sent30: Next, we embed the presented ""principles of persuasion"" into related theories on persuasion (Cameron 2009).
sent31: Festinger's Theory of Cognitive Dissonance (1957) focuses on the relationships among cognitive elements, which include beliefs, opinions, attitudes, or knowledge (O'Keefe 1990).
sent32: This relates most to Aristotle's cairos , and the need to create benevolence for the persuadee.
sent33: Evaluating the 'business principles', this theory resonates best with what DeMers (2016) defines as 'making [the cognition] appealing to the other party'.
sent34: However cognitions, and thus, a persuadee's perceived benevolence, can be dissonant, consonant, or irrelevant to each other.
sent35: If a persuadee is presented with a sufficiently vast cognitive inconsistency, then they will perceive psychological discomfort, leading to an attempt to restore their cognitive balance through a reduction or elimination of the inconsistency (Stiff 1994, Harmon-Jones 2002.
sent36: The magnitude of dissonance determines one's motivation to reduce it (Stiff 1994, Festinger 1957.
sent37: Approaches towards reducing dissonance are: changing terms to make them more consonant, adding additional consonant percipience, or altering the magnitude of the percipience (Harmon-Jones 2002, Stiff 1994, O'Keefe 1990.
sent38: In 'principles of persuasion', DeMers (2016) contends that appropriate flattering and the usage of so-called high value words contribute to persuasive acts in business contexts (cf. Aristotle's ethos ).
sent39: Accordingly, Language Expectancy Theory (LET) identifies written or spoken language as a rule-based system through which persuadees develop expectations and preferences towards ''appropriate'' linguistic usage of words in varying situations (Burgoon & Miller 1985).
sent40: Such expectations are frequently consistent with sociological and cultural norms, while preferences tend to relate to societal standards and cultural values (Burgoon & Miller 1985, Burgoon et al. 2002.
sent41: Positive expectations that facilitate a persuasive act are, for instance, if a persuader stylizes a behavior that is perceived as more preferred than expected by the persuadee.
sent42: In contrast, negative ones are inhibiting persuasion, e.g., when the persuader makes use of language that is considered to be socially unacceptable (Burgoon & Miller 1985, Burgoon et al. 2002.
sent43: Next, the 'principles of persuasion' confer that an argument based on logic is persuasive (DeMers 2016).
sent44: What Aristotle terms logos is consistent with the theory of probabilistic models.
sent45: Probabilistic models (McGuire 1981, Wyer 1970) are based on the rules of formal logic and probability, and predict beliefs regarding the conclusion of reasoning.
sent46: These predictions are based on mathematical probability, and as such this theory is consistent with what Aristotle defines as logos .
sent47: An exemplary belief syllogism (McGuire 1981) is composed of two premises that lead to a logical conclusion.
sent48: The theorists (McGuire 1981, Wyer 1970 explain that believing in the premises leads to the expectation that the identified conclusion will follow.
sent49: However, rather than solely thinking in all-or-nothing scenarios, beliefs can be ascertained through subjective probabilities: one's judgment of the probability that each of the premises is true (McGuire 1981, Wyer 1970.
sent50: Furthermore, if a message evokes a perceptual change in the truth of the premise, or additional premises are supplemented, following this theory, a change in perceiving the conclusion is expected.
sent51: Last, Balance Theory focuses on the triadic relationship involving two individuals (e.g., persuader and persuadee), the persuadee's attitude toward the persuader (Aristotle's pathos ), and their attitudes toward an attitudinal object (Heider 1958).
sent52: The resulting triad can be balanced or unbalanced: This triad is in balance if all three relationships are positive, or one is positive and two are negative.
sent53: If all three relationships are negative, or one is negative and two are positive, an unbalanced triad results, often motivating one to alter one of the three relationships (Heider 1958).
sent54: Building on this theory, we aim to identify relational determinants that relate to improving the relationship between the persuader and the persuadee towards the attitudinal object (Heider 1958).
sent55: In the business framework, we relate those determinants towards the pattern of 'trustworthiness', that represent the persuadee's attitude towards the persuader.
sent56: In his persuasion attempt, the persuader wants the persuadee to have a positive attitude."
231603122,Persuasive Natural Language Generation -A Literature Review,"Computer Science, Business, Linguistics",https://www.semanticscholar.org/paper/e9ac35f68ef4290d74551bafdfb09a4ea0983940,s5,Benevolence,"Determinants that aim at creating value for the persuadee are subsumed in this category (DeMers 2016, Voss & Raz 2016). In line with Cognitive Dissonance Theory (Festinger 1957) the identified eight determinants relate to altering a persuadee's perceived benevolence through dissonant or consonant measures (summarized in Table 3). An implementation in a persuasive NLG AI can be facilitated through identifying their absence or impact (Hunter et al 2019, Zarouali et al. 2020. The benevolence determinants are ordered alphabetically to not imply a specific order. The first column presents the determinants, the second column concisely defines each, and the third provides examples for all determinants that were identified. The last column states the corresponding citations. Linguistic Appropriacy

This category subsumes fourteen determinants that facilitate an individual's stylome and aim at matching this with linguistic appropriacy. Such a stylome can be quantified and identified linguistically (Zarouali et al. 2020). Aforementioned Language Expectation Theory identifies written or spoken language as a rule-based system through which persuadees develop expectations (Burgoon & Miller 1985). The reason for profiling the stylome of an individual is to match these expectations (Park et al. 2015). Once implemented, a persuasive NLG AI can achieve congruence between a persuasive message and the persuadee, and thus generate persuasiveness. Table 2 introduces fourteen determinants of linguistic appropriacy in alphabetical order, provides a synopsis (i.e., brief summary, column two), an example for each determinant (column three), and the corresponding academic citation (in column four).  Quirk et al. 1985, Brewer 1980 Evidence Words Tendency to approve or disapprove something. Words such as: according to. Quirk et al. 1985 Familiarity Degree of familiarity of a word or how easily a word can be recognized by an adult. Word Frequency Indication of how often used words occur in a given language.

More uncommon words reflect that the writer possesses larger vocabulary. Baayen et al. 1995 Logical Argumentation

Previous academic works unveil that arguments with consistent logic in persuasive acts increase persuasiveness (Cialdini & Goldstein 2002, Walton et al. 2008, Block et al. 2019. In line with the theory of Probabilistic Models (McGuire 1981, Wyer 1970, it is assumed that conclusive statements lead to a persuadee's expectation that a conclusion will follow. Technical implementations of logical argumentation or logical meaning representations occur as first order logic or semantic argumentation graphs (Moens 2018, Block et al. 2019). The first column of Table 4 enumerates our fourteen logical argumentation determinants, while the second provides a synopsis. Column three provides an example, and column four the corresponding citation in which the factor was identified. As in previous tables, the determinants are merely sorted alphabetically. ","[['b76', 'b41', 'b86'], ['b64', 'b8', 'b62', None, 'b86'], ['b5'], ['b56', 'b7', 'b58', None, 'b83', 'b77']]","[['b76', 'b41', 'b86'], ['b64', 'b8', 'b62', None, 'b86'], ['b5'], ['b56', 'b7', 'b58', None, 'b83', 'b77']]",15,"sent1: Determinants that aim at creating value for the persuadee are subsumed in this category (DeMers 2016, Voss & Raz 2016).
sent2: In line with Cognitive Dissonance Theory (Festinger 1957)
sent3: the identified eight determinants relate to altering a persuadee's perceived benevolence through dissonant or consonant measures (summarized in Table 3).
sent4: An implementation in a persuasive NLG AI can be facilitated through identifying their absence or impact (Hunter et al 2019, Zarouali et al. 2020.
sent5: The benevolence determinants are ordered alphabetically to not imply a specific order.
sent6: The first column presents the determinants, the second column concisely defines each, and the third provides examples for all determinants that were identified.
sent7: The last column states the corresponding citations.
sent8: Linguistic AppropriacyThis category subsumes fourteen determinants that facilitate an individual's stylome and aim at matching this with linguistic appropriacy.
sent9: Such a stylome can be quantified and identified linguistically (Zarouali et al. 2020).
sent10: Aforementioned Language Expectation Theory identifies written or spoken language as a rule-based system through which persuadees develop expectations (Burgoon & Miller 1985).
sent11: The reason for profiling the stylome of an individual is to match these expectations (Park et al. 2015).
sent12: Once implemented, a persuasive NLG AI can achieve congruence between a persuasive message and the persuadee, and thus generate persuasiveness.
sent13: Table 2 introduces fourteen determinants of linguistic appropriacy in alphabetical order, provides a synopsis (i.e., brief summary, column two), an example for each determinant (column three), and the corresponding academic citation (in column four).
sent14: Quirk et al. 1985, Brewer 1980 Evidence Words Tendency to approve or disapprove something.
sent15: Words such as: according to. Quirk et al. 1985 Familiarity Degree of familiarity of a word or how easily a word can be recognized by an adult.
sent16: Word Frequency Indication of how often used words occur in a given language.
sent17: More uncommon words reflect that the writer possesses larger vocabulary.
sent18: Baayen et al. 1995 Logical ArgumentationPrevious academic works unveil that arguments with consistent logic in persuasive acts increase persuasiveness (Cialdini & Goldstein 2002, Walton et al. 2008, Block et al. 2019. In line with the theory of Probabilistic Models (McGuire 1981, Wyer 1970, it is assumed that conclusive statements lead to a persuadee's expectation that a conclusion will follow. Technical implementations of logical argumentation or logical meaning representations occur as first order logic or semantic argumentation graphs (Moens 2018, Block et al. 2019).
sent19: The first column of Table 4 enumerates our fourteen logical argumentation determinants, while the second provides a synopsis.
sent20: Column three provides an example, and column four the corresponding citation in which the factor was identified.
sent21: As in previous tables, the determinants are merely sorted alphabetically."
231603122,Persuasive Natural Language Generation -A Literature Review,"Computer Science, Business, Linguistics",https://www.semanticscholar.org/paper/e9ac35f68ef4290d74551bafdfb09a4ea0983940,s6,Trustworthiness,"Trust plays an important role in the persuader-persuadee relationship. If established, the persuadee's attitude toward the persuader -as identified in Balance Theory (Heider 1958) -helps a persuadee to reason about the reciprocative nature, honesty or reliability of the counterpart (Kim & Duhachek 2020). An implementation of trustworthiness can, amongst others, be realized through identifying a persuadee's psychological profile (e.g., extroverted individuals respond better to texts that have a positive valence, and are in that case more persuadable, Zarouali et al. 2020, Park et al. 2015 to influence the degree of persuasiveness of a persuasive act. This category collates fifteen determinants pertaining to the increase 8 Determinant","[['b62', 'b86', 'b35', 'b48']]","[['b62', 'b86', 'b35', 'b48']]",4,"sent1: Trust plays an important role in the persuader-persuadee relationship.
sent2: If established, the persuadee's attitude toward the persuader -as identified in Balance Theory (Heider 1958) -helps a persuadee to reason about the reciprocative nature, honesty or reliability of the counterpart (Kim & Duhachek 2020).
sent3: An implementation of trustworthiness can, amongst others, be realized through identifying a persuadee's psychological profile (e.g., extroverted individuals respond better to texts that have a positive valence, and are in that case more persuadable, Zarouali et al. 2020, Park et al. 2015 to influence the degree of persuasiveness of a persuasive act.
sent4: This category collates fifteen determinants pertaining to the increase 8 Determinant"
231603122,Persuasive Natural Language Generation -A Literature Review,"Computer Science, Business, Linguistics",https://www.semanticscholar.org/paper/e9ac35f68ef4290d74551bafdfb09a4ea0983940,s10,Tools & Datasets,"In the analyzed academic studies, we found that the authors use different datasets and tools to computationally process data for technical analyses of persuasion in NLP or NLG (e.g., in Guerini et al 2008a/b, Li et al. 2020, Iyer/Sycara 2019. Logically, the implementation of a persuasive NLG AI also depends on a variety of relevant tools and datasets which we identified and consolidated in Table 6. This table classifies our findings in types which are either tool or datasets (column one). We identified six tools and seventeen persuasion or message datasets. A software tool that is used in the context of persuasion and NLP, and datasets were chosen if they were used in the context of persuasion, textual exchange/debate and NLP. We further added a synopsis (column three) explaining every tool and Authority Appealing or making reference to higher authority or experts to persuade.

We called your mom Mariam and she says please put the gun down and come outside. Cialdini & Goldstein 2002, Catellani et al. 2020 Seeking Comprehension

Instead of prioritizing own arguments, it is wise to focus on understanding the persuadee.

What do you mean by that ? Fisher Uri 1981, Kouzehgar et al. 2015 Construal Learning involves the generalization and abstraction from one's repeated experiences which is a high-construal mental process.

A short-term investor as opposed to long-term investor may rely more on a financial artificial intelligence. Kim & Duhachek 2020, Abdallah et al. 2009 Emotionality The elicitation of positive or negative emotions to impose more weight on words.

Inclusion of words or expressions such as ""amazing"" or ""excellent"". Rocklage et al. 2018 Empathy This is a very good positive book that will make you very happy. Guerini et al. 2008b, Zarouali et al. 2020 dataset, providing a link (column four, if applicable) and the respective citation of the tool or dataset (column five). The tools and datasets are sorted alphabetically. ","[['b43', 'b30', None], [None, 'b14'], [], [None, 'b50'], ['b0', 'b48'], ['b66', 'b31', 'b86']]","[['b43', 'b30', None], [None, 'b14'], [], [None, 'b50'], ['b0', 'b48'], ['b66', 'b31', 'b86']]",12,"sent1: In the analyzed academic studies, we found that the authors use different datasets and tools to computationally process data for technical analyses of persuasion in NLP or NLG (e.g., in Guerini et al 2008a/b, Li et al. 2020, Iyer/Sycara 2019. Logically, the implementation of a persuasive NLG AI also depends on a variety of relevant tools and datasets which we identified and consolidated in Table 6. This table classifies our findings in types which are either tool or datasets (column one).
sent2: We identified six tools and seventeen persuasion or message datasets.
sent3: A software tool that is used in the context of persuasion and NLP, and datasets were chosen if they were used in the context of persuasion, textual exchange/debate and NLP.
sent4: We further added a synopsis (column three) explaining every tool and Authority Appealing or making reference to higher authority or experts to persuade.
sent5: We called your mom Mariam and she says please put the gun down and come outside.
sent6: Cialdini & Goldstein 2002, Catellani et al. 2020
sent7: Seeking ComprehensionInstead of prioritizing own arguments, it is wise to focus on understanding the persuadee.
sent8: What do you mean by that ? Fisher Uri 1981, Kouzehgar et al. 2015 Construal Learning involves the generalization and abstraction from one's repeated experiences which is a high-construal mental process.
sent9: A short-term investor as opposed to long-term investor may rely more on a financial artificial intelligence.
sent10: Kim & Duhachek 2020, Abdallah et al. 2009
sent11: Emotionality The elicitation of positive or negative emotions to impose more weight on words.
sent12: Inclusion of words or expressions such as ""amazing"" or ""excellent"".
sent13: Rocklage et al. 2018 Empathy This is a very good positive book that will make you very happy.
sent14: Guerini et al. 2008b, Zarouali et al. 2020 dataset, providing a link (column four, if applicable) and the respective citation of the tool or dataset (column five).
sent15: The tools and datasets are sorted alphabetically."
231925325,Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition,"Computer Science, Engineering",https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23,s1,Attention,"Offline ASR Streaming ASR RNN-based [10], [11], [24], [30], [25], [23], [31], [32], [33], [34], [35], [36], [37] [38], [27], [39], [28], [40], [29], [41], [42], [43], [44], [45] Transformerbased [26], [46], [47], [48], [49], [50], [51], [52], [53], [53], [54], [55], [56], [57], [58], [59] [60], [61], [62], [63], [57], [64], [52], [65], [66], [67], [68], [69], [70] An encoder processes X to a high level representation (hidden states) and passes it to the decoder where prediction of Y happens. In most cases, the information required to predict a particular frame y t is confined within a small number of input frames. Therefore, for decoding y t , it is not required to look at each input frames. The Attention model aligns the input frames with y t by assigning match scores to each pair of input frame and y t . The match scores convey how much a particular input frame is relevant to y t and accordingly, the decoder decides the degree of focus on each input frame for predicting y t .

Depending on how the alignments between output and input frames are designed, different types of attention mechanism are presented in the literature. A list of existing attention models along with short descriptions is provided in Table  I. The detailed explanation of different attention models is discussed throughout the paper. In this survey, we have considered the models which are built within RNN or Transformer architecture. Table II provides the list of literature which we have reviewed in the later sections of this paper.","[['b67', 'b41', 'b35', 'b63', 'b61', 'b53', 'b48', 'b69', 'b57', 'b22', 'b45', 'b58', 'b30', 'b32', 'b64', 'b29', 'b43', 'b50', 'b59', 'b28', 'b25', 'b51', 'b46', 'b42', 'b54', 'b9', 'b65', 'b39', 'b72', 'b33', 'b27', 'b52', 'b10', 'b49', 'b56', 'b55', 'b62', 'b47', 'b70', 'b71', 'b26', 'b23', 'b31', 'b34', 'b36', 'b44', 'b40', 'b24'], []]","[['b67', 'b41', 'b35', 'b63', 'b61', 'b53', 'b48', 'b69', 'b57', 'b22', 'b45', 'b58', 'b30', 'b32', 'b64', 'b29', 'b43', 'b50', 'b59', 'b28', 'b25', 'b51', 'b46', 'b42', 'b54', 'b9', 'b65', 'b39', 'b72', 'b33', 'b27', 'b52', 'b10', 'b49', 'b56', 'b55', 'b62', 'b47', 'b70', 'b71', 'b26', 'b23', 'b31', 'b34', 'b36', 'b44', 'b40', 'b24'], []]",48,"sent1: Offline ASR Streaming ASR RNN-based [10], [11], [24], [30], [25], [23], [31], [32], [33], [34], [35], [36], [37] [38], [27], [39], [28], [40], [29], [41], [42], [43], [44], [45] Transformerbased [26], [46], [47], [48], [49], [50], [51], [52], [53], [53], [54], [55], [56], [57], [58], [59] [60], [61], [62], [63], [57], [64], [52], [65], [66], [67], [68], [69], [70] An encoder processes X to a high level representation (hidden states) and passes it to the decoder where prediction of Y happens.
sent2: In most cases, the information required to predict a particular frame y t is confined within a small number of input frames.
sent3: Therefore, for decoding y t , it is not required to look at each input frames.
sent4: The Attention model aligns the input frames with y t by assigning match scores to each pair of input frame and y t .
sent5: The match scores convey how much a particular input frame is relevant to y t and accordingly, the decoder decides the degree of focus on each input frame for predicting y t .
sent6: Depending on how the alignments between output and input frames are designed, different types of attention mechanism are presented in the literature.
sent7: A list of existing attention models along with short descriptions is provided in Table  I.
sent8: The detailed explanation of different attention models is discussed throughout the paper.
sent9: In this survey, we have considered the models which are built within RNN or Transformer architecture.
sent10: Table II provides the list of literature which we have reviewed in the later sections of this paper."
231925325,Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition,"Computer Science, Engineering",https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23,s7,C. Joint attention-CTC with RNN,"Two main approaches for end-to-end encoder-decoder ASR are attention-based and CTC [75]-based. In attention-based approach, the decoder network finds an alignment of the encoder hidden states during the prediction of each element of output sequence. The task of speech recognition is mostly monotonic. Therefore, the possibility of right to left dependency is significantly lesser compared to left to right dependency in ASR tasks. However, due to the flexible nature of attention mechanism, non-sequential alignments are also considered. Therefore, noise and irrelevant frames (encoder hidden states) may result in misalignment. This issue becomes worse for longer sequences as the length of input and output sequences vary due to factors, e.g. the rate of speech, accent, and pronunciation. Therefore, the risk of misalignment in longer sequences is higher. In contrast, CTC allows strict monotonic alignment of speech frames using forward-backward algorithm [9], [76] but assumes targets are conditionally independent on each other. Therefore, temporal dependencies are not properly utilised in CTC, unlike in attention mechanism. For effective ASR performance, many researchers have combined the advantages of both attention and CTC in a single model and therefore, the CTC probabilities replaces the incorrect predictions by the attention mechanism.

The discussion on CTC and its application on ASR is beyond the scope of this paper. However, in this section a brief introduction to CTC and how it is jointly used with attention is provided [33], [34]. CTC monotonically maps an input sequence to output sequence. Considering the model outputs Llength letter sequence Y {y l ∈ U |l = 1, · · · , L} with a set of distinct characters U , given the input sequence is X. CTC introduces frame-wise letter sequence with an additional ""blank"" symbol Z = {z t ∈ U ∪ blank|t = 1, · · · , T }. By using conditional independence assumptions, the posterior distribution p(Y |X) is factorized as follows:

CTC has three distribution components by the Bayes theorem similar to the traditional or hybrid ASR. They are frame-wise posterior distribution p(z t |X) -acoustic module, transition probability p(z t |z t−1 , C) -pronunciation module, and letter-based language module p(Y ).

Compared with CTC approaches, the attention-based approach does not make any conditional independence assumptions, and directly estimates the posterior p(Y |X) based on the chain rule:

p ctc (Y |X) and p att (Y |X) are the CTC-based and attentionbased objective functions, respectively. Finally, the logarithmic linear combination of CTC-and attention-based objective functions given by (17) is maximised to leverage the CTC and attention mechanism together in a ASR model.

λ is a tunable parameter in the range [0, 1].

In [33], [34], the CTC objective function was incorporated in the attention-based model during the training only. However, motivated by the effectiveness of this joint approach, in [35], [36], it is used for decoding or inferencing phase as well.

A triggered attention mechanism is proposed in [37]. At each decoder time step, the encoder states which the attention model looks upon are controlled by a trigger model. The encoder states are shared with the trigger model which is a CTC-based network as well as with the attention model. The trigger sequence which is computed based on the CTC generated sequence provides alignment information that controls the attention mechanism. Finally, the objective functions of CTC and attention model are optimised jointly.","[['b8', 'b77', 'b78'], ['b33', 'b32'], [], [], [], [], ['b34', 'b35', 'b33', 'b32'], ['b36']]","[['b8', 'b77', 'b78'], ['b33', 'b32'], [], [], [], [], ['b34', 'b35', 'b33', 'b32'], ['b36']]",10,"sent1: Two main approaches for end-to-end encoder-decoder ASR are attention-based and CTC [75]-based.
sent2: In attention-based approach, the decoder network finds an alignment of the encoder hidden states during the prediction of each element of output sequence.
sent3: The task of speech recognition is mostly monotonic.
sent4: Therefore, the possibility of right to left dependency is significantly lesser compared to left to right dependency in ASR tasks.
sent5: However, due to the flexible nature of attention mechanism, non-sequential alignments are also considered.
sent6: Therefore, noise and irrelevant frames (encoder hidden states) may result in misalignment.
sent7: This issue becomes worse for longer sequences as the length of input and output sequences vary due to factors, e.g.
sent8: the rate of speech, accent, and pronunciation.
sent9: Therefore, the risk of misalignment in longer sequences is higher.
sent10: In contrast, CTC allows strict monotonic alignment of speech frames using forward-backward algorithm [9], [76] but assumes targets are conditionally independent on each other.
sent11: Therefore, temporal dependencies are not properly utilised in CTC, unlike in attention mechanism.
sent12: For effective ASR performance, many researchers have combined the advantages of both attention and CTC in a single model and therefore, the CTC probabilities replaces the incorrect predictions by the attention mechanism.
sent13: The discussion on CTC and its application on ASR is beyond the scope of this paper.
sent14: However, in this section a brief introduction to CTC and how it is jointly used with attention is provided [33], [34].
sent15: CTC monotonically maps an input sequence to output sequence.
sent16: Considering the model outputs Llength letter sequence Y {y l ∈ U |l = 1, · · · , L} with a set of distinct characters U , given the input sequence is X. CTC introduces frame-wise letter sequence with an additional ""blank"" symbol Z = {z t ∈ U ∪ blank|t = 1, · · · , T }.
sent17: By using conditional independence assumptions, the posterior distribution p(Y |X) is factorized as follows:CTC has three distribution components by the Bayes theorem similar to the traditional or hybrid ASR.
sent18: They are frame-wise posterior distribution p(z t |X) -acoustic module, transition probability p(z t |z t−1 , C) -pronunciation module, and letter-based language module p(Y ).
sent19: Compared with CTC approaches, the attention-based approach does not make any conditional independence assumptions, and directly estimates the posterior p(Y |X) based on the chain rule:p ctc (Y |X) and p att (Y |X) are the CTC-based and attentionbased objective functions, respectively.
sent20: Finally, the logarithmic linear combination of CTC-and attention-based objective functions given by (17) is maximised to leverage the CTC and attention mechanism together in a ASR model.
sent21: λ is a tunable parameter in the range [0, 1].
sent22: In [33], [34], the CTC objective function was incorporated in the attention-based model during the training only.
sent23: However, motivated by the effectiveness of this joint approach, in [35], [36], it is used for decoding or inferencing phase as well.
sent24: A triggered attention mechanism is proposed in [37].
sent25: At each decoder time step, the encoder states which the attention model looks upon are controlled by a trigger model.
sent26: The encoder states are shared with the trigger model which is a CTC-based network as well as with the attention model.
sent27: The trigger sequence which is computed based on the CTC generated sequence provides alignment information that controls the attention mechanism.
sent28: Finally, the objective functions of CTC and attention model are optimised jointly."
231925325,Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition,"Computer Science, Engineering",https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23,s10,A. RNN-based models,"In this section, we will discuss the literature where attention mechanism is applied for streaming speech recognition with RNN-based encoder decoder models. To work with streaming speech, it is first required to obtain the speech frame or the set of speech frames on which attention mechanism will work. A Gaussian prediction-based attention mechanism is proposed in [38] for streaming speech recognition. Instead of looking at the entire encoder hidden states, at each decoder time step, only a set of encoder hidden states are attended based on a Gaussian window. The centre and the size of window at a particular decoder time step, t are determined by its mean (µ t ) and variance (σ t ) which are predicted given the previous decoder state. Specifically, the current window centre is determined by a predicted moving forward increment ( µ t ) and last window centre. µ t = µ t + µ t−1 . A different approach compared to (5) has been considered to calculate the similarity between j th encoder state (within the current window) and i th encoder state and it is given by (20):

A hard monotonic attention mechanism is proposed in [27]. Only a single encoder hidden state h i (i represents a decoder time step and h i represents the only encoder state selected for output prediction at i th decoder time step) which scores the highest similarity with the current decoder state is selected by passing the concerned attention probabilities through a categorical function. A stochastic process is used to enable attending encoder hidden states only from left to right direction. At each decoder time step, the attention mechanism starts processing from h i−1 to the proceeding states. h i−1 is the encoder state which was attended at last decoder time step. Each calculated similarity score (e i,j ) is then sequentially passed through a logistic sigmoid function to produce selection probabilities (p i,j ) followed by a Bernoulli distribution and once it outputs 1, the attention process stops. The last attended encoder hidden state, h i at the current decoder time step is then set as the context for the current decoder time step, i.e. c i = h i . Although the encoder states within the window of boundary [h i−1 , h i ] are processed, only a single encoder state is finally selected for the current prediction.

[27] provides linear time complexity and online speech decoding, it only attends a single encoer state for each output prediction and it may cause degradation to the performance. Therefore, monotonic chunkwise attention (MoChA) is proposed in [28] where decoder attends small ""chunks"" of encoder states within a window containing a fixed number of encoder states prior to and including h i . Due to its effectiveness, MoChA is also used to develop an on-device commercialised ASR system [40]. To increase the effectiveness of the matching scores obtained to calculate the attention probabilities between the decoder state and the chunk encoder states, multi-head monotonic chunkwise attention (MTH-MoChA) is proposed in [39]. MTH-MoChA splits the encoder and decoder hidden states into K heads. K is experimentally set as 4. For each head, matching scores, attention probabilities and the context vectors are calculated to extract the dependencies between the encoder and decoder hidden states. Finally, the average context vector over all the heads takes part in decoding.

The pronunciation rate among different speakers may vary and therefore, the attention calculated over the fixed chunk size may not be effective. To overcome this, in [29] an adaptive monotonic chunkwise attention (AMoChA) was proposed where attention at current decoder time step is computed over a window whose boundary [h i−1 , h i ] is computed as in [27]. Within the window, whichever encoder states results in p i,j > 0.5 or e i,j > 0 are attended. Hence, the chunk size is adaptive instead of constant.

The input sequence or the encoder states of length L is divided equally into W in [41]. So, each block contains B = L W encoder states, while the last block may contain fewer than B encoder states. In this model, each block is responsible for a set of output predictions and attention is computed over only the concerned blocks and not the entire encoder states. Once the model has finished attending all the encoder states of a block and predicting the required outputs, it emits a special symbol called < epsilon > which marks the end of the corresponding block processing and the model proceeds to attend the next block. The effectiveness of this model has been enhanced in [42] by extending the attention span. Specifically, the attention mechanism looks at not only the current block but the k previous blocks. Experimentally, k is set as 20.

The authors of [44] have identified the latency issue in streaming attention-based models. In most streaming models, the encoder states are attended based on a local window. Computing the precise boundaries of these local windows is a computational expensive process which in turn causes a delay in the speech-to-text conversion. To overcome this issue, in [44] external hard alignments obtained from a hybrid ASR system is used for frame-wise supervision to force the MoChA model to learn accurate boundaries and alignments. In [80] performance latency is reduced by proposing a unidirectional encoder with no future dependency. Since each position does not depend on future context, the decoder hidden states are not required to be re-computed every time a new input chunk arrives and therefore, the overall delay is reduced.

In [43], attention mechanism has been incorporated in RNN-Transducer (RNN-T) [12], [13] to make streaming speech recognition more effective and efficient. RNN-T consists of three sections: (i) a RNN encoder which processes an input sequence to encoder hidden states, (ii) a RNN decoder which is analogues to a language model takes the previous predicted symbol as input and outputs decoder hidden states, and (iii) a joint network that takes encoder and decoder hidden states at the current time step to compute output logit which is responsible to predict the output symbol when passed through a softmax layer. In [43], at the encoder side, to learn contextual dependency, a multi-head self-attention layer is added on the top of RNN layers. In addition, the joint network attends a chunk of encoder hidden states instead of attending only the current hidden state at each time step.

LAS model is primarily proposed for offline speech recognition. However, it has been modified with silence modelling for working in the streaming environment in [45]. Given streamable encoder and a suitable attention mechanism (hard monotonic, chunkwise or local window-based instead of global), the main limitation of LAS model to perform in streaming environment is a long enough silence between the utterances to make decoder believe it is the end of speech. Therefore, the LAS decoder terminates the transcription process while the speaker is still active (i.e. early stopping). This limitation is addressed in [45] by incorporating reference silence tokens during the training phase to supervise the model when to output a silence token instead of terminating the process during the inference phase.","[['b4', 'b38', 'b19'], ['b26'], ['b39', 'b27', 'b40'], ['b26', 'b28'], ['b41', 'b42'], ['b82', 'b44'], ['b43', 'b11', 'b12'], ['b45']]","[['b4', 'b38', 'b19'], ['b26'], ['b39', 'b27', 'b40'], ['b26', 'b28'], ['b41', 'b42'], ['b82', 'b44'], ['b43', 'b11', 'b12'], ['b45']]",17,"sent1: In this section, we will discuss the literature where attention mechanism is applied for streaming speech recognition with RNN-based encoder decoder models.
sent2: To work with streaming speech, it is first required to obtain the speech frame or the set of speech frames on which attention mechanism will work.
sent3: A Gaussian prediction-based attention mechanism is proposed in [38] for streaming speech recognition.
sent4: Instead of looking at the entire encoder hidden states, at each decoder time step, only a set of encoder hidden states are attended based on a Gaussian window.
sent5: The centre and the size of window at a particular decoder time step, t are determined by its mean (µ t ) and variance (σ t ) which are predicted given the previous decoder state.
sent6: Specifically, the current window centre is determined by a predicted moving forward increment ( µ t ) and last window centre.
sent7: µ t = µ t + µ t−1 . A different approach compared to (5) has been considered to calculate the similarity between j th encoder state (within the current window)
sent8: and i th encoder state and it is given by (20):A hard monotonic attention mechanism is proposed in [27].
sent9: Only a single encoder hidden state h i (i represents a decoder time step and h i represents the only encoder state selected for output prediction at i th decoder time step) which scores the highest similarity with the current decoder state is selected by passing the concerned attention probabilities through a categorical function.
sent10: A stochastic process is used to enable attending encoder hidden states only from left to right direction.
sent11: At each decoder time step, the attention mechanism starts processing from h i−1 to the proceeding states.
sent12: h i−1 is the encoder state which was attended at last decoder time step.
sent13: Each calculated similarity score (e i,j ) is then sequentially passed through a logistic sigmoid function to produce selection probabilities (p i,j ) followed by a Bernoulli distribution and once it outputs 1, the attention process stops.
sent14: The last attended encoder hidden state, h i at the current decoder time step is then set as the context for the current decoder time step, i.e. c
sent15: i = h i . Although the encoder states within the window of boundary [h i−1 , h i ] are processed, only a single encoder state is finally selected for the current prediction.
sent16: [27] provides linear time complexity and online speech decoding, it only attends a single encoer state for each output prediction and it may cause degradation to the performance.
sent17: Therefore, monotonic chunkwise attention (MoChA) is proposed in [28] where decoder attends small ""chunks"" of encoder states within a window containing a fixed number of encoder states prior to and including h i .
sent18: Due to its effectiveness, MoChA is also used to develop an on-device commercialised ASR system [40].
sent19: To increase the effectiveness of the matching scores obtained to calculate the attention probabilities between the decoder state and the chunk encoder states, multi-head monotonic chunkwise attention (MTH-MoChA) is proposed in [39].
sent20: MTH-MoChA splits the encoder and decoder hidden states into K heads.
sent21: K is experimentally set as 4. For each head, matching scores, attention probabilities and the context vectors are calculated to extract the dependencies between the encoder and decoder hidden states.
sent22: Finally, the average context vector over all the heads takes part in decoding.
sent23: The pronunciation rate among different speakers may vary and therefore, the attention calculated over the fixed chunk size may not be effective.
sent24: To overcome this, in [29] an adaptive monotonic chunkwise attention (AMoChA) was proposed where attention at current decoder time step is computed over a window whose boundary [h i−1 , h i ] is computed as in [27].
sent25: Within the window, whichever encoder states results in p i,j > 0.5 or e i,j > 0 are attended.
sent26: Hence, the chunk size is adaptive instead of constant.
sent27: The input sequence or the encoder states of length L is divided equally into W in [41].
sent28: So, each block contains B = L W encoder states, while the last block may contain fewer than B encoder states.
sent29: In this model, each block is responsible for a set of output predictions and attention is computed over only the concerned blocks and not the entire encoder states.
sent30: Once the model has finished attending all the encoder states of a block and predicting the required outputs, it emits a special symbol called < epsilon > which marks the end of the corresponding block processing and the model proceeds to attend the next block.
sent31: The effectiveness of this model has been enhanced in [42] by extending the attention span.
sent32: Specifically, the attention mechanism looks at not only the current block but the k previous blocks.
sent33: Experimentally, k is set as 20.
sent34: The authors of [44] have identified the latency issue in streaming attention-based models.
sent35: In most streaming models, the encoder states are attended based on a local window.
sent36: Computing the precise boundaries of these local windows is a computational expensive process which in turn causes a delay in the speech-to-text conversion.
sent37: To overcome this issue, in [44] external hard alignments obtained from a hybrid ASR system is used for frame-wise supervision to force the MoChA model to learn accurate boundaries and alignments.
sent38: In [80] performance latency is reduced by proposing a unidirectional encoder with no future dependency.
sent39: Since each position does not depend on future context, the decoder hidden states are not required to be re-computed every time a new input chunk arrives and therefore, the overall delay is reduced.
sent40: In [43], attention mechanism has been incorporated in RNN-Transducer (RNN-T) [12], [13] to make streaming speech recognition more effective and efficient.
sent41: RNN-T consists of three sections: (i) a RNN encoder which processes an input sequence to encoder hidden states, (ii) a RNN decoder which is analogues to a language model takes the previous predicted symbol as input and outputs decoder hidden states, and (iii) a joint network that takes encoder and decoder hidden states at the current time step to compute output logit which is responsible to predict the output symbol when passed through a softmax layer.
sent42: In [43], at the encoder side, to learn contextual dependency, a multi-head self-attention layer is added on the top of RNN layers.
sent43: In addition, the joint network attends a chunk of encoder hidden states instead of attending only the current hidden state at each time step.
sent44: LAS model is primarily proposed for offline speech recognition.
sent45: However, it has been modified with silence modelling for working in the streaming environment in [45].
sent46: Given streamable encoder and a suitable attention mechanism (hard monotonic, chunkwise or local window-based instead of global), the main limitation of LAS model to perform in streaming environment is a long enough silence between the utterances to make decoder believe it is the end of speech.
sent47: Therefore, the LAS decoder terminates the transcription process while the speaker is still active (i.e. early stopping).
sent48: This limitation is addressed in [45] by incorporating reference silence tokens during the training phase to supervise the model when to output a silence token instead of terminating the process during the inference phase."
231925325,Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition,"Computer Science, Engineering",https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23,s8,D. RNN-free Transformer-based models,"Self-attention is a mechanism to capture the dependencies within a sequence. It allows to compute the similarity between different frames in the same sequence. In other words, selfattention finds to what extent different positions of a sequence relate to each other. Transformer network [20] is entirely built using self-attention for seq2seq processing and has been successfully used in ASR as well.

Transformer was introduced to ASR domain in [26] by proposing Speech-transformer. Instead of capturing only temporal dependencies, the authors of [26] have also captured spectral dependencies by computing attention along time and frequency axis of input spectrogram features. Hence, this attention mechanism is named as ""2D attention"". The set of (q, k, v) for time-domain attention is computed using (8).

Here, the input embedding (X) is the convolutional features of spectrogram. For frequency-domain attention, the set of (q, k, v) are the transpose of same parameters in the timedomain. At each block of multi-head attention, the timedomain and frequency-domain attentions are computed parallelly and after that they are concatenated using (9). In this case attention heads belong to both time and frequency domains. Speech transformer was built to output word predictions and later on it is explored for different modelling units like phonemes, syllables, characters in [46], [47] and for largescale speech recognition in [48].

A very deep Transformer model for ASR is proposed in [49]. The authors have claimed that depth is an important factor for obtaining effective ASR performance using Transformer network. Therefore, instead of using the original version of six stacked layers for both encoder and decoder, more layers (deep configuration) are used in the structure. Specifically, the authors have shown 36 − 12 layers for the encoder-decoder is the most effective configuration. To facilitate the training of this deep network, around each sub-layer, a stochastic residual connection is employed before the layernormalisation. Another deep Transformer model is proposed in [50] where it has been shown that the ASR performance is continually increased with the increase of layers up to 42 and the attention heads up to 16. The effect on performance beyond 42 layers and 16 attention-heads is not provided, probably due to the increased computation complexity. The authors have also experimentally shown that sinusoidal positional encoding [20] is not required for deep Transformer model. To increase the model capacity efficiently, the deep Transformer proposed in [51] replaced the single-layer feed-forward network in each Transformer sub-layer by a deep neural network with residual connections.

Training deep Transformers can be difficult as it often gets caught in a bad local optimum. Therefore, to enable training deep Transformer, iterated loss [77] is used in [52]. It allows output of some intermediate transformer layers to calculate auxiliary cross entropy losses which are interpolated to configure the final loss function. Apart from that, ""gelu"" (Gaussian error linear units) [78] activation function is used in the feed-forward network of each Transformer layer. Out of the different explored approaches, positional embedding with a convolutional block before each Transformer layer has shown the best performance.

A self-attention based ASR model has been proposed in [53] by replacing the pyramidal recurrent block of LAS model at the encoder side with multi-head self-attention block. As self-attention computes similarity of each pair of input frames, the memory grows quadratically with respect to the sequence length. To overcome this, authors have applied a downsampling to the sequence length before feeding it to every self-attention block. This downsampling is done by reshaping the sequences and it is a trade-off between the sequence length and the dimension. If the sequence length is reduced by a factor a, then the dimension increased by the same factor. Specifically, X ∈ R l×d → reshapeX ∈ R l a ×ad . Therefore, memory consumption to compute the attention matrices is reduced by a 2 . Unlike in [20] where position information is added to input sequence before feeding to the self-attention block, in [53], authors have claimed that adding positional information to the acoustic sequence makes the model difficult to read content. Therefore, position information is concatenated to the acoustic sequence representation and this concatenated sequence is passed to the self-attention blocks. In addition, to enhance the context relevance while calculating the similarity between speech frames, a Gaussian diagonal mask with learnable variance is added to the attention heads. Specifically, an additional bias matrix is added to Equation (7) as given by (18).

where M is matrix whose values around the diagonal are set to a higher value to force the self-attention attending in a local range around each speech frame. The elements of this matrix are calculated by a Gaussian function:

, σ is a learnable parameter.

The quadratic computation complexity during the selfattention computation using (7) has been reduced down to linear in [54] where the authors have proposed to use the dot product of kernel feature maps for the similarity calculation between the speech frames followed by the use of associative property of matrix products.

For better incorporating long-term dependency using Transformers, in [55] Transformer-XL was proposed for machinetranslation. In Transformer-XL, a segment-level recurrence mechanism is introduced which enables the reuse of past encoder states (output of the previous layers) at the training time to maintain a longer history of contexts until they become sufficiently old. Therefore, queries at current layer have access to the key-value pairs of current layer as well as previous layers. Based on this concept, Compressive Transformer [56] was proposed and it was applied to ASR to effectively incorporate long-term dependencies. In [56], instead of discarding older encoder states, they were preserved in a compressed form. [51] also explored sharing previous encoder states but reused only key vectors from previous layers.

Another Transformer-based ASR model is proposed in [57] as an adaptation of RNN-Transducer based model [79] which uses two RNN-based encoders for audio and labels respectively to learn the alignment between them. In [57], audio and label encoders are designed with Transformer networks. Given the previous predicted label from the target label space, the two encoder outputs are combined by a joint network.

Vanilla Transformer and the deep Transformer models have a number of layers stacked in both encoder and decoder sides. Each layers and their sub-layers have their own parameters and processing them is computationally expensive. In [58], a parameter sharing approach has been proposed for Transformer network. The parameters are initialised at the first encoder and decoder layers and thereafter, re-used in the other layers. If the number of encoder and decoder layers is N and the total number of parameters in each layer is M , then instead of using N ×M parameters in both encoder and decoder sides, in [58] only M parameters are used. There is a performance degradation due to sharing the parameters. To overcome that, speech attributes such as, duration of the utterance, sex and age of the speaker are augmented with the ground truth labels during training.

In self-attention based Transformer models, each speech frame attends all other speech frames of the entire sequence or within a window. However, some of them like frames representing silence are not crucial for modelling longrange dependencies and may present multiple times in the attended sequence. Therefore, these frames should be avoided. The attention weights (or probabilities) are obtained using sof tmax function which generates non-zero probabilities and therefore, insignificant frames are also assigned to some attention weights. To overcome this, in [59] weak-attention suppression (WAS) mechanism is proposed. WAS induced sparsity over the attention probability distribution by setting attention probabilities to zero which are smaller than a dynamically determined threshold. More specifically, the threshold is determined by (19). After that, the rest non-zero probabilities are re-normalised by passing through a sof tmax function.

where θ i is the threshold, m i and σ i are the mean and standard deviation of the attention probability for the i th frame in the query sequence. γ is a scaling factor which ranges from 0 to 1 and experimentally, 0.5 provided the best result.","[['b19'], ['b25', 'b7'], ['b47', 'b8', 'b46', 'b48'], ['b50', 'b51', 'b49', 'b19'], ['b80', 'b79', 'b52'], ['b53', 'b17', 'b19'], [], [], ['b54', 'b6'], ['b51', 'b56', 'b55'], ['b81', 'b57'], ['b58'], ['b18', 'b59'], []]","[['b19'], ['b25', 'b7'], ['b47', 'b8', 'b46', 'b48'], ['b50', 'b51', 'b49', 'b19'], ['b80', 'b79', 'b52'], ['b53', 'b17', 'b19'], [], [], ['b54', 'b6'], ['b51', 'b56', 'b55'], ['b81', 'b57'], ['b58'], ['b18', 'b59'], []]",27,"sent1: Self-attention is a mechanism to capture the dependencies within a sequence.
sent2: It allows to compute the similarity between different frames in the same sequence.
sent3: In other words, selfattention finds to what extent different positions of a sequence relate to each other.
sent4: Transformer network [20] is entirely built using self-attention for seq2seq processing and has been successfully used in ASR as well.
sent5: Transformer was introduced to ASR domain in [26] by proposing Speech-transformer.
sent6: Instead of capturing only temporal dependencies, the authors of [26] have also captured spectral dependencies by computing attention along time and frequency axis of input spectrogram features.
sent7: Hence, this attention mechanism is named as ""2D attention"".
sent8: The set of (q, k, v) for time-domain attention is computed using (8).
sent9: Here, the input embedding (X) is the convolutional features of spectrogram.
sent10: For frequency-domain attention, the set of (q, k, v) are the transpose of same parameters in the timedomain.
sent11: At each block of multi-head attention, the timedomain and frequency-domain attentions are computed parallelly and after that they are concatenated using (9).
sent12: In this case attention heads belong to both time and frequency domains.
sent13: Speech transformer was built to output word predictions and later on it is explored for different modelling units like phonemes, syllables, characters in [46], [47] and for largescale speech recognition in [48].
sent14: A very deep Transformer model for ASR is proposed in [49].
sent15: The authors have claimed that depth is an important factor for obtaining effective ASR performance using Transformer network.
sent16: Therefore, instead of using the original version of six stacked layers for both encoder and decoder, more layers (deep configuration) are used in the structure.
sent17: Specifically, the authors have shown 36 − 12 layers for the encoder-decoder is the most effective configuration.
sent18: To facilitate the training of this deep network, around each sub-layer, a stochastic residual connection is employed before the layernormalisation.
sent19: Another deep Transformer model is proposed in [50] where it has been shown that the ASR performance is continually increased with the increase of layers up to 42 and the attention heads up to 16.
sent20: The effect on performance beyond 42 layers and 16 attention-heads is not provided, probably due to the increased computation complexity.
sent21: The authors have also experimentally shown that sinusoidal positional encoding [20] is not required for deep Transformer model.
sent22: To increase the model capacity efficiently, the deep Transformer proposed in [51] replaced the single-layer feed-forward network in each Transformer sub-layer by a deep neural network with residual connections.
sent23: Training deep Transformers can be difficult as it often gets caught in a bad local optimum.
sent24: Therefore, to enable training deep Transformer, iterated loss [77] is used in [52].
sent25: It allows output of some intermediate transformer layers to calculate auxiliary cross entropy losses which are interpolated to configure the final loss function.
sent26: Apart from that, ""gelu"" (Gaussian error linear units) [78] activation function is used in the feed-forward network of each Transformer layer.
sent27: Out of the different explored approaches, positional embedding with a convolutional block before each Transformer layer has shown the best performance.
sent28: A self-attention based ASR model has been proposed in [53] by replacing the pyramidal recurrent block of LAS model at the encoder side with multi-head self-attention block.
sent29: As self-attention computes similarity of each pair of input frames, the memory grows quadratically with respect to the sequence length.
sent30: To overcome this, authors have applied a downsampling to the sequence length before feeding it to every self-attention block.
sent31: This downsampling is done by reshaping the sequences and it is a trade-off between the sequence length and the dimension.
sent32: If the sequence length is reduced by a factor a, then the dimension increased by the same factor.
sent33: Specifically, X ∈ R l×d → reshapeX ∈ R l a ×ad .
sent34: Therefore, memory consumption to compute the attention matrices is reduced by a 2 .
sent35: Unlike in [20] where position information is added to input sequence before feeding to the self-attention block, in [53], authors have claimed that adding positional information to the acoustic sequence makes the model difficult to read content.
sent36: Therefore, position information is concatenated to the acoustic sequence representation and this concatenated sequence is passed to the self-attention blocks.
sent37: In addition, to enhance the context relevance while calculating the similarity between speech frames, a Gaussian diagonal mask with learnable variance is added to the attention heads.
sent38: Specifically, an additional bias matrix is added to Equation (7) as given by (18).where M is matrix whose values around the diagonal are set to a higher value to force the self-attention attending in a local range around each speech frame.
sent39: The elements of this matrix are calculated by a Gaussian function:, σ is a learnable parameter.
sent40: The quadratic computation complexity during the selfattention computation using (7) has been reduced down to linear in [54] where the authors have proposed to use the dot product of kernel feature maps for the similarity calculation between the speech frames followed by the use of associative property of matrix products.
sent41: For better incorporating long-term dependency using Transformers, in [55] Transformer-XL was proposed for machinetranslation.
sent42: In Transformer-XL, a segment-level recurrence mechanism is introduced which enables the reuse of past encoder states (output of the previous layers) at the training time to maintain a longer history of contexts until they become sufficiently old.
sent43: Therefore, queries at current layer have access to the key-value pairs of current layer as well as previous layers.
sent44: Based on this concept, Compressive Transformer [56] was proposed and it was applied to ASR to effectively incorporate long-term dependencies.
sent45: In [56], instead of discarding older encoder states, they were preserved in a compressed form.
sent46: [51] also explored sharing previous encoder states but reused only key vectors from previous layers.
sent47: Another Transformer-based ASR model is proposed in [57] as an adaptation of RNN-Transducer based model [79] which uses two RNN-based encoders for audio and labels respectively to learn the alignment between them.
sent48: In [57], audio and label encoders are designed with Transformer networks.
sent49: Given the previous predicted label from the target label space, the two encoder outputs are combined by a joint network.
sent50: Vanilla Transformer and the deep Transformer models have a number of layers stacked in both encoder and decoder sides.
sent51: Each layers and their sub-layers have their own parameters and processing them is computationally expensive.
sent52: In [58], a parameter sharing approach has been proposed for Transformer network.
sent53: The parameters are initialised at the first encoder and decoder layers and thereafter, re-used in the other layers.
sent54: If the number of encoder and decoder layers is N and the total number of parameters in each layer is M , then instead of using N ×M parameters in both encoder and decoder sides, in [58] only M parameters are used.
sent55: There is a performance degradation due to sharing the parameters.
sent56: To overcome that, speech attributes such as, duration of the utterance, sex and age of the speaker are augmented with the ground truth labels during training.
sent57: In self-attention based Transformer models, each speech frame attends all other speech frames of the entire sequence or within a window.
sent58: However, some of them like frames representing silence are not crucial for modelling longrange dependencies and may present multiple times in the attended sequence.
sent59: Therefore, these frames should be avoided.
sent60: The attention weights (or probabilities) are obtained using sof tmax function which generates non-zero probabilities and therefore, insignificant frames are also assigned to some attention weights.
sent61: To overcome this, in [59] weak-attention suppression (WAS) mechanism is proposed.
sent62: WAS induced sparsity over the attention probability distribution by setting attention probabilities to zero which are smaller than a dynamically determined threshold.
sent63: More specifically, the threshold is determined by (19).
sent64: After that, the rest non-zero probabilities are re-normalised by passing through a sof tmax function.
sent65: where θ i is the threshold, m i and
sent66: σ i are the mean and standard deviation of the attention probability for the i th frame in the query sequence.
sent67: γ is a scaling factor which ranges from 0 to 1 and experimentally, 0.5 provided the best result."
231925325,Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition,"Computer Science, Engineering",https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23,s5,A. Global Attention with RNN,"Global attention is computed over the entire encoder hidden states at every decoder time step. The mechanism illustrated in Section III-A as per [11] is an example of global attention. Since [11], a lot of progress has been made by many researchers.

The authors of [24] presented a global attention mechanism in their Listen, Attend and Spell (LAS) model. Here, Spell function takes inputs as current decoder state s i and the context c i . y i = Spell(s i , c i ). s i is computed using a recurrent function which takes inputs as previous decoder state (s i−1 ), previous output prediction (y i−1 ) and previous context (c i−1 ).

. The authors have used the content information only to calculate the matching scores given by (10). Attention probabilities are then calculated by (4) using the matching scores.

A similar content-based global attention have been proposed in [30] where a feedback factor is incorporated in addition to the content information in calculating the matching scores for better numerical stability. In generalised form, it is given by (11) 

where β i,j is the attention weight feedback computed using the previously aligned attention vectors and it is given by (12).

where w b is a trainable weight vector. Here, Spell function is computed over s i , y i−1 and c i , i.e. y i = Spell(s i , y i−1 , c i ) A character-aware (CA) attention is proposed in [25] to incorporate morphological relations for predicting words and sub-word units (WSU). A separate RNN (named as CA-RNN by the author) which dynamically generates WSU representations connected to the decoder in parallel with the encoder network. The decoder hidden state s t−1 is required to obtain the attention weights at t time step. s t is computed using the recurrent function over s t−1 , w t−1 (WSU represenation) and c t−1 . The matching scores required to compute attention vectors at decoder t time step is calculated using (6). In contrast to [11], the authors have used RELU instead of tanh function and claimed it provides better ASR performance.","[['b10'], ['b23'], ['b9'], ['b29', 'b10'], ['b11'], ['b5', 'b10', 'b24']]","[['b10'], ['b23'], ['b9'], ['b29', 'b10'], ['b11'], ['b5', 'b10', 'b24']]",9,"sent1: Global attention is computed over the entire encoder hidden states at every decoder time step.
sent2: The mechanism illustrated in Section III-A as per [11] is an example of global attention.
sent3: Since [11], a lot of progress has been made by many researchers.
sent4: The authors of [24] presented a global attention mechanism in their Listen, Attend and Spell (LAS) model.
sent5: Here, Spell function takes inputs as current decoder state s
sent6: i and the context c i . y i = Spell(s i , c i ).
sent7: s i is computed using a recurrent function which takes inputs as previous decoder state (s i−1 ), previous output prediction (y i−1 ) and previous context (c i−1 )..
sent8: The authors have used the content information only to calculate the matching scores given by (10).
sent9: Attention probabilities are then calculated by (4) using the matching scores.
sent10: A similar content-based global attention have been proposed in [30] where a feedback factor is incorporated in addition to the content information in calculating the matching scores for better numerical stability.
sent11: In generalised form, it is given by (11) where β
sent12: i,j is the attention weight feedback computed using the previously aligned attention vectors and it is given by (12).
sent13: where w b is a trainable weight vector.
sent14: Here, Spell function is computed over s i , y i−1 and c i , i.e. y i =
sent15: Spell(s i , y i−1 , c i ) A character-aware (CA) attention is proposed in [25] to incorporate morphological relations for predicting words and sub-word units (WSU).
sent16: A separate RNN (named as CA-RNN by the author) which dynamically generates WSU representations connected to the decoder in parallel with the encoder network.
sent17: The decoder hidden state s t−1 is required to obtain the attention weights at t time step.
sent18: s t is computed using the recurrent function over s t−1 , w t−1 (WSU represenation) and c t−1 .
sent19: The matching scores required to compute attention vectors at decoder t time step is calculated using (6).
sent20: In contrast to [11], the authors have used RELU instead of tanh function and claimed it provides better ASR performance."
231925325,Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition,"Computer Science, Engineering",https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23,s6,B. Local attention with RNN,"In global attention model, each encoder hidden states are attended at each decoder time step. This results in a quadratic computation complexity. In addition, the prediction of a particular decoder output mostly depends on a small number of encoder hidden states. Therefore, it is not necessary to attend the entire set of encoder hidden states at each decoder time step. The application of local attention fulfils the requirement of reducing the computation complexity by focusing on relevant encoder hidden states. Local attention mechanism is mostly popular in streaming speech recognition but, it has been applied to offline speech recognition as well. The core idea of local attention is to attend a set of encoder hidden states within a window or range at each decoder time step instead of attending the entire set of encoder hidden states. Local attention was introduced in [74] for machine translation and thereafter, it has been applied to ASR as well.

In [23], the window upon which the attention probabilities are computed is considered as

where m t−1 is the median of previous alignment α t−1 (i.e. the attention probabilities computed at the last decoder time step). w l and w r are the user-defined fixed parameters which determine the span of the window in left and right directions, respectively. A similar local attention was proposed in [31].

To obtain the attention window, position difference p t is calculated for the prediction at the t decoder time step in [32]. p t is the position difference between the centre of attention windows of previous and current decoder time steps. Therefore, given p t−1 (the centre of previous attention window) and p t , the centre of current attention window can be calculated. After that, the attention window at the t th decoder time step is set as [p t − p t , p t + p t ]. Two methods were proposed to estimate p t as given by (13) and (14).

where V p and W p are a trainable vector and matrix respectively. C max is a hyper parameter to maintain the condition:

Equations (13) and (14) are named as Constrained and Unconstrained position predictions respectively.","[['b76'], ['b22'], ['b30'], ['b31'], [], []]","[['b76'], ['b22'], ['b30'], ['b31'], [], []]",4,"sent1: In global attention model, each encoder hidden states are attended at each decoder time step.
sent2: This results in a quadratic computation complexity.
sent3: In addition, the prediction of a particular decoder output mostly depends on a small number of encoder hidden states.
sent4: Therefore, it is not necessary to attend the entire set of encoder hidden states at each decoder time step.
sent5: The application of local attention fulfils the requirement of reducing the computation complexity by focusing on relevant encoder hidden states.
sent6: Local attention mechanism is mostly popular in streaming speech recognition but, it has been applied to offline speech recognition as well.
sent7: The core idea of local attention is to attend a set of encoder hidden states within a window or range at each decoder time step instead of attending the entire set of encoder hidden states.
sent8: Local attention was introduced in [74] for machine translation and thereafter, it has been applied to ASR as well.
sent9: In [23], the window upon which the attention probabilities are computed is considered aswhere m t−1 is the median of previous alignment α t−1 (i.e. the attention probabilities computed at the last decoder time step).
sent10: w l and w r are the user-defined fixed parameters which determine the span of the window in left and right directions, respectively.
sent11: A similar local attention was proposed in [31].
sent12: To obtain the attention window, position difference p t is calculated for the prediction at the t decoder time step in [32].
sent13: p t is the position difference between the centre of attention windows of previous and current decoder time steps.
sent14: Therefore, given p t−1 (the centre of previous attention window) and p t , the centre of current attention window can be calculated.
sent15: After that, the attention window at the t th decoder time step is set as [p t − p t , p t + p t ].
sent16: Two methods were proposed to estimate p t as given by (13) and (14).where V p and W p are a trainable vector and matrix respectively.
sent17: C max is a hyper parameter to maintain the condition:Equations (13) and (14) are named as Constrained and Unconstrained position predictions respectively."
231925325,Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition,"Computer Science, Engineering",https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23,s11,B. RNN-free Transformer-based models,"In this section, we will discuss the literature where RNNfree self-attention models are used for streaming speech recognition. Self-attention aligner [60] which is designed based on the Transformer model proposes a chunk hoping mechanism to provide support to online speech recognition. Transformerbased network requires the entire sequence to be obtained before the prediction starts and hence, not suitable for online speech recognition. In [60], the entire sequence is partitioned into several overlapped chunks, each of which contains three parts belonging to current, past and future. Speech frames or encoder states of the current part are attended to provide the output predictions belonging to the corresponding chunk. The past and future parts provide contexts to the identification of the current part. After attending a chunk, the mechanism hops to a new chunk to attend. The number of speech frames or encoder states hopped between two chunks is same as the current part of each chunk. A similar method was proposed in augmented memory Transformer [61] where an augmented memory bank is included apart from partitioning the input speech sequence. The augmented memory bank is used for carrying the information over the chunks, specifically by extracting key-value pairs from the projection of concatenated augmented memory bank and the relevant chunk (including past, current and future parts).

Transformer transducer model [62] uses truncated selfattention to support streaming ASR. Instead of attending the entire speech sequence at each time step t, truncated selfattention mechanism allows attending speech frames within the window of [t − L, t + R] frames. L and R represent the frame limits to the left and right respectively. In [62], positional encoding in input embedding is done by causal convolution [63] to support online ASR. In another variation of Transformer transducer [57], the model restricts attending to the left side of the current frame only by masking the attention scores to the right of the current frame. The attention span is further restricted by attending the frames within a fixed-size window at each time step.

A chunk-flow mechanism is proposed in [64] to support streaming speech recognition in self-attention based transducer model. The chunk-flow mechanism restricts the span of selfattention to a fixed length chunk instead the whole input sequence. The fixed length chunk proceeds along time over the input sequence. Not attending the entire input sequence may degrade the performance. However, it is still kept satisfactory by using multiple self-attention heads to model longer dependencies. The chunk-flow mechanism at time t for the attention head h i is given by (21) h i,t = t+Nr τ =t−N l α i,τ s τ , (21) where N l and N r represent the number of speech frames to the left and right of the current time t. N l and N r determine the chunk span and experimentally they are chosen as 20 and 10 respectively. s τ represents the τ th vector in the input sequence and α i,τ = Attention(s τ , K, V ); K = V = chunk τ A streaming friendly self-attention mechanism, named as time-restricted self-attention is proposed in [65]. It works by restricting the speech frame at current time step to attend only a fixed number of frames to its left and right and thus it does not allow attending each speech frame to attend all other speech frames. Experimentally, these numbers are set to 15 and 6 for left and right sides, respectively. Similarly, in [52], each Transformer layer is restricted to attend a fixed limited right context during inference. A special position embedding approach also has been proposed by adding a one-hot encoder vector with the value vectors. The one-hot encoder vector consists of all zeros except a single one corresponding to the attending time step with respect to all the time steps in the current attention span. This mechanism is also used in the encoder side of streaming transformer model [66].

Synchronous Transformer [67] is proposed to support streamable speech recognition using self-attention mechanism to overcome the requirement of processing all speech frames before decoding starts. While calculating the self-attention, every speech frame is restricted to process only the frames left to it and ignore the right side. Also, at the decoder time step, encoded speech frames are processed chunkwise. The encoded speech frames are divided into overlapped chunks to maintain the smooth transition of information between chunks. At each decoder time step, the decoder predicts an output based on the last predicted output and the attention calculated over the frames belonging to a chunk only and therefore, avoids attending the entire speech sequence.

To make Transformer streamable, chunk self-attention encoder and monotonic truncated attention-based self-attention decoder is proposed in [68]. At the encoder side, the input speech is split into isolated chunks of fixed length inspired by MoChA. At the decoder side, encoder-decoder attention mechanism [20] is replaced by truncated attention [69]. The encoder embedding is truncated in a monotonic left to right approach and then attention applied over the trunacted outputs. After that, the model is optimised by online joint CTCattention method [69].

Monotonic multihead attention (MMA) is proposed in [81] to enable online decoding in Transformer network by replacing each encoder-decoder attention head with a monotonic attention (MA) head. Each MA head needs to be activated to predict a output symbol. If any MA head failed or delayed to learn alignments, it causes delay during inference. The authors of [70] have found that only few MA heads (dominant ones) learn alignments effectively and others do not. To prevent this and to let each head learning alignments effectively, HeadDrop regularisation is proposed. It entirely masks a part of the heads at random and forces the rest of non-masked heads to learn alignment effectively. In addition, the redundant MA heads are pruned in the lower layers to further improve the team work among the attention heads. Since MA is hard attention, chunkwise attention is applied on the top of each MA head to enhance the quality of context information.","[['b61', 'b60'], ['b57', 'b62', 'b63'], ['b64', 'b67', 'b52', 'b20', 'b65'], ['b69'], ['b70', 'b19', 'b71'], ['b72', 'b83']]","[['b61', 'b60'], ['b57', 'b62', 'b63'], ['b64', 'b67', 'b52', 'b20', 'b65'], ['b69'], ['b70', 'b19', 'b71'], ['b72', 'b83']]",16,"sent1: In this section, we will discuss the literature where RNNfree self-attention models are used for streaming speech recognition.
sent2: Self-attention aligner [60] which is designed based on the Transformer model proposes a chunk hoping mechanism to provide support to online speech recognition.
sent3: Transformerbased network requires the entire sequence to be obtained before the prediction starts and hence, not suitable for online speech recognition.
sent4: In [60], the entire sequence is partitioned into several overlapped chunks, each of which contains three parts belonging to current, past and future.
sent5: Speech frames or encoder states of the current part are attended to provide the output predictions belonging to the corresponding chunk.
sent6: The past and future parts provide contexts to the identification of the current part.
sent7: After attending a chunk, the mechanism hops to a new chunk to attend.
sent8: The number of speech frames or encoder states hopped between two chunks is same as the current part of each chunk.
sent9: A similar method was proposed in augmented memory Transformer [61] where an augmented memory bank is included apart from partitioning the input speech sequence.
sent10: The augmented memory bank is used for carrying the information over the chunks, specifically by extracting key-value pairs from the projection of concatenated augmented memory bank and the relevant chunk (including past, current and future parts).
sent11: Transformer transducer model [62] uses truncated selfattention to support streaming ASR.
sent12: Instead of attending the entire speech sequence at each time step t, truncated selfattention mechanism allows attending speech frames within the window of [t − L, t + R] frames.
sent13: L and R represent the frame limits to the left and right respectively.
sent14: In [62], positional encoding in input embedding is done by causal convolution [63] to support online ASR.
sent15: In another variation of Transformer transducer [57], the model restricts attending to the left side of the current frame only by masking the attention scores to the right of the current frame.
sent16: The attention span is further restricted by attending the frames within a fixed-size window at each time step.
sent17: A chunk-flow mechanism is proposed in [64] to support streaming speech recognition in self-attention based transducer model.
sent18: The chunk-flow mechanism restricts the span of selfattention to a fixed length chunk instead the whole input sequence.
sent19: The fixed length chunk proceeds along time over the input sequence.
sent20: Not attending the entire input sequence may degrade the performance.
sent21: However, it is still kept satisfactory by using multiple self-attention heads to model longer dependencies.
sent22: The chunk-flow mechanism at time t for the attention head h
sent23: i is given by (21) h i,t = t+Nr τ
sent24: =t−N l α i,τ s τ , (21) where N l and N r represent the number of speech frames to the left and right of the current time t. N l and N r determine the chunk span and experimentally they are chosen as 20 and 10 respectively.
sent25: s τ represents the τ th vector in the input sequence and α i,τ = Attention(s τ , K, V ); K = V = chunk τ
sent26: A streaming friendly self-attention mechanism, named as time-restricted self-attention is proposed in [65].
sent27: It works by restricting the speech frame at current time step to attend only a fixed number of frames to its left and right and thus it does not allow attending each speech frame to attend all other speech frames.
sent28: Experimentally, these numbers are set to 15 and 6 for left and right sides, respectively.
sent29: Similarly, in [52], each Transformer layer is restricted to attend a fixed limited right context during inference.
sent30: A special position embedding approach also has been proposed by adding a one-hot encoder vector with the value vectors.
sent31: The one-hot encoder vector consists of all zeros except a single one corresponding to the attending time step with respect to all the time steps in the current attention span.
sent32: This mechanism is also used in the encoder side of streaming transformer model [66].
sent33: Synchronous Transformer [67] is proposed to support streamable speech recognition using self-attention mechanism to overcome the requirement of processing all speech frames before decoding starts.
sent34: While calculating the self-attention, every speech frame is restricted to process only the frames left to it and ignore the right side.
sent35: Also, at the decoder time step, encoded speech frames are processed chunkwise.
sent36: The encoded speech frames are divided into overlapped chunks to maintain the smooth transition of information between chunks.
sent37: At each decoder time step, the decoder predicts an output based on the last predicted output and the attention calculated over the frames belonging to a chunk only and therefore, avoids attending the entire speech sequence.
sent38: To make Transformer streamable, chunk self-attention encoder and monotonic truncated attention-based self-attention decoder is proposed in [68].
sent39: At the encoder side, the input speech is split into isolated chunks of fixed length inspired by MoChA. At the decoder side, encoder-decoder attention mechanism [20] is replaced by truncated attention [69].
sent40: The encoder embedding is truncated in a monotonic left to right approach and then attention applied over the trunacted outputs.
sent41: After that, the model is optimised by online joint CTCattention method [69].
sent42: Monotonic multihead attention (MMA) is proposed in [81] to enable online decoding in Transformer network by replacing each encoder-decoder attention head with a monotonic attention (MA) head.
sent43: Each MA head needs to be activated to predict a output symbol.
sent44: If any MA head failed or delayed to learn alignments, it causes delay during inference.
sent45: The authors of [70] have found that only few MA heads (dominant ones) learn alignments effectively and others do not.
sent46: To prevent this and to let each head learning alignments effectively, HeadDrop regularisation is proposed.
sent47: It entirely masks a part of the heads at random and forces the rest of non-masked heads to learn alignment effectively.
sent48: In addition, the redundant MA heads are pruned in the lower layers to further improve the team work among the attention heads.
sent49: Since MA is hard attention, chunkwise attention is applied on the top of each MA head to enhance the quality of context information."
231925325,Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition,"Computer Science, Engineering",https://www.semanticscholar.org/paper/274c50e6819d8654a6cdc918d2d6d3ad02ce6c23,s3,A. RNN-based encoder-decoder architecture,"Sequence-to-sequence RNN-based ASR models are based on an encoder-decoder architecture. The encoder is an RNN which takes input sequence and converts it into hidden states. The decoder is also an RNN which takes the last encoder hidden state as input and process it to decoder hidden states which in turn used for output predictions. This traditional encoder-decoder structure has some limitations:

• The encoder hidden state, h T (last one) which is fed to the decoder has the entire input sequence information compressed into it. For longer input sequences, it may cause information loss as h T may not capture long-range dependencies effectively. • There is no alignment between the input sequence frames and the output. For predicting each output symbol, instead The above issues can be overcome by letting the decoder to access all the encoder hidden states (instead of the last one) and at each decoder time step, relevant input frames are given higher priorities than others. It is achieved by incorporating attention mechanism to the encoder-decoder model. As a part of sequence-to-sequence modelling, attention mechanism was introduced in [71] for machine translation. Inspired by the effectiveness in [71], the attention mechanism was introduced to ASR in [11]. An earlier version of this work has been presented in [10].

The model in [11] is named as attention-based recurrent sequence generator (ASRG). The graphical representation of this model is shown in Figure 1. The encoder of ASRG processes the input audio frames to encoder hidden states which are then used to predict output phonemes. By focusing on the relevant encoder hidden states, at i th decoder time step, prediction of phoneme y i is given by (1)

where c i is the context given by (2) generated by attention mechanism at the i th decoder time step. s i given by (3) is the decoder hidden state at i th time step. It is the output of a recurrent function like LSTM or GRU. Spell(., .) is a feedforward neural network with softmax output activation.

where h j is the encoder hidden state at the j th encoder time step. α i,j given by (4) is the attention probability belonging to the j th encoder hidden state for the output prediction at i th decoder time step. In other words, α i,j captures the importance of the j th input speech frame (or encoder hidden state) for decoding the i th output word (or phoneme or character). α i values are also considered as the alignment of encoder hidden states (h j∈[1,··· ,L] ) to predict an output at i th decoder time step. Therefore, c i is the sum of the products (SOP) of attention probabilities and the hidden states belonging to all encoder time steps at the i th decoder time step and it provides a context to the decoder to decode (or predict) the corresponding output.

where e i,j is the matching score between the i th decoder hidden state and the j th encoder hidden state. It is computed using a hybrid attention mechanism given by (5) in a general form and by (6) in a parametric form.

where w and b are vectors and W , V and U are matrices. These are all trainable parameters. f i = F * α i−1 is a set of vectors which are extracted for every encoder state h j of the previous alignment α i−1 which is convolved with a trainable matrix F . The tanh function produces a vector. However, e i,j is a single score. Therefore, a dot product of tanh outcome and w is performed. The mechanism in (5) is referred to as hybrid attention as it considers both location (α) and content (h) information. By dropping either α i−1 or h j , the Attend mechanism is called content-based or location-based attention.

B. Transformer-based encoder-decoder architeture RNN-based encoder-decoder architecture is sequential in nature. To capture the dependencies, hidden states are generated sequentially and at each time step, the generated hidden state is the output of a function of previous hidden state. This sequential process is time consuming. Also, during the training, error back propagates through time and this process is again time consuming.

To overcome the limitations of RNN, Transformer network is proposed completely based on attention mechanism. In Transformer network, no recurrent connection is used. Instead, the input farmes are processed parallelly at the same time, and during training, no back propagation through time is applicable.

Transormer network was introduced in [20] for machine translation and later it is successfully applied to ASR tasks. In this section, the idea of Transformer is given as described in [20]. The graphical representation of Transformer is shown in Figure 2.

The Transformer network is composed of an encoderdecoder architecture but there is no recurrent or convolutional neural network involved here. Instead, the authors have used self-attention to incorporate the dependencies in the seq2seq framework. The encoder is composed of six identical layers where each layer is divided into two sub-layers. The first sublayer is a multi-head self-attention module and the second one is a position-wise feed-forward neural network. The decoder is also composed of six identical layers but has an additional sub-layer to perform multi-head self-attention over the encoder  [20] output. Around each sub-layer, a residual connection [72] is employed followed by a layer-normalisation [73]. In the decoder section, out of two multi-head attention blocks, the first one is masked to prevent positions from attending subsequent positions.

The attention function is considered here as to obtain an output which is the weighted sum of values based on matching a query with keys from the corresponding key-value pairs using scaled dot-product. The dimensionalities of query, key and value vectors are d k , d k and d v , respectively. In practice, attention is computed on a set of query, key and value together by stacking these vectors in a matrix form. Mathematically, it is given by (7).

where Q, K, V are matrices which represent Query, Key and Value, respectively. Positional information is added to the input sequence to generate the input embedding upon which the attention will be performed. Instead of directly applying attention on input embeddings, they are linearly projected to d k and d v dimensional vectors using learned projections given by (8) 

where W q ∈ R d model ×d k , W k ∈ R d model ×d k and W v ∈ R d model ×dv are trainable parameters. d model is the dimension of input embeddings. X is the input embedding for the encoder section and the output embedding for the masked multi-head block for the decoder section. For the second multi-head block of the decoder section, X is the encoder output for k and v projection. However, for q projection, X is the output from the masked multi-head section.

In Transformer network [20], the attention mechanism have been used in three different ways. They are as follows.

1) Encoder self-attention: In the encoder section, attention mechanism is applied over the input sequences to find the similarity of each token of a sequence with rest of the tokens. 2) Decoder masked self-attention: Similar to the encoder self-attention, output (target) sequence tokens attend each other in this stage. However, instead of accessing the entire output sequence at a time, the decoder can only access the tokens preceding the token which decoder attempts to predict. This is done by masking current and all the future tokens of a particular decoder time step. This approach prevents the training phase to be biased. 3) Encoder-decoder attention: This occurs at the decoder section after decoder masked self-attention stage. With reference to (7), at this stage, Q is the linear projection of the vector coming from decoder's masked self-attention block. Whereas, K and V are obtained by linearly projecting the vector resulting from encoder self-attention block. This is the stage where the mapping between input and output (target) sequences happens. The output of this block is the attention vectors containing the relationship between tokens of input and output sequences. At each sub-layer, the attention is performed h-times in parallel. Hence, the name ""multi-head attention"" is given. In [20], the value of h is 8. According to the authors, multihead attention allows the model to jointly attend to information from different representation subspaces at different positions. The outputs from each attention head are then concatenated and projected using (9) to obtain the final output of the corresponding sub-layer.

where head i∈ [1,h] is computed using (8) and W o ∈ R hdv×d model is a trainable parameter.","[[], ['b73', 'b9', 'b10'], ['b10'], [], [], ['b5'], [], [], [], ['b19'], ['b74', 'b19', 'b75'], ['b6'], ['b7'], [], ['b19'], ['b8', 'b19', 'b6'], [None, 'b7']]","[[], ['b73', 'b9', 'b10'], ['b10'], [], [], ['b5'], [], [], [], ['b19'], ['b74', 'b19', 'b75'], ['b6'], ['b7'], [], ['b19'], ['b8', 'b19', 'b6'], [None, 'b7']]",17,"sent1: Sequence-to-sequence RNN-based ASR models are based on an encoder-decoder architecture.
sent2: The encoder is an RNN which takes input sequence and converts it into hidden states.
sent3: The decoder is also an RNN which takes the last encoder hidden state as input and process it to decoder hidden states which in turn used for output predictions.
sent4: This traditional encoder-decoder structure has some limitations:• The encoder hidden state, h T (last one) which is fed to the decoder has the entire input sequence information compressed into it.
sent5: For longer input sequences, it may cause information loss as h T may not capture long-range dependencies effectively.
sent6: • There is no alignment between the input sequence frames and the output.
sent7: For predicting each output symbol, instead The above issues can be overcome by letting the decoder to access all the encoder hidden states (instead of the last one) and at each decoder time step, relevant input frames are given higher priorities than others.
sent8: It is achieved by incorporating attention mechanism to the encoder-decoder model.
sent9: As a part of sequence-to-sequence modelling, attention mechanism was introduced in [71] for machine translation.
sent10: Inspired by the effectiveness in [71], the attention mechanism was introduced to ASR in [11].
sent11: An earlier version of this work has been presented in [10].
sent12: The model in [11] is named as attention-based recurrent sequence generator (ASRG).
sent13: The graphical representation of this model is shown in Figure 1.
sent14: The encoder of ASRG processes the input audio frames to encoder hidden states which are then used to predict output phonemes.
sent15: By focusing on the relevant encoder hidden states, at i th decoder time step, prediction of phoneme y i is given by (1)where c
sent16: i is the context given by (2) generated by attention mechanism at the i th decoder time step.
sent17: s i given by (3) is the decoder hidden state at i
sent18: th time step. It is the output of a recurrent function like LSTM or GRU.
sent19: Spell(., .) is a feedforward neural network with softmax output activation.
sent20: where h j is the encoder hidden state at the j th encoder time step.
sent21: α i,j given by (4) is the attention probability belonging to the j th encoder hidden state for the output prediction at i th decoder time step.
sent22: In other words, α i,j captures the importance of the j th input speech frame (or encoder hidden state) for decoding the i th output word (or phoneme or character).
sent23: α i values are also considered as the alignment of encoder hidden states (h j∈[1,··· ,L] ) to predict an output at i th decoder time step.
sent24: Therefore, c i is the sum of the products (SOP) of attention probabilities and the hidden states belonging to all encoder time steps at the i th decoder time step and it provides a context to the decoder to decode (or predict) the corresponding output.
sent25: where e i,j is the matching score between the i th decoder hidden state and the j th encoder hidden state.
sent26: It is computed using a hybrid attention mechanism given by (5) in a general form and by (6) in a parametric form.
sent27: where w and b are vectors and W , V and U are matrices.
sent28: These are all trainable parameters.
sent29: f i = F * α i−1 is a set of vectors which are extracted for every encoder state h j of the previous alignment α i−1 which is convolved with a trainable matrix F .
sent30: The tanh function produces a vector.
sent31: However, e i,j is a single score.
sent32: Therefore, a dot product of tanh outcome and w is performed.
sent33: The mechanism in (5) is referred to as hybrid attention as it considers both location (α) and content (h) information.
sent34: By dropping either α i−1 or h j , the Attend mechanism is called content-based or location-based attention.B.
sent35: Transformer-based encoder-decoder architeture RNN-based encoder-decoder architecture is sequential in nature.
sent36: To capture the dependencies, hidden states are generated sequentially and at each time step, the generated hidden state is the output of a function of previous hidden state.
sent37: This sequential process is time consuming.
sent38: Also, during the training, error back propagates through time and this process is again time consuming.
sent39: To overcome the limitations of RNN, Transformer network is proposed completely based on attention mechanism.
sent40: In Transformer network, no recurrent connection is used.
sent41: Instead, the input farmes are processed parallelly at the same time, and during training, no back propagation through time is applicable.
sent42: Transormer network was introduced in [20] for machine translation and later it is successfully applied to ASR tasks.
sent43: In this section, the idea of Transformer is given as described in [20].
sent44: The graphical representation of Transformer is shown in Figure 2.
sent45: The Transformer network is composed of an encoderdecoder architecture but there is no recurrent or convolutional neural network involved here.
sent46: Instead, the authors have used self-attention to incorporate the dependencies in the seq2seq framework.
sent47: The encoder is composed of six identical layers where each layer is divided into two sub-layers.
sent48: The first sublayer is a multi-head self-attention module and the second one is a position-wise feed-forward neural network.
sent49: The decoder is also composed of six identical layers but has an additional sub-layer to perform multi-head self-attention over the encoder  [20] output.
sent50: Around each sub-layer, a residual connection [72] is employed followed by a layer-normalisation [73].
sent51: In the decoder section, out of two multi-head attention blocks, the first one is masked to prevent positions from attending subsequent positions.
sent52: The attention function is considered here as to obtain an output which is the weighted sum of values based on matching a query with keys from the corresponding key-value pairs using scaled dot-product.
sent53: The dimensionalities of query, key and value vectors are d k , d k and d v , respectively.
sent54: In practice, attention is computed on a set of query, key and value together by stacking these vectors in a matrix form.
sent55: Mathematically, it is given by (7).where Q, K, V are matrices which represent Query, Key and Value, respectively.
sent56: Positional information is added to the input sequence to generate the input embedding upon which the attention will be performed.
sent57: Instead of directly applying attention on input embeddings, they are linearly projected to d k and d v dimensional vectors using learned projections given by (8) where W q ∈ R d model ×d k , W k ∈ R d model ×d k and W v ∈ R d model ×dv are trainable parameters.
sent58: d model is the dimension of input embeddings.
sent59: X is the input embedding for the encoder section and the output embedding for the masked multi-head block for the decoder section.
sent60: For the second multi-head block of the decoder section, X is the encoder output for k and v projection.
sent61: However, for q projection, X is the output from the masked multi-head section.
sent62: In Transformer network [20], the attention mechanism have been used in three different ways.
sent63: They are as follows. 1) Encoder self-attention: In the encoder section, attention mechanism is applied over the input sequences to find the similarity of each token of a sequence with rest of the tokens.
sent64: 2) Decoder masked self-attention: Similar to the encoder self-attention, output (target) sequence tokens attend each other in this stage.
sent65: However, instead of accessing the entire output sequence at a time, the decoder can only access the tokens preceding the token which decoder attempts to predict.
sent66: This is done by masking current and all the future tokens of a particular decoder time step.
sent67: This approach prevents the training phase to be biased.
sent68: 3) Encoder-decoder attention: This occurs at the decoder section after decoder masked self-attention stage.
sent69: With reference to (7), at this stage, Q is the linear projection of the vector coming from decoder's masked self-attention block.
sent70: Whereas, K and V are obtained by linearly projecting the vector resulting from encoder self-attention block.
sent71: This is the stage where the mapping between input and output (target) sequences happens.
sent72: The output of this block is the attention vectors containing the relationship between tokens of input and output sequences.
sent73: At each sub-layer, the attention is performed h-times in parallel.
sent74: Hence, the name ""multi-head attention"" is given.
sent75: In [20], the value of h is 8. According to the authors, multihead attention allows the model to jointly attend to information from different representation subspaces at different positions.
sent76: The outputs from each attention head are then concatenated and projected using (9) to obtain the final output of the corresponding sub-layer.
sent77: where head i∈ [1,h] is computed using (8) and W o ∈ R hdv×d model is a trainable parameter."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s2,Source(s) Target,"Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011) Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b) Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article * 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection. #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds. * the article's body is summarised. Sources: Twitter, ǌ News, ɀikipedia, Reddit. Evidence: Single, Multiple, Thread.

2 What is Stance?

In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process. Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition. Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc. Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither. Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016). Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).

Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text.","[[None, 'b12', 'b17', 'b57'], [], [None], []]","[[None, 'b12', 'b17', 'b57'], [], [None], []]",5,"sent1: Context Evidence #Instances Task English Datasets Rumour Has It (Qazvinian et al., 2011)
sent2: Topic Tweet 10K Rumours PHEME (Zubiaga et al., 2016b)
sent3: Claim Tweet 4.5K Rumours Emergent (Ferreira and Vlachos, 2016) ǌ Headline Article *
sent4: 2.6K Rumours FNC-1 (Pomerleau and Rao, 2017) ǌ Headline Article 75K Fake news RumourEval '
sent5: 17 (Derczynski et al., 2017) Implicit 1 Tweet 7.1K Rumours FEVER  ɀ  Table 1: Key characteristics of stance detection datasets for mis-and disinformation detection.
sent6: #Instances denotes dataset size as a whole; the numbers are in thousands (K) and are rounded to the hundreds.
sent7: * the article's body is summarised.
sent8: Sources: Twitter, ǌ News, ɀikipedia, Reddit.
sent9: Evidence: Single, Multiple, Thread.
sent10: 2 What is Stance? In order to understand the task of stance detection, we first provide definitions of stance and the stance-taking process.
sent11: Biber and Finegan (1988) define stance as the expression of a speaker's standpoint and judgement towards a given proposition.
sent12: Further, Du Bois (2007)) define stance as ""a public act by a social actor, achieved dialogically through overt communicative means, of simultaneously evaluating objects, positioning subjects (self and others), and aligning with other subjects, with respect to any salient dimension of the sociocultural field"", showing that the stance-taking process is affected not only by personal opinions, but also by other external factors such as cultural norms, roles in the institution of the family, etc.
sent13: Here, we adopt the general definition of stance detection by Küçük and Can (2020): ""for an input in the form of a piece of text and a target pair, stance detection is a classification problem where the stance of the author of the text is sought in the form of a category label from this set: Favor, Against, Neither.
sent14: Occasionally, the category label of Neutral is also added to the set of stance categories (Mohammad et al., 2016), and the target may or may not be explicitly mentioned in the text"" (Augenstein et al., 2016;Mohammad et al., 2016).
sent15: Note that the stance detection definitions and the label inventories vary somewhat, depending on the target application (see Section 3).
sent16: Finally, stance detection can be distinguished from several other closely related NLP tasks: (i) biased language detection, where the existence of an inclination or tendency towards a particular perspective within a text is explored, (ii) emotion recognition, where the goal is to recognise emotions such as love, anger, etc. in the text, (iii) perspective identification, which aims to find the pointof-view of the author (e.g., Democrat vs. Republican) and the target is always explicit, (iv) sarcasm detection, where the interest is in satirical or ironic pieces of text, often written with the intent of ridicule or mockery, and (v) sentiment analysis, which checks the polarity of a piece of text."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s6,Multiple languages,"In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.

Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1. One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance. Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead. In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a). Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help. Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful. All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance. In contrast, Unrelated is detected almost perfectly by all models (over 99 F1). Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general. This can be attributed to the use of n-grams, topic models, and lexica.

Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents. Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t. the target claim.

More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.

Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help. In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree. The success of these models is also seen in cross-lingual settings. For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT. Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets. Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).

Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER . To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted. The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021). Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score. Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points. Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70. Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes. Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information. Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.

Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021). Such models do not require a retrieval step, as they use the knowledge stored in language models. However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"". Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type. Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).

Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer."" vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club."", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers."" (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm."" is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.

Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.

These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours). A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017). Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets. More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020). Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1). Li et al. (2020) deviated from this structure and modelled the conversations as a graph. Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection. Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task. Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification . Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).

A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0. Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1. Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class. On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).

Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.

Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness. However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.

Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance. Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models. They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets. Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.  They showed that MDL helps for low-resource and substantively for full-resource scenarios. Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance. Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1. The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc. Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets.","[[], [None, 'b19'], [], ['b31'], [None], ['b43', 'b52', 'b8', 'b50', 'b51', 'b47', None, 'b30'], [None], [], [None, 'b57'], [None, 'b36', 'b49', 'b55'], [None, 'b49', 'b57'], [None, 'b38'], [], ['b25', None, 'b24']]","[[], [None, 'b19'], [], ['b31'], [None], ['b43', 'b52', 'b8', 'b50', 'b51', 'b47', None, 'b30'], [None], [], [None, 'b57'], [None, 'b36', 'b49', 'b55'], [None, 'b49', 'b57'], [None, 'b38'], [], ['b25', None, 'b24']]",27,"sent1: In this section, we discuss various ways to use stance detection for mis-and disinformation detection, and list the state-of-the-art results in Table 2.Fact-Checking as Stance Detection Here, we discuss approaches for stance detection in the context of mis-and disinformation detection, where veracity is modelled as stance detection as outlined in Section 3.1.
sent2: One such line of research is the Fake News Challenge, which used weighted accuracy as an evaluation measure (FNC score), to mitigate the impact of class imbalance.
sent3: Subsequently, Hanselowski et al. (2018a) criticized the FNC score and F1-micro, and argued in favour of F1-macro (F1) instead.
sent4: In the competition, most teams used hand-crafted features such as words, word embeddings, and sentiment lexica (Riedel et al., 2017;Hanselowski et al., 2018a).
sent5: Hanselowski et al. (2018a) showed that the most important group of features were the lexical ones, followed by features from topic models, while sentiment analysis did not help.
sent6: Ghanem et al. (2018) investigated the importance of lexical cues, and found that report and negation are most beneficial, while knowledge and denial are least useful.
sent7: All these models struggle to learn the Disagree class, achieving up to 18 F1 due to major class imbalance.
sent8: In contrast, Unrelated is detected almost perfectly by all models (over 99 F1).
sent9: Hanselowski et al. (2018a) showed that these models exploit the lexical overlap between the headline and the document, but fail when there is a need to model semantic relations or complex negation, or to understand propositional content in general.
sent10: This can be attributed to the use of n-grams, topic models, and lexica.
sent11: Mohtarami et al. (2018) investigated memory networks, aiming to mitigate the impact of irrelevant and noisy information by learning a similarity matrix and a stance filtering component, and taking a step towards explaining the stance of a given claim by extracting meaningful snippets from evidence documents.
sent12: Like previous work, their model performs poorly on the Agree/Disagree classes, due to the unsupervised way of training the memory networks, i.e., there are no gold snippets justifying the document's stance w.r.t.
sent13: the target claim. More recently, transfer learning with pre-trained Transformers has been explored (Slovikovskaya and Attardi, 2020), significantly improving the performance of previous state-of-the-art approaches.
sent14: Guderlei and Aßenmacher (2020) showed the most important hyper-parameter to be learning rate, while freezing layers did not help.
sent15: In particular, using the pre-trained Transformer RoBERTa improved F1 from 18 to 58 for Disagree, and from 50 to 70 for Agree.
sent16: The success of these models is also seen in cross-lingual settings.
sent17: For Arabic, Khouja (2020) achieved 76.7 F1 for stance detection on the ANS dataset using mBERT.
sent18: Similarly, Hardalov et al. (2022) applied pattern-exploiting training (PET) with sentiment pre-training in a cross-lingual setting showing sizeable improvements on 15 datasets.
sent19: Alhindi et al. (2021) showed that language-specific pre-training was pivotal, outperforming the state of the art on AraStance (52 F1) and Arabic FC (78 F1).
sent20: Some formulations include an extra step for evidence retrieval, e.g., retrieving Wikipedia snippets for FEVER .
sent21: To evaluate the whole fact-checking pipeline, they introduced the FEVER score -the proportion of claims for which both correct evidence is returned and a correct label is predicted.
sent22: The top systems that participated in the FEVER competition Hanselowski et al. More recent approaches used bi-directional attention (Li et al., 2018), a GPT language model (Malon, 2018;, and graph neural networks (Zhou et al., 2019;Atanasov et al., 2019;Liu et al., 2020b;Zhong et al., 2020;Weinzierl et al., 2021;Si et al., 2021).
sent23: Zhou et al. (2019) showed that adding graph networks on top of BERT can improve performance, reaching 67.1 FEVER score.
sent24: Yet, the retrieval model is also important, e.g., using the gold evidence set adds 1.4 points.
sent25: Liu et al. (2020b); Zhong et al. (2020) replaced the retrieval model with a BERT-based one, in addition to using an improved mechanism to propagate the information between nodes in the graph, boosting the score to 70.
sent26: Recently, Ye et al. (2020) experimented with a retriever that incorporates co-reference in distantsupervised pre-training, namely, CorefRoBERTa.  added external knowledge to build a contextualized semantic graph, setting a new SOTA on Snopes.
sent27: Si et al. (2021) and Ostrowski et al. (2021) improved multi-hop reasoning using a model with eXtra Hop attention (Zhao et al., 2020), a capsule network aggregation layer, and LDA topic information.
sent28: Atanasova et al. (2022) introduced the task of evidence sufficiency prediction to more reliably predict the NOT ENOUGH INFO class.
sent29: Another notable idea is to use pre-trained language models as fact-checkers based on a masked language modelling objective (Lee et al., 2020), or to use the perplexity of the entire claim with respect to the target document (Lee et al., 2021).
sent30: Such models do not require a retrieval step, as they use the knowledge stored in language models.
sent31: However, they are prone to biases in the patterns used, e.g., they can predict date instead of city/country and vice-versa when using ""born in/on"".
sent32: Moreover, the insufficient context can seriously confuse them, e.g., for short claims with uncommon words such as ""Sarawak is a ..."", where it is hard to detect the entity type.
sent33: Finally, the performance of such models remains well below supervised approaches; even though recent work shows that few-shot training can improve results (Lee et al., 2021).
sent34: Error analysis suggests that the main challenges are (i) confusing semantics at the sentence level, e.g., ""Andrea Pirlo is an American professional footballer.""
sent35: vs. ""Andrea Pirlo is an Italian professional footballer who plays for an American club.
sent36: "", (ii) sensitivity to spelling errors, (iii) lack of relation between the article and the entities in the claim, (vi) dependence on syntactic overlaps, e.g., ""Terry Crews played on the Los Angeles Chargers.""
sent37: (NotE-noughInfo) is classified as refuted, given the sentence ""In football, Crews played ... for the Los Angeles Rams, San Diego Chargers and Washington Redskins, ..."", (v) embedding-level confusion, e.g., numbers tend to have similar embeddings, ""The heart beats at a resting rate close to 22 bpm.""
sent38: is not classified as refuted based on the evidence sentence ""The heart beats at a resting rate close to 72 bpm."", and similarly for months.
sent39: Threaded Stance In the setting of conversational threads (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), in contrast to the single-task setup, which ignores or does not provide further context, important knowledge can be gained from the structure of user interactions.
sent40: These approaches are mostly applied as part of a larger system, e.g., for detecting and debunking rumours (see Section 3.2, Rumours).
sent41: A common pattern is to use tree-like structured models, fed with lexicon-based content formatting (Zubiaga et al., 2016a) or dictionary-based token scores (Aker et al., 2017).
sent42: Kumar and Carley (2019) replaced CRFs with Binarised Constituency Tree LSTMs, and used pre-trained embeddings to encode the tweets.
sent43: More recently, Tree (Ma and Gao, 2020) and Hierarchical (Yu et al., 2020) Transformers were proposed, which combine post-and threadlevel representations for rumour debunking, improving previous results on RumourEval '17 (Yu et al., 2020).
sent44: Kochkina et al. (2017Kochkina et al. ( , 2018 split conversations into branches, modelling each branch with branched-LSTM and hand-crafted features, outperforming other systems at RumourEval '17 on stance detection (43.4 F1).
sent45: Li et al. (2020) deviated from this structure and modelled the conversations as a graph.
sent46: Tian et al. (2020) showed that pre-training on stance data yielded better representations for threaded tweets for downstream rumour detection.
sent47: Yang et al. (2019) took a step further and curated per-class pre-training data by adapting examples, not only from stance datasets, but also from tasks such as question answering, achieving the highest F1 (57.9) on the RumourEval '19 stance detection task.
sent48: Li et al. (2019a,b) additionally incorporated user credibility information, conversation structure, and other content-related features to predict the rumour veracity, ranking 3rd on stance detection and 1st on veracity classification .
sent49: Finally, the stance of a post might not be expressed directly towards the root of the thread, thus the preceding posts must be also taken into account (Gorrell et al., 2019).
sent50: A major challenge for all rumour detection datasets is the class distribution (Zubiaga et al., 2016b;Derczynski et al., 2017;Gorrell et al., 2019), e.g., the minority class denying is extremely hard for models to learn, as even for strong systems such as Kochkina et al. (2017) the F1 for it is 0.
sent51: Label semantics also appears to play a role as the querying label has a similar distribution, but much higher F1.
sent52: Yet another factor is thread depth, as performance drops significant at higher depth, especially for the supporting class.
sent53: On the positive side, using multitask learning and incorporating stance detection labels into veracity detection yields a huge boost in performance (Gorrell et al., 2019;Yu et al., 2020).
sent54: Another factor, which goes hand in hand with the threaded structure, is the temporal dimension of posts in a thread (Lukasik et al., 2016;Veyseh et al., 2017;Dungs et al., 2018;. In-depth data analysis (Zubiaga et al. (2016a,b); Kochkina et al. (2017); ; Ma and Gao (2020); Li et al. (2020); among others) shows interesting patterns along the temporal dimension: (i) source tweets (at zero depth) usually support the rumour and models often learn to detect that, (ii) it takes time for denying tweets to emerge, afterwards for false rumors their number increases quite substantially, (iii) the proportion of querying tweets towards unverified rumors also shows an upward trend over time, but their overall number decreases.
sent55: Multi-Dataset Learning (MDL) Mixing data from different domains and sources can improve robustness.
sent56: However, setups that combine mis-and disinformation identification with stance detection, outlined in Section 3, vary in their annotation and labelling schemes, which poses many challenges.
sent57: Earlier approaches focused on pre-training models on multiple tasks, e.g., Fang et al. (2019) achieved state-of-the-art results on FNC-1 by finetuning on multiple tasks such as question answering, natural language inference, etc., which are weakly related to stance.
sent58: Recently, Schiller et al. (2021) proposed a benchmark to evaluate the robustness of stance detection models.
sent59: They leveraged a pre-trained multi-task deep neural network, MT-DNN (Liu et al., 2019), and continued its training on all datasets simultaneously using multitask learning, showing sizeable improvements over models trained on individual datasets.
sent60: Hardalov et al. (2021) (Schick and Schütze, 2021) in a cross-lingual setting, combining datasets with different label inventories by modelling the task as a cloze question answering one.
sent61: They showed that MDL helps for low-resource and substantively for full-resource scenarios.
sent62: Moreover, transferring knowledge from English stance datasets and noisily generated sentiment-based stance data can further boost performance.
sent63: Table 2 shows the state-of-theart (SOTA) results for each dataset discussed in Section 3 and Table 1.
sent64: The datasets vary in their task formulation and composition in terms of size, number of classes, class imbalance, topics, evaluation measures, etc.
sent65: Each of these factors impacts the performance, leading to sizable differences in the final score, as discussed in Section 4, and hence rendering the reported results hard to compare directly across these datasets."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s9,Shades of Truth,"The notion of shades of truth is important in mis-and disinformation detection. For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false. We believe that such shades could be applied to stance and used in a larger pipeline. In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).

Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance. As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance. Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022.","[['b20', 'b3', 'b9', 'b18'], ['b25', None]]","[['b20', 'b3', 'b9', 'b18'], ['b25', None]]",6,"sent1: The notion of shades of truth is important in mis-and disinformation detection.
sent2: For example, fact-checking often goes beyond binary true/false labels, e.g., Nakov et al. (2018) used a third category half-true, Rashkin et al. (2017) included mixed and no factual evidence, and Wang (2017); Santia and Williams (2018) adopted an even finer-grained schema with six labels, including barely true and utterly false.
sent3: We believe that such shades could be applied to stance and used in a larger pipeline.
sent4: In fact, fine-grained labels are common for the related task of Sentiment Analysis (Pang and Lee, 2005;Rosenthal et al., 2017).
sent5: Label Semantics As research in stance detection has evolved, so has the definition of the task and the label inventories, but they still do not capture the strength of the expressed stance.
sent6: As shown in Section 3 (also Appendix 2, labels can vary based on the use case and the setting they are used in. Most researchers have adopted a variant of the Favour, Against, and Neither labels, or an extended schema such as (S)upport, (Q)uery, (D)eny, and (C)omment (Mohammad et al., 2016), but that is not enough to accurately assess stance.
sent7: Moreover, adding label granularity can further improve the transfer between datasets, as the stance labels already share some semantic similarities, but there can be mismatches in the label definitions (Schiller et al., 2021;Hardalov et al., 2021Hardalov et al., , 2022."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s10,Explainability,"The ability for a model to be able to explain its decisions is getting increasingly important, especially for mis-and disinformation detection, as one could argue that it is a crucial step towards adopting fully automated fact-checking. The FEVER 2.0 task formulation (Thorne et al., 2019) can be viewed as a step towards obtaining such explanations, e.g., there have been efforts to identify adversarial triggers that offer explanations for the vulnerabilities at the model level (Atanasova et al., 2020b). However, FEVER is artificially created and is limited to Wikipedia, which may not reflect real-world settings. To mitigate this, explanation by professional journalists can be found on fact-checking websites, and can be further combined with stance detection in an automated system. In a step in this direction, Atanasova et al. (2020a) generated natural language explanations for claims from PolitiFact 4 given gold evidence document summaries by journalists.

Moreover, partial explanations can be obtained automatically from the underlying models, e.g., from memory networks (Mohtarami et al., 2018), attention weights (Zhou et al., 2019;Liu et al., 2020b), or topic relations (Si et al., 2021). However, such approaches are limited as they can require gold snippets justifying the document's stance, attention weights can be misleading (Jain and Wallace, 2019), and topics might be noisy due to their unsupervised nature. Other existing systems (Popat et al., 2017(Popat et al., , 2018Nadeem et al., 2019) offer explanations to a more limited extent, highlighting span overlaps between the target text and the evidence documents. Overall, there is a need for holistic and realistic explanations of how a factchecking model arrived at its prediction.

Integration People question false information more and tend to confirm true information (Mendoza et al., 2010). Thus, stance can play a vital role in verifying dubious content. In Appendix C, we discuss existing systems and real-world applications of stance for mis-and disinformation identification in more detail. However, we argue that a tighter integration between stance and factchecking is needed. Stance can be expressed in different forms, e.g., tweets, news articles, user posts, sentences in Wikipedia, and Wiki tables, among others and can have different formulations as part of the fact-checking pipeline (see Section 3). All these can guide human fact-checkers through the process of fact-checking, and can point them to relevant evidence. Moreover, the wisdom of the crowd can be a powerful instrument in the fight against mis-and disinformation (Pennycook and Rand, 2019), but we should note that vocal minorities can derail public discourse (Scannell et al., 2021). Nevertheless, these risks can be mitigated by taking into account the credibility of the user or of the information source, which can be done automatically or with the help of human fact-checkers.","[[None], ['b14', 'b52', 'b1', None, 'b30', 'b13'], [None, 'b10', 'b23']]","[[None], ['b14', 'b52', 'b1', None, 'b30', 'b13'], [None, 'b10', 'b23']]",10,"sent1: The ability for a model to be able to explain its decisions is getting increasingly important, especially for mis-and disinformation detection, as one could argue that it is a crucial step towards adopting fully automated fact-checking.
sent2: The FEVER 2.0 task formulation (Thorne et al., 2019) can be viewed as a step towards obtaining such explanations, e.g., there have been efforts to identify adversarial triggers that offer explanations for the vulnerabilities at the model level (Atanasova et al., 2020b).
sent3: However, FEVER is artificially created and is limited to Wikipedia, which may not reflect real-world settings.
sent4: To mitigate this, explanation by professional journalists can be found on fact-checking websites, and can be further combined with stance detection in an automated system.
sent5: In a step in this direction, Atanasova et al. (2020a) generated natural language explanations for claims from PolitiFact 4 given gold evidence document summaries by journalists.
sent6: Moreover, partial explanations can be obtained automatically from the underlying models, e.g., from memory networks (Mohtarami et al., 2018), attention weights (Zhou et al., 2019;Liu et al., 2020b), or topic relations (Si et al., 2021).
sent7: However, such approaches are limited as they can require gold snippets justifying the document's stance, attention weights can be misleading (Jain and Wallace, 2019), and topics might be noisy due to their unsupervised nature.
sent8: Other existing systems (Popat et al., 2017(Popat et al., , 2018Nadeem et al., 2019) offer explanations to a more limited extent, highlighting span overlaps between the target text and the evidence documents.
sent9: Overall, there is a need for holistic and realistic explanations of how a factchecking model arrived at its prediction.
sent10: Integration People question false information more and tend to confirm true information (Mendoza et al., 2010).
sent11: Thus, stance can play a vital role in verifying dubious content.
sent12: In Appendix C, we discuss existing systems and real-world applications of stance for mis-and disinformation identification in more detail.
sent13: However, we argue that a tighter integration between stance and factchecking is needed.
sent14: Stance can be expressed in different forms, e.g., tweets, news articles, user posts, sentences in Wikipedia, and Wiki tables, among others and can have different formulations as part of the fact-checking pipeline (see Section 3).
sent15: All these can guide human fact-checkers through the process of fact-checking, and can point them to relevant evidence.
sent16: Moreover, the wisdom of the crowd can be a powerful instrument in the fight against mis-and disinformation (Pennycook and Rand, 2019), but we should note that vocal minorities can derail public discourse (Scannell et al., 2021).
sent17: Nevertheless, these risks can be mitigated by taking into account the credibility of the user or of the information source, which can be done automatically or with the help of human fact-checkers."
232075945,A Survey on Stance Detection for Mis-and Disinformation Identification,Computer Science,https://www.semanticscholar.org/paper/14ba97c7e4c7d370965333ecf3835e514c664106,s14,B Additional Formulations of Stance as a Component for Fact-Checking,"Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline. Below, we describe some work that follows these formulations.

Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia. They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions. A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020). This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.

Media Profiling Stance detection has also been used for media profiling. Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics. They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020). This is an important step towards understanding media biases. Tweet: Wow, that is fascinating! I hope you never mock our proud Scandi heritage again.

(b) Examples from Qazvinian et al. (2011) andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992. ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.

(c) Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how do you know its an ISIS flag? Can you actually confirm that? ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table 3: Illustrative examples for different stance detection scenarios included in our survey. We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).

The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.

More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information. In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.

There is a well-known connection between factuality and bias. 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.

User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user. In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017).","[[], [None], ['b32'], [None, 'b17'], [], ['b14', 'b13', None, 'b0'], ['b16'], ['b15'], [None]]","[[], [None], ['b32'], [None, 'b17'], [], ['b14', 'b13', None, 'b0'], ['b16'], ['b15'], [None]]",11,"sent1: Beyond the approaches that we outlined in Section 3.2, stance has also been used for detecting misconceptions and for profiling media sources as part of a fact-checking pipeline.
sent2: Below, we describe some work that follows these formulations.
sent3: Misconceptions Hossain et al. (2020) focused on detecting misinformation related to COVID-19, based on known misconceptions listed in Wikipedia.
sent4: They evaluated the veracity of a tweet depending on whether it agrees, disagrees, or has no stance with respect to a set of misconceptions.
sent5: A related formulation of the task is detecting previously fact-checked claims (Shaar et al., 2020).
sent6: This allows to assess the veracity of dubious content by evaluating the stance of a claim regarding already checked stories, known misconceptions, and facts.
sent7: Media Profiling Stance detection has also been used for media profiling.
sent8: Stefanov et al. (2020) explored the feasibility of an unsupervised approach for identifying the political leanings (left, center, or right bias) of media outlets and influential people on Twitter based on their stance on controversial topics.
sent9: They built clusters of users around core vocal ones based on their behaviour on Twitter such as retweeting, using the procedure proposed by Darwish et al. (2020).
sent10: This is an important step towards understanding media biases.
sent11: Tweet: Wow, that is fascinating!
sent12: I hope you never mock our proud Scandi heritage again.
sent13: (b) Examples from Qazvinian et al. (2011)
sent14: andDerczynski et al. (2017) Claim: The Rodney King riots took place in the most populous county in the USA. ɀiki Evidence 1: The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992.
sent15: ɀiki Evidence 2: Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA.(c)
sent16: Example from  Headline: Jess Smith of Chatham, Kent was the smiling sun baby in the Teletubbies TV show ǌ
sent17: Summary 1: Canterbury Christ Church University student Jess Smith, from Chatham, starred as Teletubbies sun ǌ
sent18: Summary 2: This College Student Claims She Was The Teletubbies Sun Baby (d) Example from Ferreira and Vlachos (2016) u1: We understand that there are two gunmen and up to a dozen hostages inside the cafe under siege at Sydney.. ISIS flags remain on display #7News u2: @u1 not ISIS flags u3: @u1 sorry -how
sent19: do you know its an ISIS flag? Can you actually confirm that?
sent20: ɳ u4: @u3 no she cant cos its actually not u5: @u1 More on situation at Martin Place in Sydney, AU LINK u6: @u1 Have you actually confirmed its an ISIS flag or are you talking shit ɳ  Table
sent21: 3: Illustrative examples for different stance detection scenarios included in our survey.
sent22: We annotate the expressed stance with (support, for), (deny, against), ɳ (query), and (comment).
sent23: The reliability of entire news media sources has been automatically rated based on their stance with respect to manually fact-checked claims, without access to gold labels for the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015;Popat et al., 2017Popat et al., , 2018. The assumption in such methods is that reliable media agree with true claims and disagree with false ones, while for unreliable media, the situation is reversed. The trustworthiness of Web sources has also been studied from a data analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims.
sent24: More recently, Baly et al. (2018a) used gold labels from Media Bias/Fact Check, 5 and a variety of information sources: articles published by the medium, what is said about the medium on Wikipedia, metadata from its Twitter profile, URL structure, and traffic information.
sent25: In follow-up work, Baly et al. (2019) used the same representation to jointly predict a medium's factuality of reporting (high vs. mixed vs. low) and its bias (left vs. center vs. right) on an ordinal scale, in a multi-task ordinal regression setup.  (Nakov et al., 2021) for a recent survey on media profiling.
sent26: There is a well-known connection between factuality and bias.
sent27: 6 For example, hyper-partisanship is often linked to low trustworthiness (Potthast et al., 2018), e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and also more trustworthy.
sent28: User Profiling In the case of social media and community fora, it is important to model the trustworthiness of the user.
sent29: In particular, there has been research on finding opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a), sockpuppets (Maity et al., 2017Kumar et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017)."
232116743,Use of Social Media Data in Disaster Management: A Survey,"Computer Science, Environmental Science",https://www.semanticscholar.org/paper/7595367176192a375585e18593d1a2bd3d5faccc,s16,Spatial,"Although spatial representations of social media data, such as geo-location, play an essential part in social-media-based event detection or event analysis, there are few social media data that provide information about users' locations [34]. Furthermore, there is a variety of location information represented in social media, ranging from a very precise location using geographic coordinates (e.g., longitude and latitude) to a very fuzzy location using descriptive language (e.g., city name).

Geographical information is represented in several ways on social media, as shown in   The approach can be applied to disseminate disaster early warning messages to social media users whose home locations are at risk [36] spatial pattern analysis spatial pattern analysis -

The analysis of patterns between the distance from the epicentre and time using social media can enhance situational awareness. However, the proposed method requires a sufficient number of tweets to avoid data quality issues.

[37] spatial pattern analysis spatial pattern analysis spatial pattern analysis This approach combines social media and sensor data for statistical analysis, which provides a more precise assessment of tweets' spatial patterns in a hazardous area.

[38]

Kernel density estimation (KDE) -based clustering for social media analysis from Hurricane Sandy KDE-based clustering for social media analysis from Hurricane Sandy KDE-based clustering for social media analysis from Hurricane Sandy

Associating social media data with hurricane damage data for spatial pattern analysis enabled qualified assessment of rapid damage.

[3] -Hot-spot detection Hot-spot detection

The proposed method for spatial analysis enables timely decision-making for emergency response and full awareness of public concern.

Several approaches have been proposed in the literature to address the issues of spatial representation in social media. Here, Table 4 presents the state-of-the-art methods for identifying and analysing the spatial information of social media data.

Geo-location identification: The research of [34] shows that 0.42% of all tweets use the latitude and longitude function to tag their geo-location, and out of 1 million Twitter users, only 26% have listed a city name. In most cases, general expressions (such as California) or nonsensical expressions (such as wonderland) are used. This research aimed to detect the locations of tweets that do not clearly mention geographic information. To this end, the authors proposed a method of computing the probability that a word is linked to a city. In order to improve the accuracy, the authors introduced a model of spatial variation for analysing the geographic distribution of words in tweets. The authors of [35] mentioned that geo-location information in tweet data may have noisy signals. For example, a user in the UK could tweet about a Houston Rockets game or their vacation in India. To overcome this, the authors integrate two types of signals (user's friend and user's tweet's nearby location) from social networks to predict a user's location.

Geo-location analytics: The authors of [36] discussed the effect of an earthquake on the East Coast of the United States (US) on August 23, 2011 by analysing the collected tweet data. The main finding of the paper was the patterns between the distance from the epicentre and the time after the earthquake. The authors of [37] used sensor data to identify flood-affected regions. The authors performed some statistical analyses of the collected data to find the general spatial patterns and to explore the differences between the spatial patterns among the relevant tweets. On the other hand, methods such as kernel density estimation (KDE) have been widely used for clustering of the activities during Hurricane Sandy [38] and spatial hotspot detection during the 2012 Beijing rainstorm [3].","[['b33'], ['b35'], [], [], [], [], [], [], [], [], ['b34', 'b33'], ['b36', 'b35', 'b2', 'b37']]","[['b33'], ['b35'], [], [], [], [], [], [], [], [], ['b34', 'b33'], ['b36', 'b35', 'b2', 'b37']]",8,"sent1: Although spatial representations of social media data, such as geo-location, play an essential part in social-media-based event detection or event analysis, there are few social media data that provide information about users' locations [34].
sent2: Furthermore, there is a variety of location information represented in social media, ranging from a very precise location using geographic coordinates (e.g., longitude and latitude) to a very fuzzy location using descriptive language (e.g., city name).
sent3: Geographical information is represented in several ways on social media, as shown in   The approach can be applied to disseminate disaster early warning messages to social media users whose home locations are at risk [36] spatial pattern analysis spatial pattern analysis -
sent4: The analysis of patterns between the distance from the epicentre and time using social media can enhance situational awareness.
sent5: However, the proposed method requires a sufficient number of tweets to avoid data quality issues.
sent6: [37] spatial pattern analysis spatial pattern analysis spatial pattern analysis This approach combines social media and sensor data for statistical analysis, which provides a more precise assessment of tweets' spatial patterns in a hazardous area.
sent7: [38]Kernel density estimation (KDE) -based clustering for social media analysis from Hurricane Sandy KDE-based clustering for social media analysis from Hurricane Sandy KDE-based clustering for social media analysis from Hurricane SandyAssociating social media data with hurricane damage data for spatial pattern analysis enabled qualified assessment of rapid damage.
sent8: [3] -Hot-spot detection Hot-spot detectionThe proposed method for spatial analysis enables timely decision-making for emergency response and full awareness of public concern.
sent9: Several approaches have been proposed in the literature to address the issues of spatial representation in social media.
sent10: Here, Table 4 presents the state-of-the-art methods for identifying and analysing the spatial information of social media data.
sent11: Geo-location identification: The research of [34] shows that 0.42% of all tweets use the latitude and longitude function to tag their geo-location, and out of 1 million Twitter users, only 26% have listed a city name.
sent12: In most cases, general expressions (such as California) or nonsensical expressions (such as wonderland) are used.
sent13: This research aimed to detect the locations of tweets that do not clearly mention geographic information.
sent14: To this end, the authors proposed a method of computing the probability that a word is linked to a city.
sent15: In order to improve the accuracy, the authors introduced a model of spatial variation for analysing the geographic distribution of words in tweets.
sent16: The authors of [35] mentioned that geo-location information in tweet data may have noisy signals.
sent17: For example, a user in the UK could tweet about a Houston Rockets game or their vacation in India.
sent18: To overcome this, the authors integrate two types of signals (user's friend and user's tweet's nearby location) from social networks to predict a user's location.
sent19: Geo-location analytics: The authors of [36] discussed the effect of an earthquake on the East Coast of the United States (US) on August 23, 2011 by analysing the collected tweet data.
sent20: The main finding of the paper was the patterns between the distance from the epicentre and the time after the earthquake.
sent21: The authors of [37] used sensor data to identify flood-affected regions.
sent22: The authors performed some statistical analyses of the collected data to find the general spatial patterns and to explore the differences between the spatial patterns among the relevant tweets.
sent23: On the other hand, methods such as kernel density estimation (KDE) have been widely used for clustering of the activities during Hurricane Sandy [38] and spatial hotspot detection during the 2012 Beijing rainstorm [3]."
232116743,Use of Social Media Data in Disaster Management: A Survey,"Computer Science, Environmental Science",https://www.semanticscholar.org/paper/7595367176192a375585e18593d1a2bd3d5faccc,s12,Social Media Platforms,"Generally, social media data are directly accessible on social media platforms (e.g., Facebook, Twitter, and Instagram). These platforms are considered as major data sources for social media data analytics in disaster management. Most social media platforms provide HTTP-based Application Programming Interfaces (APIs) for data consumers to access their social media services (e.g., data service and analytics services). Data consumers can use their tools to communicate with the respective APIs to collect and store social media data for their purposes [28]. For example, Twitter provides search APIs that enable consumers to find historical or real-time data by using keywords or hashtags. Much research on using social media data for disaster management utilises such APIs to access social media data directly from the social media platforms [22][23][24][25]. Due to the unstructured characteristics of social media data and the indeterminacy of the sources, the quality and trustworthiness of the collected social media data become significant issues [20]. Based on this, additional processes (e.g., data filtering, data classification, and data extraction) for data preparation are required. Due to privacy concerns, some of the social media platforms (e.g., Facebook and Twitter) have put several restrictions on data access.","[['b27', 'b22', 'b21', 'b23', 'b19', 'b24']]","[['b27', 'b22', 'b21', 'b23', 'b19', 'b24']]",6,"sent1: Generally, social media data are directly accessible on social media platforms (e.g., Facebook, Twitter, and Instagram).
sent2: These platforms are considered as major data sources for social media data analytics in disaster management.
sent3: Most social media platforms provide HTTP-based Application Programming Interfaces (APIs) for data consumers to access their social media services (e.g., data service and analytics services).
sent4: Data consumers can use their tools to communicate with the respective APIs to collect and store social media data for their purposes [28].
sent5: For example, Twitter provides search APIs that enable consumers to find historical or real-time data by using keywords or hashtags.
sent6: Much research on using social media data for disaster management utilises such APIs to access social media data directly from the social media platforms [22][23][24][25].
sent7: Due to the unstructured characteristics of social media data and the indeterminacy of the sources, the quality and trustworthiness of the collected social media data become significant issues [20].
sent8: Based on this, additional processes (e.g., data filtering, data classification, and data extraction) for data preparation are required.
sent9: Due to privacy concerns, some of the social media platforms (e.g., Facebook and Twitter) have put several restrictions on data access."
232116743,Use of Social Media Data in Disaster Management: A Survey,"Computer Science, Environmental Science",https://www.semanticscholar.org/paper/7595367176192a375585e18593d1a2bd3d5faccc,s13,Third Parties,"Social media data are also collected and organised by organisations and institutions for specific purposes. Due to the benefits of Open Data, some of them have been interested in opening their collected social media data for others [20]. These organisations are considered as alternative data sources for social media data. This section discusses the different methods of accessing social media data. Third parties who provide their collected social media data are considered as alternative data sources for conducting research on social media data analytics for disaster management. This social media data are collected and organised in a specific way to be used for a specific purpose. For example, CrisisLexT26 [26] provides crisis-related tweets during emergency events, which are collected from Twitter by using crisis-specific keywords. CrowdFlower [29] provides the Figure Eight platform for free open datasets. These include tweets relevant to various kinds of disasters. Most social media data collected by third-party data sources are usually prepared using additional processes to provide higher-quality datasets. In addition to using datasets from third-party data sources for disaster management, such datasets can be used as training datasets and for evaluation in data analysis in many studies on disaster management. For example, the authors of [27] utilised datasets from CrisisLexT26 and CrowdFlower as training datasets for identifying disaster-related tweets. Table 2 depicts some existing works that use social media data sources for disaster management. Twitter is a major source of social media data, which can be accessed via the Twitter API. ","[['b25', 'b26', 'b19', 'b28']]","[['b25', 'b26', 'b19', 'b28']]",4,"sent1: Social media data are also collected and organised by organisations and institutions for specific purposes.
sent2: Due to the benefits of Open Data, some of them have been interested in opening their collected social media data for others [20].
sent3: These organisations are considered as alternative data sources for social media data.
sent4: This section discusses the different methods of accessing social media data.
sent5: Third parties who provide their collected social media data are considered as alternative data sources for conducting research on social media data analytics for disaster management.
sent6: This social media data are collected and organised in a specific way to be used for a specific purpose.
sent7: For example, CrisisLexT26 [26] provides crisis-related tweets during emergency events, which are collected from Twitter by using crisis-specific keywords.
sent8: CrowdFlower [29] provides the Figure Eight platform for free open datasets.
sent9: These include tweets relevant to various kinds of disasters.
sent10: Most social media data collected by third-party data sources are usually prepared using additional processes to provide higher-quality datasets.
sent11: In addition to using datasets from third-party data sources for disaster management, such datasets can be used as training datasets and for evaluation in data analysis in many studies on disaster management.
sent12: For example, the authors of [27] utilised datasets from CrisisLexT26 and CrowdFlower as training datasets for identifying disaster-related tweets.
sent13: Table 2 depicts some existing works that use social media data sources for disaster management.
sent14: Twitter is a major source of social media data, which can be accessed via the Twitter API."
232116743,Use of Social Media Data in Disaster Management: A Survey,"Computer Science, Environmental Science",https://www.semanticscholar.org/paper/7595367176192a375585e18593d1a2bd3d5faccc,s17,Temporal,"Most social media applications attach a time stamp to the posted data. The temporal relation between events can be derived from the time stamp of the event and the content. We studied how temporal information is used in the existing systems for event detection. In the context of event detection, we categorised the temporal information into three categories as shown in Figure 5.  • Pre-event: This represents the time period before the occurrence of the event of interest. In general, a social media message that is posted before the event occurrence can be analysed to derive the following information: (i) warnings-e.g., a post about bad weather from the Met-Office before heavy rainfall, a cyclone alert, etc. serves as a warning message for an impending natural disaster, (ii) precursor event detectione.g., a social media post about a leaning electric pole in a location can serve as a precursor for landslide event detection, and (iii) temporal offset-pre-event posts from social media are analysed to determine the offset between the time of the post and the time of the actual event, such as, for instance, the time taken after the leaning pole post and the actual landslide in that locality. Pre-event posts from social media can thus be utilised for serving the mitigation and preparedness phases of emergency management. • Real-time: This represents the time span during which the event is happening. In the real time of the event occurrence, social media may be widely used for information sharing about the incidents related to the event. Generally, the real-time posts from social media during the occurrence of the event can be analysed for: (i) obtaining situational awareness-e.g., ""trains cancelled, schools closed in Kerala due to heavy rains"", or a social media post about ""roadblock due to landslides on NH-8"", (ii) deriving/issuing warnings about the after-effects/impacts of a disaster-e.g., ""high tides are expected in coastal areas after the tremors"", and (iii) response, relief, and recovery-e.g., a tweet during the Kerala flood in 2018: ""shortage of bubble wrap and ready-to-eat items in Sanskrit College Palayam"". • Post-event: This represents the time period after the occurrence of the event of interest. Often, after disasters, social media is widely used to communicate about required supplies, information about missing people, death tolls, property losses, relief operations planned by the government and NGOs, protective measures to be undertaken while returning home, funds donated by various authorities, etc. Thus, the post-event data can generally be analysed for: (i) warnings of further events, (ii) deriving information on the impact of the event, (iii) identifying the relief and recovery measures required, and (iv) determining the temporal offset between the time of the post and time of the actual event.

It is important to analyse the behaviour of the public/communities before, during, and after disasters in order to bring in effective disaster response, management, planning, and mitigation. Since social networks serve as the easiest and most common way to sample public opinion, we can make use of the time-stamped, geo-tagged data from social media for this purpose. Table 5 summarises the temporal the state-of-the-art methods for temporal information analysis. Chae et al. [39] explain the temporal analysis of Twitter data related to hurricane Sandy, wherein they analyse the Twitter user density distribution two weeks before and after the date of the event, as well as for a time period on the day of the event, right after the announcement of the evacuation order. A similar study on the spatiotemporal analysis of Twitter data for the same disaster event was performed by Kryvasheyeu et al. [40], according to whom the persistence of the Twitter activity levels in the time frame immediate to the occurrence of the event (post-event) was a good indicator for determining which areas were likely to need the most assistance. Further, during a disaster, normalised activity levels, rates of original content creation, and rates of content rebroadcasting must be considered to identify the hardest-hit areas in real time. The number of tweets during the Christchurch, New Zealand earthquakes were analysed over time in a window of five minutes in [41]. The analysis indicated that when an earthquake with a magnitude of 4.2 or stronger occurred at a particular time, it correlated with a spike in the number of tweets over that time frame.

Another crucial factor to be considered while choosing the time frame for social media data collection is the type of disaster. For disasters like landslides, floods, and storms, we may be able to capture some of the warning signs for these events from social media posts before the actual occurrence of these events, whereas for other events, such as wildfires and earthquakes, the posts relevant or related to them may surface only after the occurrence of these events. Wang et al. [42] analysed wildfire-related tweets of some of the major wildfires that occurred in San Diego County, USA with respect to space, time, content, and network by collecting Twitter data from the day when the first wildfire occurred until the date when most of these wildfires were 100% contained. The temporal evolution of wildfire-related tweets obtained using different keywords, with and without the location, gave an insight into the time lag taken for the spreading of the information. Furthermore, as Granell and Ostermann mentioned in [43], the durations of the impacts of these events also affect the temporal and contextual variation in the data related to these events. For instance, real-time and post-event data can be utilised for disaster response and recovery, whereas pre-event data can be utilised for preparedness and planning. A case study to analyse the social media text during and after the 2012 Beijing rainstorm was described in [3], where the authors performed time-series decomposition of the data to identify the overall trend and variations with respect to different developmental stages of the event, as well as the cyclical trends of microblogging activity. They concluded that the trend analysis of text streams for different topics over time corresponded well with different development stages of the event. For example, texts related to the event increased in the week after the rainstorm, following which they began to slowly subside, and, finally, faded out. The classification of these social media messages into different contextual categories and their analysis over time help to identify the transition between various phases of disaster management and support effective decision-making for disaster preparedness, response, and recovery. The authors of [44] present a classifier based on logistic regression that automatically classifies the gathered social media data into various topic categories during various disaster phases and classifies the temporal trends of these topic categories in different phases. The experimentation using tweets related to Hurricane Sandy revealed that: (i) Tweets regarding preparedness reached their peak on the day before the event when the emergency declaration was issued, (ii) a large proportion of tweets related to impact were observed within a few days of the event's occurrence, and (iii) the largest peak of tweets related to disaster recovery was observed five days after the event.","[[], ['b39', 'b38', 'b40'], ['b43', 'b41', 'b2', 'b42']]","[[], ['b39', 'b38', 'b40'], ['b43', 'b41', 'b2', 'b42']]",7,"sent1: Most social media applications attach a time stamp to the posted data.
sent2: The temporal relation between events can be derived from the time stamp of the event and the content.
sent3: We studied how temporal information is used in the existing systems for event detection.
sent4: In the context of event detection, we categorised the temporal information into three categories as shown in Figure 5.  • Pre-event: This represents the time period before the occurrence of the event of interest.
sent5: In general, a social media message that is posted before the event occurrence can be analysed to derive the following information: (i) warnings-e.g., a post about bad weather from the Met-Office before heavy rainfall, a cyclone alert, etc. serves as a warning message for an impending natural disaster, (ii) precursor event detectione.g., a social media post about a leaning electric pole in a location can serve as a precursor for landslide event detection, and (iii) temporal offset-pre-event posts from social media are analysed to determine the offset between the time of the post and the time of the actual event, such as, for instance, the time taken after the leaning pole post and the actual landslide in that locality.
sent6: Pre-event posts from social media can thus be utilised for serving the mitigation and preparedness phases of emergency management.
sent7: • Real-time: This represents the time span during which the event is happening.
sent8: In the real time of the event occurrence, social media may be widely used for information sharing about the incidents related to the event.
sent9: Generally, the real-time posts from social media during the occurrence of the event can be analysed for: (i) obtaining situational awareness-e.g., ""trains cancelled, schools closed in Kerala due to heavy rains"", or a social media post about ""roadblock due to landslides on NH-8"", (ii) deriving/issuing warnings about the after-effects/impacts of a disaster-e.g., ""high tides are expected in coastal areas after the tremors"", and (iii) response, relief, and recovery-e.g., a tweet during the Kerala flood in 2018: ""shortage of bubble wrap and ready-to-eat items in Sanskrit College Palayam"".
sent10: • Post-event: This represents the time period after the occurrence of the event of interest.
sent11: Often, after disasters, social media is widely used to communicate about required supplies, information about missing people, death tolls, property losses, relief operations planned by the government and NGOs, protective measures to be undertaken while returning home, funds donated by various authorities, etc.
sent12: Thus, the post-event data can generally be analysed for: (i) warnings of further events, (ii) deriving information on the impact of the event, (iii) identifying the relief and recovery measures required, and (iv) determining the temporal offset between the time of the post and time of the actual event.
sent13: It is important to analyse the behaviour of the public/communities before, during, and after disasters in order to bring in effective disaster response, management, planning, and mitigation.
sent14: Since social networks serve as the easiest and most common way to sample public opinion, we can make use of the time-stamped, geo-tagged data from social media for this purpose.
sent15: Table 5 summarises the temporal the state-of-the-art methods for temporal information analysis.
sent16: Chae et al. [39] explain the temporal analysis of Twitter data related to hurricane Sandy, wherein they analyse the Twitter user density distribution two weeks before and after the date of the event, as well as for a time period on the day of the event, right after the announcement of the evacuation order.
sent17: A similar study on the spatiotemporal analysis of Twitter data for the same disaster event was performed by Kryvasheyeu et al. [40], according to whom the persistence of the Twitter activity levels in the time frame immediate to the occurrence of the event (post-event) was a good indicator for determining which areas were likely to need the most assistance.
sent18: Further, during a disaster, normalised activity levels, rates of original content creation, and rates of content rebroadcasting must be considered to identify the hardest-hit areas in real time.
sent19: The number of tweets during the Christchurch, New Zealand earthquakes were analysed over time in a window of five minutes in [41].
sent20: The analysis indicated that when an earthquake with a magnitude of 4.2 or stronger occurred at a particular time, it correlated with a spike in the number of tweets over that time frame.
sent21: Another crucial factor to be considered while choosing the time frame for social media data collection is the type of disaster.
sent22: For disasters like landslides, floods, and storms, we may be able to capture some of the warning signs for these events from social media posts before the actual occurrence of these events, whereas for other events, such as wildfires and earthquakes, the posts relevant or related to them may surface only after the occurrence of these events.
sent23: Wang et al. [42] analysed wildfire-related tweets of some of the major wildfires that occurred in San Diego County, USA with respect to space, time, content, and network by collecting Twitter data from the day when the first wildfire occurred until the date when most of these wildfires were 100% contained.
sent24: The temporal evolution of wildfire-related tweets obtained using different keywords, with and without the location, gave an insight into the time lag taken for the spreading of the information.
sent25: Furthermore, as Granell and Ostermann mentioned in [43], the durations of the impacts of these events also affect the temporal and contextual variation in the data related to these events.
sent26: For instance, real-time and post-event data can be utilised for disaster response and recovery, whereas pre-event data can be utilised for preparedness and planning.
sent27: A case study to analyse the social media text during and after the 2012 Beijing rainstorm was described in [3], where the authors performed time-series decomposition of the data to identify the overall trend and variations with respect to different developmental stages of the event, as well as the cyclical trends of microblogging activity.
sent28: They concluded that the trend analysis of text streams for different topics over time corresponded well with different development stages of the event.
sent29: For example, texts related to the event increased in the week after the rainstorm, following which they began to slowly subside, and, finally, faded out.
sent30: The classification of these social media messages into different contextual categories and their analysis over time help to identify the transition between various phases of disaster management and support effective decision-making for disaster preparedness, response, and recovery.
sent31: The authors of [44] present a classifier based on logistic regression that automatically classifies the gathered social media data into various topic categories during various disaster phases and classifies the temporal trends of these topic categories in different phases.
sent32: The experimentation using tweets related to Hurricane Sandy revealed that: (i) Tweets regarding preparedness reached their peak on the day before the event when the emergency declaration was issued, (ii) a large proportion of tweets related to impact were observed within a few days of the event's occurrence, and (iii) the largest peak of tweets related to disaster recovery was observed five days after the event."
232116743,Use of Social Media Data in Disaster Management: A Survey,"Computer Science, Environmental Science",https://www.semanticscholar.org/paper/7595367176192a375585e18593d1a2bd3d5faccc,s19,Methodologies Used for Data Management,"Data management for social media includes collecting, indexing, storing, and querying social media data for the accessibility, reliability, and timeliness of the data. Social media generates a large volume of data every day. For instance, according to [45], Facebook generates around four petabytes of data every day. The sheer amount of data itself poses a significant challenge in social media data management, making it a Big Data problem. Data management and analysis systems for social media data must, therefore, be able to handle the ""four Vs"" of Big Data analytics-namely, volume, variety, velocity, and veracity. In this section, we present the state of the art in various systems and in research involving social media analytics for disaster management. From the data management perspective, we reviewed how data are collected, filtered, pre-processed, localised, stored, indexed, and queried.

Maynard et al. [15] presented a framework for real-time semantic social media analysis that is based on a popular open-source framework for natural language processing, GATE [46]. For the evaluation of the framework, they used the Twitter streaming API for data collection. Both streaming and batch processing approaches were evaluated. The GATE Cloud Paralleliser (GCP) [47] was used to perform batch processing of text, as it supports execution of NLP pipelines with millions of documents. It performs the pre-processing and transformations required to load into the main information management system in the GATE pipeline, Mímir (Multi-paradigm Information Management Index and Repository). It also supports indexing of text, annotations, and semantics. In real-time stream analysis, the Twitter client is used to capture data from the Twitter streaming API to feed into a message queue. Separate semantic analysis processors analyse and annotate the text and push it into Mímir, which, in turn, enables semantic search using the knowledge encoded in knowledge graphs or ontologies [48]. This enables the indexed documents to form semantic relationships and, thus, makes it easy to perform complex semantic searches over the indexed dataset. GATE Prospector [14] is used for exploring and searching datasets in the Mímir system. This system is reviewed in the next section.

Kim et al. proposed a conceptual framework [49] for social media data collection and quality assessment. The framework's strategy consists of three major steps to develop, apply, and validate search filters. Retrieval precision and retrieval recall are measured. Quality assessment in data collection is an important aspect of analysing a large amount of data, such as social media content. This is very relevant in the disaster management scenario. Search filter development is performed with keyword selection, which includes disambiguations and slang words, and this procedure was generally performed manually by domain experts. Search filters are developed using standard logical operators, such as AND, OR, and NOT, and by involving data preprocessing techniques, such as n-gram analysis and proximity operators. D-record [50] utilises three data sources: Twitter, Open-StreetMap, and satellite images. A set of keywords for a needed concept was expanded using topic modelling learned using an support vector machine (SVM)-based classifier with Synthetic Minority Over-Sampling Technique (SMOTE). In ""Twitter Analytics: A big data management perspective"" [13], Goonetilleke et al. reviewed several open-source and commercial tools for data collection, management, and querying for Twitter, many of which have been used in disaster management applications. Wang et al. (2013) and Wang et al. (2010) [51,52] developed a scalable Cyber Infrastructure-based Geographic Information Systems (CyberGIS) for analysing large amounts of social media content in a natural disaster context. The system employs data fusion techniques to fuse social media data with census data and remote-sensing imagery. Slamet et al. proposed a system design [53] to find a secure place locator (SPL), which covers a system information engineering aspect. It involves combining multiple data sources, such as location databases, governmental information, and information from the community, as it uses a relational database model to store and process the data. Yates et al. performed a case study on emergency knowledge management and social media technologies [54]. The study investigated social media and related tools for effective knowledge management. It discussed how US government agencies use social media data as an informal information dispersal mechanism, and also studied how visual information layering helped the disaster management scenario.

Apart from text information, the use of multimedia data, such as images, audio, and video, in extreme event management [55] remains challenging due to the variety and complexity of social media content. Such events require sophisticated techniques to represent and analyse the multimedia contents to better understand extreme events. The authors of [56] proposed a novel data model based on a hypergraph structure to manage the massive amount of multimedia data produced by social media. The proposed data model comprises three different entities-users, multimedia objects, and annotation objects-to represent the variety and complexity of relationships of the multimedia contents. This approach enables merging of social media contents from different social media platforms in a single data structure. Here, the influence diffusion algorithm [57] was proposed to investigate social media users who have significant interactions on a particular social media object. Table 6. Methodologies.","[['b44'], ['b14', 'b45', 'b47', 'b46', 'b13'], ['b52', 'b50', 'b49', 'b51', 'b53', 'b48', 'b12'], ['b54', 'b56', 'b55']]","[['b44'], ['b14', 'b45', 'b47', 'b46', 'b13'], ['b52', 'b50', 'b49', 'b51', 'b53', 'b48', 'b12'], ['b54', 'b56', 'b55']]",16,"sent1: Data management for social media includes collecting, indexing, storing, and querying social media data for the accessibility, reliability, and timeliness of the data.
sent2: Social media generates a large volume of data every day.
sent3: For instance, according to [45], Facebook generates around four petabytes of data every day.
sent4: The sheer amount of data itself poses a significant challenge in social media data management, making it a Big Data problem.
sent5: Data management and analysis systems for social media data must, therefore, be able to handle the ""four Vs"" of Big Data analytics-namely, volume, variety, velocity, and veracity.
sent6: In this section, we present the state of the art in various systems and in research involving social media analytics for disaster management.
sent7: From the data management perspective, we reviewed how data are collected, filtered, pre-processed, localised, stored, indexed, and queried.
sent8: Maynard et al. [15] presented a framework for real-time semantic social media analysis that is based on a popular open-source framework for natural language processing, GATE [46].
sent9: For the evaluation of the framework, they used the Twitter streaming API for data collection.
sent10: Both streaming and batch processing approaches were evaluated.
sent11: The GATE Cloud Paralleliser (GCP) [47] was used to perform batch processing of text, as it supports execution of NLP pipelines with millions of documents.
sent12: It performs the pre-processing and transformations required to load into the main information management system in the GATE pipeline, Mímir (Multi-paradigm Information Management Index and Repository).
sent13: It also supports indexing of text, annotations, and semantics.
sent14: In real-time stream analysis, the Twitter client is used to capture data from the Twitter streaming API to feed into a message queue.
sent15: Separate semantic analysis processors analyse and annotate the text and push it into Mímir, which, in turn, enables semantic search using the knowledge encoded in knowledge graphs or ontologies [48].
sent16: This enables the indexed documents to form semantic relationships and, thus, makes it easy to perform complex semantic searches over the indexed dataset.
sent17: GATE Prospector [14] is used for exploring and searching datasets in the Mímir system.
sent18: This system is reviewed in the next section.
sent19: Kim et al. proposed a conceptual framework [49] for social media data collection and quality assessment.
sent20: The framework's strategy consists of three major steps to develop, apply, and validate search filters.
sent21: Retrieval precision and retrieval recall are measured.
sent22: Quality assessment in data collection is an important aspect of analysing a large amount of data, such as social media content.
sent23: This is very relevant in the disaster management scenario.
sent24: Search filter development is performed with keyword selection, which includes disambiguations and slang words, and this procedure was generally performed manually by domain experts.
sent25: Search filters are developed using standard logical operators, such as AND, OR, and NOT, and by involving data preprocessing techniques, such as n-gram analysis and proximity operators.
sent26: D-record [50] utilises three data sources: Twitter, Open-StreetMap, and satellite images.
sent27: A set of keywords for a needed concept was expanded using topic modelling learned using an support vector machine (SVM)-based classifier with Synthetic Minority Over-Sampling Technique (SMOTE).
sent28: In ""Twitter Analytics: A big data management perspective"" [13], Goonetilleke et al. reviewed several open-source and commercial tools for data collection, management, and querying for Twitter, many of which have been used in disaster management applications.
sent29: Wang et al. (2013) and Wang et al. (2010) [51,52] developed a scalable Cyber Infrastructure-based Geographic Information Systems (CyberGIS) for analysing large amounts of social media content in a natural disaster context.
sent30: The system employs data fusion techniques to fuse social media data with census data and remote-sensing imagery.
sent31: Slamet et al. proposed a system design [53] to find a secure place locator (SPL), which covers a system information engineering aspect.
sent32: It involves combining multiple data sources, such as location databases, governmental information, and information from the community, as it uses a relational database model to store and process the data.
sent33: Yates et al. performed a case study on emergency knowledge management and social media technologies [54].
sent34: The study investigated social media and related tools for effective knowledge management.
sent35: It discussed how US government agencies use social media data as an informal information dispersal mechanism, and also studied how visual information layering helped the disaster management scenario.
sent36: Apart from text information, the use of multimedia data, such as images, audio, and video, in extreme event management [55] remains challenging due to the variety and complexity of social media content.
sent37: Such events require sophisticated techniques to represent and analyse the multimedia contents to better understand extreme events.
sent38: The authors of [56] proposed a novel data model based on a hypergraph structure to manage the massive amount of multimedia data produced by social media.
sent39: The proposed data model comprises three different entities-users, multimedia objects, and annotation objects-to represent the variety and complexity of relationships of the multimedia contents.
sent40: This approach enables merging of social media contents from different social media platforms in a single data structure.
sent41: Here, the influence diffusion algorithm [57] was proposed to investigate social media users who have significant interactions on a particular social media object.
sent42: Table 6. Methodologies."
232116743,Use of Social Media Data in Disaster Management: A Survey,"Computer Science, Environmental Science",https://www.semanticscholar.org/paper/7595367176192a375585e18593d1a2bd3d5faccc,s22,Methodologies Used for Data Analysis,"In [59], the researchers showed an example of an earthquake event and early warning using a social approach. This was accomplished by integrating semantic analysis and real-time data from Twitter. They made two primary assumptions: that each Twitter user is a sensor, and each tweet is associated with a time and location. Semantic analysis was used to classify tweets into positive and negative classes. Tweets related to earthquake events were classified as a positive class, while tweets unrelated to earthquake events were classified as a negative class. Furthermore, they used a machine learning algorithm, support vector machine (SVM), for tweet classification.

On the other hand, Latent Dirichlet Allocation (LDA), a topic modelling technique in the information retrieval domain, was used in [60]. LDA was used to extract the inherent topic structure from a set of social media messages, and the extracted topic referred to an event (e.g., 2011 Virginia earthquake).

The authors gave an example of topics and the proportions of each topic to all messages and showed how the earthquake events captured from the topics constituted a small proportion of messages. Using the LDA topic model approach, meaningful topics with many iterations were discovered. Abnormal events captured from extracted topics did not happen frequently and covered only a small fraction of the social media data stream. In order to identify such abnormal events, the authors used seasonal trend decomposition based on locally weighted regression (Loess), which they called STL. In STL, the reminder component is used to implement control charts. The detected anomaly events are compared with other social media data to confirm the anomalies.

A candidate retrieval algorithm was used in [61] for retrieving events from the database. The authors implemented feature extraction to extract spatial, temporal, and textual information, and then used scoring and ranking to determine which document belonged to which event. SVM-based classification was the methodology used in this paper for event detection.

An architecture for a public health surveillance process using SMART-C was presented in [63]. The architecture explains the data sources with their modalities, users, and services provided by the underlying system to enable enhanced situational awareness and informed decision-making during all phases of disaster management. The authors discussed the requirements for implementing the following services: event classification/grouping, semantic reasoning, location determination, event extraction, speech analysis, text analysis, video analysis, sensor analysis, geospatial analysis, response planning and generation, and alert dissemination service. They also presented a discussion on security and privacy, event detection, and correlation.

Classification and information extraction from Twitter were carried out in [30]. The authors used free part-of-speech-tagging software for Twitter and Weka data mining tools. For classification purposes, they first broadly classified the tweets into personal, informative, and other tweets. They further classified the informative tweets into: (i) caution and advice, (ii) damage, (iii) donations, (iv) people, and (v) other. They used a Naive Bayesian classifier for feature extraction and used unigram, bigram, and partof-speech (POS) tagging to provide a rich set of features in the classifier. Once a tweet was classified, a sequence labelling task identified relevant information using conditional random fields.

A participatory sensing-based model for mining spatial information of urban emergency events was discussed in [64]. The researchers conducted simulations on a typhoon event, Typhoon Chan-hom. They proposed a hierarchical data model with three different layers: a (i) social user layer, (ii) crowdsourcing layer, and (iii) spatial information layer. In the social user layer, the proposed method collected data related to emergency events. In the crowdsourcing layer, the positive samples were collected, and the address and Geographic Information Systems (GIS) data is mined. Information related to the same emergency events was clustered in this layer. In the spatial information layer, the spatial information of the emergency event was mined. Semantic analysis of the geo-tagged microblog data helped obtain a public opinion from the spatial perspective, and assistance could be offered where it was required. From the collected data, it was observed that the risk was high in Beijing, Zhejiang, Jiangsu, and Shanghai.

In [34], the authors proposed a probabilistic framework for identifying the location of a Twitter user based on the content of their tweet. The authors used a simple cart classifier to classify the tweets with strong geo-scope, and then use a lattice-based neighbourhood smoothing model to refine the user location. They also showed that, with an increase in the number of tweets, the location estimation process converges.

The authors of [35] proposed a unified discriminative influence model to solve the problem of profiling users' home locations on Twitter. They adapted probabilistic methods for local prediction and global prediction to profile user location. Local-prediction-based profiling uses the user's friends, followers, and their tweets to efficiently profile the user's location, whereas global prediction, in addition, uses unlabelled users to accurately profile user location. In D-record [50], text sentences are vectorised to capture their semantics. Before featuring, the text is pre-processed by stemming, case folding, and removing noisy lexical elements using an SVM classifier with a lexicon-based feature, Term Frequency-Inverse Document Frequency (TF-IDF) vectors, and gensim's word2vec embedding.","[['b58'], ['b59'], [], ['b60'], ['b62'], ['b29'], ['b63'], ['b33'], ['b49', 'b34']]","[['b58'], ['b59'], [], ['b60'], ['b62'], ['b29'], ['b63'], ['b33'], ['b49', 'b34']]",9,"sent1: In [59], the researchers showed an example of an earthquake event and early warning using a social approach.
sent2: This was accomplished by integrating semantic analysis and real-time data from Twitter.
sent3: They made two primary assumptions: that each Twitter user is a sensor, and each tweet is associated with a time and location.
sent4: Semantic analysis was used to classify tweets into positive and negative classes.
sent5: Tweets related to earthquake events were classified as a positive class, while tweets unrelated to earthquake events were classified as a negative class.
sent6: Furthermore, they used a machine learning algorithm, support vector machine (SVM), for tweet classification.
sent7: On the other hand, Latent Dirichlet Allocation (LDA), a topic modelling technique in the information retrieval domain, was used in [60].
sent8: LDA was used to extract the inherent topic structure from a set of social media messages, and the extracted topic referred to an event (e.g., 2011 Virginia earthquake).
sent9: The authors gave an example of topics and the proportions of each topic to all messages and showed how the earthquake events captured from the topics constituted a small proportion of messages.
sent10: Using the LDA topic model approach, meaningful topics with many iterations were discovered.
sent11: Abnormal events captured from extracted topics did not happen frequently and covered only a small fraction of the social media data stream.
sent12: In order to identify such abnormal events, the authors used seasonal trend decomposition based on locally weighted regression (Loess), which they called STL.
sent13: In STL, the reminder component is used to implement control charts.
sent14: The detected anomaly events are compared with other social media data to confirm the anomalies.
sent15: A candidate retrieval algorithm was used in [61] for retrieving events from the database.
sent16: The authors implemented feature extraction to extract spatial, temporal, and textual information, and then used scoring and ranking to determine which document belonged to which event.
sent17: SVM-based classification was the methodology used in this paper for event detection.
sent18: An architecture for a public health surveillance process using SMART-C was presented in [63].
sent19: The architecture explains the data sources with their modalities, users, and services provided by the underlying system to enable enhanced situational awareness and informed decision-making during all phases of disaster management.
sent20: The authors discussed the requirements for implementing the following services: event classification/grouping, semantic reasoning, location determination, event extraction, speech analysis, text analysis, video analysis, sensor analysis, geospatial analysis, response planning and generation, and alert dissemination service.
sent21: They also presented a discussion on security and privacy, event detection, and correlation.
sent22: Classification and information extraction from Twitter were carried out in [30].
sent23: The authors used free part-of-speech-tagging software for Twitter and Weka data mining tools.
sent24: For classification purposes, they first broadly classified the tweets into personal, informative, and other tweets.
sent25: They further classified the informative tweets into: (i) caution and advice, (ii) damage, (iii) donations, (iv) people, and (v) other.
sent26: They used a Naive Bayesian classifier for feature extraction and used unigram, bigram, and partof-speech (POS) tagging to provide a rich set of features in the classifier.
sent27: Once a tweet was classified, a sequence labelling task identified relevant information using conditional random fields.
sent28: A participatory sensing-based model for mining spatial information of urban emergency events was discussed in [64].
sent29: The researchers conducted simulations on a typhoon event, Typhoon Chan-hom.
sent30: They proposed a hierarchical data model with three different layers: a (i) social user layer, (ii) crowdsourcing layer, and (iii) spatial information layer.
sent31: In the social user layer, the proposed method collected data related to emergency events.
sent32: In the crowdsourcing layer, the positive samples were collected, and the address and Geographic Information Systems (GIS) data is mined.
sent33: Information related to the same emergency events was clustered in this layer.
sent34: In the spatial information layer, the spatial information of the emergency event was mined.
sent35: Semantic analysis of the geo-tagged microblog data helped obtain a public opinion from the spatial perspective, and assistance could be offered where it was required.
sent36: From the collected data, it was observed that the risk was high in Beijing, Zhejiang, Jiangsu, and Shanghai.
sent37: In [34], the authors proposed a probabilistic framework for identifying the location of a Twitter user based on the content of their tweet.
sent38: The authors used a simple cart classifier to classify the tweets with strong geo-scope, and then use a lattice-based neighbourhood smoothing model to refine the user location.
sent39: They also showed that, with an increase in the number of tweets, the location estimation process converges.
sent40: The authors of [35] proposed a unified discriminative influence model to solve the problem of profiling users' home locations on Twitter.
sent41: They adapted probabilistic methods for local prediction and global prediction to profile user location.
sent42: Local-prediction-based profiling uses the user's friends, followers, and their tweets to efficiently profile the user's location, whereas global prediction, in addition, uses unlabelled users to accurately profile user location.
sent43: In D-record [50], text sentences are vectorised to capture their semantics.
sent44: Before featuring, the text is pre-processed by stemming, case folding, and removing noisy lexical elements using an SVM classifier with a lexicon-based feature, Term Frequency-Inverse Document Frequency (TF-IDF) vectors, and gensim's word2vec embedding."
232116743,Use of Social Media Data in Disaster Management: A Survey,"Computer Science, Environmental Science",https://www.semanticscholar.org/paper/7595367176192a375585e18593d1a2bd3d5faccc,s29,References,"Disaster Management Phase Disaster Management Type [16] Preparedness, Response Technological, Natural [39] Preparedness, Response, Recovery Natural disaster [40] Preparedness, Response, Recovery Natural disaster [3] Response Natural disaster [44] Preparedness, Response Natural disaster

The authors of [16] proposed a novel approach to viewing social media data as a human sensor and using social media to observe technological disasters (sightings of oil) and natural disasters (earthquakes and air quality). Geo-locations were extracted and used as a boundary for the prediction of an oil spill. The authors of [39] and [40] analysed Twitter data to identify public behaviour patterns from both spatial and temporal perspectives during a natural disaster-Hurricane Sandy. An investigation of the emergency information distribution using social media during an emergency event was performed in [3]. This work analysed the social media stream during the 2012 Beijing rainstorm by using classification and location models. The authors of [44] analysed tweets about Hurricane Sandy to find temporal trends using a classifier based on logistic regression. The applications of social media mentioned in Table 7 were proposed to address problems in a different phase of disaster management. The work in [16,39,40,44] addresses problems in the preparedness phase, while the outcomes in [3,16,39,40,44] are utilised for the response phase. The applications outlined in [39,40] are used for the recovery phase.

It can be seen that most research has focused on the application of social media data for natural disasters rather than technological disasters. However, the approaches presented in most of these works can be applied to multiple phases of disaster management.

Interestingly, the response phase is the most popular aspect in which to exploit social media data, while there are no available publications that are applicable to the mitigation phase.","[['b43', 'b38', 'b39', 'b2', 'b15'], ['b43', 'b38', 'b39', 'b2', 'b15'], [], []]","[['b43', 'b38', 'b39', 'b2', 'b15'], ['b43', 'b38', 'b39', 'b2', 'b15'], [], []]",10,"sent1: Disaster Management Phase Disaster Management Type [16] Preparedness, Response Technological, Natural [39]
sent2: Preparedness, Response, Recovery Natural disaster [40] Preparedness, Response, Recovery Natural disaster [3] Response Natural disaster [44] Preparedness, Response Natural disaster
sent3: The authors of [16] proposed a novel approach to viewing social media data as a human sensor and using social media to observe technological disasters (sightings of oil) and natural disasters (earthquakes and air quality).
sent4: Geo-locations were extracted and used as a boundary for the prediction of an oil spill.
sent5: The authors of [39] and [40] analysed Twitter data to identify public behaviour patterns from both spatial and temporal perspectives during a natural disaster-Hurricane Sandy.
sent6: An investigation of the emergency information distribution using social media during an emergency event was performed in [3].
sent7: This work analysed the social media stream during the 2012 Beijing rainstorm by using classification and location models.
sent8: The authors of [44] analysed tweets about Hurricane Sandy to find temporal trends using a classifier based on logistic regression.
sent9: The applications of social media mentioned in Table 7 were proposed to address problems in a different phase of disaster management.
sent10: The work in [16,39,40,44] addresses problems in the preparedness phase, while the outcomes in [3,16,39,40,44] are utilised for the response phase.
sent11: The applications outlined in [39,40] are used for the recovery phase.
sent12: It can be seen that most research has focused on the application of social media data for natural disasters rather than technological disasters.
sent13: However, the approaches presented in most of these works can be applied to multiple phases of disaster management.
sent14: Interestingly, the response phase is the most popular aspect in which to exploit social media data, while there are no available publications that are applicable to the mitigation phase."
232116743,Use of Social Media Data in Disaster Management: A Survey,"Computer Science, Environmental Science",https://www.semanticscholar.org/paper/7595367176192a375585e18593d1a2bd3d5faccc,s1,Background,"During natural disasters, social media can play an essential role in the emergency response and provide a complete picture of situational awareness during and after the disaster. There are several challenges in acquiring and extracting hazard-related information from social media, including volume, unstructured data sources, signal-to-noise ratio, ungrammatical and multilingual data, and fraudulent message identification and removal. The massive amounts and variety of data generated by social media lead to different levels of information being extracted from the social media data. For instance, geographical information (geo-tagging) attached to a tweet about a roadblock on a hilly road provides more useful contextual information than a similar tweet without geo-tagging. Similarly, a tweet with attached images could potentially provide more situational awareness. For example, a tweet with photos of a roadblock on a hilly road can help people who are driving on the road nearby to understand the current situation of the roadblock and change to a new route away from the blocked area.

Due to the volume and complexity in such large amounts of social media data, it is crucial to have tools and systems that can automatically classify and extract information, which could turn data into meaningful, actionable information for those attempting to manage the situation. This information has to be systematically managed and made available upon request and to be queried based on different query conditions. The main dimensions of a query include the geo-location/geo-fence, keywords and their disambiguations, user type (e.g., government, non-governmental organisations (NGOs), news agencies, public, etc.), and type of message (e.g., warning, news, SOS, request for supplies, or general posts/tweets about an ongoing or impending situation). It is also important for the system to remove common false-positive patterns. For instance, the word ""Landslide"" in a tweet talking about a landslide victory of a sports team could potentially be classified as a tweet about a landslide hazard. To support this, the use of a tool such as an ontology can be applied to yield meaningful information from complex data. For instance, an ontology of landslides would represent the domain of landslide hazards through relevant terms and relationships between them. These relationships provide formal definitions to the domain terms, thereby enabling machines to understand and analyse them. Thus, the knowledge represented in an ontology enables machines to perform intelligent tasks, such as interactively communicating with social media users to extract contextual information related to an event of interest, identifying the relation of this information with the hazard of interest, and providing this to the decision-maker as a complete picture to enable informed decision-making.

An ontology-based approach is thus more sophisticated than traditional data management approaches, since it combines the data model with the associated domain knowledge that can be processed by machines to obtain semantically rich and meaningful information [12]. Systematic extraction of important information and semantic meaning from the free text in social media will help make the systems intelligent enough to organise and present data in an actionable form. Similarly, natural language processing (NLP) is an important technology for understanding and extracting information from user-generated text content. We reviewed several natural language processing methods and case studies [13][14][15].

In this survey, the applications described above, including ontological support, NLP, and data mining, are reviewed in the context of social media and natural hazard response and recovery.","[[], [], ['b14', 'b11', 'b13', 'b12'], []]","[[], [], ['b14', 'b11', 'b13', 'b12'], []]",4,"sent1: During natural disasters, social media can play an essential role in the emergency response and provide a complete picture of situational awareness during and after the disaster.
sent2: There are several challenges in acquiring and extracting hazard-related information from social media, including volume, unstructured data sources, signal-to-noise ratio, ungrammatical and multilingual data, and fraudulent message identification and removal.
sent3: The massive amounts and variety of data generated by social media lead to different levels of information being extracted from the social media data.
sent4: For instance, geographical information (geo-tagging) attached to a tweet about a roadblock on a hilly road provides more useful contextual information than a similar tweet without geo-tagging.
sent5: Similarly, a tweet with attached images could potentially provide more situational awareness.
sent6: For example, a tweet with photos of a roadblock on a hilly road can help people who are driving on the road nearby to understand the current situation of the roadblock and change to a new route away from the blocked area.
sent7: Due to the volume and complexity in such large amounts of social media data, it is crucial to have tools and systems that can automatically classify and extract information, which could turn data into meaningful, actionable information for those attempting to manage the situation.
sent8: This information has to be systematically managed and made available upon request and to be queried based on different query conditions.
sent9: The main dimensions of a query include the geo-location/geo-fence, keywords and their disambiguations, user type (e.g., government, non-governmental organisations (NGOs), news agencies, public, etc.), and type of message (e.g., warning, news, SOS, request for supplies, or general posts/tweets about an ongoing or impending situation).
sent10: It is also important for the system to remove common false-positive patterns.
sent11: For instance, the word ""Landslide"" in a tweet talking about a landslide victory of a sports team could potentially be classified as a tweet about a landslide hazard.
sent12: To support this, the use of a tool such as an ontology can be applied to yield meaningful information from complex data.
sent13: For instance, an ontology of landslides would represent the domain of landslide hazards through relevant terms and relationships between them.
sent14: These relationships provide formal definitions to the domain terms, thereby enabling machines to understand and analyse them.
sent15: Thus, the knowledge represented in an ontology enables machines to perform intelligent tasks, such as interactively communicating with social media users to extract contextual information related to an event of interest, identifying the relation of this information with the hazard of interest, and providing this to the decision-maker as a complete picture to enable informed decision-making.
sent16: An ontology-based approach is thus more sophisticated than traditional data management approaches, since it combines the data model with the associated domain knowledge that can be processed by machines to obtain semantically rich and meaningful information [12].
sent17: Systematic extraction of important information and semantic meaning from the free text in social media will help make the systems intelligent enough to organise and present data in an actionable form.
sent18: Similarly, natural language processing (NLP) is an important technology for understanding and extracting information from user-generated text content.
sent19: We reviewed several natural language processing methods and case studies [13][14][15].
sent20: In this survey, the applications described above, including ontological support, NLP, and data mining, are reviewed in the context of social media and natural hazard response and recovery."
232116743,Use of Social Media Data in Disaster Management: A Survey,"Computer Science, Environmental Science",https://www.semanticscholar.org/paper/7595367176192a375585e18593d1a2bd3d5faccc,s11,Social Media Users,"Messages originating from different accounts on social media have different qualities and trustworthiness [20]. For instance, official accounts used by government agencies are likely to have more trustworthiness than public users with personal accounts. However, although government agencies that are responsible for the management of disasters use social media to disseminate disaster-related information, they still play a limited role in the communities. Instead, it is the public users that play a significant role in contributing to information networks during disaster events. The authors of [21] showed different distributions of Twitter users participating in various disaster events, with public users having a clearly greater percentage of participation. To summarise, different types of social media users play different roles in disaster management, each providing different context, quality, and trustworthiness of social media data. In this paper, we classify types of social media users as government authorities, research/academic institutions, non-governmental organisations (NGO), and the public, as shown in Figure 2.  • Government Authority-refers to government organisations involved in disaster response and support. These organisations are authorised to: (i) disseminate official announcement and actionable warning information to people in a disaster risk area, e.g., National Disaster Management Authority (NDMA), and (ii) provide supporting information for disaster management, e.g., Geological Survey of India (GSI), British Geological Survey (BGS), and national meteorological offices. •

Research/Academic Institution-refers to institutions or research groups who are conducting research on disaster management. • Non-Governmental Organisation (NGO)-refers to private-sector organisations that are disseminating disaster-related information on social media. This user type contributes a greater percentage of information than the government authorities and provides a higher quality of information compared to the information provided by individual users. Examples of NGOs include Save the Hills, Cable News Network (CNN), and Asian News International (ANI). •

Public-refers to individual users with personal social media accounts. This user type makes the greatest contribution to social media by sharing disaster-related information.

With a huge number of users in this category, it constitutes the greatest percentage in information networks compared to other types of users. Most research [22][23][24][25][26][27] relies on information contributed by public users, even though the information may be of uncertain quality and trustworthiness. As a consequence, social media data preparation techniques (e.g., data filtering, data classification, and data extraction) to improve data quality and enhance the accuracy of social media data analysis prove challenging.","[['b20', 'b19'], [], [], ['b22', 'b25', 'b21', 'b26', 'b23', 'b24']]","[['b20', 'b19'], [], [], ['b22', 'b25', 'b21', 'b26', 'b23', 'b24']]",8,"sent1: Messages originating from different accounts on social media have different qualities and trustworthiness [20].
sent2: For instance, official accounts used by government agencies are likely to have more trustworthiness than public users with personal accounts.
sent3: However, although government agencies that are responsible for the management of disasters use social media to disseminate disaster-related information, they still play a limited role in the communities.
sent4: Instead, it is the public users that play a significant role in contributing to information networks during disaster events.
sent5: The authors of [21] showed different distributions of Twitter users participating in various disaster events, with public users having a clearly greater percentage of participation.
sent6: To summarise, different types of social media users play different roles in disaster management, each providing different context, quality, and trustworthiness of social media data.
sent7: In this paper, we classify types of social media users as government authorities, research/academic institutions, non-governmental organisations (NGO), and the public, as shown in Figure 2.  • Government Authority-refers to government organisations involved in disaster response and support.
sent8: These organisations are authorised to: (i) disseminate official announcement and actionable warning information to people in a disaster risk area, e.g., National Disaster Management Authority (NDMA), and (ii) provide supporting information for disaster management, e.g., Geological Survey of India (GSI), British Geological Survey (BGS), and national meteorological offices.
sent9: •Research/Academic Institution-refers to institutions or research groups who are conducting research on disaster management.
sent10: • Non-Governmental Organisation (NGO)-refers to private-sector organisations that are disseminating disaster-related information on social media.
sent11: This user type contributes a greater percentage of information than the government authorities and provides a higher quality of information compared to the information provided by individual users.
sent12: Examples of NGOs include Save the Hills, Cable News Network (CNN), and Asian News International (ANI).
sent13: •Public-refers to individual users with personal social media accounts.
sent14: This user type makes the greatest contribution to social media by sharing disaster-related information.
sent15: With a huge number of users in this category, it constitutes the greatest percentage in information networks compared to other types of users.
sent16: Most research [22][23][24][25][26][27] relies on information contributed by public users, even though the information may be of uncertain quality and trustworthiness.
sent17: As a consequence, social media data preparation techniques (e.g., data filtering, data classification, and data extraction) to improve data quality and enhance the accuracy of social media data analysis prove challenging."
232895648,Generative Adversarial Networks: A Literature Review,Computer Science,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,s5,Variational Autoencoders (VAE),"Autoencoder is a data compression algorithm, in which the data compression and decompression are realized by neural network self-learning [34]. The encoder maps the input data to the low-latitude features we need, and then reconstructs the original input data through the decoder. Variational Autoencoder [35] is a method that adds ""Gaussian noise"" to the result of the encoder in Autoencoder to make the result of decoder robust to noise.

In the formula above, X is training data, and Z is the hidden feature that cannot be observed in X data. The characteristic of VAE is that every one-dimensional distribution of Z conforms to a normal distribution, and the learning of characteristics is introduced to make the decoding effect better [36,37]. However, VAE adopted the Variational Inference [38] for approximation. Compared with GAN, the fitting of the real data is not as good as GAN. From the result of the generated picture, the picture clarity of GAN is also better than that of VAE.","[['b34', 'b33'], ['b36', 'b35', 'b37']]","[['b34', 'b33'], ['b36', 'b35', 'b37']]",5,"sent1: Autoencoder is a data compression algorithm, in which the data compression and decompression are realized by neural network self-learning [34].
sent2: The encoder maps the input data to the low-latitude features we need, and then reconstructs the original input data through the decoder.
sent3: Variational Autoencoder [35] is a method that adds ""Gaussian noise"" to the result of the encoder in Autoencoder to make the result of decoder robust to noise.
sent4: In the formula above, X is training data, and Z is the hidden feature that cannot be observed in X data.
sent5: The characteristic of VAE is that every one-dimensional distribution of Z conforms to a normal distribution, and the learning of characteristics is introduced to make the decoding effect better [36,37].
sent6: However, VAE adopted the Variational Inference [38] for approximation.
sent7: Compared with GAN, the fitting of the real data is not as good as GAN.
sent8: From the result of the generated picture, the picture clarity of GAN is also better than that of VAE."
232895648,Generative Adversarial Networks: A Literature Review,Computer Science,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,s8,Training Process,"Since it is difficult for the first generated sample to reach the level of real data, we need to continuously train and optimize the GAN. However, the training of GAN is different from the previous single neural network training, and we adopt separate alternating iterative training [39][40][41]. In GAN, we use fixed generator to optimize the discriminator, or fixed discriminator to optimize the generator [21]. The formula for the entire GAN is as follows.

E is the expected value of the distribution function,

is the distribution that generates the sample.","[['b20', 'b39', 'b38', 'b40'], [], []]","[['b20', 'b39', 'b38', 'b40'], [], []]",4,"sent1: Since it is difficult for the first generated sample to reach the level of real data, we need to continuously train and optimize the GAN.
sent2: However, the training of GAN is different from the previous single neural network training, and we adopt separate alternating iterative training [39][40][41].
sent3: In GAN, we use fixed generator to optimize the discriminator, or fixed discriminator to optimize the generator [21].
sent4: The formula for the entire GAN is as follows.
sent5: E is the expected value of the distribution function,is the distribution that generates the sample."
232895648,Generative Adversarial Networks: A Literature Review,Computer Science,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,s12,Inception Score (IS),"During the ILSVRC competition in 2014, Google proposed a network called Inception Net [42]. It not only controlled the number of parameters, but also achieved good classification performance. In 2015, Inception net-v3 was proposed and it was exactly the model required for IS calculation in this part [43,44].

As analyzed above, IS focuses on the sharpness and diversity of images, which IS is not high resolution but clear classification [45]. The formula for IS is as follows.

Generate the 1000-dimensional vector y obtained from the input model of picture x, and the probability distribution of x belonging to each category is

is the marginal distribution of all the images obtained after the input model of a large number of images is generated.

is related to variety.

However, the evaluation index of IS was limited to the use of data set, because the Inception V3 model adopted by IS was trained on ImageNet [46], so the image generation of GAN also needs to rely on ImageNet to serve as a real sample. It no longer makes sense to use the classification model and the generation model in different data sets. At the same time, in the case of insufficient samples, the estimation of sample distribution will become very difficult, so we also need other indicators to evaluate the capability of GAN.","[['b43', 'b41', 'b42'], ['b44'], [], [], [], ['b45']]","[['b43', 'b41', 'b42'], ['b44'], [], [], [], ['b45']]",5,"sent1: During the ILSVRC competition in 2014, Google proposed a network called Inception Net [42].
sent2: It not only controlled the number of parameters, but also achieved good classification performance.
sent3: In 2015, Inception net-v3 was proposed and it was exactly the model required for IS calculation in this part [43,44].
sent4: As analyzed above, IS focuses on the sharpness and diversity of images, which IS is not high resolution but clear classification [45].
sent5: The formula for IS is as follows.
sent6: Generate the 1000-dimensional vector y obtained from the input model of picture x, and the probability distribution of x belonging to each category isis the marginal distribution of all the images obtained after the input model of a large number of images is generated.
sent7: is related to variety. However, the evaluation index of IS was limited to the use of data set, because the Inception V3 model adopted by IS was trained on ImageNet [46], so the image generation of GAN also needs to rely on ImageNet to serve as a real sample.
sent8: It no longer makes sense to use the classification model and the generation model in different data sets.
sent9: At the same time, in the case of insufficient samples, the estimation of sample distribution will become very difficult, so we also need other indicators to evaluate the capability of GAN."
232895648,Generative Adversarial Networks: A Literature Review,Computer Science,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,s13,Fréchet Inception Distance (FID),"FID [47] also uses the Inception V3 model. Due to the limitations of ImageNet dataset, images that do not exist in the data set are uniformly judged to be not real images. FID chooses the method of extracting image features to calculate the distance between the generated samples and the real samples in the feature space. The closer the distance is, the better the effect and diversity of the naturally generated images will be.

According to the formula, FID is negatively correlated with picture quality. Compared with IS, it is more robust to noise. Even in the case of insufficient samples (only one type of picture is produced), it will have a high FID value [48].

The following table lists FID values of the more classical generation models in recent years under different datasets [49]. MM GAN and NS GAN are expressed as mini-max [50] loss function and non-saturating [51] loss function respectively. It can be seen that FID values of different GAN models are quite different under different data sets. In recent years, more and more GAN models take FID value as the index of GAN model optimization. In addition, Mode Score (MS) [61], Kernel Maximum Mean Discrepancy (Kernel MMD) [62] and 1-nearest Neighbor (1-NN) [63] classifier are also the evaluation indicators of GAN model. With the unceasing development of GAN, more accurate evaluation criteria will be continuously proposed to evaluate the model.","[['b46'], ['b47'], ['b50', 'b49', 'b60', 'b62', 'b61', 'b48']]","[['b46'], ['b47'], ['b50', 'b49', 'b60', 'b62', 'b61', 'b48']]",8,"sent1: FID [47] also uses the Inception V3 model.
sent2: Due to the limitations of ImageNet dataset, images that do not exist in the data set are uniformly judged to be not real images.
sent3: FID chooses the method of extracting image features to calculate the distance between the generated samples and the real samples in the feature space.
sent4: The closer the distance is, the better the effect and diversity of the naturally generated images will be.
sent5: According to the formula, FID is negatively correlated with picture quality.
sent6: Compared with IS, it is more robust to noise.
sent7: Even in the case of insufficient samples (only one type of picture is produced), it will have a high FID value [48].
sent8: The following table lists FID values of the more classical generation models in recent years under different datasets [49].
sent9: MM GAN and NS GAN are expressed as mini-max [50] loss function and non-saturating [51] loss function respectively.
sent10: It can be seen that FID values of different GAN models are quite different under different data sets.
sent11: In recent years, more and more GAN models take FID value as the index of GAN model optimization.
sent12: In addition, Mode Score (MS) [61], Kernel Maximum Mean Discrepancy (Kernel MMD) [62]
sent13: and 1-nearest Neighbor (1-NN) [63] classifier are also the evaluation indicators of GAN model.
sent14: With the unceasing development of GAN, more accurate evaluation criteria will be continuously proposed to evaluate the model."
232895648,Generative Adversarial Networks: A Literature Review,Computer Science,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,s16,Deep Convolution Generative Adversarial Networks (DCGAN),"The traditional neural network is simply composed of three sections: input layer, hidden layer and output layer [68]. Within each layer, neurons with weights are mapped to the next layer by activation function. This way of receiving input from the previous layer's output to the next layer is also called full connection. The disadvantage of this way is that it is affected by a large number of parameters such as weight, the convergence of training is too slow, and the generalization effect is not good.

Convolutional neural network (CNN) is a feed forward neural network with convolution calculation [69]. It trains the weights of the CNN through the back-propagation algorithm, and finally obtains the classification results. CNN is similar to the traditional neural network structure, which is composed of input layer, hidden layer and output layer. The hidden layer in the traditional neural network structure includes convolutional layer, pooling layer and full connection layer, among which weight sharing of convolution layer solves the problem of low training efficiency caused by excessive parameters of traditional neural network. The birth of CNN provides a new idea for the development of deep learning.

CNN has a good effect in supervised learning [70], but it is seldom used in unsupervised direction. At the beginning, some researchers tried to combine CNN and GAN, but no good effect was achieved [71]. Until 2015, Alec Radford et al. proposed DCGAN [72], which improved the network structure of GAN and greatly improved the quality of GAN generated images and the stability of training.

DCGAN chose to remove the hidden layer of full connection because the full connection mode of GAN made the training inefficient. Batch Normalization [73] is used in the convergence of the model and avoids the collapse of the generator, allowing for deeper gradient propagation.

Another feature of the model is that in the generator, except for the tangent h function used for output, the activation function selects ReLU function, while in the discriminator, Leaky ReLU function is selected to prevent gradient sparse selection [74]. The reason for the use of the tangent h function is briefly explained here. Since the pixel range is 0 to 255, the ReLU function's result may exceed this range. It is beneficial to fix the final output value with a function whose range is -1 to 1.

The birth of DCGAN realized unsupervised feature extraction, and the image realized the addition and subtraction function similar to the word vector and this idea is widely used in image synthesis.","[['b67'], ['b68'], ['b70', 'b71', 'b69'], ['b72'], ['b73'], []]","[['b67'], ['b68'], ['b70', 'b71', 'b69'], ['b72'], ['b73'], []]",7,"sent1: The traditional neural network is simply composed of three sections: input layer, hidden layer and output layer [68].
sent2: Within each layer, neurons with weights are mapped to the next layer by activation function.
sent3: This way of receiving input from the previous layer's output to the next layer is also called full connection.
sent4: The disadvantage of this way is that it is affected by a large number of parameters such as weight, the convergence of training is too slow, and the generalization effect is not good.
sent5: Convolutional neural network (CNN) is a feed forward neural network with convolution calculation [69].
sent6: It trains the weights of the CNN through the back-propagation algorithm, and finally obtains the classification results.
sent7: CNN is similar to the traditional neural network structure, which is composed of input layer, hidden layer and output layer.
sent8: The hidden layer in the traditional neural network structure includes convolutional layer, pooling layer and full connection layer, among which weight sharing of convolution layer solves the problem of low training efficiency caused by excessive parameters of traditional neural network.
sent9: The birth of CNN provides a new idea for the development of deep learning.
sent10: CNN has a good effect in supervised learning [70], but it is seldom used in unsupervised direction.
sent11: At the beginning, some researchers tried to combine CNN and GAN, but no good effect was achieved [71].
sent12: Until 2015, Alec Radford et al. proposed DCGAN [72], which improved the network structure of GAN and greatly improved the quality of GAN generated images and the stability of training.
sent13: DCGAN chose to remove the hidden layer of full connection because the full connection mode of GAN made the training inefficient.
sent14: Batch Normalization [73] is used in the convergence of the model and avoids the collapse of the generator, allowing for deeper gradient propagation.
sent15: Another feature of the model is that in the generator, except for the tangent h function used for output, the activation function selects ReLU function, while in the discriminator, Leaky ReLU function is selected to prevent gradient sparse selection [74].
sent16: The reason for the use of the tangent h function is briefly explained here.
sent17: Since the pixel range is 0 to 255, the ReLU function's result may exceed this range.
sent18: It is beneficial to fix the final output value with a function whose range is -1 to 1.
sent19: The birth of DCGAN realized unsupervised feature extraction, and the image realized the addition and subtraction function similar to the word vector and this idea is widely used in image synthesis."
232895648,Generative Adversarial Networks: A Literature Review,Computer Science,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,s17,Wasserstein GAN (WGAN),"In the previous part, we constructed a relatively stable network structure by introducing DCGAN. While Martin Arjovsky, the author of WGAN [57], did an experiment on DCGAN, in which the generator was fixed, the discriminator was trained iteratively, and the relation graph of the gradient of the generator's objective function and the number of iterations was established. From this experiment, with the iterative training of discriminator, the gradient of generator decays rapidly. It can be known that the poorly trained discriminator will make the generator gradient unstable. Sufficient training of discriminator will cause generator gradient to disappear. Therefore, the training degree of discriminator is one of the important reasons for GAN instability.

According to Goodfellow's paper, under the condition of optimal discriminator, the gradient of generator can be composed of the Kullback-Leibler (KL) divergence and the Jensen-Shannon (JS) divergence [21]. In this case, mode collapse is easily caused, which means that the generated sample focuses on part of mode and lacks diversity. For this reason, the author proposed the Earth-Mover (EM) distance and compared it with KL divergence and JS divergence, and found that the change of the EM distance was more sensitive and more useful gradient could be proposed.

The Kullback-Leibler (KL) divergence

The Earth-Mover (EM) distance or Wasserstein-1

Due to the advantages of EM distance, Arjovsky tried to apply it on the GAN, borrow the Kantorovich -Rubinstein duality theory [75] will get EM distance formula for the deformation, and then adapt the method of neural network to solve, the writer named the neural network Critic. To satisfy the following equation, the Critic truncates the parameter to a range called weight clipping after each update.

There are three main differences between the Critic and the traditional discriminator of GAN. First, since the discriminator is responsible for dichotomies in GAN, and the Critic's function is to fit EM distance, sigmoid function is removed, the probability is no longer output, but the general score. Then, the target function of the Critic no longer contains log functions. Last but not least, there is no need to worry about the effect of excessive training of the discriminator on the generation of samples. The more training of the Critic, the better samples will be generated.

However, WGAN also has problems such as difficult convergence and poor sample quality, so the researchers finally focused on the weight clipping method in WGAN. In other words, this method will lead to the weakening of model generation ability or gradient explosion (disappeared). Gradient penalty was considered as one of the ways to speed up a solution to the problem, and WGAN-GP [58] was proposed.

The EM distance is proposed under the Lipschitz constraint, and the Lipschitz constraint requires that the discriminator gradient not exceed K. The authors of WGAN-GP believe that Lipschitz restriction does not need to be added to the entire sample space. Instead, they focus on the generated samples, real samples and their intermediate regions.

The first two parts on the right of the equal sign of the above equation are the loss of the Critic, and the third part is the added gradient penalty item proposed by the author. By adding gradient penalty term, WGAN-GP improves the slow convergence of WGAN model and the training speed.

The experiment from this paper shows that in terms of Generator iterations and Wall clock time (in seconds), the performance of WGAN-GP is close to that of DCGAN and far better than that of WGAN. At the same time, due to the balance problem between discriminator and generator in DCGAN, WGAN-GP is a better choice.","[['b56'], ['b20'], [], [], ['b74'], [], ['b57'], [], [], []]","[['b56'], ['b20'], [], [], ['b74'], [], ['b57'], [], [], []]",4,"sent1: In the previous part, we constructed a relatively stable network structure by introducing DCGAN.
sent2: While Martin Arjovsky, the author of WGAN [57], did an experiment on DCGAN, in which the generator was fixed, the discriminator was trained iteratively, and the relation graph of the gradient of the generator's objective function and the number of iterations was established.
sent3: From this experiment, with the iterative training of discriminator, the gradient of generator decays rapidly.
sent4: It can be known that the poorly trained discriminator will make the generator gradient unstable.
sent5: Sufficient training of discriminator will cause generator gradient to disappear.
sent6: Therefore, the training degree of discriminator is one of the important reasons for GAN instability.
sent7: According to Goodfellow's paper, under the condition of optimal discriminator, the gradient of generator can be composed of the Kullback-Leibler (KL) divergence and the Jensen-Shannon (JS) divergence [21].
sent8: In this case, mode collapse is easily caused, which means that the generated sample focuses on part of mode and lacks diversity.
sent9: For this reason, the author proposed the Earth-Mover (EM) distance and compared it with KL divergence and JS divergence, and found that the change of the EM distance was more sensitive and more useful gradient could be proposed.
sent10: The Kullback-Leibler (KL) divergenceThe Earth-Mover (EM) distance or Wasserstein-1Due to the advantages of EM distance, Arjovsky tried to apply it on the GAN, borrow the Kantorovich -Rubinstein duality theory [75] will get EM distance formula for the deformation, and then adapt the method of neural network to solve, the writer named the neural network Critic.
sent11: To satisfy the following equation, the Critic truncates the parameter to a range called weight clipping after each update.
sent12: There are three main differences between the Critic and the traditional discriminator of GAN.
sent13: First, since the discriminator is responsible for dichotomies in GAN, and the Critic's function is to fit EM distance, sigmoid function is removed, the probability is no longer output, but the general score.
sent14: Then, the target function of the Critic no longer contains log functions.
sent15: Last but not least, there is no need to worry about the effect of excessive training of the discriminator on the generation of samples.
sent16: The more training of the Critic, the better samples will be generated.
sent17: However, WGAN also has problems such as difficult convergence and poor sample quality, so the researchers finally focused on the weight clipping method in WGAN.
sent18: In other words, this method will lead to the weakening of model generation ability or gradient explosion (disappeared).
sent19: Gradient penalty was considered as one of the ways to speed up a solution to the problem, and WGAN-GP [58] was proposed.
sent20: The EM distance is proposed under the Lipschitz constraint, and the Lipschitz constraint requires that the discriminator gradient not exceed K. The authors of WGAN-GP believe that Lipschitz restriction does not need to be added to the entire sample space.
sent21: Instead, they focus on the generated samples, real samples and their intermediate regions.
sent22: The first two parts on the right of the equal sign of the above equation are the loss of the Critic, and the third part is the added gradient penalty item proposed by the author.
sent23: By adding gradient penalty term, WGAN-GP improves the slow convergence of WGAN model and the training speed.
sent24: The experiment from this paper shows that in terms of Generator iterations and Wall clock time (in seconds), the performance of WGAN-GP is close to that of DCGAN and far better than that of WGAN.
sent25: At the same time, due to the balance problem between discriminator and generator in DCGAN, WGAN-GP is a better choice."
232895648,Generative Adversarial Networks: A Literature Review,Computer Science,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,s19,Natural Language Processing (NLP),"Natural language processing is the study of normal communication between humans and computers using natural language [78]. At present, NLP is mostly based on statistical machine learning and applied in emotional processing, machine translation, text extraction and other directions [79][80][81]. However, NLP did not make great progress in the early days of GAN. The reason is that GAN is mainly applied to continuous data, while text is mainly discrete data. According to the discriminant result, the discriminator will give feedback to the generator after the sequence generated by the generator is inputted. With the efforts of researchers, GAN has made some achievements on NLP in recent years.","[['b78', 'b77', 'b80', 'b79']]","[['b78', 'b77', 'b80', 'b79']]",4,"sent1: Natural language processing is the study of normal communication between humans and computers using natural language [78].
sent2: At present, NLP is mostly based on statistical machine learning and applied in emotional processing, machine translation, text extraction and other directions [79][80][81].
sent3: However, NLP did not make great progress in the early days of GAN.
sent4: The reason is that GAN is mainly applied to continuous data, while text is mainly discrete data.
sent5: According to the discriminant result, the discriminator will give feedback to the generator after the sequence generated by the generator is inputted.
sent6: With the efforts of researchers, GAN has made some achievements on NLP in recent years."
232895648,Generative Adversarial Networks: A Literature Review,Computer Science,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,s21,Audio Generated,"With the development of advanced learning, the ability of computer natural language processing is gradually improved, but it is seldom applied in audio processing. Previous audio generation methods are generally based on text. This approach requires humans to record a large number of voice databases, which is inefficient and produces unnatural audio [85]. In 2016, Google DeepMind proposed a deep generation model of raw audio waveforms called WaveNet [86], which chose to directly model the original waveform of audio signals, expanding the variety of audio and increasing the authenticity of audio generation. However, since the audio has a sequence, WaveNet, which belongs to the autoregressive model, needs a long time to conduct continuous sampling, and researchers have been trying more generation methods, such as MelodyRNN, DeepBach and so on [87].

Due to GAN's high efficiency and quality in image generation, researchers have been trying to use GAN to generate music. SeqGAN proposed by Yu et al. in the last session can also generate audio, but it does not show the generated samples. Inspired by WaveNet, Yang et al. proposed a MidiNet model that combines CNN with GAN and generates a music that reaches the level of MelodyRNN in realism and pleasant [88]. Jesse et al. proposed a method to quickly generate high fidelity audio, which could be 50,000 times faster than current most commonly used WaveNet methods named GANSynth which adopts the architecture of Progressive GAN [89]. Different from WaveNet, GANSynth adopts the method of parallel sequence generation, and makes use of convolution to generate audio fragments on a single potential vector, so as to separate global features such as pitch and timbre.","[['b84', 'b85', 'b86'], ['b88', 'b87']]","[['b84', 'b85', 'b86'], ['b88', 'b87']]",5,"sent1: With the development of advanced learning, the ability of computer natural language processing is gradually improved, but it is seldom applied in audio processing.
sent2: Previous audio generation methods are generally based on text.
sent3: This approach requires humans to record a large number of voice databases, which is inefficient and produces unnatural audio [85].
sent4: In 2016, Google DeepMind proposed a deep generation model of raw audio waveforms called WaveNet [86], which chose to directly model the original waveform of audio signals, expanding the variety of audio and increasing the authenticity of audio generation.
sent5: However, since the audio has a sequence, WaveNet, which belongs to the autoregressive model, needs a long time to conduct continuous sampling, and researchers have been trying more generation methods, such as MelodyRNN, DeepBach and so on [87].
sent6: Due to GAN's high efficiency and quality in image generation, researchers have been trying to use GAN to generate music.
sent7: SeqGAN proposed by Yu et al. in the last session can also generate audio, but it does not show the generated samples.
sent8: Inspired by WaveNet, Yang et al. proposed a MidiNet model that combines CNN with GAN and generates a music that reaches the level of MelodyRNN in realism and pleasant [88].
sent9: Jesse et al. proposed a method to quickly generate high fidelity audio, which could be 50,000 times faster than current most commonly used WaveNet methods named GANSynth which adopts the architecture of Progressive GAN [89].
sent10: Different from WaveNet, GANSynth adopts the method of parallel sequence generation, and makes use of convolution to generate audio fragments on a single potential vector, so as to separate global features such as pitch and timbre."
232895648,Generative Adversarial Networks: A Literature Review,Computer Science,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,s25,Super-Resolution (SR),"Super-resolution usually is the reconstruction of one or more low-resolution images to generate high-resolution images [94]. In deep learning, super-resolution adopts SRCNN, DRCN and other methods [95]. In terms of super-resolution reconstruction, Wang et al. proposed Pix2pixHD [96], which further optimized Pix2pix by using a coarse-thin generator, a multi-scale discriminator structure and a robust antagonism learning objective function to achieve the purpose of high-resolution reconstruction and generate high-resolution images. In addition, SRGAN [97] based on GAN model training uses generator to generate detailed parts of images, and adopts perceptive loss function and counter loss function to increase the sense of reality of images. This method can not only be applied in the sharpening of old photos, but also to the interface optimization of some early games. Subsequently, based on the idea of SRGAN, ESRGAN [98] and other optimization models with clearer edge images were gradually proposed, which promoted the development of SR.","[['b95', 'b96', 'b97', 'b93', 'b94']]","[['b95', 'b96', 'b97', 'b93', 'b94']]",5,"sent1: Super-resolution usually is the reconstruction of one or more low-resolution images to generate high-resolution images [94].
sent2: In deep learning, super-resolution adopts SRCNN, DRCN and other methods [95].
sent3: In terms of super-resolution reconstruction, Wang et al. proposed Pix2pixHD [96], which further optimized Pix2pix by using a coarse-thin generator, a multi-scale discriminator structure and a robust antagonism learning objective function to achieve the purpose of high-resolution reconstruction and generate high-resolution images.
sent4: In addition, SRGAN [97] based on GAN model training uses generator to generate detailed parts of images, and adopts perceptive loss function and counter loss function to increase the sense of reality of images.
sent5: This method can not only be applied in the sharpening of old photos, but also to the interface optimization of some early games.
sent6: Subsequently, based on the idea of SRGAN, ESRGAN [98] and other optimization models with clearer edge images were gradually proposed, which promoted the development of SR."
232895648,Generative Adversarial Networks: A Literature Review,Computer Science,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,s28,Information Security,"Information is universal, shareable, value-added, manageable and multifunctional, which makes it especially important for human beings [102]. The meaning of information security is to protect all kinds of information resources from all kinds of threats, interference and destruction [103,104]. The game mechanism of attackers and defenders is similar to GAN, which has made some contributions to the improvement of information security, especially in cryptography.

In 2016, Google proposed an encryption technology based on GAN [105], which could effectively solve the data protection problem in the process of data sharing. Cryptography can be both defensive and offensive, and GAN is also applied to decryption technology. Briland et al. raised a way of cryptography generation based upon machine learning theory to replace artificially generated cryptography rules, which was named PassGAN [106]. By using the leaked password list as the real sample to train discriminator, the sample generated by the generator will be closer and closer to the real user's password and complete the password guessing process. In 2018, Aidan N came up with an unsupervised method of deciphering the code, named cipherGAN [107]. After the training of unmatched plaintext ciphertext, this method can decode the Caesar shift code or Virginia code with high fidelity. Inspired by CycleGAN, this model adopts unmatched plaintext and ciphertext, and completes the key decoding of long word level without parallel text.","[['b102', 'b101', 'b103'], ['b104', 'b105', 'b106']]","[['b102', 'b101', 'b103'], ['b104', 'b105', 'b106']]",6,"sent1: Information is universal, shareable, value-added, manageable and multifunctional, which makes it especially important for human beings [102].
sent2: The meaning of information security is to protect all kinds of information resources from all kinds of threats, interference and destruction [103,104].
sent3: The game mechanism of attackers and defenders is similar to GAN, which has made some contributions to the improvement of information security, especially in cryptography.
sent4: In 2016, Google proposed an encryption technology based on GAN [105], which could effectively solve the data protection problem in the process of data sharing.
sent5: Cryptography can be both defensive and offensive, and GAN is also applied to decryption technology.
sent6: Briland et al. raised a way of cryptography generation based upon machine learning theory to replace artificially generated cryptography rules, which was named PassGAN [106].
sent7: By using the leaked password list as the real sample to train discriminator, the sample generated by the generator will be closer and closer to the real user's password and complete the password guessing process.
sent8: In 2018, Aidan N came up with an unsupervised method of deciphering the code, named cipherGAN [107].
sent9: After the training of unmatched plaintext ciphertext, this method can decode the Caesar shift code or Virginia code with high fidelity.
sent10: Inspired by CycleGAN, this model adopts unmatched plaintext and ciphertext, and completes the key decoding of long word level without parallel text."
232895648,Generative Adversarial Networks: A Literature Review,Computer Science,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,s4,Auto-regressive Network,"Auto-regressive models are often applied to the prediction of economics, informatics and natural phenomena [26]. The auto-regressive model is a directed probability model without potential random variables and belongs to the category of supervised learning. Auto-regressive models are often used to deal with problems of time series [27].

Pixel recursive neural network (Pixel RNN) [28] is a relatively new generation method in recent years, the basic idea of this model is to generate images from one pixel to another, and the former also can be a reference for the latter. Finally, the prediction of the joint distribution of each pixel on the image is converted into the conditional distribution. The specific prediction is as follows.

In addition to RNN [29,30], the author also adopted the method of convolutional neural network (CNN) to carry out convolution around the generated pixel points and later researchers also improved and optimized this kind of approach [31,32]. For DeepMind's latest research, an Autoregressive Model of 3D Meshes was proposed [33]. This method improves the quality of grid vertex prediction from 2D to 3D. At the same time, the development of autoregressive networks proves that this method is still applicable to the latest generation requirements.

Compared with GAN, the advantage of the Auto-regressive network is to explicitly calculate likelihood and put forward a good evaluation measure explicitly. The disadvantage is that the generation speed is slow, and the resolution of the picture is not high.","[['b25', 'b26'], ['b27'], ['b29', 'b28', 'b31', 'b30', 'b32'], []]","[['b25', 'b26'], ['b27'], ['b29', 'b28', 'b31', 'b30', 'b32'], []]",8,"sent1: Auto-regressive models are often applied to the prediction of economics, informatics and natural phenomena [26].
sent2: The auto-regressive model is a directed probability model without potential random variables and belongs to the category of supervised learning.
sent3: Auto-regressive models are often used to deal with problems of time series [27].
sent4: Pixel recursive neural network (Pixel RNN)
sent5: [28] is a relatively new generation method in recent years, the basic idea of this model is to generate images from one pixel to another, and the former also can be a reference for the latter.
sent6: Finally, the prediction of the joint distribution of each pixel on the image is converted into the conditional distribution.
sent7: The specific prediction is as follows.
sent8: In addition to RNN [29,30], the author also adopted the method of convolutional neural network (CNN) to carry out convolution around the generated pixel points and later researchers also improved and optimized this kind of approach [31,32].
sent9: For DeepMind's latest research, an Autoregressive Model of 3D Meshes was proposed [33].
sent10: This method improves the quality of grid vertex prediction from 2D to 3D.
sent11: At the same time, the development of autoregressive networks proves that this method is still applicable to the latest generation requirements.
sent12: Compared with GAN, the advantage of the Auto-regressive network is to explicitly calculate likelihood and put forward a good evaluation measure explicitly.
sent13: The disadvantage is that the generation speed is slow, and the resolution of the picture is not high."
232895648,Generative Adversarial Networks: A Literature Review,Computer Science,https://www.semanticscholar.org/paper/c7db4378b9a677a301e25f7340e5ee1db904d571,s29,Cyber Security,"Nowadays, Cyber Security has attracted more and more attention from researchers because of the progress of big data, Internet of things (IoT), blockchain and other hot spots [108]. At the same time, the gradually increasing numbers of network anomalies threaten the normal operation of the network, such as Challenge Collapsar (CC) attack, distributed denial of service (DDoS) attack, malware, worm [109]. However, network abnormal behavior is not a simple image or text, and it is difficult to process with GAN. Meanwhile, as the detection technology of abnormal network behaviors, the researchers turn to the source of the attack and plan to use the method of generating abnormal behavior samples to simulate the attack, so as to improve the detection ability of the existing detection technology.

Hu et al. proposed MalGAN [110], a generation model of malware. This model used a neural network-based alternative detector to match the black box detection to generate samples that could fool the detector so as to by passing the black box detection. The DeepDGA [111] algorithm can generate a large number of pseudo-random domain names through the training of GAN. Kim et al. proposed tDCGAN [112], which turned the malicious software into pictures, adopted self-encoder as the generator of GAN, and finally trained discriminator that adopted transfer learning method to detect zero-day malicious software.","[['b107', 'b108'], ['b111', 'b110', 'b109']]","[['b107', 'b108'], ['b111', 'b110', 'b109']]",5,"sent1: Nowadays, Cyber Security has attracted more and more attention from researchers because of the progress of big data, Internet of things (IoT), blockchain and other hot spots [108].
sent2: At the same time, the gradually increasing numbers of network anomalies threaten the normal operation of the network, such as Challenge Collapsar (CC) attack, distributed denial of service (DDoS) attack, malware, worm [109].
sent3: However, network abnormal behavior is not a simple image or text, and it is difficult to process with GAN.
sent4: Meanwhile, as the detection technology of abnormal network behaviors, the researchers turn to the source of the attack and plan to use the method of generating abnormal behavior samples to simulate the attack, so as to improve the detection ability of the existing detection technology.
sent5: Hu et al. proposed MalGAN [110], a generation model of malware.
sent6: This model used a neural network-based alternative detector to match the black box detection to generate samples that could fool the detector so as to by passing the black box detection.
sent7: The DeepDGA [111] algorithm can generate a large number of pseudo-random domain names through the training of GAN.
sent8: Kim et al. proposed tDCGAN [112], which turned the malicious software into pictures, adopted self-encoder as the generator of GAN, and finally trained discriminator that adopted transfer learning method to detect zero-day malicious software."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s46,Text Summarization,"In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature. Researchers and domain experts need to go through a number of biomedical documents. As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186]. Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187]. There are no standard datasets for biomedical text summarization. Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].

Moradi et al. [188] proposed a novel approach to summarize biomedical scientific articles. They embedded sentences, generated clusters, and then extracted the most informative sentences from each of the clusters. They showed that BERT-large outperformed other models including the in-domain BERT models like BioBERT.

In the case of small models, BioBERT outperformed others. Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text. They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences. Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text. BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence. The sentences with the highest score are considered as the summary. Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT.","[['b189', 'b190', 'b186', 'b187', 'b188'], ['b189'], ['b190', 'b191', 'b75']]","[['b189', 'b190', 'b186', 'b187', 'b188'], ['b189'], ['b190', 'b191', 'b75']]",9,"sent1: In general, sources of biomedical information like clinical notes, scientific papers, radiology reports are length in nature.
sent2: Researchers and domain experts need to go through a number of biomedical documents.
sent3: As biomedical documents are length in nature, there is a need for automatic biomedical text summarization which reduces the effort and time for researchers and domain experts [185], [186].
sent4: Text summarization falls into two broad categories namely extractive summarization which identifies the most relevant sentences in the document while abstractive summarization generates new text which represents the summary of the document [187].
sent5: There are no standard datasets for biomedical text summarization.
sent6: Researchers usually treat scientific papers as documents and their abstracts as summaries [188], [189].Moradi et al. [188] proposed a novel approach to summarize biomedical scientific articles.
sent7: They embedded sentences, generated clusters, and then extracted the most informative sentences from each of the clusters.
sent8: They showed that BERT-large outperformed other models including the in-domain BERT models like BioBERT.
sent9: In the case of small models, BioBERT outperformed others.
sent10: Moradi et al. [189] proposed a novel approach based on word embeddings and graph ranking to summarize the biomedical text.
sent11: They generated a graph with sentences as nodes and edges as relations whose strength is measured by cosine similarity between sentence vectors generated by averaging BioBERT and Glove embeddings and finally, graph ranking algorithms identify the important sentences.
sent12: Du et al. [190] introduced a novel approach called BioBERTSum to summarize the biomedical text.
sent13: BioBERTSum uses BioBERT to encode sentences, transformer decoder + sigmoid to calculate the scores for each sentence.
sent14: The sentences with the highest score are considered as the summary.
sent15: Chen et al. [75] proposed a novel clinical text summarization based on AlphaBERT."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s36,Green Models,"CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus. As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords. This kind of representation increases the overall length of the input as well as hinders the model learning. DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text. However, both these approaches involve learning the model parameters from scratch which is highly expensive. These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].

Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123]. These models are referred to as Green Models as they are developed in a low cost environment-friendly approach. GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors. The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors. With the addition of domain-specific word vectors, the model acquires domain-specific knowledge. The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction. This approach is completely inexpensive as it requires only CPU. exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module. The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text. During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive. Table 9 contains summary of Green T-BPLMs.","[['b123', 'b122'], ['b123', 'b122']]","[['b123', 'b122'], ['b123', 'b122']]",4,"sent1: CPT allows the general T-PLMs to adapt to in-domain by further pretraining on large volumes of the in-domain corpus.
sent2: As these models contain vocabulary, which is learned over general text, they cannot represent indomain words in a meaningful way as many of the indomain words are split into a number of subwords.
sent3: This kind of representation increases the overall length of the input as well as hinders the model learning.
sent4: DSPT or SPT allows the model to have an in-domain vocabulary that is learned over in-domain text.
sent5: However, both these approaches involve learning the model parameters from scratch which is highly expensive.
sent6: These approaches being expensive, are far away from the small research labs, and with long runtimes, they are environmentally unfriendly also [122], [123].
sent7: Recently there is raising interest in the biomedical research community to adapt general T-PLMs models to in-domain in a low cost way and the models contain in-domain vocabulary also [122], [123].
sent8: These models are referred to as Green Models as they are developed in a low cost environment-friendly approach.
sent9: GreenBioBERT [122] -extends general BERT to the biomedical domain by refining the embedding layer with domain-specific word vectors.
sent10: The authors generated indomain vocabulary using Word2Vec and then aligned them with general WordPiece vectors.
sent11: With the addition of domain-specific word vectors, the model acquires domain-specific knowledge.
sent12: The authors showed that GreenBioBERT achieves competitive performance compared to BioBERT in entity extraction.
sent13: This approach is completely inexpensive as it requires only CPU.
sent14: exBERT [123] -extends general BERT to the biomedical domain by refining the model with two additions a) in-domain WordPiece vocabulary in addition to existing general domain WordPiece vocabulary b) extension module.
sent15: The in-domain WordPiece vectors and extension module parameters are learned during pretraining on biomedical text.
sent16: During pretraining, as the parameters of general BERT are kept fixed, this approach is quite inexpensive.
sent17: Table 9 contains summary of Green T-BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s35,Ontology Enriched,"T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks. These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text. Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] . Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets. Here, pretraining involves three loss functions namely MLM, NSP, and triple classification. Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model. Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP. The novel multi-label loss function allows the model to connect all the words under the same CUI. CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss. Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge. SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss.  ","[['b45', 'b46', 'b120', 'b44', 'b33']]","[['b45', 'b46', 'b120', 'b44', 'b33']]",5,"sent1: T-BPLMs like BioBERT, BlueBERT and PubMedBERT have achieved good results in many biomedical NLP tasks.
sent2: These models acquire domain-specific knowledge by pretraining on large volumes of biomedical text.
sent3: Recent works [34], [45]- [47] showed that these models acquire only the knowledge available in pretraining corpora and the performance of these models can be   [34], [45]- [47] .
sent4: Clinical Kb-BERT and Clinical Kb-ALBERT [45] are obtained by further pretraining BioBERT and ALBERT models on MIMIC-III clinical notes and UMLS relation triplets.
sent5: Here, pretraining involves three loss functions namely MLM, NSP, and triple classification.
sent6: Triple classification involves identifying whether two concepts are connected by the relation or not and helps to inject UMLS relationship knowledge into the model.
sent7: Umls-BERT [46] is initialized from ClinicalBERT and further pretrained on MIMIC-III clinical notes using novel multilabel loss-based MLM and NSP.
sent8: The novel multi-label loss function allows the model to connect all the words under the same CUI.
sent9: CoderBERT [47] is initialized from BioBERT and further pre-trained on UMLS concepts and relations using multi-similarity loss and knowledge embedding loss.
sent10: Multi-similarity loss helps to learn close embeddings for entities under the same CUI and Knowledge embedding loss helps to inject relationship knowledge.
sent11: SapBERT [120] is initialized from PubMedBERT and further pre-trained on UMLS synonyms using multisimilarity loss."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s34,Language-Specific,"Following the success of BioBERT, ClinicalBERT, Pub-MedBERT in English biomedical tasks, researchers focused on developing T-BPLMs for other languages also by pretraining from scratch [41], [91], [111] or pretraining from Multilingual BERT [95], [97] or pretraining from monolingual BERT [48], [91], [112] models. For example, CHMBERT [112] is the first Chinese medical BERT model which is initialized from the general Chinese BERT model and further pretrained on a huge (185GB) corpus of Chinese medical text gathered from more than 100 hospitals. MC-BERT [48] is also initialized from general Chinese BERT and further pre-trained on hybrid corpora which includes general, biomedical, and medical texts. Table 7 contains a summary of language-specific T-BPLMs.","[['b111', 'b47', 'b95', 'b97', 'b91', 'b112', 'b40']]","[['b111', 'b47', 'b95', 'b97', 'b91', 'b112', 'b40']]",7,"sent1: Following the success of BioBERT, ClinicalBERT, Pub-MedBERT in English biomedical tasks, researchers focused on developing T-BPLMs for other languages also by pretraining from scratch [41], [91], [111] or pretraining from Multilingual BERT [95], [97] or pretraining from monolingual BERT [48], [91], [112] models.
sent2: For example, CHMBERT [112] is the first Chinese medical BERT model which is initialized from the general Chinese BERT model and further pretrained on a huge (185GB) corpus of Chinese medical text gathered from more than 100 hospitals.
sent3: MC-BERT [48] is also initialized from general Chinese BERT and further pre-trained on hybrid corpora which includes general, biomedical, and medical texts.
sent4: Table 7 contains a summary of language-specific T-BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s32,Hybrid Corpora,"It is difficult to obtain a large amount of in-domain text in some cases. For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.  The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC). However, to pretrain a transformer-based PLM from scratch, we require large volumes of text. To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97]. For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs.","[['b88', 'b27', 'b104', 'b17', 'b97', 'b42', None, 'b40', 'b87']]","[['b88', 'b27', 'b104', 'b17', 'b97', 'b42', None, 'b40', 'b87']]",9,"sent1: It is difficult to obtain a large amount of in-domain text in some cases.
sent2: For example, MIMIC [87], [88] is the largest publicly available dataset of medical records.
sent3: The MIMIC dataset is small compared to the general Wikipedia corpus or biomedical scientific literature (from PubMed + PMC).
sent4: However, to pretrain a transformer-based PLM from scratch, we require large volumes of text.
sent5: To overcome this, some of the models are pretrained on general + in-domain text [41], [104] or, in-domain + related domain text [18], [28], [43], [97].
sent6: For example, BERT (jpCR+jpW) [ Table 6 contains hybrid corpora-based T-BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s31,Scientific Literature,"In the last few decades, the amount of biomedical literature is growing at a rapid scale. As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16]. However, biomedical text significantly differs from the general text with a lot of domain-specific words. As a result, the performance of general T-PLMs is limited in many of the tasks. So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text. PubMed and PMC are the two popular sources of biomedical text. PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles. As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles. Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].

As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text. BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature. BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts). BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus. BioMedBERT outperformed BioBERT on biomedical question answering.

The key reason for the better performance of BioMed-BERT is the diversity of biomedical text in the BREATHE corpus. The main drawback in developing biomedical models by further pretraining general models is the general vocabulary. To overcome this, researchers started to develop biomedical models by using DSPT. Microsoft researchers developed PubMed [20] model by DSPT with in-domain vocabulary and whole word masking strategy. OuBioBERT [21] is trained from scratch using focused PubMed abstracts (280M words) as core corpora and PubMed abstracts (2800M words) as satellite corpora. It outperforms BioBERT and BlueBERT in many of the tasks in the BLUE benchmark. Table 5 contains summary of scientific literature-based T-BPLMs.","[['b96', 'b15', 'b42'], ['b97', 'b15', 'b100'], ['b20', 'b19']]","[['b96', 'b15', 'b42'], ['b97', 'b15', 'b100'], ['b20', 'b19']]",8,"sent1: In the last few decades, the amount of biomedical literature is growing at a rapid scale.
sent2: As knowledge discovery from biomedical literature is useful in many applications, biomedical text mining is gaining popularity in the research community [16].
sent3: However, biomedical text significantly differs from the general text with a lot of domain-specific words.
sent4: As a result, the performance of general T-PLMs is limited in many of the tasks.
sent5: So, biomedical researchers focused on developing in-domain T-PLMs to handle biomedical text.
sent6: PubMed and PMC are the two popular sources of biomedical text.
sent7: PubMed con-  tains only biomedical literature citations and abstracts only while PMC contains full-text biomedical articles.
sent8: As of March 2020, PubMed includes 30M citations and abstracts while PMC contains 7.5M full-text articles.
sent9: Due to the large collection and broad coverage, these two are the first choice to pretrain T-BPLMs [16], [43], [96].
sent10: As DSPT is expensive, most of the works developed in-domain T-PLMs by initializing from general BERT models and then further pretraining on biomedical text.
sent11: BioBERT [16] is the first biomedical pre-trained language model which is obtained by further pretraining general BERT on biomedical literature.
sent12: BioBERTpt-bio [97] is obtained by further pretraining BERT Multilingual (base) on Brazilian biomedical corpus -scientific papers from PubMed (0.8M-only literature titles) + Scielo (health -12.4M + biological-3.2M: both titles and abstracts).
sent13: BioMedBERT [100] is obtained by further pretraining BERT-large on BREATHE 1.0 corpus.
sent14: BioMedBERT outperformed BioBERT on biomedical question answering.
sent15: The key reason for the better performance of BioMed-BERT is the diversity of biomedical text in the BREATHE corpus.
sent16: The main drawback in developing biomedical models by further pretraining general models is the general vocabulary.
sent17: To overcome this, researchers started to develop biomedical models by using DSPT.
sent18: Microsoft researchers developed PubMed [20] model by DSPT with in-domain vocabulary and whole word masking strategy.
sent19: OuBioBERT [21] is trained from scratch using focused PubMed abstracts (280M words) as core corpora and PubMed abstracts (2800M words) as satellite corpora.
sent20: It outperforms BioBERT and BlueBERT in many of the tasks in the BLUE benchmark.
sent21: Table 5 contains summary of scientific literature-based T-BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s30,Social Media,"In the last decade, social media has become the first choice for internet users to express their thoughts. Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers [107], [108]. Recent works have shown that health-related social media data is useful in many applications to provide better health-related services [109], [110]. The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets [92]. This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos. Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts. CT-BERT [92] is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30% improvement compared to BERT-large, on five different classification datasets. BioRedditBERT [94] is initialized from BioBERT and further pretrained on healthrelated Reddit posts. The authors showed that BioRed-ditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in normalizing health-related entity mentions. RuDR-BERT [95] is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews). The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins. EnRuDR-BERT [95] and EnDR-BERT [95] are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively. Table 4 contains summary of social media text-based BPLMs.","[['b95', 'b107', 'b110', 'b109', 'b108', 'b92', 'b94']]","[['b95', 'b107', 'b110', 'b109', 'b108', 'b92', 'b94']]",7,"sent1: In the last decade, social media has become the first choice for internet users to express their thoughts.
sent2: Apart from views about general topics, various social media platforms like Twitter, Reddit, AskAPatient, WebMD are used to share health-related experiences in the form of tweets, reviews, questions, and answers [107], [108].
sent3: Recent works have shown that health-related social media data is useful in many applications to provide better health-related services [109], [110].
sent4: The performance of general T-PLMs on Wikipedia and Books Corpus is limited on health-related social media datasets [92].
sent5: This is because social media text is highly informal with a lot of nonstandard abbreviations, irregular grammar, and typos.
sent6: Researchers working at the intersection of social media and health, trained social media text-based T-BPLMs to handle social media texts.
sent7: CT-BERT [92] is initialized from BERT-large and further pretrained on a corpus of 22.5M covid related tweets and this model showed up to 30% improvement compared to BERT-large, on five different classification datasets.
sent8: BioRedditBERT [94] is initialized from BioBERT and further pretrained on healthrelated Reddit posts.
sent9: The authors showed that BioRed-ditBERT outperforms in-domains models like BioBERT, BlueBERT, PubMedBERT, ClinicalBERT by up to 1.8% in normalizing health-related entity mentions.
sent10: RuDR-BERT [95] is initialized from Multilingual BERT and pretrained on the raw part of the RuDReC corpus (1.4M reviews).
sent11: The authors showed that RuDR-BERT outperforms multilingual BERT and Russian BERT on Russian sentence classification and clinical entity extraction datasets by large margins.
sent12: EnRuDR-BERT [95] and EnDR-BERT [95] are obtained by further pretraining multilingual BERT on Russian and English health reviews and English health reviews respectively.
sent13: Table 4 contains summary of social media text-based BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s44,Text Classification,"Text classification involves assigning one of the predefined labels to variable-length texts like phrases, sentences, paragraphs, or documents. Text classification involves an encoder which is usually a transformerbased PLM and a task-specific softmax classifier. '[CLS]' vector or weighted sum of final hidden state vectors is treated as an aggregate representation of given text. The fully connected dense layer in the classifier projects text representation vector into n-dimensional vector space where 'n' represents the number of predefined labels. Finally, the softmax function is applied to get the probabilities of all the labels. Garadi et al. [172] formulated prescription medication (PM) identification from tweets as four-way text classification. They achieved good results using models like BERT, RoBERTa, XLNet, ALBERT, and DistillBERT. They trained different ML-based meta classifiers with predictions from pre-trained models as inputs and further improved the results.

Shen et al. [173] applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT. They generated labels for the training instances using a rule-based NLP algorithm. Chen et al. [174] further pretrained the general BERT model on 1.5 drugrelated tweets and showed that further pretraining improves the performance of the model on ADR tweets classification. Recent works [175], [176] showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification. They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability.","[['b172'], ['b175', 'b177', 'b176', 'b174']]","[['b172'], ['b175', 'b177', 'b176', 'b174']]",5,"sent1: Text classification involves assigning one of the predefined labels to variable-length texts like phrases, sentences, paragraphs, or documents.
sent2: Text classification involves an encoder which is usually a transformerbased PLM and a task-specific softmax classifier.
sent3: '[CLS]' vector or weighted sum of final hidden state vectors is treated as an aggregate representation of given text.
sent4: The fully connected dense layer in the classifier projects text representation vector into n-dimensional vector space where 'n' represents the number of predefined labels.
sent5: Finally, the softmax function is applied to get the probabilities of all the labels.
sent6: Garadi et al. [172] formulated prescription medication (PM) identification from tweets as four-way text classification.
sent7: They achieved good results using models like BERT, RoBERTa, XLNet, ALBERT, and DistillBERT.
sent8: They trained different ML-based meta classifiers with predictions from pre-trained models as inputs and further improved the results.
sent9: Shen et al. [173] applied various in-domain BERT for Alzheimer disease clinical notes classification and achieved the best results using PubMedBERT and BioBERT.
sent10: They generated labels for the training instances using a rule-based NLP algorithm.
sent11: Chen et al. [174] further pretrained the general BERT model on 1.5 drugrelated tweets and showed that further pretraining improves the performance of the model on ADR tweets classification.
sent12: Recent works [175], [176] showed that adding attention on the top of the BERT model improves the performance of the model in clinical text classification.
sent13: They introduced a custom attention model to aggregate the encoder output and showed that this leads to improved performance and interpretability."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s28,Electronic Health Records,"In the last decade, most hospitals have been using Electronic Health Records (EHRs) to record patient as well as treatment details right from admission to discharge [82]. EHRs contain a vast amount of medical data which can be used to provide better patient care by knowledge discovery and the development of better algorithms. As EHR contains sensitive information related to patients, medical data must be de-identified before sharing. EHRs include both structured and unstructured data [83], [84]. Structured data includes laboratory test results, various medical codes, etc. Unstructured data include clinical notes like medication instructions, progress notes, discharge summaries, etc. Clinical notes include the most valuable patient information which is difficult and expensive to extract manually. So, there is a need for automatic information extraction methods to utilize the abundant medical data from EHRs in research as well as applications [84]- [86]. Following the success of transformer-based PLMs in the general domain, researchers in biomedical NLP also developed EHR-based T-BPLMs by pretraining over clinical notes or medical codes or both. MIMIC [87], [88] [19] further pre-trained XLNetbase on MIMIC-III Clinical Notes. The authors released two models namely a) pretrained model on nursing notes and b) pretrained model on discharge summaries. Yang et al. [39] further pre-trained general models like BERT, ELECTRA, RoBERTa, XLNet, and ALBERT on MIMIC-III and released in-domain PLMs. It is the first work to release in-domain models based on all the popular transformer-based PLMs. Unlike the above pretrained models which are pretrained on clinical text, recent works [27], [31], [39] released models which are pre-trained on disease codes or multi-modal EHR data. BEHRT [27] is trained from scratch using 1.6 million patient EHR data with MLM as pretraining task. The authors used code, position, age, and segment embeddings. Med-BERT [31] is trained from scratch using 28,490,650 patient EHR data with MLM and LOS (Length of Stay) as pretraining tasks. The authors used code, serialization and visit embeddings. BERT-EHR [32] is trained from scratch using multi-modal data from 43967 patient records with MLM as a pretraining task. Table 2 contains summary of various EHR based BPLMs.","[['b88', 'b82', 'b84', 'b26', 'b38', 'b18', 'b83', 'b31', 'b30', 'b87', 'b86']]","[['b88', 'b82', 'b84', 'b26', 'b38', 'b18', 'b83', 'b31', 'b30', 'b87', 'b86']]",11,"sent1: In the last decade, most hospitals have been using Electronic Health Records (EHRs) to record patient as well as treatment details right from admission to discharge [82].
sent2: EHRs contain a vast amount of medical data which can be used to provide better patient care by knowledge discovery and the development of better algorithms.
sent3: As EHR contains sensitive information related to patients, medical data must be de-identified before sharing.
sent4: EHRs include both structured and unstructured data [83], [84].
sent5: Structured data includes laboratory test results, various medical codes, etc.
sent6: Unstructured data include clinical notes like medication instructions, progress notes, discharge summaries, etc.
sent7: Clinical notes include the most valuable patient information which is difficult and expensive to extract manually.
sent8: So, there is a need for automatic information extraction methods to utilize the abundant medical data from EHRs in research as well as applications [84]- [86].
sent9: Following the success of transformer-based PLMs in the general domain, researchers in biomedical NLP also developed EHR-based T-BPLMs by pretraining over clinical notes or medical codes or both.
sent10: MIMIC [87], [88] [19] further pre-trained XLNetbase on MIMIC-III Clinical Notes.
sent11: The authors released two models namely a) pretrained model on nursing notes and b) pretrained model on discharge summaries.
sent12: Yang et al. [39] further pre-trained general models like BERT, ELECTRA, RoBERTa, XLNet, and ALBERT on MIMIC-III and released in-domain PLMs.
sent13: It is the first work to release in-domain models based on all the popular transformer-based PLMs.
sent14: Unlike the above pretrained models which are pretrained on clinical text, recent works [27], [31], [39] released models which are pre-trained on disease codes or multi-modal EHR data.
sent15: BEHRT [27] is trained from scratch using 1.6 million patient EHR data with MLM as pretraining task.
sent16: The authors used code, position, age, and segment embeddings.
sent17: Med-BERT [31] is trained from scratch using 28,490,650 patient EHR data with MLM and LOS (Length of Stay) as pretraining tasks.
sent18: The authors used code, serialization and visit embeddings.
sent19: BERT-EHR [32] is trained from scratch using multi-modal data from 43967 patient records with MLM as a pretraining task.
sent20: Table 2 contains summary of various EHR based BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s37,Debiased Models,"T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. Zhang et al. [127] further pretrained SciBERT [105] on MIMIC-III clinical notes and showed that the performance of the model is different for different groups. The authors applied adversarial pretraining debiasing to reduce the gender bias in the model. The authors released both the models publicly to encourage further research in debiasing T-BPLMs.","[['b126', 'b105', 'b124', 'b127']]","[['b126', 'b105', 'b124', 'b127']]",4,"sent1: T-PLMs are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others.
sent2: The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126].
sent3: It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group.
sent4: Zhang et al. [127] further pretrained SciBERT [105] on MIMIC-III clinical notes and showed that the performance of the model is different for different groups.
sent5: The authors applied adversarial pretraining debiasing to reduce the gender bias in the model.
sent6: The authors released both the models publicly to encourage further research in debiasing T-BPLMs."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s25,Auxiliary Embeddings,"Main embeddings represent the given input sequence in low dimensional space. The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better. For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings. The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence. As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings. Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].

Segment Embeddings -Segment embeddings help to Age Embeddings -In models like BEHRT [27] and BERT-EHR [32], age embeddings are used in addition to other embeddings. Age embeddings provide the age of the patient and help the model to leverage temporal information. Age embedding is the same for all the codes in a single patient visit.

Gender Embeddings -In models like BEHRT [27] and BERT-EHR [32], gender embeddings are used in addition to other embeddings. Gender embeddings provide the gender information of the patient to the model. Gender embedding is the same for all the codes in all the patient visits.

Semantic Group Embeddings -Semantic group embeddings are used in UmlsBERT [46] to explicitly inform the model to learn similar representations for words from the same semantic group i.e., semantic group embedding is same for all the words which fall into the same semantic group. Besides, it also helps to provide better representations for rare words. Figure 14 shows transformer-based BPLMs taxonomy.","[['b1', 'b31', 'b26'], ['b31', 'b26'], ['b31', 'b26'], ['b45']]","[['b1', 'b31', 'b26'], ['b31', 'b26'], ['b31', 'b26'], ['b45']]",8,"sent1: Main embeddings represent the given input sequence in low dimensional space.
sent2: The purpose of auxiliary embeddings is to provide additional information to the model so that the model can learn better.
sent3: For each input token, a representation vector is obtained by summing the main and two or more auxiliary embeddings.
sent4: The various auxiliary embeddings are Position Embeddings -Position embeddings enhance the final input representation of a token by providing its position information in the input sequence.
sent5: As there is no convolution or recurrence layers which can learn the order of input tokens automatically, we need to explicitly provide the location of each token in the input sequence through position embeddings.
sent6: Position embeddings can be pre-determined [27], [32] or learned during model pretraining [2].
sent7: Segment Embeddings -Segment embeddings help to Age Embeddings -In models like BEHRT [27] and BERT-EHR [32], age embeddings are used in addition to other embeddings.
sent8: Age embeddings provide the age of the patient and help the model to leverage temporal information.
sent9: Age embedding is the same for all the codes in a single patient visit.
sent10: Gender Embeddings -In models like BEHRT [27] and BERT-EHR [32], gender embeddings are used in addition to other embeddings.
sent11: Gender embeddings provide the gender information of the patient to the model.
sent12: Gender embedding is the same for all the codes in all the patient visits.
sent13: Semantic Group Embeddings -Semantic group embeddings are used in UmlsBERT [46] to explicitly inform the model to learn similar representations for words from the same semantic group i.e., semantic group embedding is same for all the words which fall into the same semantic group.
sent14: Besides, it also helps to provide better representations for rare words.
sent15: Figure 14 shows transformer-based BPLMs taxonomy."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s22,Multi-Task Fine-Tuning,"Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. At the same time, due to the increase in training set size, the model is less prone to over-fitting. Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]. Moreover, having a single model for multitasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs [70]. Multi-task fine-tuning may not provide the best results all the time [70]. In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks [71]. For example, Mahanjan et al. [71] applied multi-task finetuning iteratively to choose the best subset of related datasets. Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS [72] dataset. After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model [73].","[['b67', 'b70', 'b71', 'b73', 'b72', 'b69']]","[['b67', 'b70', 'b71', 'b73', 'b72', 'b69']]",6,"sent1: Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]-[69].
sent2: Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer.
sent3: Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks.
sent4: At the same time, due to the increase in training set size, the model is less prone to over-fitting.
sent5: Multi-task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69].
sent6: Moreover, having a single model for multitasks eliminates the need of deploying separate models for each task which saves computational resources, time, and deployment costs [70].
sent7: Multi-task fine-tuning may not provide the best results all the time [70].
sent8: In such cases, multi-task fine tuning can be applied iteratively to identify the best possible subset of tasks [71].
sent9: For example, Mahanjan et al. [71] applied multi-task finetuning iteratively to choose the best subset of related datasets.
sent10: Finally, the authors fine-tuned the model on best subset of related datasets and achieved the best results on the Clinical STS [72] dataset.
sent11: After multi-task fine-tuning, the model can be further fine-tuned on the target specific dataset separately to further enhance the performance of the model [73]."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s21,Intermediate Fine-Tuning (IFT),"IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets. IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains. Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58]. For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].

Same Task Same Domain -Here, the source and target datasets are from the same task and domain. But the source dataset is a more generic one while the target dataset is more specific [62], [63]. For example, Gao et al. [63] fine-tuned BlueBERT on a large general biomedical NER corpus like MedMentions [64] or Semantic Medline before fine-tuning on the small target NER corpus.

Different Task Same Domain -Here, the source and target datasets are from different tasks but the same domain. Fine-tuning on source dataset which is from the same domain allows the model to gain more domainspecific knowledge which improves the performance of the model on the same domain target task [65]. McCreery et al. [65] fine-tuned the model on the medical questionanswer pairs dataset to enhance its performance on the medical question similarity dataset.

Different Task Different Domain -Here, the source and target datasets are from different tasks and different domains. For example, Jeong et al. [66] fine-tuned BioBERT on a general MultiNLI dataset to improve the performance of the model in biomedical QA. Here the model learns sentence level reasoning skills which are useful in biomedical QA.","[['b59', 'b57', 'b60', 'b54', 'b58'], ['b64', 'b62', 'b63'], ['b65'], ['b66']]","[['b59', 'b57', 'b60', 'b54', 'b58'], ['b64', 'b62', 'b63'], ['b65'], ['b66']]",10,"sent1: IFT on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets.
sent2: IFT can be done in following four ways Same Task Different Domain -Here, the source and target datasets are from the same task but different domains.
sent3: Model can be fine-tuned on general domain datasets before fine-tuning on small in-domain datasets [55]- [58].
sent4: For example, Cengiz et al. [55] fine-tuned indomain model on general NLI datasets like SNLI [59] and MNLI [60] before fine-tuning on MedNLI [61].
sent5: Same Task Same Domain -Here, the source and target datasets are from the same task and domain.
sent6: But the source dataset is a more generic one while the target dataset is more specific [62], [63].
sent7: For example, Gao et al. [63] fine-tuned BlueBERT on a large general biomedical NER corpus like MedMentions [64] or Semantic Medline before fine-tuning on the small target NER corpus.
sent8: Different Task Same Domain -Here, the source and target datasets are from different tasks but the same domain.
sent9: Fine-tuning on source dataset which is from the same domain allows the model to gain more domainspecific knowledge which improves the performance of the model on the same domain target task [65].
sent10: McCreery et al. [65] fine-tuned the model on the medical questionanswer pairs dataset to enhance its performance on the medical question similarity dataset.
sent11: Different Task Different Domain -Here, the source and target datasets are from different tasks and different domains.
sent12: For example, Jeong et al. [66] fine-tuned BioBERT on a general MultiNLI dataset to improve the performance of the model in biomedical QA.
sent13: Here the model learns sentence level reasoning skills which are useful in biomedical QA."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s19,Auxiliary Pretraining Tasks,"Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them. For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45]. This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models. Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.

Similarly, Liu et al. [34] used multi-similarity loss-based pretraining task to inject UMLS synonym knowledge into PubMedBERT.","[['b53', 'b46', 'b44'], ['b33']]","[['b53', 'b46', 'b44'], ['b33']]",4,"sent1: Auxiliary pretraining tasks help to inject knowledge from human-curated sources like UMLS [54] into indomain models to further enhance them.
sent2: For example, the triple classification pretraining task involves identifying whether two concepts are connected by the relation or not [45].
sent3: This auxiliary task is used by Hao et al. [45] to inject UMLS relation knowledge into in-domain models.
sent4: Yuan et al. [47] used two auxiliary pretraining tasks based on multi-similarity Loss and Knowledge embedding loss to further pretrain BioBERT on UMLS.
sent5: Similarly, Liu et al. [34] used multi-similarity loss-based pretraining task to inject UMLS synonym knowledge into PubMedBERT."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s18,BERT [2],"SOP sentence-level Allows the model to learn sentence-level reasoning skills by modeling intersentence coherence. More challenging compared to NSP as SOP involves only sentence coherence.

ALBERT [15] SBO phrase-level Model predicts the masked tokens in a span based on boundary token representations and position embeddings.

SpanBERT [49] RTD word-level Model checks every token whether it is replaced or not. More efficient compared to MLM as it involves all the tokens in the input.

ELECTRA [50]  {x 1 , x 2 , . . . , x m }, a subset of tokens is randomly chosen and these tokens are replaced. The authors replaced tokens, 80% of the time with a special token '[MASK]', 10% of the time with a random token, and 10% of the time with the same token. This is done to handle the mismatch between pretraining and fine-tuning phases. Formally,

wherex is the masked version of x and m(x) represents the set of masked token positions. Some of the improvements like dynamic masking [3], whole word masking [2], [20], [51], whole entity masking [48], [52], and whole span masking [48] are introduced in MLM to further improve its efficiency as a pretraining task. Delvin et al. [2] used static masking to replace the tokens i.e., the input sentences are masked once during pre-processing and the model predicts the same masked tokens in the input sentences for every epoch during pretraining. In the case of dynamic masking [3], different tokens are masked in the input sentence for different epochs which prevents the model from predicting the same masked tokens in every epoch and hence it learns more. Whole word masking is much more challenging as the model has to predict the entire word rather than part of a word. In the case of the whole entity and span maskings, in-domain entities and phrases in the input sentences are identified and then masked rather than masking the randomly chosen tokens. As a result, the model learns entity-centric and in-domain linguistic knowledge during pretraining which enhances the performance of the model in downstream tasks [48], [52]. For example, Zhang et al. [48] trained MC-BERT using NSP and MLM with whole entity and span maskings. Michalopoulos et al. [46] used novel multi-label loss-based MLM along with NSP to further pretrain ClinicalBERT on MIMIC-III clinical notes to get UmlsBERT. Novel multilabel lossbased MLM allows the model to connect all the words under the same concept. Sequence-to-Sequence MLM (Seq2SeqLM) is an extension of MLM to models based on encoder-decoder architecture. Models like T5 [4] are pretrained using Seq2SeqLM pretraining task.

Replaced Token Detection (RTD) [50]. It is a novel pretraining task that involves verifying whether each token in the input is replaced or not. Initially, some of the tokens in the input sentences are replaced with words predicted by a small generator network, and then the model (discriminator) is asked to predict the status of each word as replaced or not. The two advantages of RTD over MLM are a) RTD provides more training signal compared to MLM as RTD involves checking the status of every token in the input rather than a subset of randomly chosen tokens like MLM. and b) Unlike MLM, RTD does not use any special tokens like '[MASK]' to corrupt the input. So, it avoids the mismatch problem that the special token '[MASK]' is seen only during pretraining but not during fine-tuning. Formally,

wherex is the corrupted version of x and t = 1 when the token is not a replaced one. Span Boundary Objective (SBO) [49]. It is a novel pretraining task that involves predicting the entire masked span based on the context. Initially, a contiguous span of tokens is randomly chosen and masked and then the model is asked to predict the masked tokens in the span based on the token representations at the boundary. In the case of MLM, the model predicts the masked token based on the final hidden vector of the masked token. However, in the case of SBO, the model predicts the masked token in the span based on the final hidden vectors of the boundary tokens and the position embedding of the masked token. SBO is more challenging as it is difficult to predict the entire span ""frequent bathroom runs"" than predicting ""frequent"" when the model already sees ""bathroom runs"". SBO helps the model to achieve better results in span extraction-based tasks like entity extraction and question answering [49], [53]. Let s and e represent the start and end indices of the span in the input sequence. Then, each token x i in the span is predicted based on the final hidden vectors of the boundary tokens x s−1 , x e+1 and its position embedding p i−s+1 . Then

where y i = g(x s−1 , x e+1 , p i−s+1 ), g() represents feedforward network of two layers and S represents the positions of tokens in contiguous span.

Next Sentence Prediction (NSP) [2]. NSP is a sentence-level pretraining task that involves predicting whether given two sentences appear consecutively or not . It is basically a two-way sentence pair classification task. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsN ext, IsN otN ext} depending on whether the two sentences are consecutive or not. NSP helps the model to learn sentence-level reasoning skills which are useful in downstream tasks involving sentence pairs like natural language inference, text similarity, and question answering [2]. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are positive and the rest negative. Let z represents aggregate vector representation of the sentence pair (x, y). Then,

where t = 1 when the two sentences x and y are consecutive. Sentence Order Prediction (SOP) [15]. SOP is a novel sentence-level pretraining task which models intersentence coherence. Like NSP, SOP is a two-way sentence pair classification. Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsSwapped, IsN otSwapped} depending on whether the sentences are swapped or not. For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are swapped and the rest are not swapped. Unlike NSP which involves the prediction of both topic and coherence, SOP involves only sentence coherence prediction [15]. Topic prediction is comparatively easier which questions the effectiveness of NSP as a pretraining task [3], [15], [49]. Let z represent aggregate vector representation of the sentence pair (x, y). Then,

where t = 1 when the two sentences x and y are not swapped.","[[], ['b14'], ['b48'], ['b49'], ['b3', 'b50', 'b45', 'b47', 'b51', 'b1', 'b2', 'b19'], ['b49'], ['b52', 'b48'], [], ['b1'], ['b14', 'b2', 'b48'], []]","[[], ['b14'], ['b48'], ['b49'], ['b3', 'b50', 'b45', 'b47', 'b51', 'b1', 'b2', 'b19'], ['b49'], ['b52', 'b48'], [], ['b1'], ['b14', 'b2', 'b48'], []]",18,"sent1: SOP sentence-level Allows the model to learn sentence-level reasoning skills by modeling intersentence coherence.
sent2: More challenging compared to NSP as SOP involves only sentence coherence.
sent3: ALBERT [15] SBO phrase-level Model predicts the masked tokens in a span based on boundary token representations and position embeddings.
sent4: SpanBERT [49] RTD word-level Model checks every token whether it is replaced or not.
sent5: More efficient compared to MLM as it involves all the tokens in the input.
sent6: ELECTRA [50]  {x 1 , x 2 , . . . , x m }, a subset of tokens is randomly chosen and these tokens are replaced.
sent7: The authors replaced tokens, 80% of the time with a special token '[MASK]', 10% of the time with a random token, and 10% of the time with the same token.
sent8: This is done to handle the mismatch between pretraining and fine-tuning phases.
sent9: Formally,wherex is the masked version of x and m(x) represents the set of masked token positions.
sent10: Some of the improvements like dynamic masking [3], whole word masking [2], [20], [51], whole entity masking [48], [52], and whole span masking [48] are introduced in MLM to further improve its efficiency as a pretraining task.
sent11: Delvin et al. [2] used static masking to replace the tokens i.e., the input sentences are masked once during pre-processing and the model predicts the same masked tokens in the input sentences for every epoch during pretraining.
sent12: In the case of dynamic masking [3], different tokens are masked in the input sentence for different epochs which prevents the model from predicting the same masked tokens in every epoch and hence it learns more.
sent13: Whole word masking is much more challenging as the model has to predict the entire word rather than part of a word.
sent14: In the case of the whole entity and span maskings, in-domain entities and phrases in the input sentences are identified and then masked rather than masking the randomly chosen tokens.
sent15: As a result, the model learns entity-centric and in-domain linguistic knowledge during pretraining which enhances the performance of the model in downstream tasks [48], [52].
sent16: For example, Zhang et al. [48] trained MC-BERT using NSP and MLM with whole entity and span maskings.
sent17: Michalopoulos et al. [46] used novel multi-label loss-based MLM along with NSP to further pretrain ClinicalBERT on MIMIC-III clinical notes to get UmlsBERT.
sent18: Novel multilabel lossbased MLM allows the model to connect all the words under the same concept.
sent19: Sequence-to-Sequence MLM (Seq2SeqLM) is an extension of MLM to models based on encoder-decoder architecture.
sent20: Models like T5 [4] are pretrained using Seq2SeqLM pretraining task.
sent21: Replaced Token Detection (RTD) [50].
sent22: It is a novel pretraining task that involves verifying whether each token in the input is replaced or not.
sent23: Initially, some of the tokens in the input sentences are replaced with words predicted by a small generator network, and then the model (discriminator) is asked to predict the status of each word as replaced or not.
sent24: The two advantages of RTD over MLM are a) RTD provides more training signal compared to MLM as RTD involves checking the status of every token in the input rather than a subset of randomly chosen tokens like MLM.
sent25: and b) Unlike MLM, RTD does not use any special tokens like '[MASK]' to corrupt the input.
sent26: So, it avoids the mismatch problem that the special token '[MASK]' is seen only during pretraining but not during fine-tuning.
sent27: Formally,wherex is the corrupted version of x and t = 1 when the token is not a replaced one.
sent28: Span Boundary Objective (SBO) [49].
sent29: It is a novel pretraining task that involves predicting the entire masked span based on the context.
sent30: Initially, a contiguous span of tokens is randomly chosen and masked and then the model is asked to predict the masked tokens in the span based on the token representations at the boundary.
sent31: In the case of MLM, the model predicts the masked token based on the final hidden vector of the masked token.
sent32: However, in the case of SBO, the model predicts the masked token in the span based on the final hidden vectors of the boundary tokens and the position embedding of the masked token.
sent33: SBO is more challenging as it is difficult to predict the entire span ""frequent bathroom runs"" than predicting ""frequent"" when the model already sees ""bathroom runs"".
sent34: SBO helps the model to achieve better results in span extraction-based tasks like entity extraction and question answering [49], [53].
sent35: Let s and e represent the start and end indices of the span in the input sequence.
sent36: Then, each token x i in the span is predicted based on the final hidden vectors of the boundary tokens x s−1 , x e+1 and its position embedding p i−s+1 .
sent37: Thenwhere y i = g(x s−1 , x e+1 , p i−s+1 ), g() represents feedforward network of two layers and S represents the positions of tokens in contiguous span.
sent38: Next Sentence Prediction (NSP) [2].
sent39: NSP is a sentence-level pretraining task that involves predicting whether given two sentences appear consecutively or not .
sent40: It is basically a two-way sentence pair classification task.
sent41: Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsN ext, IsN otN
sent42: ext} depending on whether the two sentences are consecutive or not.
sent43: NSP helps the model to learn sentence-level reasoning skills which are useful in downstream tasks involving sentence pairs like natural language inference, text similarity, and question answering [2].
sent44: For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are positive and the rest negative.
sent45: Let z represents aggregate vector representation of the sentence pair (x, y).
sent46: Then,where t = 1 when the two sentences x and y are consecutive.
sent47: Sentence Order Prediction (SOP) [15].
sent48: SOP is a novel sentence-level pretraining task which models intersentence coherence.
sent49: Like NSP, SOP is a two-way sentence pair classification.
sent50: Formally, for a given sentence pair (x, y), the model has to predict one of the two labels {IsSwapped, IsN otSwapped} depending on whether the sentences are swapped or not.
sent51: For a balanced pretraining, the training examples are chosen in a 1:1 ratio i.e., 50% are swapped and the rest are not swapped.
sent52: Unlike NSP which involves the prediction of both topic and coherence, SOP involves only sentence coherence prediction [15].
sent53: Topic prediction is comparatively easier which questions the effectiveness of NSP as a pretraining task [3], [15], [49].
sent54: Let z represent aggregate vector representation of the sentence pair (x, y).
sent55: Then,where t = 1 when the two sentences x and y are not swapped."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s17,Main Pretraining Tasks,"The main pretraining tasks allow the model to learn language representations. Some of the commonly used main pretraining tasks are masked language modelling (MLM) [2], replaced token detection (RTD) [50], sentence boundary objective (SBO) [49], next sentence prediction (NSP) [2] and sentence order prediction (SOP) [15].

Masked Language Modeling (MLM). It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2]. The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens. However, the meaning of a word depends on both the left and right contexts. Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model. Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI. Less challenging as it involves topic prediction which is a relatively easy task.","[['b1', 'b14', 'b49', 'b48'], ['b1']]","[['b1', 'b14', 'b49', 'b48'], ['b1']]",5,"sent1: The main pretraining tasks allow the model to learn language representations.
sent2: Some of the commonly used main pretraining tasks are masked language modelling (MLM) [2], replaced token detection (RTD)
sent3: [50], sentence boundary objective (SBO) [49], next sentence prediction (NSP) [2] and sentence order prediction (SOP) [15].Masked Language Modeling (MLM).
sent4: It is an improved version of Language Modeling which utilizes both left and right contexts to predict the missing tokens [2].
sent5: The main drawback in Unidirectional LM (Forward LM or Backward LM) is the inability to utilize both left and right contexts at the same time to predict the tokens.
sent6: However, the meaning of a word depends on both the left and right contexts.
sent7: Devlin et al. [2] utilized MLM as a pretraining task for learning the parameters of the BERT model.
sent8: Formally, for a given sequence x with tokens  NSP sentence-level Allows the model to learn sentence-level reasoning skills which are useful in tasks like NLI.
sent9: Less challenging as it involves topic prediction which is a relatively easy task."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s14,Domain-Specific Pretraining (DSPT),"The main drawback in continual pretraining is the general domain vocabulary. For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2]. As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning. Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords. DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10). For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles [20]. PubMed achieved state-of-the-art results in the BLURB benchmark. Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11). In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained. TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences.","[['b1', 'b43', 'b19', 'b42']]","[['b1', 'b43', 'b19', 'b42']]",4,"sent1: The main drawback in continual pretraining is the general domain vocabulary.
sent2: For example, the WordPiece vocabulary in BERT is learned over English Wikipedia and Books Corpus [2].
sent3: As a result, the vocabulary does not represent the biomedical domain and hence many of the biomedical words are split into several subwords which hinders the model learning during pretraining and fine-tuning.
sent4: Moreover, the length of the input sequence also increases as many of the in-domain words are split into several subwords.
sent5: DSPT over in-domain text allows the model to have the in-domain vocabulary (refer Figure 10).
sent6: For example, PubMedBERT is trained from scratch using PubMed abstracts and PMC full-text articles [20].
sent7: PubMed achieved state-of-the-art results in the BLURB benchmark.
sent8: Similarly, RoBERTa-base-PM-M3-Voc is trained from scratch over PubMed and PMC and MIMIC-III clinical notes [43]. is based on the hypothesis that pretraining over taskrelated unlabelled text allows the model to learn both domain and task-specific knowledge [44] (refer Figure  11).
sent9: In TAPT, task-related unlabelled sentences are gathered, and then the model is further pretrained.
sent10: TAPT is less expensive compared to other pretraining methods as it involves pretraining the model over a relatively small corpus of task-related unlabelled sentences."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s13,Mixed-Domain Pretraining (MDPT),"Mixed domain pretraining involves training the model using both general and in-domain text. Depending on whether the pretraining is done simultaneously or not, mixed domain pretraining can be classified into a) Continual pretraining -initially the model is pre-trained over general domain text and then adapted to the biomedical domain [16] and b) Simultaneous pretraining -the model is pre-trained over the combined corpora having both general and in-domain text where the indomain text is up sampled to ensure balanced pretraining [21].

Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs. It is also referred to as further pretraining. In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7). For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16]. In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].

Simultaneous Pretraining (SPT) : Continual pretraining achieved good results by adapting general models to the biomedical domain [16], [18], [19], [39], [40]. However, it requires large volumes of in-domain text. Otherwise, CPT may result in suboptimal performance. Simultaneous pretraining comes to the rescue when only a small amount of in-domain text is available. Here, the pretraining corpora consist of both in-domain and general domain text where the in-domain text is up sampled to ensure a balanced pretraining (refer Figure  8). For example, BERT (jpCR+jpW) [41] is developed by simultaneous pretraining over a small amount of Japanese clinical text and a large amount of Japanese Wikipedia text. This model outperformed UTH-BERT in clinical text classification. UTH-BERT [42] is trained from scratch over Japanese clinical text. ","[['b20', 'b15'], ['b17', 'b15'], ['b41', 'b17', 'b40', 'b38', 'b18', 'b39', 'b15']]","[['b20', 'b15'], ['b17', 'b15'], ['b41', 'b17', 'b40', 'b38', 'b18', 'b39', 'b15']]",11,"sent1: Mixed domain pretraining involves training the model using both general and in-domain text.
sent2: Depending on whether the pretraining is done simultaneously or not, mixed domain pretraining can be classified into a) Continual pretraining -initially the model is pre-trained over general domain text and then adapted to the biomedical domain [16] and b)
sent3: Simultaneous pretraining -the model is pre-trained over the combined corpora having both general and in-domain text where the indomain text is up sampled to ensure balanced pretraining [21].
sent4: Continual Pretraining (CPT) : It is the standard approach followed by the biomedical NLP research community to develop transformer-based BPLMs.
sent5: It is also referred to as further pretraining.
sent6: In this approach, the model is initialized with general PLM weights and then the model is adapted to in-domain by further pretraining on large volumes of in-domain text (refer Figure  7).
sent7: For example, BioBERT is initialized with general BERT weights and then further pretrained on PubMed abstracts and PMC full-text articles [16].
sent8: In the case of BlueBERT, the authors used both PubMed abstracts and MIMIC-III clinical notes for continual pretraining [18].
sent9: Simultaneous Pretraining (SPT) : Continual pretraining achieved good results by adapting general models to the biomedical domain [16], [18], [19], [39], [40].
sent10: However, it requires large volumes of in-domain text.
sent11: Otherwise, CPT may result in suboptimal performance.
sent12: Simultaneous pretraining comes to the rescue when only a small amount of in-domain text is available.
sent13: Here, the pretraining corpora consist of both in-domain and general domain text where the in-domain text is up sampled to ensure a balanced pretraining (refer Figure  8).
sent14: For example, BERT (jpCR+jpW) [41] is developed by simultaneous pretraining over a small amount of Japanese clinical text and a large amount of Japanese Wikipedia text.
sent15: This model outperformed UTH-BERT in clinical text classification.
sent16: UTH-BERT [42] is trained from scratch over Japanese clinical text."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s10,Self-Supervised Learning,"Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade. This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions. The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances. However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data. In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples. This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].

Robotics is the first AI field to use self-supervised learning methods [34]. Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38]. SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods. SSL is similar to unsupervised learning as it does not depend on human-labeled instances. It is also similar to supervised learning as it learns using supervision. However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data. SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks. The pseudo labels are generated depending on the definitions of pre-training tasks. SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34]. In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling). In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization). In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection). For more details about different SSL methods, please refer to the survey paper written by Liu et al. [34].","[['b33'], ['b37', 'b25', 'b35', 'b33', 'b36', 'b34', 'b13', 'b12']]","[['b33'], ['b37', 'b25', 'b35', 'b33', 'b36', 'b34', 'b13', 'b12']]",9,"sent1: Deep learning algorithms dominated rule-based and machine learning algorithms in the last decade.
sent2: This is because deep learning models can learn features automatically which eliminates the requirement of expensive feature engineering and process the inputs in an end-to-end manner i.e., take raw inputs and give the decisions.
sent3: The success of the deep learning algorithms comes from the knowledge gained during training from human-labeled instances.
sent4: However, supervised learning has a lot of limitations a) with less data, the model may get overfitted and prone to bias b) some of the domains like biomedical are supervision starved i.e., difficult to get labeled data.
sent5: In general, we expect models to be close to human intelligence i.e., more general and make decisions with just a few samples.
sent6: This desire of developing models with more generalization ability and learning from fewer samples has made the researchers focus on other learning paradigms like Self-Supervised Learning [34].Robotics is the first AI field to use self-supervised learning methods [34].
sent7: Over the last five years, selfsupervised learning has become popular in other AI fields like natural language processing [13], [14], [26], computer vision [35], [36], and speech processing [37], [38].
sent8: SSL is a new learning paradigm that draws inspiration from both supervised and unsupervised learning methods.
sent9: SSL is similar to unsupervised learning as it does not depend on human-labeled instances.
sent10: It is also similar to supervised learning as it learns using supervision.
sent11: However, in SSL the supervision is provided by the pseudo labels which are generated automatically from the pretraining data.
sent12: SSL involves pretraining the model over a large unlabelled corpus using one or more pretraining tasks.
sent13: The pseudo labels are generated depending on the definitions of pre-training tasks.
sent14: SSL methods fall into three categories namely Generative, Contrastive, and Generate-Contrastive [34].
sent15: In Generative SSL, encoder maps input vector x to vector y, and decoder recovers x from y (e.g., masked language modeling).
sent16: In Contrastive SSL, the encoder maps input vector x to vector y to measure similarity (e.g., mutual information maximization).
sent17: In Generate-Contrastive SSL, fake samples are generated using encoder-decoder while the discriminator identifies the fake samples (e.g., replaced token detection).
sent18: For more details about different SSL methods, please refer to the survey paper written by Liu et al. [34]."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s24,Main Embeddings,"Text embeddings map the given sequence of words into a sequence of vectors. Text embeddings can be char, subword or code-based.

Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only. Each character is represented using an embedding. These embeddings are initialized randomly and learned during model pretraining. ELMo embedding model uses CharCNN to generate word representations from character embeddings [74]. Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings. Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders. The main advantage with character embeddings is the small size of vocabulary as it includes only characters. The disadvantage is longer pretraining times [28]. As the sequence length increases with character level embeddings, models are slow to pre-train.

Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words. The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords. Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words. Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE) [76], Byte-Level BPE [77], Word-Piece [30], Unigram [78], and Sentencepiece [79].

Byte-Pair Encoding (BPE) [76] -It starts with a base vocabulary having all the unique characters in the training corpus. It augments the base vocabulary with the most frequent pairs until the desired vocabulary size is achieved. Byte-Pair Encoding algorithm can be summarized as 1) Prepare a large training corpus and fix the vocabulary size. 2) Generate a base vocabulary having all the unique characters in the training corpus. 3) Calculate the frequency of all the words in the corpus. 4) Augment the vocabulary with the most frequently occurring pair. 5) Until the desired vocabulary size is achieved, repeat step 4. Byte-Level BPE [77] -In Byte-Level BPE, each character is represented as a byte, and the rest of the procedure is the same as in BPE. Text is converted into a sequence of bytes and the most frequent byte pair is added into the base vocabulary until the desired size is achieved. Byte-Level BPE is extremely beneficial in the multilingual scenario. GPT-2 [77] and RoBERTa [3] use Byte-Level BPE embeddings.

WordPiece [30] -The working of WordPiece is almost the same as BPE. Word-Piece and BPE differ in the strategy used in selecting the symbol pair to augment the base vocabulary. BPE chooses the most frequent symbol pair while Word-Piece uses a language model to choose the symbol pair. BERT [2], DistilBERT [80], and ELECTRA model use WordPiece embeddings.

SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space. However, this assumption is not applicable in all languages. To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary. The final vocabulary is generated iteratively using BPE or Unigram. XLNet [81], ALBERT [15], and T5 [4] models use SentencePiece embeddings.

Unigram [78] -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself. BPE and Word-piece start with a small base vocabulary and then augments the base vocabulary for a certain number of iterations. Unlike BPE and Word-Piece, Unigram starts with a large base vocabulary and then iteratively trims the symbols to arrive at a small final vocabulary. It is not used directly in any of the models. SentencePiece uses the Unigram algorithm to generate the final vocabulary.

Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors. For example, in the case of models like BERT-EHR [32], MedBERT [31], and BEHRT [27], the input is not a sequence of words. Instead, input is patient visits. Each patient visit is represented as a sequence of codes. The number of code embeddings varies from model to model. For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes.","[[], ['b74', 'b27', 'b75'], ['b29', 'b79', 'b76', 'b78', 'b77'], ['b76', 'b2', 'b77'], ['b1', 'b29', 'b80'], ['b79', 'b14', 'b81'], ['b78'], ['b30', 'b31', 'b26']]","[[], ['b74', 'b27', 'b75'], ['b29', 'b79', 'b76', 'b78', 'b77'], ['b76', 'b2', 'b77'], ['b1', 'b29', 'b80'], ['b79', 'b14', 'b81'], ['b78'], ['b30', 'b31', 'b26']]",21,"sent1: Text embeddings map the given sequence of words into a sequence of vectors.
sent2: Text embeddings can be char, subword or code-based.
sent3: Character Embeddings -In character embeddings, the vocabulary consists of letters, punctuation symbols, special characters and numbers only.
sent4: Each character is represented using an embedding.
sent5: These embeddings are initialized randomly and learned during model pretraining.
sent6: ELMo embedding model uses CharCNN to generate word representations from character embeddings [74].
sent7: Inspired by ELMo, BioCharBERT also uses CharCNN on the top of character embeddings to generate word representations [28]. AlphaBERT [75] also uses character embeddings.
sent8: Unlike CharacterBERT, AlphaBERT directly combines character embeddings with position embeddings and then applies a stack of transformer encoders.
sent9: The main advantage with character embeddings is the small size of vocabulary as it includes only characters.
sent10: The disadvantage is longer pretraining times [28].
sent11: As the sequence length increases with character level embeddings, models are slow to pre-train.
sent12: Subword Embeddings-In subword embeddings, the vocabulary consists of characters and the most frequent subwords and words.
sent13: The main principle driving the subword embedding vocabulary construction is that frequent words should be represented as a single word and rare words should be represented in terms of meaningful subwords.
sent14: Subword embedding vocabularies are always moderate in size as they use sub-words to represent rare and misspelled words.
sent15: Some of the popular algorithms to generate vocabulary for sub-word embeddings are Byte-Pair Encoding (BPE) [76], Byte-Level BPE [77], Word-Piece [30], Unigram [78], and Sentencepiece [79].
sent16: Byte-Pair Encoding (BPE) [76] -It starts with a base vocabulary having all the unique characters in the training corpus.
sent17: It augments the base vocabulary with the most frequent pairs until the desired vocabulary size is achieved.
sent18: Byte-Pair Encoding algorithm can be summarized as 1)
sent19: Prepare a large training corpus and fix the vocabulary size.
sent20: 2) Generate a base vocabulary having all the unique characters in the training corpus.
sent21: 3) Calculate the frequency of all the words in the corpus.
sent22: 4) Augment the vocabulary with the most frequently occurring pair.
sent23: 5) Until the desired vocabulary size is achieved, repeat step 4.
sent24: Byte-Level BPE [77] -In Byte-Level BPE, each character is represented as a byte, and the rest of the procedure is the same as in BPE.
sent25: Text is converted into a sequence of bytes and the most frequent byte pair is added into the base vocabulary until the desired size is achieved.
sent26: Byte-Level BPE is extremely beneficial in the multilingual scenario.
sent27: GPT-2 [77] and RoBERTa [3] use Byte-Level BPE embeddings.
sent28: WordPiece [30] -The working of WordPiece is almost the same as BPE.
sent29: Word-Piece and BPE differ in the strategy used in selecting the symbol pair to augment the base vocabulary.
sent30: BPE chooses the most frequent symbol pair while Word-Piece uses a language model to choose the symbol pair.
sent31: BERT [2], DistilBERT [80], and ELECTRA model use WordPiece embeddings.
sent32: SentencePiece [79] -A common problem in BPE and WordPiece is that they assume that words in the input sentences are separated by space.
sent33: However, this assumption is not applicable in all languages.
sent34: To overcome this, SentencePiece treats space as a character and includes it in the base vocabulary.
sent35: The final vocabulary is generated iteratively using BPE or Unigram.
sent36: XLNet [81], ALBERT [15], and T5 [4] models use SentencePiece embeddings.
sent37: Unigram [78] -Unigram is similar to BPE and Word-Piece in fixing the vocabulary size at the beginning itself.
sent38: BPE and Word-piece start with a small base vocabulary and then augments the base vocabulary for a certain number of iterations.
sent39: Unlike BPE and Word-Piece, Unigram starts with a large base vocabulary and then iteratively trims the symbols to arrive at a small final vocabulary.
sent40: It is not used directly in any of the models.
sent41: SentencePiece uses the Unigram algorithm to generate the final vocabulary.
sent42: Code embeddings -Code embeddings map the given sequence of codes into a sequence of vectors.
sent43: For example, in the case of models like BERT-EHR [32], MedBERT [31], and BEHRT [27], the input is not a sequence of words.
sent44: Instead, input is patient visits.
sent45: Each patient visit is represented as a sequence of codes.
sent46: The number of code embeddings varies from model to model.
sent47: For example, MedBERT and BEHRT include embeddings only for disease codes while BERT-EHR includes embeddings for disease, medication, procedure, and clinical notes."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s38,Multi-Modal Models,"T-PLMs have achieved success in many of the NLP tasks including in specific domains like Biomedical. Recent research works have focused on developing pretrained models that can handle multi-modal data i.e., video + text [128], [129], image + text [130]- [134] etc. In Biomedical domain, models like BERTHop [134] and Medical-VLBERT [133] have been proposed recently to handle image + text data. BERTHop [134] is a multi-modal T-BPLM developed for Chest X-ray disease diagnosis. Like ViLBERT [131] and LXMERT [132], BERTHop uses separate encoder to encoder image and text inputs. BERTHop uses PixelHop++ [135] to encode image data and BlueBERT as text encoder. Medical-VLBERT is developed for automatic report generation from COVID-19 scans. Unlike BERTHop, Medical-VLBERT [133] uses shared encoder based on VL-BERT [130] to encode image and text data.  ","[['b132', 'b129', 'b133', 'b128', 'b131', 'b134', 'b130', 'b135']]","[['b132', 'b129', 'b133', 'b128', 'b131', 'b134', 'b130', 'b135']]",8,"sent1: T-PLMs have achieved success in many of the NLP tasks including in specific domains like Biomedical.
sent2: Recent research works have focused on developing pretrained models that can handle multi-modal data i.e., video + text [128], [129], image + text [130]- [134] etc.
sent3: In Biomedical domain, models like BERTHop [134] and Medical-VLBERT [133] have been proposed recently to handle image + text data.
sent4: BERTHop [134] is a multi-modal T-BPLM developed for Chest X-ray disease diagnosis.
sent5: Like ViLBERT [131] and LXMERT [132], BERTHop uses separate encoder to encoder image and text inputs.
sent6: BERTHop uses PixelHop++ [135] to encode image data and BlueBERT as text encoder.
sent7: Medical-VLBERT is developed for automatic report generation from COVID-19 scans.
sent8: Unlike BERTHop, Medical-VLBERT [133] uses shared encoder based on VL-BERT [130] to encode image and text data."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s2,FOUNDATIONS,"In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4). The embedding layer takes input tokens and returns a vector for each. The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens. The final input vector for each token is obtained by summing all the vectors of each embedding type. The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism. By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors. Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type. In some models, there are more than three also. For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings. A detailed description of various embedding types is presented in Section 3.4. The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation. The first sublayer can be char, sub-word, or code embedding based. For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings. Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes. The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5).","[['b27', 'b29', 'b28', 'b26', 'b31', 'b30']]","[['b27', 'b29', 'b28', 'b26', 'b31', 'b30']]",6,"sent1: In general, the core components of transformer-based PLMs like BERT and RoBERTa are embedding and transformer encoder layers (refer Figure 4).
sent2: The embedding layer takes input tokens and returns a vector for each.
sent3: The embedding layer has three or more sub-layers each of which provides a vector of specific embedding type for each of the input tokens.
sent4: The final input vector for each token is obtained by summing all the vectors of each embedding type.
sent5: The transformer encoder layer enhances each input token vector by encoding global contextual information using the self-attention mechanism.
sent6: By applying a sequence of such transformer encoder layers, the model can encode complex language information in the input token vectors.
sent7: Usually, the embedding layer consists of three sublayers with each sub-layer representing a particular embedding type.
sent8: In some models, there are more than three also.
sent9: For example, embedding layer of BERT-EHR [27] contains code, position, segment, age and gender embeddings.
sent10: A detailed description of various embedding types is presented in Section 3.4.
sent11: The first sublayer converts input tokens to a sequence of vectors while the other two sub-layers provide auxiliary information like position and segmentation.
sent12: The first sublayer can be char, sub-word, or code embedding based.
sent13: For example, BioCharBERT [28] uses CharCNN [29] on the top of character embeddings, BERT uses WordPiece [30] embeddings while BEHRT [27], MedBERT [31] and BERT-EHR [32] models use code embeddings.
sent14: Unlike BioCharBERT and BERT models, the input for BEHRT, MedBERT, BERT-EHR models is patient visits where each patient visit is expressed as a sequence of codes.
sent15: The final input representation X for the given input tokens {x 1 , x 2 , . . . x n }is obtained by adding the embeddings from the three sub-layers (for simplicity, we have included only three embedding types -refer Figure 5)."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s40,Natural Language Inference,"Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics. It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence. Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain. NLI is framed as a three-way sentence pair classification problem. Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair. MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes. As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%. Cengiz et al. [55] applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%. The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [59] and MultiNLI [60] and then finetuned them on MedNLI.","[['b88', 'b59', 'b60', 'b66', 'b137', 'b136', 'b54', 'b58', 'b140', 'b138', 'b15', 'b139']]","[['b88', 'b59', 'b60', 'b66', 'b137', 'b136', 'b54', 'b58', 'b140', 'b138', 'b15', 'b139']]",12,"sent1: Natural Language Inference (NLI) is an important NLP task that requires sentence-level semantics.
sent2: It involves identifying the relationship between a pair of sentences i.e., whether the second sentence entails or contradicts or neural with the first sentence.
sent3: Training the model on NLI datasets helps the models to learn sentence-level semantics, which is useful in many tasks like paraphrase mining, information retrieval [136] in the general domain and medical concept normalization [137], [138], semantic relatedness [139], question answering [66] in the biomedical domain.
sent4: NLI is framed as a three-way sentence pair classification problem.
sent5: Here models like BERT learn the representation of given two sentences jointly and the three-way task-specific softmax classifier predicts the relationship between the given sentence pair.
sent6: MedNLI [61] is the in-domain NLI dataset with around 14k instances generated from MIMIC-III [88] clinical notes.
sent7: As MedNLI contains sentence pairs taken from EHRs, Kanakarajan et al. [140] further pretrained BioBERT [16] on MIMIC-III and then fine-tuned the model on MedNLI to achieve an accuracy of 83.45%.
sent8: Cengiz et al. [55] applied an ensemble of two BioBERT models and achieved an accuracy of 84.7%.
sent9: The authors fine-tuned each of the BioBERT models on general NLI datasets like SNLI [59] and MultiNLI [60] and then finetuned them on MedNLI."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s65,Intrinsic Probes,"During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14]. Intrinsic probes through light on the knowledge learned by PLMs during pretraining. In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models. For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models. However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining. This is an area which requires much attention from Biomedical NLP community.","[['b213', 'b214', 'b212', 'b215', 'b13']]","[['b213', 'b214', 'b212', 'b215', 'b13']]",5,"sent1: During pretraining, PLMs learn syntactic, semantic knowledge along with factual and common-sense knowledge available in the pretraining corpus [14].
sent2: Intrinsic probes through light on the knowledge learned by PLMs during pretraining.
sent3: In general NLP, researchers proposed several intrinsic probes like LAMA, Negated and Misprimed LAMA [211], XLAMA [212], X-FACTR [213], MickeyProbe [214] to understand the knowledge encoded in pretrained models.
sent4: For example, LAMA [211] probes the factual and common-sense knowledge of English pretrained models, while X-FACTR [213] probes the factual knowledge of multi-lingual pretrained models.
sent5: However, there is no such intrinsic probes in Biomedical domain to through light on the knowledge learned by BPLMs during pretraining.
sent6: This is an area which requires much attention from Biomedical NLP community."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s64,Benchmarks,"In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks. A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks. A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models. In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching). In biomedical domain there are three benchmarks namely BLUE [18], BLURB [20] and ChineseBLUE [48]. BLUE introduced by Peng et al. [18] contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets. BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets. The semantics of EHR and medical social media texts are different from biomedical scientific literature. So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets.","[['b17', 'b208', 'b211', 'b47', 'b209', 'b210', 'b19']]","[['b17', 'b208', 'b211', 'b47', 'b209', 'b210', 'b19']]",7,"sent1: In general, a benchmark is a tool to evaluate the performance of models across different NLP tasks.
sent2: A benchmark is required because we expect the pre-trained language models to be general and robust i.e., the models perform well across tasks rather than on one or two specific tasks.
sent3: A benchmark with one or more datasets for multiple NLP tasks helps to assess the general ability and robustness of models.
sent4: In general domain, we have a number of benchmarks like GLUE [207] and Super-GLUE [208] (general language understanding), XGLUE [209] (cross lingual language understanding) and LinCE [210] (code switching).
sent5: In biomedical domain there are three benchmarks namely BLUE [18], BLURB [20] and ChineseBLUE [48].
sent6: BLUE introduced by Peng et al. [18] contains ten datasets for five biomedical NLP tasks, while BLURB contains thirteen datasets for six tasks and ChineseBLUE contains eight tasks with nine datasets.
sent7: BLUE and ChineseBLUE include both EHR and scientific literature-based datasets, while BLURB includes only biomedical scientific literature-based datasets.
sent8: The semantics of EHR and medical social media texts are different from biomedical scientific literature.
sent9: So, there is a need of exclusive benchmarks for EHR and medical social media-based datasets."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s60,Mitigating Bias,"With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation [112], disease prediction [31], [32] etc. However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others. The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126]. Real-world datasets have a bias in many forms. It can be based on various attributes like gender, age, ethnicity, and marital status. These attributes are considered as protected or sensitive [201]. For example, in the MIMIC-III dataset [88] a) heart disease is more common in males compared to females-an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups -an example of ethnicity bias. It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group. There are few works that identified and addressed bias in transformer-based biomedical language models. Zhang et al. [127] using a simple word competition task showed that SciBERT [105] exhibits ethnicity bias. Moreover, the authors showed SciBERT model when further-pretrained on clinical notes exhibits performance differences for different protected attributes. They further showed that adversarial pretraining debiasing has little impact in reducing bias. Minot et al. [202] proposed an approach based on data augmentation to identify and reduce gender bias in patient notes. This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions.","[['b88', 'b105', 'b126', 'b124', 'b112', 'b31', 'b202', 'b30', 'b203', 'b127']]","[['b88', 'b105', 'b126', 'b124', 'b112', 'b31', 'b202', 'b30', 'b203', 'b127']]",10,"sent1: With the success of deep learning models in various tasks, deep learning-based systems are used to automate the decisions like department recommendation [112], disease prediction [31], [32] etc.
sent2: However, these models are shown to exhibit bias i.e., the decisions taken by the models may favor a particular group of people compared to others.
sent3: The main reason for unfair decisions of the models is the bias in the datasets on which the models are trained [124]- [126].
sent4: Real-world datasets have a bias in many forms.
sent5: It can be based on various attributes like gender, age, ethnicity, and marital status.
sent6: These attributes are considered as protected or sensitive [201].
sent7: For example, in the MIMIC-III dataset [88] a) heart disease is more common in males compared to females-an example of gender bias b) there are fewer clinical studies involving black patients compared to other groups -an example of ethnicity bias.
sent8: It is necessary to identify and reduce any form of bias that allows the model to take fair decisions without favoring any group.
sent9: There are few works that identified and addressed bias in transformer-based biomedical language models.
sent10: Zhang et al. [127] using a simple word competition task showed that SciBERT [105] exhibits ethnicity bias.
sent11: Moreover, the authors showed SciBERT model when further-pretrained on clinical notes exhibits performance differences for different protected attributes.
sent12: They further showed that adversarial pretraining debiasing has little impact in reducing bias.
sent13: Minot et al. [202] proposed an approach based on data augmentation to identify and reduce gender bias in patient notes.
sent14: This is an area that needs to be explored further to improve reduce bias and improve the fairness in model decisions."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s55,Robustness to Noise,"Transformed based PLMs have achieved the best results in many of the tasks. However, the performance of these models on noisy test instances is limited [197]- [200]. This is because the model is mostly trained on less noisy instances. As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances. Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning. The robustness of models is crucial particularly insensitive domains like biomedical [199], [200]. Two possible solutions are a) CharBERT [28] -replaced the WordPiece based embedding layer with CharCNN based embedding layer. Here word representation is generated from character embeddings using CharCNN. b) Adversarial Training [200] -Here, the training set is augmented with the noisy instances. Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances.","[['b200', 'b198', 'b201', 'b27']]","[['b200', 'b198', 'b201', 'b27']]",4,"sent1: Transformed based PLMs have achieved the best results in many of the tasks.
sent2: However, the performance of these models on noisy test instances is limited [197]- [200].
sent3: This is because the model is mostly trained on less noisy instances.
sent4: As these models mostly never encounter noisy instances during fine-tuning, the performance is significantly reduced on noisy instances.
sent5: Apart from this, the noisy words in the instances are split into several subtokens which affect the model learning.
sent6: The robustness of models is crucial particularly insensitive domains like biomedical [199], [200].
sent7: Two possible solutions are a) CharBERT [28] -replaced the WordPiece based embedding layer with CharCNN based embedding layer.
sent8: Here word representation is generated from character embeddings using CharCNN.
sent9: b) Adversarial Training [200] -Here, the training set is augmented with the noisy instances.
sent10: Training the model on an augmented training set exposes the model to noisy instances and hence the model performs better on noisy instances."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s54,Small Datasets,"Pretraining on large volumes of in-domain text allows the model language representations while fine-tuning allows the model to learn task-specific knowledge. During fine-tuning the model must learn sufficient task-specific knowledge to achieve good results on the task where the input distribution, as well as label space, is different from pretraining [193], [194]. With small target datasets, the models are not able to learn enough task-specific which limits the performance. To over the small size of target datasets, the possible solutions are Intermediate Fine-Tuning -Intermediate fine-tuning on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets [55]- [58], [62], [63], [65], [66] and

Multi-Task Fine-Tuning -Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69]. Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer. Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks. Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]- [71], [73].

Data Augmentation -Data augmentation helps us to create new training instances from existing instances. These newly creating training instances are close to original training data and helpful in low resource scenarios. Back translation and EDA [195] are the top popular techniques for data augmentation. For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model. The domain-specific ontologies like UMLS can also be used to augment the training instances [153].

Semi-Supervised Learning -Semi-supervised learning augments the training set with pseudo-labeled instances. The model which is fine-tuned on the original training set is used to label the task-related unlabelled instances. The model is again fine-tuned on the augmented training set and this process is repeated until the model converges [63], [196] . Table 11 contains a brief summary of these approaches.","[['b62', 'b57', 'b66', 'b63', 'b54', 'b65', 'b195', 'b194'], ['b73', 'b67', 'b71', 'b69'], ['b159', 'b196', 'b153'], ['b197', 'b63']]","[['b62', 'b57', 'b66', 'b63', 'b54', 'b65', 'b195', 'b194'], ['b73', 'b67', 'b71', 'b69'], ['b159', 'b196', 'b153'], ['b197', 'b63']]",17,"sent1: Pretraining on large volumes of in-domain text allows the model language representations while fine-tuning allows the model to learn task-specific knowledge.
sent2: During fine-tuning the model must learn sufficient task-specific knowledge to achieve good results on the task where the input distribution, as well as label space, is different from pretraining [193], [194].
sent3: With small target datasets, the models are not able to learn enough task-specific which limits the performance.
sent4: To over the small size of target datasets, the possible solutions are Intermediate Fine-Tuning -Intermediate fine-tuning on large, related datasets allows the model to learn more domain or task-specific knowledge which improves the performance on small target datasets [55]- [58], [62], [63], [65], [66] andMulti-Task Fine-Tuning -Multi-task fine-tuning allows the model to be fine-tuned on multiple tasks simultaneously [67]- [69].
sent5: Here the embedding and transformer encoder layers are common for all the tasks and each task has a separate task-specific layer.
sent6: Multi-task fine-tuning allows the model to gain domain as well as task-specific reasoning knowledge from multiple tasks.
sent7: Multi-Task fine-tuning is more useful in low resource scenarios which are common in the biomedical domain [69]- [71], [73].Data Augmentation -Data augmentation helps us to create new training instances from existing instances.
sent8: These newly creating training instances are close to original training data and helpful in low resource scenarios.
sent9: Back translation and EDA [195] are the top popular techniques for data augmentation.
sent10: For example, Wang et al. [159] used back translation to augment the training instances to train the clinical text similarity model.
sent11: The domain-specific ontologies like UMLS can also be used to augment the training instances [153].
sent12: Semi-Supervised Learning -Semi-supervised learning augments the training set with pseudo-labeled instances.
sent13: The model which is fine-tuned on the original training set is used to label the task-related unlabelled instances.
sent14: The model is again fine-tuned on the augmented training set and this process is repeated until the model converges [63], [196] .
sent15: Table 11 contains a brief summary of these approaches."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s58,Quality Sequence Representation,"For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation. According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information. The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space. Finally, a softmax is applied to convert it into a vector of probabilities. However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector.","[['b56', 'b159', 'b171', 'b136', 'b1', 'b177', 'b176']]","[['b56', 'b159', 'b171', 'b136', 'b1', 'b177', 'b176']]",7,"sent1: For text classification or sentence pair classification tasks like NLI and STS, Devlin et al. [2] suggested to use the final hidden vector of the special token added at the beginning of the input sequence as the final input sequence representation.
sent2: According to Devlin et al. [2], the final hidden vector of the special token aggregates the entire sequence information.
sent3: The final hidden vector is given to a linear layer which projects into ndimensional vector space whether n represents the size of label space.
sent4: Finally, a softmax is applied to convert it into a vector of probabilities.
sent5: However, some of the recent works showed that involving all the final hidden vectors using max-pooling [136], attention [171], [175], [176], or hierarchical convolution layers [57], [159] gives a much better final sequence representation compared to using only special token vector."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s53,Ontology Knowledge Injection,"Models like BioBERT, PubMedBERT have achieved good results in many of the tasks. However, these models lack knowledge from human-curated knowledge sources. These models can be further enhanced by ontology knowledge injection. Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174].","[['b45', 'b46', 'b193', 'b175', 'b44', 'b33']]","[['b45', 'b46', 'b193', 'b175', 'b44', 'b33']]",6,"sent1: Models like BioBERT, PubMedBERT have achieved good results in many of the tasks.
sent2: However, these models lack knowledge from human-curated knowledge sources.
sent3: These models can be further enhanced by ontology knowledge injection.
sent4: Ontology knowledge injection can be done in many ways namely a) continual pretraining the models on UMLS synonyms [34], [46] or relations [45] or both [47] b) continual pretraining the models on UMLS concept definitions [192] and c) feature vector constructed using ontology is added to the sequence vector learned by the models [174]."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s52,Semi-Supervised Learning,"Fine-tunes the model on training instances along with pseudo labeled instances

Allows the model to leverage task-related unlabelled instances.

Fine-tuning must be done iteratively to reduce the noisy labeled instances. BERT models to the biomedical domain. Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44]. TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances. The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191]. b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123]. The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123].","[[], [], ['b123', 'b43', 'b192', 'b122']]","[[], [], ['b123', 'b43', 'b192', 'b122']]",4,"sent1: Fine-tunes the model on training instances along with pseudo labeled instancesAllows the model to leverage task-related unlabelled instances.
sent2: Fine-tuning must be done iteratively to reduce the noisy labeled instances.
sent3: BERT models to the biomedical domain.
sent4: Two such lowcost domain adaptation methods are a) Task Adaptative Pretraining (TAPT) -It involves further pretraining on task-related unlabelled instances [44].
sent5: TAPT allows the models to learn domain as well as task-specific knowledge by pretraining on a relatively small number of taskrelated unlabelled instances.
sent6: The number of unlabelled instances can be increased by retrieving task-related unlabelled sentences using lightweight approaches like VAMPIRE [191].
sent7: b) Extending embedding layer -General T-PLMs can be adapted to the biomedical domain by refining the embedding layer with the addition of new in-domain vocabulary [122], [123].
sent8: The new in-domain vocabulary can be i) generated over in-domain text using Word2Vec and then aligned with general word-piece vocabulary [122] or ii) generated over in-domain text using word-piece [123]."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s42,Semantic Textual Similarity,"Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences. Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity. Both NLI and STS require sentence-level semantics. STS is useful in tasks like concept relatedness [139], medical concept normalization [137], [138] , duplicate text detection [155], question answering [65], [156] and text summarization [157]. Moreover, Reimers et al. [136] showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences. Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value. BIOSSES [158] and Clinical STS [72] are the commonly used datasets to train and evaluate indomain STS models.

Recent works exploited general models for clinical STS [56], [57], [65], [159]. For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model. As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71]. Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations. Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets. Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset. Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning. The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer. This is because the '[CLS]' vector contains only partial information. Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results.","[['b158', 'b72', 'b157', 'b155', 'b156', 'b137', 'b136', 'b65', 'b138', 'b139'], ['b71', 'b161', 'b56', 'b55', 'b159', 'b155', 'b65', 'b160']]","[['b158', 'b72', 'b157', 'b155', 'b156', 'b137', 'b136', 'b65', 'b138', 'b139'], ['b71', 'b161', 'b56', 'b55', 'b159', 'b155', 'b65', 'b160']]",18,"sent1: Semantic Textual Similarity quantifies the degree of semantic similarity between two phrases or sentences.
sent2: Unlike NLI which classifies the given sentence pair into one of three classes, semantic textual similarity returns a value that indicates the degree of similarity.
sent3: Both NLI and STS require sentence-level semantics.
sent4: STS is useful in tasks like concept relatedness [139], medical concept normalization [137], [138] , duplicate text detection [155], question answering [65], [156] and text summarization [157].
sent5: Moreover, Reimers et al. [136] showed that training transformer-based PLMs on STS datasets allow the model to learn sentence-level semantics and hence better represent variable-length texts like phrases or sentences.
sent6: Models like BERT learn the joint representation of given sentence pair and a task-specific sigmoid layer gives the similarity value.
sent7: BIOSSES [158] and Clinical STS [72] are the commonly used datasets to train and evaluate indomain STS models.
sent8: Recent works exploited general models for clinical STS [56], [57], [65], [159].
sent9: For example, Yang et al. [56] achieved the best results on the 2019 N2C2 STS dataset using Roberta-large model.
sent10: As clinical STS datasets are small in size, recent works initially fine-tuned the models on general STS datasets and then fine-tuned them on clinical datasets [56], [57], [65], [71].
sent11: Xiong et al. [160] enhanced in-domain BERT-based text similarity using CNN-based character level representations and TransE [161] based entity representations.
sent12: Mutinda et al. [155] achieved a Pearson correlation score of 0.8320 by finetuning Clinical BERT on a combined training set having instances from both general and clinical STS datasets.
sent13: Mahajan et al. [71] proposed a novel approach based on ClinicalBERT fine-tuned using iterative multi-task learning and achieved the best results on the Clinical STS dataset.
sent14: Iterative multi-task learning a) helps the model to learn task-specific knowledge from related datasets and b) choose the best-related datasets for intermediate multi-task fine-tuning.
sent15: The main drawback in the above existing works is giving '[CLS]' vector as sentence pair representation to the sigmoid layer.
sent16: This is because the '[CLS]' vector contains only partial information.
sent17: Unlike existing works, Wang et al. [159] applied hierarchical convolution on the final hidden state vectors and then applied max and min pooling to get the final sentence pair representation and achieved better results."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s41,Entity Extraction,"Entity Extraction is the first step in unlocking valuable information in unstructured text data. Entity Extraction is useful in many tasks like entity linking, relation extraction, knowledge graph construction, etc. Extracting clinical entities like drug and adverse drug reactions is useful in pharmacovigilance and biomedical entities like proteins, chemicals and drugs is useful to discover knowledge in scientific literature. Some of the popular entity extraction datasets are I2B2 2010 [141], VAERS [142], CADEC [143], N2C2 2018 [144] , BC4CHEMD [145], B5CDR-Chem [146], JNLPBA [147] and NCBI-Disease [148].

Most of the existing work approaches entity extraction as sequence labeling or machine reading comprehension. In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer [62], [149], [150], BiLSTM+Softmax [150], BiLSTM+CRF [62], [151]- [153] or CRF [53], [62], [151] is applied. Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance [151], [152]. This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM. Some of the works experimented with general BERT for extracting clinical and biomedical entities. For example, Portelli et al. [53] showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text. Boudjellal et al. [104] developed ABioNER by further pretraining AraBERT [41] on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT [2] and AraBERT on Arabic biomedical entity extraction. This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model. As in-domain datasets are comparatively small, some of the recent works [62], [63], [154] initially fine-tuned the models on similar datasets before fine-tuning on small target datasets. This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets. For example, Gao et al. [63] proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning. Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels. Recently Sun et al. [62] formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ (Softmax / CRF / BiLSTM-CRF) on six datasets.","[['b142', 'b143', 'b141', 'b147', 'b148', 'b145', 'b144', 'b146'], ['b153', 'b52', 'b104', 'b62', 'b151', 'b63', 'b1', 'b152', 'b149', 'b40', 'b150', 'b154']]","[['b142', 'b143', 'b141', 'b147', 'b148', 'b145', 'b144', 'b146'], ['b153', 'b52', 'b104', 'b62', 'b151', 'b63', 'b1', 'b152', 'b149', 'b40', 'b150', 'b154']]",20,"sent1: Entity Extraction is the first step in unlocking valuable information in unstructured text data.
sent2: Entity Extraction is useful in many tasks like entity linking, relation extraction, knowledge graph construction, etc.
sent3: Extracting clinical entities like drug and adverse drug reactions is useful in pharmacovigilance and biomedical entities like proteins, chemicals and drugs is useful to discover knowledge in scientific literature.
sent4: Some of the popular entity extraction datasets are I2B2 2010 [141], VAERS [142], CADEC [143], N2C2 2018 [144] , BC4CHEMD [145], B5CDR-Chem [146], JNLPBA [147] and NCBI-Disease [148].
sent5: Most of the existing work approaches entity extraction as sequence labeling or machine reading comprehension.
sent6: In case of sequence labelling approach, BERT based models generate contextual representations for each token and then softmax layer [62], [149], [150], BiLSTM+Softmax [150], BiLSTM+CRF [62], [151]- [153] or CRF [53], [62], [151] is applied.
sent7: Recent works showed adding BiLSTM on the top of the BERT model does not show much difference in performance [151], [152].
sent8: This is because transformer encoder layers in BERT based models do the same job of encoding contextual in token representations like BiLSTM.
sent9: Some of the works experimented with general BERT for extracting clinical and biomedical entities.
sent10: For example, Portelli et al. [53] showed that SpanBERT+CRF outperformed in-domain BERT models also in extracting clinical entities in social media text.
sent11: Boudjellal et al. [104] developed ABioNER by further pretraining AraBERT [41] on general Arabic corpus + biomedical Arabic text and showed that ABioNER outperformed both multilingual BERT [2] and AraBERT on Arabic biomedical entity extraction.
sent12: This shows that further pretraining general AraBERT on small in-domain text corpus improves the performance of the model.
sent13: As in-domain datasets are comparatively small, some of the recent works [62], [63], [154] initially fine-tuned the models on similar datasets before fine-tuning on small target datasets.
sent14: This intermediate fine-tuning allows the model to learn more task-specific knowledge which improves the performance of the model on small target datasets.
sent15: For example, Gao et al. [63] proposed a novel approach entity extraction approach based on intermediate fine-tuning and semi-supervised learning.
sent16: Here intermediate fine-tuning allows the model to learn more task-specific knowledge while semi-supervised learning allows to leverage task-related unlabelled data by assigning pseudo labels.
sent17: Recently Sun et al. [62] formulated biomedical entity extraction as question answering and showed that BioBERT+QA outperformed BioBERT+ (Softmax / CRF / BiLSTM-CRF) on six datasets."
233481730,AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models,"Computer Science, Linguistics, Medicine",https://www.semanticscholar.org/paper/420c897bc67e6f438db522d919d925df1a10aa8c,s43,Relation Extraction,"Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text. Entity extraction followed by relation extraction helps to convert unstructured text into structured data. Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering. Some of relation extraction datasets are I2B2 2012 [162], AIMED [163], ChemProt [164], DDI [165], I2B2 2010 [141] and EU-ADR [166]. Wei et al. [167] achieved the best results on two datasets using MIMIC-BERT [40] +Softmax. Thillaisundaram and Togia [168] applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks [169]. Liu et al. [170] proposed SciBERT+Softmax for relation extraction in biomedical text. They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets. The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of '[CLS]' vector. Su et al. [171] added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets. The authors generated the final representation by concatenating '[CLS]' vector and weighted sum vector of final hidden state vectors. When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results.","[['b169', 'b165', 'b163', 'b171', 'b141', 'b170', 'b39', 'b166', 'b162', 'b168', 'b167', 'b164']]","[['b169', 'b165', 'b163', 'b171', 'b141', 'b170', 'b39', 'b166', 'b162', 'b168', 'b167', 'b164']]",12,"sent1: Relation extraction is a crucial task in information extraction which identifies the semantic relations between entities in text.
sent2: Entity extraction followed by relation extraction helps to convert unstructured text into structured data.
sent3: Extracting relations between entities is useful in many tasks like knowledge graph construction, text summarization, and question answering.
sent4: Some of relation extraction datasets are I2B2 2012 [162], AIMED [163], ChemProt [164], DDI [165], I2B2 2010 [141] and EU-ADR [166].
sent5: Wei et al. [167] achieved the best results on two datasets using MIMIC-BERT [40] +Softmax.
sent6: Thillaisundaram and Togia [168] applied SciBERT +Softmax to extract relations from biomedical abstracts as part of AGAC track of BioNLP-OST 2019 shared tasks [169].
sent7: Liu et al. [170] proposed SciBERT+Softmax for relation extraction in biomedical text.
sent8: They showed SciBERT+Softmax outperforms BERT+Softmax on three biomedical relation extraction datasets.
sent9: The main drawback in the above existing works is that they utilize only the partial knowledge from the last layer in the form of '[CLS]' vector.
sent10: Su et al. [171] added attention on the top of BioBERT to fully utilize the information in the last layer and achieved the best results on three biomedical extraction datasets.
sent11: The authors generated the final representation by concatenating '[CLS]' vector and weighted sum vector of final hidden state vectors.
sent12: When compared to applying LSTM on the final hidden state vectors, the attention layer gives better results."
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,Computer Science,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,s30,Deep feature-based methodologies,"Deep learning-based methodologies are becoming popular in the past few years due to their efficient feature learning ability. The deep Convolutional Neural Network (DCNN) based approach has gained more attention in the computer vision community. Table 4 shows an overview of the deep feature-based person retrieval methodologies. The performance column shows the highest value reported in the relevant literature in the case of multiple scenario-based analyses. Semantic Retrieval Convolutional Neural Network (SRCNN) developed by Martinho et al. in [124] shows the evaluation of a similar setup of [106]. Binary Cross-Entropy (BCE) and Mean Squared Error (MSE) loss functions quantify binary classification and regression. SRCNN achieves 35.7% and 46.4% at rank-1 accuracy for one-shot and multi-shot identification, respectively. Thus, a deep feature based SRCNN approach demonstrates a rank-1 accuracy improvement of 23.2% and 26.3% over a handcrafted feature-based system of [106].

The baseline method [76] of AVSS 2018 challenge II [5] dataset is implementable using handcrafted features, while all participants [69,79,80] of the challenge have evaluations based on deep features. The following discussions in this section consist of methodologies implemented on AVSS 2018 challenge II [5] dataset as well as some other approaches on large scale datasets like Market-1501 [105], DukeMTMC [107], PA100K [117] and CUHK03 [113].

Galiyawala et al. [69] use height, clothing colour, and gender for person retrieval. Person detection and semantic segmentation using Mask R-CNN [98] help to remove the cluttered background. It results in a clutter-free torso patch for efficient colour classification. Height is available using a camera calibration approach [120]. Torso clothing colour and gender are classified using a fine-tuned AlexNet [125] based individual model. Sequential implementation of height, colour, and gender filter aims to eliminate the detected persons and leave only the target person. This linear filtering-based approach achieves an average IoU of 0.36. Schumann et al. [80] detect the person using the Single-Shot Multibox Detector (SSD) [126]. Early-stage false positives are eliminated by background modelling based on a mixture of Gaussian distribution. The strategy of the ensemble of classifiers is adapted, and predictions are fused by computing mean or weighted mean. The evaluation is based on the Euclidean distance between a query vector and each detected person's attribute probability to produce the final result.  Fig. 16 Sample output frames for the person retrieved using semantic description by approach in [78]: person from Test Sequence 11 and Frame 66 with semantic description height (very short; 140-160 cm), torso type (short sleeve), torso colour-1 (yellow), torso colour-2 (NA) and gender (male). Images from left to right are Mask R-CNN person detection, height filtering, clothing colour filtering and gender filtering.

Yaguchi et al. [79] also use mask R-CNN for person detection and DenseNet-161 for attribute classification. Initially, they estimate all the attributes of the detected persons, and then the matching score is calculated using a Hamming loss. The person with the minimum loss is the target. Galiyawala et al. [78] further improve the linear filtering approach of [69] by introducing adaptive torso patch extraction and bounding box regression. Torso patch extraction is undertaken by deciding the torso region according to clothing type attribute. Thus, it removes noisy pixels from the torso patch and provides better colour classification. An IoU based box regression predicts the bounding box in the frame where soft biometric attribute-based retrieval fails. The approaches in [69] and [78] follow a linear filtering approach that filters out the non-matching person according to attributes and leaves the target in the end. The other two methods in [79] and [80] estimate all the detected attributes of a person in parallel. These methods fuse the characteristics in the end to retrieve the target. The linear filtering approach does not need to estimate all the attributes for all detection persons. However, an error in the first filter will propagate to further attribute filtering and reduce retrieval accuracy.

The AVSS 2018 challenge II [5] dataset is evaluated based on two metrics; an average IoU and percent of frames with an IoU ≥ 0.4. State-of-the-art average IoU of 0.569 is achieved by a linear filtering approach [78]. 75.9% of frames with an IoU ≥ 0.4 is achieved by [80]. Some sample qualitative results of person retrieval using the method in [78] are shown in Fig. 16. It offers a surveillance frame from AVSS 2018 challenge II datasets for test sequence 40 and frame 66 with a semantic description, namely, height (very short; 140-160 cm), torso type (short sleeve), torso colour-1 (yellow), torso colour-2 (NA) and gender (male). The first image shows the person detection output using Mask R-CNN. Height filter output shows that many persons match the given query. Among these persons, the colour filter output produces a couple of matches and this is further refined by using a gender filter. It is to be noted that all deep features-based approaches [59,78,79,80] perform better than the handcrafted feature-based baseline approach [76].

Sun et al. [81] apply part level features because they provide fine-grained information for person description. The authors propose a Part-based Convolutional Baseline (PCB) network for part-based feature extraction. Part-level Table 5 Comparison of handcrafted and deep feature based methods.","[['b121', 'b103'], ['b3', 'b104', 'b102', 'b76', 'b66', 'b114', 'b110', 'b73', 'b77'], ['b95', 'b66', 'b123', 'b75', 'b77', 'b117', 'b122'], ['b76', 'b66', 'b77', 'b75'], ['b3', 'b56', 'b76', 'b75', 'b73', 'b77'], ['b78']]","[['b121', 'b103'], ['b3', 'b104', 'b102', 'b76', 'b66', 'b114', 'b110', 'b73', 'b77'], ['b95', 'b66', 'b123', 'b75', 'b77', 'b117', 'b122'], ['b76', 'b66', 'b77', 'b75'], ['b3', 'b56', 'b76', 'b75', 'b73', 'b77'], ['b78']]",29,"sent1: Deep learning-based methodologies are becoming popular in the past few years due to their efficient feature learning ability.
sent2: The deep Convolutional Neural Network (DCNN) based approach has gained more attention in the computer vision community.
sent3: Table 4 shows an overview of the deep feature-based person retrieval methodologies.
sent4: The performance column shows the highest value reported in the relevant literature in the case of multiple scenario-based analyses.
sent5: Semantic Retrieval Convolutional Neural Network (SRCNN) developed by Martinho et al. in [124] shows the evaluation of a similar setup of [106]. Binary Cross-Entropy (BCE) and Mean Squared Error (MSE) loss functions quantify binary classification and regression.
sent6: SRCNN achieves 35.7% and 46.4% at rank-1 accuracy for one-shot and multi-shot identification, respectively.
sent7: Thus, a deep feature based SRCNN approach demonstrates a rank-1 accuracy improvement of 23.2% and 26.3% over a handcrafted feature-based system of [106].
sent8: The baseline method [76] of AVSS 2018 challenge II [5] dataset is implementable using handcrafted features, while all participants [69,79,80] of the challenge have evaluations based on deep features.
sent9: The following discussions in this section consist of methodologies implemented on AVSS 2018 challenge II [5] dataset as well as some other approaches on large scale datasets like Market-1501 [105], DukeMTMC [107], PA100K [117] and CUHK03 [113].
sent10: Galiyawala et al. [69] use height, clothing colour, and gender for person retrieval.
sent11: Person detection and semantic segmentation using Mask R-CNN [98] help to remove the cluttered background.
sent12: It results in a clutter-free torso patch for efficient colour classification.
sent13: Height is available using a camera calibration approach [120].
sent14: Torso clothing colour and gender are classified using a fine-tuned AlexNet [125] based individual model.
sent15: Sequential implementation of height, colour, and gender filter aims to eliminate the detected persons and leave only the target person.
sent16: This linear filtering-based approach achieves an average IoU of 0.36.
sent17: Schumann et al. [80] detect the person using the Single-Shot Multibox Detector (SSD) [126].
sent18: Early-stage false positives are eliminated by background modelling based on a mixture of Gaussian distribution.
sent19: The strategy of the ensemble of classifiers is adapted, and predictions are fused by computing mean or weighted mean.
sent20: The evaluation is based on the Euclidean distance between a query vector and each detected person's attribute probability to produce the final result.
sent21: Fig. 16 Sample output frames for the person retrieved using semantic description by approach in [78]: person from Test Sequence 11 and Frame 66 with semantic description height (very short; 140-160 cm), torso type (short sleeve), torso colour-1
sent22: (yellow), torso colour-2 (NA) and gender (male).
sent23: Images from left to right are Mask R-CNN person detection, height filtering, clothing colour filtering and gender filtering.
sent24: Yaguchi et al. [79] also use mask R-CNN for person detection and DenseNet-161 for attribute classification.
sent25: Initially, they estimate all the attributes of the detected persons, and then the matching score is calculated using a Hamming loss.
sent26: The person with the minimum loss is the target.
sent27: Galiyawala et al. [78] further improve the linear filtering approach of [69] by introducing adaptive torso patch extraction and bounding box regression.
sent28: Torso patch extraction is undertaken by deciding the torso region according to clothing type attribute.
sent29: Thus, it removes noisy pixels from the torso patch and provides better colour classification.
sent30: An IoU based box regression predicts the bounding box in the frame where soft biometric attribute-based retrieval fails.
sent31: The approaches in [69] and [78] follow a linear filtering approach that filters out the non-matching person according to attributes and leaves the target in the end.
sent32: The other two methods in [79] and [80] estimate all the detected attributes of a person in parallel.
sent33: These methods fuse the characteristics in the end to retrieve the target.
sent34: The linear filtering approach does not need to estimate all the attributes for all detection persons.
sent35: However, an error in the first filter will propagate to further attribute filtering and reduce retrieval accuracy.
sent36: The AVSS 2018 challenge II [5] dataset is evaluated based on two metrics; an average IoU and percent of frames with an IoU ≥ 0.4.
sent37: State-of-the-art average IoU of 0.569 is achieved by a linear filtering approach [78].
sent38: 75.9% of frames with an IoU ≥ 0.4 is achieved by [80].
sent39: Some sample qualitative results of person retrieval using the method in [78] are shown in Fig. 16.
sent40: It offers a surveillance frame from AVSS 2018 challenge II datasets for test sequence 40 and frame 66 with a semantic description, namely, height (very short; 140-160 cm), torso type (short sleeve), torso colour-1
sent41: (yellow), torso colour-2 (NA) and gender (male).
sent42: The first image shows the person detection output using Mask R-CNN.
sent43: Height filter output shows that many persons match the given query.
sent44: Among these persons, the colour filter output produces a couple of matches and this is further refined by using a gender filter.
sent45: It is to be noted that all deep features-based approaches [59,78,79,80] perform better than the handcrafted feature-based baseline approach [76].Sun et al. [81] apply part level features because they provide fine-grained information for person description.
sent46: The authors propose a Part-based Convolutional Baseline (PCB) network for part-based feature extraction.
sent47: Part-level Table 5 Comparison of handcrafted and deep feature based methods."
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,Computer Science,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,s34,Natural language description-based person retrieval,"Cross-modal retrieval-based applications are drawing attention due to the rapid growth of multimodal data like text, image, video, and audio. Features from different modalities like text and image are not directly comparable as they lie entirely in other spaces. Hence, it is a challenging problem due to the sizeable heterogeneous gap between different text and image modalities. One such issue of person retrieval from surveillance video using natural language description is in this section. Table 6 shows an overview of different methodologies for person retrieval using natural language descriptions.

Zhou et al. [89] develop an attention-based algorithm that localises a person in the surveillance frame using attributes and natural language query. The author annotated the cityscapes dataset [132] surveillance frame with attributes and descriptions because the dataset did not have natural language descriptions. Matrix representations of sentence expressions use the Skip-gram model [143]. Attributes and descriptions are by bidirectional Long-Short Term Memory (BLSTM) [130,131] network. Visual features extraction is done using Faster R-CNN [97] and ResNet152 [127] with the algorithm achieving 74.6% recall@1 i.e., 74.6% of the highest scoring box is correct. The Cityscape dataset contains only street views, i.e., frontal view camera. Hence, it does not cover various view challenges for surveillance. Also, the description annotated dataset is not available publicly and it limits the usability.

Li et al. [85] first published a large-scale person dataset with natural language description, CUHK-PEDES (publicly available), as discussed in Sect. 2.4. The author proposes a Recurrent Neural Network with a Gated Neural Attention mechanism (GNA-RNN) to establish the baseline on CUHK-PEDES. The network consists of a visual sub-network for visual feature extraction from an image and a language sub-network for textual feature extraction from a description. The visual sub-network consists of VGG-16 [133] as a backbone network and generates 512 graphical units. Each optical unit determines the existence of a specific appearance pattern. RNN with Long Short-Term Memory (LSTM) is useful in language sub-network, which takes words and images as input. It outputs unit level attention that decides which visual units should pay more attention to the word. The word-level gate determines the importance of the word, e.g., the word ""white""has more weightage than the word ""the"". The GNA-RNN network is trained in an end-to-end manner and provides a top-1 accuracy of 19.05%. This approach creates the baseline for further research in cross-modal person retrieval based on natural language descriptions. Li et al. [85] 2017 VGG-16 [133] LSTM [130] GNA-RNN [85] CUHK-PEDES [85] Top-1 acc.

(19.05%)

Li et al. [84] 2017 VGG-16 [133] LSTM [130], word2vec [144] IATV [84] CUHK-PEDES [85] Top-1 acc.

(25.94%)

Chen et al. [82] 2018

ResNet50 [127] LSTM [130] GLIA [82] CUHK-PEDES [85] Top-1 acc. Bi-GRU [145,146] MIA [135] CUHK-PEDES [85] Recall@1 (48.00%)

Zhang et al. [136] 2018

MobileNet [128] Bi-LSTM [130,131] CMPC + CMPM [136] CUHK-PEDES [85] Recall@1 (49.37%)

Wang et al. [137] 2019

MobileNet [128] Bi-LSTM [130,131] MCCL [137] CUHK-PEDES [85] Top-1 acc.

(50.58%)

Sarafianos et al. [138] 2019 ResNet-101 [127] BERT [148], LSTM [130] TIMAM [138] CUHK-PEDES [85] Top-1 acc.

(54.51%)

Aggarwal et al. [139] 2020

MobileNet [128] NLTK [149], Bi-LSTM [130,131] CMAAM [139] CUHK-PEDES The approaches discussed below are evaluated on this baseline. Most of the large-scale dataset contains identity level annotations considered by Li et al. [84] to match visual and textual domains. Identity-aware textual-visual matching is done in a two-stage network. Identity-level annotations are effectively utilized by introducing a Cross-Modal Cross-Entropy (CMCE) loss in the stage-1 network. The CMCE loss implicitly maximizes inter-identity feature distances and minimizes intra-identity feature distances. However, the coupling between visual and textual features generated through CMCE loss is loose. Hence, the initial matching of stage-1 is further refined by stage-2 CNN-LSTM with an underlying co-attention mechanism that produces the final textual-visual matching confidence. This two-stage framework achieves 25.94% of top-1 accuracy.

Global and local level image-language association (GLIA) is proposed by Chen et al. [82] to exploit the semantic information available in the description. The GLIA approach gains a significant boost to the top-1 accuracy from the baselines and achieves 43.58% top-1 accuracy. Zheng et al. [134] focus on the limitation of ranking loss. The authors do not explicitly consider the feature distribution in a single modality. They overcome it by considering the problem as instance-level retrieval and propose the instance loss. Each image-text query pair is regarded as an instance and observed as a class during training to learn finer granularity. Instead of considering pre-trained models for feature extraction, the method view adopts end-to-end learning from the data itself. Dual-path CNN-CNN i.e. Dual-Path Convolutional Image-Text Embedding (DPCE) architecture is proposed instead of the CNN-RNN approach for image-text matching. DPCE [134] achieves 44.40% of top-1 rank accuracy.

Multi-granularity Image-text Alignments (MIA) framework [135] adopts a multiple granularities (i.e., global-global, global-local, and local-local alignments) based approach for better similarity evaluations between text and image. The global context of image and description matches global-global granularity. On the other hand, relations between the global context and local components establishes the global-local alignment. Visual human parts fit with noun phrases in the final local-local granularity. The algorithm achieves 48.00% of recall@1. Zhang et al. [136] focus on learning discriminative features by proposing two loss functions, i.e., cross-modal projection matching (CMPM) loss and a cross-modal projection classification (CMPC) loss. Natural language description is first tokenized into words and processed sequentially using Bi-LSTM. Visual features are extractable from the last pooling layer of MobileNet [128]. The association module embeds visual and textual elements into a shared latent space. 49.37% of recall@1 is achieved by the algorithm while considering both CMPM and CMPC losses. Similar to work in [84], Wang et al. [137] also utilize identity-level information and propose Mutually Connected Classification Loss (MCCL). They first create a baseline approach before applying MCCL for better feature embedding. This baseline approach uses MobileNet pre-trained on ImageNet [147] for visual features, Bi-LSTM for textual element, and triplet loss function for cross-modal feature embedding. Triplet loss does not fully exploit feature distribution. The MCCL classifica-tion weight is shared between both modalities. Only the baseline approach with triplet loss achieves 45.55% of recall@1 while MCCL achieves 50.58% of recall@1.

The majority of approaches so far introduce a new loss function for the network to learn better feature representations without the complexity of textual phrases. For example, the word ""t-shirt""is an adjective, but it might be useful as a noun in the description. Such limitations go away by introducing a Text-Image Modality Adversarial Matching (TIMAM) framework [138]. Sarafianos et al. [138] propose adversarial representation learning, which helps bring features from different modalities very close. TIMAM attains top-1 accuracy of 54.51%. Aggarwal et al. [139] use attribute classification as an additional task and identity for bridging the gap between different modalities to improve representation learning. The method uses Deep Coral loss [150] to reduce the modality gap. They achieve state-of-the-art top-1 accuracy of 56.61% on CUHK-PEDES dataset. Thus, most of the work [82,84,85,134,135,136,137,138,139] shows cross-modal person retrieval on the only publicly available CHUHK-PEDES dataset except [89]. CUHK-PEDES contains only the image gallery of cropped persons, limiting practical usage in real-time scenarios to retrieve the person from the input of full surveillance frames.","[[], ['b129', 'b124', 'b128', 'b140', 'b86', 'b94', 'b127'], ['b130', 'b82', 'b127'], [], ['b82', 'b81', 'b141', 'b130', 'b127'], [], ['b79'], ['b132', 'b82', 'b124', 'b142', 'b143', 'b79', 'b127'], ['b133'], ['b125', 'b82', 'b133', 'b128', 'b127'], ['b134'], ['b125', 'b82', 'b128', 'b134', 'b127'], [], ['b82', 'b124', 'b145', 'b135', 'b127'], [], ['b136'], ['b125', 'b128', 'b81', 'b136', 'b146', 'b127'], ['b79', None], ['b132', 'b125', 'b133', 'b81', 'b134', 'b144'], ['b132', 'b82', 'b133', 'b79', 'b81', 'b136', 'b147', None, 'b134', 'b86', 'b135']]","[[], ['b129', 'b124', 'b128', 'b140', 'b86', 'b94', 'b127'], ['b130', 'b82', 'b127'], [], ['b82', 'b81', 'b141', 'b130', 'b127'], [], ['b79'], ['b132', 'b82', 'b124', 'b142', 'b143', 'b79', 'b127'], ['b133'], ['b125', 'b82', 'b133', 'b128', 'b127'], ['b134'], ['b125', 'b82', 'b128', 'b134', 'b127'], [], ['b82', 'b124', 'b145', 'b135', 'b127'], [], ['b136'], ['b125', 'b128', 'b81', 'b136', 'b146', 'b127'], ['b79', None], ['b132', 'b125', 'b133', 'b81', 'b134', 'b144'], ['b132', 'b82', 'b133', 'b79', 'b81', 'b136', 'b147', None, 'b134', 'b86', 'b135']]",66,"sent1: Cross-modal retrieval-based applications are drawing attention due to the rapid growth of multimodal data like text, image, video, and audio.
sent2: Features from different modalities like text and image are not directly comparable as they lie entirely in other spaces.
sent3: Hence, it is a challenging problem due to the sizeable heterogeneous gap between different text and image modalities.
sent4: One such issue of person retrieval from surveillance video using natural language description is in this section.
sent5: Table 6 shows an overview of different methodologies for person retrieval using natural language descriptions.
sent6: Zhou et al. [89] develop an attention-based algorithm that localises a person in the surveillance frame using attributes and natural language query.
sent7: The author annotated the cityscapes dataset [132] surveillance frame with attributes and descriptions because the dataset did not have natural language descriptions.
sent8: Matrix representations of sentence expressions use the Skip-gram model [143].
sent9: Attributes and descriptions are by bidirectional Long-Short Term Memory (BLSTM) [130,131] network.
sent10: Visual features extraction is done using Faster R-CNN [97] and ResNet152 [127] with the algorithm achieving 74.6% recall@1 i.e., 74.6% of the highest scoring box is correct.
sent11: The Cityscape dataset contains only street views, i.e., frontal view camera.
sent12: Hence, it does not cover various view challenges for surveillance.
sent13: Also, the description annotated dataset is not available publicly and it limits the usability.
sent14: Li et al. [85] first published a large-scale person dataset with natural language description, CUHK-PEDES (publicly available), as discussed in Sect. 2.4.
sent15: The author proposes a Recurrent Neural Network with a Gated Neural Attention mechanism (GNA-RNN) to establish the baseline on CUHK-PEDES.
sent16: The network consists of a visual sub-network for visual feature extraction from an image and a language sub-network for textual feature extraction from a description.
sent17: The visual sub-network consists of VGG-16 [133] as a backbone network and generates 512 graphical units.
sent18: Each optical unit determines the existence of a specific appearance pattern.
sent19: RNN with Long Short-Term Memory (LSTM) is useful in language sub-network, which takes words and images as input.
sent20: It outputs unit level attention that decides which visual units should pay more attention to the word.
sent21: The word-level gate determines the importance of the word, e.g., the word ""white""has more weightage than the word ""the"".
sent22: The GNA-RNN network is trained in an end-to-end manner and provides a top-1 accuracy of 19.05%.
sent23: This approach creates the baseline for further research in cross-modal person retrieval based on natural language descriptions.
sent24: Li et al. [85] 2017 VGG-16 [133] LSTM [130] GNA-RNN [85] CUHK-PEDES [85]
sent25: Top-1 acc. (19.05%)Li et al. [84] 2017 VGG-16 [133] LSTM [130], word2vec [144] IATV [84] CUHK-PEDES [85]
sent26: Top-1 acc.(25.94%)Chen et al. [82] 2018ResNet50 [127] LSTM [130] GLIA [82] CUHK-PEDES [85]
sent27: Top-1 acc. Bi-GRU [145,146] MIA [135] CUHK-PEDES [85] Recall@1
sent28: (48.00%)Zhang et al. [136] 2018
sent29: MobileNet [128] Bi-LSTM [130,131] CMPC + CMPM [136] CUHK-PEDES [85] Recall@1 (49.37%)Wang et al. [137] 2019MobileNet [128] Bi-LSTM [130,131] MCCL [137] CUHK-PEDES [85]
sent30: Top-1 acc.(50.58%)Sarafianos et al. [138] 2019 ResNet-101 [127] BERT [148], LSTM [130] TIMAM [138] CUHK-PEDES [85]
sent31: Top-1 acc. (54.51%)Aggarwal et al. [139] 2020MobileNet [128] NLTK [149], Bi-LSTM [130,131] CMAAM [139] CUHK-PEDES
sent32: The approaches discussed below are evaluated on this baseline.
sent33: Most of the large-scale dataset contains identity level annotations considered by Li et al. [84] to match visual and textual domains.
sent34: Identity-aware textual-visual matching is done in a two-stage network.
sent35: Identity-level annotations are effectively utilized by introducing a Cross-Modal Cross-Entropy (CMCE) loss in the stage-1 network.
sent36: The CMCE loss implicitly maximizes inter-identity feature distances and minimizes intra-identity feature distances.
sent37: However, the coupling between visual and textual features generated through CMCE loss is loose.
sent38: Hence, the initial matching of stage-1 is further refined by stage-2 CNN-LSTM with an underlying co-attention mechanism that produces the final textual-visual matching confidence.
sent39: This two-stage framework achieves 25.94% of top-1 accuracy.
sent40: Global and local level image-language association (GLIA) is proposed by Chen et al. [82] to exploit the semantic information available in the description.
sent41: The GLIA approach gains a significant boost to the top-1 accuracy from the baselines and achieves 43.58% top-1 accuracy.
sent42: Zheng et al. [134] focus on the limitation of ranking loss.
sent43: The authors do not explicitly consider the feature distribution in a single modality.
sent44: They overcome it by considering the problem as instance-level retrieval and propose the instance loss.
sent45: Each image-text query pair is regarded as an instance and observed as a class during training to learn finer granularity.
sent46: Instead of considering pre-trained models for feature extraction, the method view adopts end-to-end learning from the data itself.
sent47: Dual-path CNN-CNN i.e. Dual-Path Convolutional Image-Text Embedding (DPCE) architecture is proposed instead of the CNN-RNN approach for image-text matching.
sent48: DPCE [134] achieves 44.40% of top-1 rank accuracy.
sent49: Multi-granularity Image-text Alignments (MIA) framework [135] adopts a multiple granularities (i.e., global-global, global-local, and local-local alignments) based approach for better similarity evaluations between text and image.
sent50: The global context of image and description matches global-global granularity.
sent51: On the other hand, relations between the global context and local components establishes the global-local alignment.
sent52: Visual human parts fit with noun phrases in the final local-local granularity.
sent53: The algorithm achieves 48.00% of recall@1.
sent54: Zhang et al. [136] focus on learning discriminative features by proposing two loss functions, i.e., cross-modal projection matching (CMPM) loss and a cross-modal projection classification (CMPC) loss.
sent55: Natural language description is first tokenized into words and processed sequentially using Bi-LSTM.
sent56: Visual features are extractable from the last pooling layer of MobileNet [128].
sent57: The association module embeds visual and textual elements into a shared latent space.
sent58: 49.37% of recall@1 is achieved by the algorithm while considering both CMPM and CMPC losses.
sent59: Similar to work in [84], Wang et al. [137] also utilize identity-level information and propose Mutually Connected Classification Loss (MCCL).
sent60: They first create a baseline approach before applying MCCL for better feature embedding.
sent61: This baseline approach uses MobileNet pre-trained on ImageNet [147] for visual features, Bi-LSTM for textual element, and triplet loss function for cross-modal feature embedding.
sent62: Triplet loss does not fully exploit feature distribution.
sent63: The MCCL classifica-tion weight is shared between both modalities.
sent64: Only the baseline approach with triplet loss achieves 45.55% of recall@1 while MCCL achieves 50.58% of recall@1.
sent65: The majority of approaches so far introduce a new loss function for the network to learn better feature representations without the complexity of textual phrases.
sent66: For example, the word ""t-shirt""is an adjective, but it might be useful as a noun in the description.
sent67: Such limitations go away by introducing a Text-Image Modality Adversarial Matching (TIMAM) framework [138].
sent68: Sarafianos et al. [138] propose adversarial representation learning, which helps bring features from different modalities very close.
sent69: TIMAM attains top-1 accuracy of 54.51%.
sent70: Aggarwal et al. [139] use attribute classification as an additional task and identity for bridging the gap between different modalities to improve representation learning.
sent71: The method uses Deep Coral loss [150] to reduce the modality gap.
sent72: They achieve state-of-the-art top-1 accuracy of 56.61% on CUHK-PEDES dataset.
sent73: Thus, most of the work [82,84,85,134,135,136,137,138,139] shows cross-modal person retrieval on the only publicly available CHUHK-PEDES dataset except [89].
sent74: CUHK-PEDES contains only the image gallery of cropped persons, limiting practical usage in real-time scenarios to retrieve the person from the input of full surveillance frames."
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,Computer Science,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,s33,Deep features,"AlexNet [69] It is small network and 61M parameters to learn, but the classification accuracy is low. MobileNet [80] It's a light weight network and 4.2M parameters to learn, but it is having little high error rate. ResNet-50 [80,81,83] It is having advantage of residual block which is having skip connection from input for better learning ability. Over 25M parameters to learn. DenseNet [78,79,80] It is having connection from each layer to every other layer and hence leveraging information from other layers which learns much better compare to other networks.

features are learnable by conducting uniform partition on the conv-layer without explicitly partitioning images. The spatial consistency within-part is exploitable to refine the coarse partition provided by PCB. The improvement in the uniform partition is made by the Refined Part Pooling (RPP) network. It achieves 92.3% of rank-1 accuracy with only the PCB. Further, it improves to 93.8% of rank-1 accuracy by employing RPP with PCB network on the Market-1501 [105] dataset. The text attribute combinations exist on a large scale in a real scenario. However, a minimal amount of combination with sufficient data is available for training. Except for such combinations, others are never modelled during training. Thus, Dong et al. [83] formulate a textual attribute query-based person search problem as a zero-shot (ZSL) learning problem for the first time. The authors propose Attribute-Image Hierarchical Matching (AIHM), which matches attributes and images at multiple hierarchical levels. The algorithm achieves state-of-the-art rank-1 mAP of 43.3% on Market-1501 [105], 50.5% on DukeMTMC [107] and 31.3% on PA100K [117] datasets. Table 5 shows a comparison of handcrafted and deep feature-based methods with their advantages and disadvantages.

A query-based personal retrieval system usually provides soft biometric keywords as the input query. Improvisation in natural language processing now allows automatic extraction of the keywords from sentences. It will move the system towards full automation.","[['b76', 'b66', 'b78', 'b80', 'b75', 'b77'], ['b102', 'b80', 'b104', 'b114'], []]","[['b76', 'b66', 'b78', 'b80', 'b75', 'b77'], ['b102', 'b80', 'b104', 'b114'], []]",10,"sent1: AlexNet [69] It is small network and 61M parameters to learn, but the classification accuracy is low.
sent2: MobileNet [80] It's a light weight network and 4.2M parameters to learn, but it is having little high error rate.
sent3: ResNet-50 [80,81,83] It is having advantage of residual block which is having skip connection from input for better learning ability.
sent4: Over 25M parameters to learn. DenseNet [78,79,80]
sent5: It is having connection from each layer to every other layer and hence leveraging information from other layers which learns much better compare to other networks.
sent6: features are learnable by conducting uniform partition on the conv-layer without explicitly partitioning images.
sent7: The spatial consistency within-part is exploitable to refine the coarse partition provided by PCB.
sent8: The improvement in the uniform partition is made by the Refined Part Pooling (RPP) network.
sent9: It achieves 92.3% of rank-1 accuracy with only the PCB.
sent10: Further, it improves to 93.8% of rank-1 accuracy by employing RPP with PCB network on the Market-1501 [105] dataset.
sent11: The text attribute combinations exist on a large scale in a real scenario.
sent12: However, a minimal amount of combination with sufficient data is available for training.
sent13: Except for such combinations, others are never modelled during training.
sent14: Thus, Dong et al. [83] formulate a textual attribute query-based person search problem as a zero-shot (ZSL) learning problem for the first time.
sent15: The authors propose Attribute-Image Hierarchical Matching (AIHM), which matches attributes and images at multiple hierarchical levels.
sent16: The algorithm achieves state-of-the-art rank-1 mAP of 43.3% on Market-1501 [105], 50.5% on DukeMTMC [107] and 31.3% on PA100K [117] datasets.
sent17: Table 5 shows a comparison of handcrafted and deep feature-based methods with their advantages and disadvantages.
sent18: A query-based personal retrieval system usually provides soft biometric keywords as the input query.
sent19: Improvisation in natural language processing now allows automatic extraction of the keywords from sentences.
sent20: It will move the system towards full automation."
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,Computer Science,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,s32,Hand crafted features,"Haar [71] Face detection works well for near view but it is difficult to detect for far view and crowded scenarios.

Color histogram [70,71,73] Simple to implement and supports the less number of colors (i.e. primary colors) for classification but difficult if color classes increases (different shades of color) as it involves quantization process for histogram calculations. HOG [76,110] HOG is good descriptor but its coarseness leads to sub-region processing in image which increases computational complexity. Height [73,74,75] It is calculated based on camera calibration parameters, but a small error in parameters estimation leads to high error in height calculations. Moreover, the camera parameter estimation need to done again if camera is moved.","[['b68'], ['b67', 'b68', 'b70', 'b71', 'b107', 'b73', 'b72']]","[['b68'], ['b67', 'b68', 'b70', 'b71', 'b107', 'b73', 'b72']]",8,"sent1: Haar [71] Face detection works well for near view but it is difficult to detect for far view and crowded scenarios.
sent2: Color histogram [70,71,73] Simple to implement and supports the less number of colors (i.e. primary colors) for classification but difficult if color classes increases (different shades of color) as it involves quantization process for histogram calculations.
sent3: HOG [76,110] HOG is good descriptor but its coarseness leads to sub-region processing in image which increases computational complexity.
sent4: Height [73,74,75] It is calculated based on camera calibration parameters, but a small error in parameters estimation leads to high error in height calculations.
sent5: Moreover, the camera parameter estimation need to done again if camera is moved."
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,Computer Science,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,s29,Handcrafted feature-based methodologies,"Research before the era of deep learning shows promising methods for person retrieval based on handcrafted features. Fig. 15 shows the general block diagram for person retrieval methods that use handcrafted features. Person detection is done using adaptive background segmentation, face detection, frame differencing, and query-based avatar creation. Feature extraction is a critical step where hand-engineered features are extracted using popular algorithms. Further, the feature fusion and classification are done for target person retrieval.

Vaquero et al. [71] exploit the limitation of the sensitivity of face recognition technology against illumination changes, low-resolution videos, and pose variations. They are the first to implement a video-based visual surveillance system that uses a person's fine-grained parts and attributes. Person detection uses a face detector. Further, the body is divisible into three regions with soft biometrics from the areas, i.e., face (hair type, eyewear type, facial hair type), torso (clothing color), and leg (clothing color). The nine facial attributes are extracted by training an individual Viola-Jones detector using Haar features. A normalized colour histogram is in hue, saturation and luminance (HSL) space for each body part (i.e., torso and leg).

Denman et al. [73] also propose a body part segmentation-based approach that detects a person using adaptive background segmentation [91] and segments the body into three parts (head, torso, and leg) using an average gradient across rows of the image. Calculation of height uses the Tsai [120] camera calibration approach and colour histograms for clothing colour. These features are fused using a weighted sum fusion approach. The algorithm evaluation on the PETS [121] dataset achieves the best results with an Equal Error Rate (EER) of 26.7% (colour model), the worst EER rate of 39.6% (size model), and an EER rate of 29.8% (a combination of colour and size).

The methods in [71,73] detect a person first and then match the query rather than searching the person based on the query. Such limitation is removable by creating an avatar of the person from discrete soft biometric attributes [74]. The particle filter is then applied to drive a search in the surveillance frame. The methods mentioned here propose a new dataset for soft biometricbased person localisation. The height of a person is broken up into 5 classes (concise, short, average, tall, and very tall) and colour into 11 classes (black, blue, brown, green, grey, orange, pink, purple, red, yellow, and white). Localisation accuracy is measurable using Intersection-over-Union (IoU). These methods achieve 24.28% average localisation accuracy.

Pedestrian semantic attribute-based ApiS dataset is in [110] with 11 binary attributes and two multiclass attributes. AdaBoost classifiers are useful for binary attribute classification, and K Nearest Neighbours (KNN) classifier is useful for multiclass attribute classification. This paper focuses on attribute classification and is useful for person retrieval. Halstead et al. [75] further extend the surveillance dataset of [74] by annotating 12 multiclass attributes on 110 persons. The authors improve the performance of [74] by adding more attributes for avatar creation and considering the unequal reliability of each attribute. A 21% improvement is reported over the baseline [74]. Avatar further extends in the form of channel representation (CR) [76]. The algorithm also incorporates shape information in the form of HOG representation to improve the performance. CR captures the spatial characteristics of colour and texture by representing them in the form of a multi-dimensional image. Each channel in CR represents either colour or texture information. The CR based approach achieves 44% average localisation accuracy.

Martinho et al. [106] generate a feature vector of length 4704 for each 256 × 256 pre-processed image. They use CIE-LAB, and the Gabor channel filters feature for the generation of an Ensemble of Localised Features (ELF) descriptor. A HOG based descriptor is also useful for the creation of 2304 features. The Extra-Trees (ET) supervised ensemble learning algorithm is applied for classification. The algorithm achieves 12.5% and 20.1% at rank-1 accuracy for one-shot and multi-shot identification. Shah et al. [70] use clothing colour and clothing type for person retrieval. They detect the person using motion segmentation based on frame differencing. Background clutter is removable by a morphological operation. The ISCC-NBS colour model [122] and CIEDE2000 [123] distance metrics are useful for colour classification. Their method achieves a 67.96% TP rate, which is better than the GMM (44.88%) approach. Table 3 shows an overview of the handcrafted feature-based methodologies. The performance column shows the highest value reported in the relevant literature in multiple scenario-based analyses. With the advent of deep learning techniques, person retrieval methods have seen a significant improvement in accuracy measures. Most state-of-the-art person retrieval methods are based on deep learning architectures.","[[], ['b68'], ['b88', 'b70', 'b117', 'b118'], ['b68', 'b70', 'b71'], ['b73', 'b72', 'b107', 'b71'], ['b67', 'b119', 'b120', 'b103']]","[[], ['b68'], ['b88', 'b70', 'b117', 'b118'], ['b68', 'b70', 'b71'], ['b73', 'b72', 'b107', 'b71'], ['b67', 'b119', 'b120', 'b103']]",16,"sent1: Research before the era of deep learning shows promising methods for person retrieval based on handcrafted features.
sent2: Fig. 15 shows the general block diagram for person retrieval methods that use handcrafted features.
sent3: Person detection is done using adaptive background segmentation, face detection, frame differencing, and query-based avatar creation.
sent4: Feature extraction is a critical step where hand-engineered features are extracted using popular algorithms.
sent5: Further, the feature fusion and classification are done for target person retrieval.
sent6: Vaquero et al. [71] exploit the limitation of the sensitivity of face recognition technology against illumination changes, low-resolution videos, and pose variations.
sent7: They are the first to implement a video-based visual surveillance system that uses a person's fine-grained parts and attributes.
sent8: Person detection uses a face detector.
sent9: Further, the body is divisible into three regions with soft biometrics from the areas, i.e., face (hair type, eyewear type, facial hair type), torso (clothing color), and leg (clothing color).
sent10: The nine facial attributes are extracted by training an individual Viola-Jones detector using Haar features.
sent11: A normalized colour histogram is in hue, saturation and luminance (HSL) space for each body part (i.e., torso and leg).Denman et al. [73] also propose a body part segmentation-based approach that detects a person using adaptive background segmentation [91] and segments the body into three parts (head, torso, and leg) using an average gradient across rows of the image.
sent12: Calculation of height uses the Tsai [120] camera calibration approach and colour histograms for clothing colour.
sent13: These features are fused using a weighted sum fusion approach.
sent14: The algorithm evaluation on the PETS [121] dataset achieves the best results with an Equal Error Rate (EER) of 26.7% (colour model), the worst EER rate of 39.6% (size model), and an EER rate of 29.8% (a combination of colour and size).
sent15: The methods in [71,73] detect a person first and then match the query rather than searching the person based on the query.
sent16: Such limitation is removable by creating an avatar of the person from discrete soft biometric attributes [74].
sent17: The particle filter is then applied to drive a search in the surveillance frame.
sent18: The methods mentioned here propose a new dataset for soft biometricbased person localisation.
sent19: The height of a person is broken up into 5 classes (concise, short, average, tall, and very tall) and colour into 11 classes (black, blue, brown, green, grey, orange, pink, purple, red, yellow, and white).
sent20: Localisation accuracy is measurable using Intersection-over-Union (IoU).
sent21: These methods achieve 24.28% average localisation accuracy.
sent22: Pedestrian semantic attribute-based ApiS dataset is in [110] with 11 binary attributes and two multiclass attributes.
sent23: AdaBoost classifiers are useful for binary attribute classification, and K Nearest Neighbours (KNN) classifier is useful for multiclass attribute classification.
sent24: This paper focuses on attribute classification and is useful for person retrieval.
sent25: Halstead et al. [75] further extend the surveillance dataset of [74] by annotating 12 multiclass attributes on 110 persons.
sent26: The authors improve the performance of [74] by adding more attributes for avatar creation and considering the unequal reliability of each attribute.
sent27: A 21% improvement is reported over the baseline [74].
sent28: Avatar further extends in the form of channel representation (CR) [76].
sent29: The algorithm also incorporates shape information in the form of HOG representation to improve the performance.
sent30: CR captures the spatial characteristics of colour and texture by representing them in the form of a multi-dimensional image.
sent31: Each channel in CR represents either colour or texture information.
sent32: The CR based approach achieves 44% average localisation accuracy.
sent33: Martinho et al. [106] generate a feature vector of length 4704 for each 256 × 256 pre-processed image.
sent34: They use CIE-LAB, and the Gabor channel filters feature for the generation of an Ensemble of Localised Features (ELF) descriptor.
sent35: A HOG based descriptor is also useful for the creation of 2304 features.
sent36: The Extra-Trees (ET) supervised ensemble learning algorithm is applied for classification.
sent37: The algorithm achieves 12.5% and 20.1% at rank-1 accuracy for one-shot and multi-shot identification.
sent38: Shah et al. [70] use clothing colour and clothing type for person retrieval.
sent39: They detect the person using motion segmentation based on frame differencing.
sent40: Background clutter is removable by a morphological operation.
sent41: The ISCC-NBS colour model [122] and CIEDE2000
sent42: [123] distance metrics are useful for colour classification.
sent43: Their method achieves a 67.96% TP rate, which is better than the GMM (44.88%) approach.
sent44: Table 3 shows an overview of the handcrafted feature-based methodologies.
sent45: The performance column shows the highest value reported in the relevant literature in multiple scenario-based analyses.
sent46: With the advent of deep learning techniques, person retrieval methods have seen a significant improvement in accuracy measures.
sent47: Most state-of-the-art person retrieval methods are based on deep learning architectures."
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,Computer Science,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,s6,Structure of the paper,"The paper presents the use of soft biometrics for person retrieval and suggests their strengths and weaknesses. It recommends the most discriminative and robust soft biometrics during person retrieval under challenging conditions. Besides covering the general frame work for retrieval using textual query it also reviews and compares large-scale datasets. It further provides classification for retrieval methods and provides state-of-the-art reviews for soft biometric attribute-based retrieval methods. Further, the integration of benchmark datasets and methods allows an effective comparison of their performance. In this way, the presented review is entirely different from other soft biometrics basics reviews [9,10,11,12], which focus mainly on soft biometric attribute retrieval from the person's image. A review of recent datasets, state-of-theart retrieval methods including vision and natural language processing, and dataset-method integration is not available in the other review articles.

The contributions of the paper are summarised as follows:

1. Recommends the most discriminative soft biometrics under challenging conditions and show case their use for person retrieval. 2. Provides the most comprehensive coverage of soft biometrics datasets. 3. Spans the complete spectrum for state-of-the-art methods; image based, discrete attribute based and natural language description-based retrieval. 4. Integrates datasets and methods for quantitative and objective performance evaluation. 5. Discusses open research problems in person retrieval using soft biometrics and their possible solutions.","[['b8', 'b7', 'b10', 'b9'], [], ['b3']]","[['b8', 'b7', 'b10', 'b9'], [], ['b3']]",5,"sent1: The paper presents the use of soft biometrics for person retrieval and suggests their strengths and weaknesses.
sent2: It recommends the most discriminative and robust soft biometrics during person retrieval under challenging conditions.
sent3: Besides covering the general frame work for retrieval using textual query it also reviews and compares large-scale datasets.
sent4: It further provides classification for retrieval methods and provides state-of-the-art reviews for soft biometric attribute-based retrieval methods.
sent5: Further, the integration of benchmark datasets and methods allows an effective comparison of their performance.
sent6: In this way, the presented review is entirely different from other soft biometrics basics reviews [9,10,11,12], which focus mainly on soft biometric attribute retrieval from the person's image.
sent7: A review of recent datasets, state-of-theart retrieval methods including vision and natural language processing, and dataset-method integration is not available in the other review articles.
sent8: The contributions of the paper are summarised as follows:1.
sent9: Recommends the most discriminative soft biometrics under challenging conditions and show case their use for person retrieval.
sent10: 2. Provides the most comprehensive coverage of soft biometrics datasets.
sent11: 3. Spans the complete spectrum for state-of-the-art methods; image based, discrete attribute based and natural language description-based retrieval.
sent12: 4. Integrates datasets and methods for quantitative and objective performance evaluation.
sent13: 5. Discusses open research problems in person retrieval using soft biometrics and their possible solutions."
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,Computer Science,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,s26,Mean Average Precision (mAP):,"It is an average of maximum precisions at different recall values. An object detector accuracy is measured by mAP. It is calculated as eq. 4,

Intersection-over-Union (IoU):

The localisation accuracy of the object detector is measurable by Intersectionover-Union (IoU). It uses the bounding box of the object for evaluation. IoU is the fraction of the intersecting area to the union area between the output bounding box and the ground truth. It measures how good the detector is in localizing objects to the ground truth. Fig. 13 depicts the IoU metric.

D = bounding box output of algorithm and GT = ground truth bounding box.

Different person retrieval methodologies reviewed below use the evaluation metrics discussed above. Methods in [69,74,75,76,78,79,80] use IoU or average IoU, [69,70,78] use TPR and [81,82,84,85,89,134,135,136,137,138,139] use Top-1 accuracy as evaluation metrics.

Fig. 14 Classification of person retrieval methodologies based on the type of input query. Each classification category shows example of person retrieval from cropped person image gallery and full frames of surveillance video. Images shown in image gallery are adopted from RAP [108] dataset and surveillance video frames are adopted from PRW [118] dataset to showcase the example.","[[], [], [], [], ['b67', 'b132', 'b133', 'b66', 'b73', 'b86', 'b76', 'b79', 'b136', 'b75', 'b134', 'b72', 'b135', 'b71', 'b82', 'b81', 'b78', None, 'b77'], ['b115', 'b105']]","[[], [], [], [], ['b67', 'b132', 'b133', 'b66', 'b73', 'b86', 'b76', 'b79', 'b136', 'b75', 'b134', 'b72', 'b135', 'b71', 'b82', 'b81', 'b78', None, 'b77'], ['b115', 'b105']]",21,"sent1: It is an average of maximum precisions at different recall values.
sent2: An object detector accuracy is measured by mAP.
sent3: It is calculated as eq. 4,Intersection-over-Union (IoU):
sent4: The localisation accuracy of the object detector is measurable by Intersectionover-Union (IoU).
sent5: It uses the bounding box of the object for evaluation.
sent6: IoU is the fraction of the intersecting area to the union area between the output bounding box and the ground truth.
sent7: It measures how good the detector is in localizing objects to the ground truth.
sent8: Fig. 13 depicts the IoU metric.
sent9: D = bounding box output of algorithm and GT = ground truth bounding box.
sent10: Different person retrieval methodologies reviewed below use the evaluation metrics discussed above.
sent11: Methods in [69,74,75,76,78,79,80] use IoU or average IoU, [69,70,78] use TPR and [81,82,84,85,89,134,135,136,137,138,139] use Top-1 accuracy as evaluation metrics.
sent12: Fig. 14 Classification of person retrieval methodologies based on the type of input query.
sent13: Each classification category shows example of person retrieval from cropped person image gallery and full frames of surveillance video.
sent14: Images shown in image gallery are adopted from RAP [108] dataset and surveillance video frames are adopted from PRW [118] dataset to showcase the example."
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,Computer Science,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,s20,Person retrieval datasets,"A variety of challenging datasets are an essential entity for any research and development task. Datasets with extensive annotations and reliable evaluation strategies help to create a robust algorithm. Researchers have developed many public datasets for person identification and re-identification in the past two decades. Table 2 shows only datasets with soft biometric annotations, which help create textual attribute query or natural language description for person retrieval.

Dataset comparisons have the following parameters: number of persons, number of images, resolution of the image (width × height)), number of soft biometrics annotations, number of cameras used to collect the dataset, type of attribute query, challenges covered, and whether the dataset is with full surveillance frame or cropped person. Abbreviations for challenges and types of attributes are in Table 2, which are useful for further discussion. Layne et al. [102] annotate the VIPeR [99] dataset with 15 soft biometric attributes like shorts, skirt, sandals, backpack, jeans, logo, v-neck, stripes, sunglasses, openouterwear, headphones, long-hair, short-hair, gender and carrying an object. These are binary attributes e.g., gender has value from set {0, 1}, where 0 = male and 1 = female.

Zhu et al. [110] introduce the Attributed Pedestrians in Surveillance (APS) database specifically for pedestrian attribute classification, which can also be useful for person retrieval. The APS consists of 11 binary attributes and two multi-class (total 35) attributes, i.e., clothing colour of upper-body and lowerbody. APS is the first dataset to use clothing colour and covers more challenges compared to VIPeR. Layne et al. [103] further extend annotations for VIPeR [99], PRID [100], and GRID [101] from 15 to 21 by introducing shirt colour (red, blue, green), shirt shade (light and dark) and pant and hair colour. New datasets like VIPeR and PRID are having fixed image resolution, limited cameras, C2, C5, and C8. GRID upgrades such parameters and introduces C1 and C3. However, it is limited to some images. VIPeR, PRID, and GRID contain photos from outdoor scenes only.  C5 -Different illumination in the scene for a single sequence C10 -Type of camera / multiple camera Deng et al. [104] introduce the first large-scale PEdesTrian Attribute (PETA) dataset to overcome the limitations of VIPeR, APS, PRID, and GRID datasets. They introduce C1, C2, C4, C5, C7, C8 and diversity by annotating 61 binary and 4 multiclass (total 105) attributes in 19,000 images collected from surveillance datasets like 3DpeS [111], CAVIAR4ReID [112], CUHK [113], GRID [101], i-LIDS [114], PRID [100], SARC3D [115], TownCentre [116] and VIPeR [99]. Martinho et al. [106] create the SoBiR dataset with 12 multi-class attributes. It is a comparatively smaller dataset with 1,600 images. However, each soft biometric attribute possesses multiple classes which help discriminate against people better.

Recent research advancement is increasing rapidly in deep learning domains, which requires a more diverse and huge amount of data for better generalization. Datasets like VIPeR, APS GRID, and SoBiR are too small for deep learning-based frameworks. Datasets published in the last five years provide a good amount of data to create robust models and algorithms. Large-scale datasets such as Mareket-1501 [105] have 32,217 images of 1,501 persons and DukeMTMC-reID [107] provides 36,441 images of 1,812 persons. These small and large-scale datasets use discrete attribute-based queries ( Fig. 12(a)), which has limited practical usage, as discussed in Sect. 2.3. Thus, Li et al. [85] propose a large-scale dataset with natural language description-based annotations. The dataset is known as CUHK Person Description Dataset (CUHK-PEDES). It provides 40,206 images of 13,003 persons with 80,412 description sentences. It is the only large-scale dataset with a natural language description that provides an opportunity to explore the relationship between language and vision for a person retrieval problem.

Liu et al. [117] propose a large-scale pedestrian attribute (PA) dataset with 100000 images. It is known as PA-100k. It consists of the highest number of images in all the datasets with a more diverse collection from 598 scenes. It can only be used for attribute-based person retrieval, not for person reidentification. Further increasing the scale in all aspects, Li et al. [108] created a Richly Annotated Pedestrian (RAP). Li et al. [109] also make a RAP -Large-Scale Person Retrieval Challenge (RAP-LSPRC) dataset. RAP-LSPRC is a subset of RAP with a comprehensive evaluation and benchmark dataset. By far, RAP contains the highest number of images (84,928) collected from 25 surveillance cameras installed in the unconstrained indoor environment of a shopping mall. RAP and RAP-LSPRC are useful for both person retrieval and person re-identification tasks.

Datasets discussed so far contain an image gallery of cropped persons from surveillance video frames. Hence, in such image problems related to person detection, occlusion, merging with background, crowded scene, and illumination variations in a single sequence cannot be overcome. It limits the development of an end-to-end intelligent surveillance system. Halstead et al. first developed an AVSS 2018 challenge II dataset with full surveillance frames (20,453) collected from 6 indoor cameras. It is the only dataset that contains video sequences of 151 persons (110 training + 41 testing) with varying surveillance frames from 21 to 290. They provide shallow resolution (704 × 576)) frames.

They cover most real-time surveillance challenges (Sect. 2.1) to develop an end-to-end solution for person retrieval. Such surveillance frames are in Fig. 4, 5, 6, 7, and Fig. 9.

Recently, Kumar et al. [155] released a UAV-based dataset for Pedestrian Detection, Tracking, Re-Identification, and Search (P-DESTRE) from aerial devices. It also contains full surveillance frame videos. These videos are captured with 4K spatial resolution (3840 × 2160) and over different days and times. Thus, P-DESTRE is the first dataset to provide consistent ID annotations across multiple days. Other datasets discussed so far do not cover this challenge. Although this dataset covers rich annotations, it lacks clothing colour annotations. By reviewing all the datasets, the commonly used soft biometric attribute set is {gender, upper-body cloth type, lower-body cloth type, upper-body cloth color, lower-body cloth color, upper-body cloth texture, clothing style, height, hair color, hairstyle, backpack, carrying an object, eye-wear, shoe}. Such variety and diversity show the continuous evolution of challenging datasets proposed by active researchers in the vision and language fields.","[[], ['b99', 'b96'], ['b111', 'b113', 'b101', 'b96', 'b97', 'b112', 'b107', 'b110', 'b109', 'b103', 'b108', 'b98', 'b100'], ['b102', 'b104', 'b82'], ['b114', 'b105', 'b106'], [], [], ['b152']]","[[], ['b99', 'b96'], ['b111', 'b113', 'b101', 'b96', 'b97', 'b112', 'b107', 'b110', 'b109', 'b103', 'b108', 'b98', 'b100'], ['b102', 'b104', 'b82'], ['b114', 'b105', 'b106'], [], [], ['b152']]",22,"sent1: A variety of challenging datasets are an essential entity for any research and development task.
sent2: Datasets with extensive annotations and reliable evaluation strategies help to create a robust algorithm.
sent3: Researchers have developed many public datasets for person identification and re-identification in the past two decades.
sent4: Table 2 shows only datasets with soft biometric annotations, which help create textual attribute query or natural language description for person retrieval.
sent5: Dataset comparisons have the following parameters: number of persons, number of images, resolution of the image (width × height)), number of soft biometrics annotations, number of cameras used to collect the dataset, type of attribute query, challenges covered, and whether the dataset is with full surveillance frame or cropped person.
sent6: Abbreviations for challenges and types of attributes are in Table 2, which are useful for further discussion.
sent7: Layne et al. [102] annotate the VIPeR [99] dataset with 15 soft biometric attributes like shorts, skirt, sandals, backpack, jeans, logo, v-neck, stripes, sunglasses, openouterwear, headphones, long-hair, short-hair, gender and carrying an object.
sent8: These are binary attributes e.g., gender has value from set {0, 1}, where 0 = male and 1 = female.
sent9: Zhu et al. [110] introduce the Attributed Pedestrians in Surveillance (APS) database specifically for pedestrian attribute classification, which can also be useful for person retrieval.
sent10: The APS consists of 11 binary attributes and two multi-class (total 35) attributes, i.e., clothing colour of upper-body and lowerbody.
sent11: APS is the first dataset to use clothing colour and covers more challenges compared to VIPeR. Layne et al. [103] further extend annotations for VIPeR [99], PRID [100], and GRID [101] from 15 to 21 by introducing shirt colour (red, blue, green), shirt shade (light and dark) and pant and hair colour.
sent12: New datasets like VIPeR and PRID are having fixed image resolution, limited cameras, C2, C5, and C8.
sent13: GRID upgrades such parameters and introduces C1 and C3.
sent14: However, it is limited to some images.
sent15: VIPeR, PRID, and GRID contain photos from outdoor scenes only.
sent16: C5 -Different illumination in the scene for a single sequence C10 -Type of camera / multiple camera Deng et al. [104] introduce the first large-scale PEdesTrian Attribute (PETA) dataset to overcome the limitations of VIPeR, APS, PRID, and GRID datasets.
sent17: They introduce C1, C2, C4, C5, C7, C8 and diversity by annotating 61 binary and 4 multiclass (total 105) attributes in 19,000 images collected from surveillance datasets like 3DpeS [111], CAVIAR4ReID [112], CUHK [113], GRID [101], i-LIDS [114], PRID [100], SARC3D [115], TownCentre [116] and VIPeR [99].
sent18: Martinho et al. [106] create the SoBiR dataset with 12 multi-class attributes.
sent19: It is a comparatively smaller dataset with 1,600 images.
sent20: However, each soft biometric attribute possesses multiple classes which help discriminate against people better.
sent21: Recent research advancement is increasing rapidly in deep learning domains, which requires a more diverse and huge amount of data for better generalization.
sent22: Datasets like VIPeR, APS GRID, and SoBiR are too small for deep learning-based frameworks.
sent23: Datasets published in the last five years provide a good amount of data to create robust models and algorithms.
sent24: Large-scale datasets such as Mareket-1501 [105] have 32,217 images of 1,501 persons and DukeMTMC-reID [107] provides 36,441 images of 1,812 persons.
sent25: These small and large-scale datasets use discrete attribute-based queries ( Fig. 12(a)), which has limited practical usage, as discussed in Sect. 2.3.
sent26: Thus, Li et al. [85] propose a large-scale dataset with natural language description-based annotations.
sent27: The dataset is known as CUHK Person Description Dataset (CUHK-PEDES).
sent28: It provides 40,206 images of 13,003 persons with 80,412 description sentences.
sent29: It is the only large-scale dataset with a natural language description that provides an opportunity to explore the relationship between language and vision for a person retrieval problem.
sent30: Liu et al. [117] propose a large-scale pedestrian attribute (PA) dataset with 100000 images.
sent31: It is known as PA-100k. It consists of the highest number of images in all the datasets with a more diverse collection from 598 scenes.
sent32: It can only be used for attribute-based person retrieval, not for person reidentification.
sent33: Further increasing the scale in all aspects, Li et al. [108] created a Richly Annotated Pedestrian (RAP).
sent34: Li et al. [109] also make a RAP -Large-Scale Person Retrieval Challenge (RAP-LSPRC) dataset.
sent35: RAP-LSPRC is a subset of RAP with a comprehensive evaluation and benchmark dataset.
sent36: By far, RAP contains the highest number of images (84,928) collected from 25 surveillance cameras installed in the unconstrained indoor environment of a shopping mall.
sent37: RAP and RAP-LSPRC are useful for both person retrieval and person re-identification tasks.
sent38: Datasets discussed so far contain an image gallery of cropped persons from surveillance video frames.
sent39: Hence, in such image problems related to person detection, occlusion, merging with background, crowded scene, and illumination variations in a single sequence cannot be overcome.
sent40: It limits the development of an end-to-end intelligent surveillance system.
sent41: Halstead et al. first developed an AVSS 2018 challenge II dataset with full surveillance frames (20,453) collected from 6 indoor cameras.
sent42: It is the only dataset that contains video sequences of 151 persons (110 training + 41 testing) with varying surveillance frames from 21 to 290.
sent43: They provide shallow resolution (704 × 576)) frames.
sent44: They cover most real-time surveillance challenges (Sect. 2.1) to develop an end-to-end solution for person retrieval.
sent45: Such surveillance frames are in Fig. 4, 5, 6, 7, and Fig. 9.
sent46: Recently, Kumar et al. [155] released a UAV-based dataset for Pedestrian Detection, Tracking, Re-Identification, and Search (P-DESTRE) from aerial devices.
sent47: It also contains full surveillance frame videos.
sent48: These videos are captured with 4K spatial resolution (3840 × 2160) and over different days and times.
sent49: Thus, P-DESTRE is the first dataset to provide consistent ID annotations across multiple days.
sent50: Other datasets discussed so far do not cover this challenge.
sent51: Although this dataset covers rich annotations, it lacks clothing colour annotations.
sent52: By reviewing all the datasets, the commonly used soft biometric attribute set is {gender, upper-body cloth type, lower-body cloth type, upper-body cloth color, lower-body cloth color, upper-body cloth texture, clothing style, height, hair color, hairstyle, backpack, carrying an object, eye-wear, shoe}.
sent53: Such variety and diversity show the continuous evolution of challenging datasets proposed by active researchers in the vision and language fields."
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,Computer Science,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,s19,Vision based person retrieval system,"This section discusses a person retrieval system that uses a natural languagebased textual query. Researchers propose methods using handcrafted featurebased retrieval [61,62,63,64,65,66,67,70,71,72,73,74,75,76,77], deep learning feature-based linear filtering [68,69,78], parallel classification of attributes [79,80,81] utilization of Natural Language Processing (NLP) algorithms to process textual queries [82,83,84,85,86,87,88,89]. Such a plethora of methods consists of person detection, segmentation, soft biometric attributes classification, and person identification as crucial steps in the person retrieval process. These key steps are given in Fig. 10 and discussed as follows:

Step-I (Person detection): Person retrieval in surveillance is a challenging task because such scenarios are usually in the wild, containing various objects (e.g., chair, table, car, and train) apart from the person. Hence, person detection is the critical initial step before textual query-based person retrieval. Person detection is the task of locating all persons in the surveillance frame. It was a foreground-background separation problem before the era of deep learning. A person is a foreground object, which is different from the rest of the background. Early research detected the person by different methodologies like background subtraction [70,90], adaptive background segmentation [91], Gaussian Mixture Model (GMM) [92], and Histograms of Oriented Gradients (HoG) [93]. Deep learning-based methodologies are becoming popular in recent years due to their robust feature extraction and learning ability. The computer vision community considers person detection as an object detection problem. Some of the popular object detection frameworks are ""You Only Look Once (YOLO)"" [94], Single-Shot Multibox Detector (SSD) [126], Region-based Convolutional Neural Network (R-CNN) [95], Fast R-CNN [96], Faster R-CNN [97] and Mask R-CNN [98]. Person detection provides a bounding box for each person in the frame.

Step-II (Segmentation): Segmentation follows person detection. Segmentation can be either in the form of a body part from a bounding box or semantic segmentation within the bounding box. Such segmentation is shown in Fig. 11. A full-body can be segmented into three major parts: head and shoulders, upper body, and lower body (see Fig. 11(a)). Fig. 11(b) shows  semantic segmentation in which each pixel belonging to the person within the bounding box has a label. Mask R-CNN [98] provides both person detection and semantic segmentation together.

Step-III (Soft biometric attribute recognition): Full-body images and segmentation outputs are supplied to extract soft biometric attributes. Full-body images are useful for extracting characteristics like height and gender [68,69,78]. On the other hand, clothing colour, clothing texture, clothing type, shoes, hair, and accessory attributes are available from different body parts. Usually, details from video frames are visual features. Recent development shows multi-attribute learning for person attribute recognition. Researchers propose various networks like Deep learning-based Multiple Attribute Recognition (DeepMAR) [152], Attribute Convolutional Net (ACN) [153], and Multi-Label Convolutional Neural Network (ML-CNN) [154].

Step-IV (Text feature extraction): Textual attribute query or natural language description is another input to the person retrieval system. Such textual query samples are given in Fig. 12. Textual attribute query ( Fig. 12(a)) is cheaper to collect in terms of attribute wise information and has a less complicated sentence structure in comparison with natural language description ( Fig. 12(b)). The data is collected separately for all attributes which is useful in the retrieval process. For example, torso type attribute value is available from one of the predefined classes {Long Sleeve, Short Sleeve, No Sleeve}. However, such a discrete attribute query has a fragile appearance, descriptive ability, and practical usage limitation. In contrast to textual attribute query, natural language description has more complex sentence structures. However, it provides a detailed description of the person. It is essential to extract relevant information from such a human verbal description. For example, the description in Fig. 12(b), 'women', 'brown hair', 'blacktop'are more relevant information than 'the', 'a', and 'has'. Such relevant information forms textual features. NLP based algorithms help to extract textual features from natural language descriptions.

Step-V (Feature fusion and retrieval): Visual features for each person are available from surveillance videos, and textual features are extractable from the textual query. Person retrieval from surveillance using textual query covers two major problem domains: (i) computer vision and (ii) nat-ural language processing. Hence, cross-modal feature embedding is applicable in the feature fusion block (see Fig.10). Finally, a person(s) matching the textual query is retrieved. There is a possibility of multiple person retrieval as soft biometrics are not unique to an individual.

The accuracy of person retrieval depends on the complexity of the training and testing data sets. The retrieval robustness relies on the availability of the richly annotated data sets. The performance evaluation of the method is done by evaluation metrics like accuracy, True Positive Rate (TPR) and Intersection-over-Union (IoU). Therefore, the following sub-sections discuss person retrieval datasets (available in the public domain) and evaluation metrics.","[['b67', 'b85', 'b63', 'b61', 'b69', 'b84', 'b66', 'b58', 'b80', 'b73', 'b86', 'b64', 'b59', 'b60', 'b76', 'b79', 'b75', 'b65', 'b72', 'b68', 'b70', 'b71', 'b62', 'b82', 'b74', 'b81', 'b78', 'b83', 'b77'], ['b88', 'b67', 'b95', 'b91', 'b89', 'b123', 'b93', 'b90', 'b87', 'b92', 'b94'], ['b95'], ['b151', 'b66', 'b65', 'b75', 'b149', 'b150'], [], [], []]","[['b67', 'b85', 'b63', 'b61', 'b69', 'b84', 'b66', 'b58', 'b80', 'b73', 'b86', 'b64', 'b59', 'b60', 'b76', 'b79', 'b75', 'b65', 'b72', 'b68', 'b70', 'b71', 'b62', 'b82', 'b74', 'b81', 'b78', 'b83', 'b77'], ['b88', 'b67', 'b95', 'b91', 'b89', 'b123', 'b93', 'b90', 'b87', 'b92', 'b94'], ['b95'], ['b151', 'b66', 'b65', 'b75', 'b149', 'b150'], [], [], []]",47,"sent1: This section discusses a person retrieval system that uses a natural languagebased textual query.
sent2: Researchers propose methods using handcrafted featurebased retrieval [61,62,63,64,65,66,67,70,71,72,73,74,75,76,77], deep learning feature-based linear filtering [68,69,78], parallel classification of attributes [79,80,81] utilization of Natural Language Processing (NLP) algorithms to process textual queries [82,83,84,85,86,87,88,89].
sent3: Such a plethora of methods consists of person detection, segmentation, soft biometric attributes classification, and person identification as crucial steps in the person retrieval process.
sent4: These key steps are given in Fig. 10 and discussed as follows:Step-I (Person detection): Person retrieval in surveillance is a challenging task because such scenarios are usually in the wild, containing various objects (e.g., chair, table, car, and train) apart from the person.
sent5: Hence, person detection is the critical initial step before textual query-based person retrieval.
sent6: Person detection is the task of locating all persons in the surveillance frame.
sent7: It was a foreground-background separation problem before the era of deep learning.
sent8: A person is a foreground object, which is different from the rest of the background.
sent9: Early research detected the person by different methodologies like background subtraction [70,90], adaptive background segmentation [91], Gaussian Mixture Model (GMM) [92], and Histograms of Oriented Gradients (HoG) [93].
sent10: Deep learning-based methodologies are becoming popular in recent years due to their robust feature extraction and learning ability.
sent11: The computer vision community considers person detection as an object detection problem.
sent12: Some of the popular object detection frameworks are ""You Only Look Once (YOLO)"" [94], Single-Shot Multibox Detector (SSD) [126], Region-based Convolutional Neural Network (R-CNN) [95], Fast R-CNN [96], Faster R-CNN [97] and Mask R-CNN [98].
sent13: Person detection provides a bounding box for each person in the frame.
sent14: Step-II (Segmentation): Segmentation follows person detection.
sent15: Segmentation can be either in the form of a body part from a bounding box or semantic segmentation within the bounding box.
sent16: Such segmentation is shown in Fig. 11.
sent17: A full-body can be segmented into three major parts: head and shoulders, upper body, and lower body (see Fig. 11(a)).
sent18: Fig. 11(b) shows  semantic segmentation in which each pixel belonging to the person within the bounding box has a label.
sent19: Mask R-CNN [98] provides both person detection and semantic segmentation together.
sent20: Step-III (Soft biometric attribute recognition): Full-body images and segmentation outputs are supplied to extract soft biometric attributes.
sent21: Full-body images are useful for extracting characteristics like height and gender [68,69,78].
sent22: On the other hand, clothing colour, clothing texture, clothing type, shoes, hair, and accessory attributes are available from different body parts.
sent23: Usually, details from video frames are visual features.
sent24: Recent development shows multi-attribute learning for person attribute recognition.
sent25: Researchers propose various networks like Deep learning-based Multiple Attribute Recognition (DeepMAR) [152], Attribute Convolutional Net (ACN) [153], and Multi-Label Convolutional Neural Network (ML-CNN) [154].
sent26: Step-IV (Text feature extraction): Textual attribute query or natural language description is another input to the person retrieval system.
sent27: Such textual query samples are given in Fig. 12.
sent28: Textual attribute query ( Fig. 12(a)) is cheaper to collect in terms of attribute wise information and has a less complicated sentence structure in comparison with natural language description ( Fig. 12(b)).
sent29: The data is collected separately for all attributes which is useful in the retrieval process.
sent30: For example, torso type attribute value is available from one of the predefined classes {Long Sleeve, Short Sleeve, No Sleeve}.
sent31: However, such a discrete attribute query has a fragile appearance, descriptive ability, and practical usage limitation.
sent32: In contrast to textual attribute query, natural language description has more complex sentence structures.
sent33: However, it provides a detailed description of the person.
sent34: It is essential to extract relevant information from such a human verbal description.
sent35: For example, the description in Fig. 12(b), 'women', 'brown hair', 'blacktop'are more relevant information than 'the', 'a', and 'has'.
sent36: Such relevant information forms textual features.
sent37: NLP based algorithms help to extract textual features from natural language descriptions.
sent38: Step-V (Feature fusion and retrieval): Visual features for each person are available from surveillance videos, and textual features are extractable from the textual query.
sent39: Person retrieval from surveillance using textual query covers two major problem domains: (i) computer vision and (ii) nat-ural language processing.
sent40: Hence, cross-modal feature embedding is applicable in the feature fusion block (see Fig.10).
sent41: Finally, a person(s) matching the textual query is retrieved.
sent42: There is a possibility of multiple person retrieval as soft biometrics are not unique to an individual.
sent43: The accuracy of person retrieval depends on the complexity of the training and testing data sets.
sent44: The retrieval robustness relies on the availability of the richly annotated data sets.
sent45: The performance evaluation of the method is done by evaluation metrics like accuracy, True Positive Rate (TPR) and Intersection-over-Union (IoU).
sent46: Therefore, the following sub-sections discuss person retrieval datasets (available in the public domain) and evaluation metrics."
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,Computer Science,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,s17,Soft biometric attribute selection,"Soft biometric attributes are not unique; for example, there may be multiple people with a blue torso colour. Thus, it produces numerous matches for the given textual query. Therefore, it is advantageous to use the most discriminative attributes for person retrieval. Different soft biometrics have certain advantages, and they are discussed below. For example, a surveillance video may contain different view angles and distance. The person's height is invariant to such concerns [68,69,78]. Clothing colour is also one of the most discriminative attributes. It has the following advantages:

-Colour is more immune to noise.

-It is insensitive to dimension, view angle, and direction.

-Colour is recognizable from a far distance.

Clothing type is also insensitive to view angle. Gender is identifiable from near as well as far distance and different view angles. Table 1 shows the strength of soft biometric attributes against challenging conditions. "" ""indicates that a particular attribute is useful for the retrieval process against a specific challenging scenario. For example, a standing person's height estimation requires head and feet points [68,69,78]. The extraction of those points is not affected by different view angles, near or far fields, illumination, and low resolution. However, those points are non-extractable when a person is occluded or has a pose like sitting. Similarly, ""×""indicates that a particular attribute is not useful for the retrieval process against the challenge. Table 1 Soft biometric attribute selection for person retrieval against different challenges.","[['b66', 'b75', 'b65'], [], [], [], ['b66', 'b75', 'b65']]","[['b66', 'b75', 'b65'], [], [], [], ['b66', 'b75', 'b65']]",6,"sent1: Soft biometric attributes are not unique; for example, there may be multiple people with a blue torso colour.
sent2: Thus, it produces numerous matches for the given textual query.
sent3: Therefore, it is advantageous to use the most discriminative attributes for person retrieval.
sent4: Different soft biometrics have certain advantages, and they are discussed below.
sent5: For example, a surveillance video may contain different view angles and distance.
sent6: The person's height is invariant to such concerns [68,69,78].
sent7: Clothing colour is also one of the most discriminative attributes.
sent8: It has the following advantages:-Colour is more immune to noise.
sent9: -It is insensitive to dimension, view angle, and direction.
sent10: -Colour is recognizable from a far distance.
sent11: Clothing type is also insensitive to view angle.
sent12: Gender is identifiable from near as well as far distance and different view angles.
sent13: Table 1 shows the strength of soft biometric attributes against challenging conditions. ""
sent14: ""indicates that a particular attribute is useful for the retrieval process against a specific challenging scenario.
sent15: For example, a standing person's height estimation requires head and feet points [68,69,78].
sent16: The extraction of those points is not affected by different view angles, near or far fields, illumination, and low resolution.
sent17: However, those points are non-extractable when a person is occluded or has a pose like sitting.
sent18: Similarly, ""×""indicates that a particular attribute is not useful for the retrieval process against the challenge.
sent19: Table 1 Soft biometric attribute selection for person retrieval against different challenges."
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,Computer Science,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,s27,Person retrieval methodologies,"Person retrieval is a subject undergoing intense study for the past two decades due to its prime application to public safety. A large-scale intelligent surveillance system requires person retrieval using visual attributes or available person image(s). Vision community researchers are proposing a plethora of methodologies for robust person retrieval. They have two major categories based on the type of input query: -Image-based retrieval.

-Soft biometric attribute-based retrieval.

Classification of person retrieval methodologies based on the type of input query is shown in Fig. 14. A person is retrievable from the cropped person image gallery or full frames of surveillance video. The latter is more suitable for real-time scenarios. The person retrieval example of each category is also shown in Fig. 14.

Image-based person retrieval methodologies aim to retrieve the target from the large-scale image gallery. This large-scale image gallery or dataset is from different non-overlapping cameras. Such person retrieval techniques are known as person re-identification (Re-ID). These techniques require at least one image of the target person for querying the system to perform re-identification of the same identity from different cameras. A person ReID technique assumes that the image gallery contains at least one image of the target person captured from a camera other than the query image. Such methodologies fail when a probe image is not available.

In contrast, soft biometric attribute-based person retrieval methodologies do not require a probe image. It uses a semantic person description generated from soft biometric attributes. Such a description (textual query) is input to the person retrieval system. Soft-biometric attribute-based retrieval is further divisible into two categories based on the type of textual query ( Fig. 12 and  14):

1. Discrete attribute-based retrieval. 2. Natural language description based retrieval.

A discrete attribute-based person retrieval system searches for the person using soft biometrics attributes, e.g., clothing colour, clothing type, and gender. While a natural language description-based system accepts a natural description, e.g., A woman is with average height wearing a white midi with no sleeve. She has black open hair.

Applications like obvious question answering, image captioning, and person retrieval are gaining much attention due to the association of two widely researched domains, i.e., computer vision and natural language processing. Such applications expect the learning of discriminative feature representations from both images and text. The person retrieval algorithm ranks gallery images or person(s) in the surveillance frame according to their relevance to the description. The best matching images from the gallery or surveillance frame return the person of interest. This class of surveillance problems is known as textbased person search or person retrieval using natural language descriptions [85,139]. Such a person retrieval problem aims to enable retrieval of the person of interest using two different domain modalities, i.e., text and image. It takes one type of data as an input query (i.e., text). It retrieves the relevant data of another type (i.e., person image) by mapping textual and visual features (Sec. 2.3). Such kind of retrieval is also known as cross-modal retrieval [139,140,141,142].

Among all person retrieval methodologies, image gallery-based techniques are not preferable for end-to-end system-level evaluations. They do not consider challenges like occlusion, pose, and illumination in person detection from the full surveillance frame. Natural language description-based person retrieval systems are more suited for real-time person retrieval from surveillance videos. A person retrieved using a textual query is an input query to image-based person ReID systems for re-identification of the same target in the camera network. This paper discusses soft biometric attribute-based person retrieval methodologies in a further section.","[[], [], [], [], [], [], [], ['b82', 'b137', 'b136', 'b138', 'b139'], []]","[[], [], [], [], [], [], [], ['b82', 'b137', 'b136', 'b138', 'b139'], []]",5,"sent1: Person retrieval is a subject undergoing intense study for the past two decades due to its prime application to public safety.
sent2: A large-scale intelligent surveillance system requires person retrieval using visual attributes or available person image(s).
sent3: Vision community researchers are proposing a plethora of methodologies for robust person retrieval.
sent4: They have two major categories based on the type of input query: -Image-based retrieval.
sent5: -Soft biometric attribute-based retrieval.
sent6: Classification of person retrieval methodologies based on the type of input query is shown in Fig. 14.
sent7: A person is retrievable from the cropped person image gallery or full frames of surveillance video.
sent8: The latter is more suitable for real-time scenarios.
sent9: The person retrieval example of each category is also shown in Fig. 14.Image-based person retrieval methodologies aim to retrieve the target from the large-scale image gallery.
sent10: This large-scale image gallery or dataset is from different non-overlapping cameras.
sent11: Such person retrieval techniques are known as person re-identification (Re-ID).
sent12: These techniques require at least one image of the target person for querying the system to perform re-identification of the same identity from different cameras.
sent13: A person ReID technique assumes that the image gallery contains at least one image of the target person captured from a camera other than the query image.
sent14: Such methodologies fail when a probe image is not available.
sent15: In contrast, soft biometric attribute-based person retrieval methodologies do not require a probe image.
sent16: It uses a semantic person description generated from soft biometric attributes.
sent17: Such a description (textual query) is input to the person retrieval system.
sent18: Soft-biometric attribute-based retrieval is further divisible into two categories based on the type of textual query ( Fig. 12 and  14):1.
sent19: Discrete attribute-based retrieval.
sent20: 2. Natural language description based retrieval.
sent21: A discrete attribute-based person retrieval system searches for the person using soft biometrics attributes, e.g., clothing colour, clothing type, and gender.
sent22: While a natural language description-based system accepts a natural description, e.g., A woman is with average height wearing a white midi with no sleeve.
sent23: She has black open hair. Applications like obvious question answering, image captioning, and person retrieval are gaining much attention due to the association of two widely researched domains, i.e., computer vision and natural language processing.
sent24: Such applications expect the learning of discriminative feature representations from both images and text.
sent25: The person retrieval algorithm ranks gallery images or person(s) in the surveillance frame according to their relevance to the description.
sent26: The best matching images from the gallery or surveillance frame return the person of interest.
sent27: This class of surveillance problems is known as textbased person search or person retrieval using natural language descriptions [85,139].
sent28: Such a person retrieval problem aims to enable retrieval of the person of interest using two different domain modalities, i.e., text and image.
sent29: It takes one type of data as an input query (i.e., text).
sent30: It retrieves the relevant data of another type (i.e., person image) by mapping textual and visual features (Sec. 2.3).
sent31: Such kind of retrieval is also known as cross-modal retrieval [139,140,141,142].
sent32: Among all person retrieval methodologies, image gallery-based techniques are not preferable for end-to-end system-level evaluations.
sent33: They do not consider challenges like occlusion, pose, and illumination in person detection from the full surveillance frame.
sent34: Natural language description-based person retrieval systems are more suited for real-time person retrieval from surveillance videos.
sent35: A person retrieved using a textual query is an input query to image-based person ReID systems for re-identification of the same target in the camera network.
sent36: This paper discusses soft biometric attribute-based person retrieval methodologies in a further section."
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,Computer Science,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,s2,Characteristics of soft biometrics,"Research interest is now more inclined towards applications where person retrieval at a distance is necessary, based on ancillary information like soft biometrics [60,61,62]. Some early research [6,63,64,65,66,67] shows the use of soft biometrics to improve the performance of the primary biometric system by narrowing down the search space in the initial stages. Jain et al. [6] mentioned that soft biometrics are inexpensive to compute, with no person cooperation requirement, and derived at a distance. Similarly, various pros and cons of soft biometrics are in further discussion.

Registration free / enrollment free: Traditional biometric systems require prior registration of the biometric signature for identification in the future. However, prior registration of soft biometric attributes of a specific person is not required. The training is offline, and the model is useful for retrieval of the individual.

Compatible with human language: Traditional biometrics features like fingerprints are discriminative. However, they cannot be describable by linguistic labels. It creates a semantic gap between human beings and the person retrieval system. Soft biometrics is a form of biometrics that uses labels people generally use to describe or identify each other like tall, female, young and blond hair. They help to bridge the semantic gap by generating descriptions understandable by humans.

Cost and computationally effective: Face, fingerprint, palm print, and iris-like biometrics require dedicated sensors for acquisition and a constrained environment. Soft biometric attributes like height, gender, and clothing colours are extractable from single low-quality surveillance videos where the traditional biometric acquisition fails. It reduces costs on the overall system development.

Recognizable from a distance: Soft biometrics attributes are identifiable from a distance, e.g., clothing colour, clothing type, gender.

No cooperation from the subject: Soft biometrics attributes (e.g., clothing colour, gender) acquisition does not require any cooperation from the subject. Unconstrained and non-intrusive acquisition is also non-invasive, which makes it suitable for surveillance applications.","[['b4', 'b64', 'b59', 'b57', 'b60', 'b62', 'b63', 'b61', 'b58'], [], [], [], [], []]","[['b4', 'b64', 'b59', 'b57', 'b60', 'b62', 'b63', 'b61', 'b58'], [], [], [], [], []]",9,"sent1: Research interest is now more inclined towards applications where person retrieval at a distance is necessary, based on ancillary information like soft biometrics [60,61,62].
sent2: Some early research [6,63,64,65,66,67] shows the use of soft biometrics to improve the performance of the primary biometric system by narrowing down the search space in the initial stages.
sent3: Jain et al. [6] mentioned that soft biometrics are inexpensive to compute, with no person cooperation requirement, and derived at a distance.
sent4: Similarly, various pros and cons of soft biometrics are in further discussion.
sent5: Registration free / enrollment free: Traditional biometric systems require prior registration of the biometric signature for identification in the future.
sent6: However, prior registration of soft biometric attributes of a specific person is not required.
sent7: The training is offline, and the model is useful for retrieval of the individual.
sent8: Compatible with human language: Traditional biometrics features like fingerprints are discriminative.
sent9: However, they cannot be describable by linguistic labels.
sent10: It creates a semantic gap between human beings and the person retrieval system.
sent11: Soft biometrics is a form of biometrics that uses labels people generally use to describe or identify each other like tall, female, young and blond hair.
sent12: They help to bridge the semantic gap by generating descriptions understandable by humans.
sent13: Cost and computationally effective: Face, fingerprint, palm print, and iris-like biometrics require dedicated sensors for acquisition and a constrained environment.
sent14: Soft biometric attributes like height, gender, and clothing colours are extractable from single low-quality surveillance videos where the traditional biometric acquisition fails.
sent15: It reduces costs on the overall system development.
sent16: Recognizable from a distance: Soft biometrics attributes are identifiable from a distance, e.g., clothing colour, clothing type, gender.
sent17: No cooperation from the subject: Soft biometrics attributes (e.g., clothing colour, gender) acquisition does not require any cooperation from the subject.
sent18: Unconstrained and non-intrusive acquisition is also non-invasive, which makes it suitable for surveillance applications."
233864858,Person Retrieval in Surveillance Using Textual Query: A Review,Computer Science,https://www.semanticscholar.org/paper/b900a89058ff4051b9b78ff725b206fb573005e5,s1,History and background,"An early person identification system developed by Alphonse Bertillon, a police officer and biometric researcher from Paris, France, identified criminals in the 19th century. His approach is also known as Bertillonage [1], which was widely accepted in France and other countries. Criminal identification was made only by employing photographs or names before Bertillonage. He was the first to use anthropometry for law enforcement using various physical measurements. He also defined the process of physical measurements and photograph acquisition to maintain uniformity in the records [1,2] (see Fig. 1). Bertillon's system measures physical features like standing height, length and breadth of head, length of individual fingers, dimensions of the foot, dimensions of the nose, and ear. It also has descriptions of eye and hair colour, any scars, marks, and tattoos on the body [1].

Two persons may likely have the same height, but the chances of other measurements being similar is highly unlikely. Sizes are not unique to individuals and hence may fail to identify a person if used without other attributes. e.g., twins may have similar biological features. Hence, it has often been superseded by fingerprint-based identification. Biometrics is a reliable solution for person identification [4] due to properties like uniqueness, stability, universality, and accessibility. The answer is dependent on ""what you are""which uses face, fingerprint, palm print, hand geometry. It is independent of ""what you possess""like identity card, or ""what you know""e.g., password or the personal identification number [15,16]. Such traditional biometrics-based systems are successful in many applications like forensics, employee attendance, and mobile or laptop security. Biometrics-based retrieval systems have limited usage in surveillance applications due to the following:

-Biometric samples are difficult to capture from video footage of unconstrained environments. Fig. 2 Video surveillance frame samples [5].

-It is challenging to capture physiological samples (e.g., fingerprint) from an uncooperative person. -Biometric attributes (e.g., face) are challenging to acquire due to camera distance and position. -Captured biometrics have poor quality due to low-resolution cameras.

Traditional biometric-based systems fail to retrieve a person from surveillance videos due to the above reasons. Fig. 2 shows sample video surveillance frames from the AVSS 2018 challenge II database [5]. A person of interest is within a green bounding box. It shows various scenarios where traditional biometrics-based systems fail to retrieve the person. Fig. 2(a) shows the environment with poor illumination and a low-resolution frame. Fig. 2(b) shows a scenario with a crowd and a large distance between the camera and the person. Face recognition-based systems fail to retrieve the person in such scenarios. However, attributes like colour and type of clothing, height and gender can help in-person retrieval under such challenging conditions. For example, green (in Fig. 2(a)) and purple (in Fig. 2(b)) coloured clothes are identifiable. Jain et al. [6] introduce such personal attributes as soft biometrics, and many other research articles elaborate on soft biometrics [6,7,8,9,10,11,12,13]. Boston Marathon bombing [14] is a real-world example where police investigation took three days to search out the suspect from hundreds of hours of surveillance videos. Thus, automation of the person retrieval process saves time during a critical investigation. With the ever-increasing demand for security through surveillance, researchers are infusing more interest in developing soft biometric-based person retrieval from low-quality surveillance videos where traditional biometrics fail.

Biometrics are broadly classified as hard and soft biometrics, as shown in Fig. 3. Hard biometrics have physiological and behavioural attributes of a person. Physiologically based methods establish a person's identity using physiological characteristics like face, iris, fingerprint, palm print, hand geometry and DNA. Behavioural techniques perform authentication by recognizing patterns such as voice, gait, and signature. Although biometrics like face, fingerprint, and palm print are unique to an individual, a single biometric cannot meet the requirements of all applications [15,16]. For example, camera-based security systems cannot use fingerprints as a biometric for user authentication. Thus, biometric attribute selection depends on the application. One such application is person retrieval from a distance through a surveillance frame. Here traditional biometrics fail, but soft biometrics play a critical role in retrieval. Soft biometrics are the ancillary information deduced from the human body. Dantcheva et al. [12] categorized soft biometrics into demographic, anthropometric, medical, material, and behavioural attributes.

The term demographic is related to the accumulation and statistical analysis of broad characteristics about groups of people and populations. Age, ethnicity, gender, and race are such characteristics that are used as soft biometrics and also termed as global traits [17]. Gender, ethnicity and race usually do not show changes over the person's lifespan and hence are useful for search space reduction from surveillance. Gender and age estimation are the most researched demographic attributes. Researchers estimate gender from face [18,19,20,21,22,23,24,25,26], fingerprint [27,28,29,30,31,32], iris [33,34], body [35,36,37,38,39,40], hand [41,42,43,44,45,46] and speech [47,48,49,50,51]. Gender from face covers a major research segment and achieves 99% [22] classification accuracy. However, performance decreases dramatically for a realistic and unconstrained environment. It imposes illumination changes, occlusion, different views, and poses. Surveillance videos cover such unconstrained scenarios and it is also very difficult to acquire face, fingerprint, iris, hand, and speech biometrics from surveillance. Gender classification from the full human body is a more suitable way for surveillance applications.

Anthropometric measurements are a traditional way of body measurements. Since the era of Bertillonage [1,2] they have been in security practices and are currently known as anthropometric and geometric soft biometrics.

Facial landmarks related to chin, mouth, eyes, and nose are anthropometric measurements related to facial geometry [52,53,54,55]. Body height [56,57,58,59] is the most researched anthropometric attribute. Similarly, other body measurements, like torso length, leg length, and step length [58], are also useful. Medical characteristics help monitor a person [12] with the help of a computer vision-based technique. Bodyweight, body mass index (BMI), and skin quality are soft biometrics used for an application like early detection and prevention of diseases like skin cancer. Another class of soft biometrics is material and behavioural. It includes various accessories worn by a person like a hat, scarf, eyeglasses; different clothes and clothing colours; scars, marks, and tattoos on the body. Such soft biometrics also play a crucial role in retrieval. For example, a description, a male with a green t-shirt and blue jeans wearing a black cap and carrying a backpack, contains clothing colours, clothing types, accessories (i.e., cap and backpack) as soft biometrics. Material and behavioural attributes are highly time-inconsistent. E.g., a person wearing a white shirt and black pants in the morning may have different clothes (blue t-shirt and white shorts) in the afternoon of the same day. Thus, descriptions for morning and afternoon are different for person retrieval. Time consistency discussion is in Sec. 2.1.","[['b1', 'b0'], ['b2', 'b12', 'b13'], ['b3'], [], ['b4', 'b3', 'b8', 'b10', 'b5', 'b7', 'b9', 'b11', 'b6'], ['b10', 'b13', 'b12'], ['b41', 'b20', 'b35', 'b48', 'b14', 'b22', 'b45', 'b38', 'b18', 'b30', 'b32', 'b19', 'b29', 'b43', 'b16', 'b28', 'b25', 'b37', 'b21', 'b46', 'b42', 'b39', 'b15', 'b33', 'b27', 'b17', 'b47', 'b26', 'b23', 'b31', 'b34', 'b36', 'b44', 'b40', 'b24'], ['b1', 'b0'], ['b52', 'b10', 'b50', 'b49', 'b55', 'b56', 'b51', 'b54', 'b53']]","[['b1', 'b0'], ['b2', 'b12', 'b13'], ['b3'], [], ['b4', 'b3', 'b8', 'b10', 'b5', 'b7', 'b9', 'b11', 'b6'], ['b10', 'b13', 'b12'], ['b41', 'b20', 'b35', 'b48', 'b14', 'b22', 'b45', 'b38', 'b18', 'b30', 'b32', 'b19', 'b29', 'b43', 'b16', 'b28', 'b25', 'b37', 'b21', 'b46', 'b42', 'b39', 'b15', 'b33', 'b27', 'b17', 'b47', 'b26', 'b23', 'b31', 'b34', 'b36', 'b44', 'b40', 'b24'], ['b1', 'b0'], ['b52', 'b10', 'b50', 'b49', 'b55', 'b56', 'b51', 'b54', 'b53']]",64,"sent1: An early person identification system developed by Alphonse Bertillon, a police officer and biometric researcher from Paris, France, identified criminals in the 19th century.
sent2: His approach is also known as Bertillonage [1], which was widely accepted in France and other countries.
sent3: Criminal identification was made only by employing photographs or names before Bertillonage.
sent4: He was the first to use anthropometry for law enforcement using various physical measurements.
sent5: He also defined the process of physical measurements and photograph acquisition to maintain uniformity in the records [1,2] (see Fig. 1).
sent6: Bertillon's system measures physical features like standing height, length and breadth of head, length of individual fingers, dimensions of the foot, dimensions of the nose, and ear.
sent7: It also has descriptions of eye and hair colour, any scars, marks, and tattoos on the body [1].
sent8: Two persons may likely have the same height, but the chances of other measurements being similar is highly unlikely.
sent9: Sizes are not unique to individuals and hence may fail to identify a person if used without other attributes.
sent10: e.g., twins may have similar biological features.
sent11: Hence, it has often been superseded by fingerprint-based identification.
sent12: Biometrics is a reliable solution for person identification [4] due to properties like uniqueness, stability, universality, and accessibility.
sent13: The answer is dependent on ""what you are""which uses face, fingerprint, palm print, hand geometry.
sent14: It is independent of ""what you possess""like identity card, or ""what you know""e.g., password or the personal identification number [15,16].
sent15: Such traditional biometrics-based systems are successful in many applications like forensics, employee attendance, and mobile or laptop security.
sent16: Biometrics-based retrieval systems have limited usage in surveillance applications due to the following:-Biometric samples are difficult to capture from video footage of unconstrained environments.
sent17: Fig. 2 Video surveillance frame samples [5].
sent18: -It is challenging to capture physiological samples (e.g., fingerprint) from an uncooperative person.
sent19: -Biometric attributes (e.g., face) are challenging to acquire due to camera distance and position.
sent20: -Captured biometrics have poor quality due to low-resolution cameras.
sent21: Traditional biometric-based systems fail to retrieve a person from surveillance videos due to the above reasons.
sent22: Fig. 2 shows sample video surveillance frames from the AVSS 2018 challenge II database [5].
sent23: A person of interest is within a green bounding box.
sent24: It shows various scenarios where traditional biometrics-based systems fail to retrieve the person.
sent25: Fig. 2(a) shows the environment with poor illumination and a low-resolution frame.
sent26: Fig. 2(b) shows a scenario with a crowd and a large distance between the camera and the person.
sent27: Face recognition-based systems fail to retrieve the person in such scenarios.
sent28: However, attributes like colour and type of clothing, height and gender can help in-person retrieval under such challenging conditions.
sent29: For example, green (in Fig. 2(a)) and purple (in Fig. 2(b)) coloured clothes are identifiable.
sent30: Jain et al. [6] introduce such personal attributes as soft biometrics, and many other research articles elaborate on soft biometrics [6,7,8,9,10,11,12,13].
sent31: Boston Marathon bombing [14] is a real-world example where police investigation took three days to search out the suspect from hundreds of hours of surveillance videos.
sent32: Thus, automation of the person retrieval process saves time during a critical investigation.
sent33: With the ever-increasing demand for security through surveillance, researchers are infusing more interest in developing soft biometric-based person retrieval from low-quality surveillance videos where traditional biometrics fail.
sent34: Biometrics are broadly classified as hard and soft biometrics, as shown in Fig. 3.
sent35: Hard biometrics have physiological and behavioural attributes of a person.
sent36: Physiologically based methods establish a person's identity using physiological characteristics like face, iris, fingerprint, palm print, hand geometry and DNA.
sent37: Behavioural techniques perform authentication by recognizing patterns such as voice, gait, and signature.
sent38: Although biometrics like face, fingerprint, and palm print are unique to an individual, a single biometric cannot meet the requirements of all applications [15,16].
sent39: For example, camera-based security systems cannot use fingerprints as a biometric for user authentication.
sent40: Thus, biometric attribute selection depends on the application.
sent41: One such application is person retrieval from a distance through a surveillance frame.
sent42: Here traditional biometrics fail, but soft biometrics play a critical role in retrieval.
sent43: Soft biometrics are the ancillary information deduced from the human body.
sent44: Dantcheva et al. [12] categorized soft biometrics into demographic, anthropometric, medical, material, and behavioural attributes.
sent45: The term demographic is related to the accumulation and statistical analysis of broad characteristics about groups of people and populations.
sent46: Age, ethnicity, gender, and race are such characteristics that are used as soft biometrics and also termed as global traits [17].
sent47: Gender, ethnicity and race usually do not show changes over the person's lifespan and hence are useful for search space reduction from surveillance.
sent48: Gender and age estimation are the most researched demographic attributes.
sent49: Researchers estimate gender from face [18,19,20,21,22,23,24,25,26], fingerprint [27,28,29,30,31,32], iris [33,34], body [35,36,37,38,39,40], hand [41,42,43,44,45,46] and speech [47,48,49,50,51].
sent50: Gender from face covers a major research segment and achieves 99% [22] classification accuracy.
sent51: However, performance decreases dramatically for a realistic and unconstrained environment.
sent52: It imposes illumination changes, occlusion, different views, and poses.
sent53: Surveillance videos cover such unconstrained scenarios and it is also very difficult to acquire face, fingerprint, iris, hand, and speech biometrics from surveillance.
sent54: Gender classification from the full human body is a more suitable way for surveillance applications.
sent55: Anthropometric measurements are a traditional way of body measurements.
sent56: Since the era of Bertillonage [1,2] they have been in security practices and are currently known as anthropometric and geometric soft biometrics.
sent57: Facial landmarks related to chin, mouth, eyes, and nose are anthropometric measurements related to facial geometry [52,53,54,55].
sent58: Body height [56,57,58,59] is the most researched anthropometric attribute.
sent59: Similarly, other body measurements, like torso length, leg length, and step length [58], are also useful.
sent60: Medical characteristics help monitor a person [12] with the help of a computer vision-based technique.
sent61: Bodyweight, body mass index (BMI), and skin quality are soft biometrics used for an application like early detection and prevention of diseases like skin cancer.
sent62: Another class of soft biometrics is material and behavioural.
sent63: It includes various accessories worn by a person like a hat, scarf, eyeglasses; different clothes and clothing colours; scars, marks, and tattoos on the body.
sent64: Such soft biometrics also play a crucial role in retrieval.
sent65: For example, a description, a male with a green t-shirt and blue jeans wearing a black cap and carrying a backpack, contains clothing colours, clothing types, accessories (i.e., cap and backpack) as soft biometrics.
sent66: Material and behavioural attributes are highly time-inconsistent.
sent67: E.g., a person wearing a white shirt and black pants in the morning may have different clothes (blue t-shirt and white shorts) in the afternoon of the same day.
sent68: Thus, descriptions for morning and afternoon are different for person retrieval.
sent69: Time consistency discussion is in Sec. 2.1."
237532483,A Survey on Temporal Sentence Grounding in Videos,Computer Science,https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453,s16,4.2.1,"Large-scale video corpus moment retrieval. Large-scale video corpus moment retrieval (VCMR) is a research direction extended from TSGV that has been explored over the past few years [15,32,77,79]. It has more application value since it can retrieve the target segment semantically corresponding to a given text query from a large-scale video corpus (i.e., a collection of untrimmed and unsegmented videos) rather than from a single video. As compared with TSGV, VCMR has higher efficiency requirements since it not only needs to retrieve a specific segment from one single video but also locates the target video from a video corpus. Escorcia et al. [15] first extend TSGV to VCMR, introducing a model named Clip Alignment with Language (CAL) to align the query feature with a sequence of uniformly partitioned clips for moment composing. Lei et al. [32] introduce a new dataset for VCMR called TVR, which is comprised of videos and their associated subtitle texts. A Cross-modal Moment Localization (XML) network with a novel convolutional start-end detector module is also proposed to produce moment predictions in a late fusion manner. Zhang et al. [77] present a hierarchical multi-modal encoder (HAMMER) to capture both coarse-and fine-grained semantic information from the videos and train the model with three sub-tasks (i.e., video retrieval, segment temporal localization, and masked language modeling). Zhang et al. [79] introduce contrastive learning for VCMR, designing a retrieval and localization network with contrastive learning (ReLoCLNet).","[['b76', 'b14', 'b31', 'b78']]","[['b76', 'b14', 'b31', 'b78']]",4,"sent1: Large-scale video corpus moment retrieval.
sent2: Large-scale video corpus moment retrieval (VCMR) is a research direction extended from TSGV that has been explored over the past few years [15,32,77,79].
sent3: It has more application value since it can retrieve the target segment semantically corresponding to a given text query from a large-scale video corpus (i.e., a collection of untrimmed and unsegmented videos) rather than from a single video.
sent4: As compared with TSGV, VCMR has higher efficiency requirements since it not only needs to retrieve a specific segment from one single video but also locates the target video from a video corpus.
sent5: Escorcia et al. [15] first extend TSGV to VCMR, introducing a model named Clip Alignment with Language (CAL) to align the query feature with a sequence of uniformly partitioned clips for moment composing.
sent6: Lei et al. [32] introduce a new dataset for VCMR called TVR, which is comprised of videos and their associated subtitle texts.
sent7: A Cross-modal Moment Localization (XML) network with a novel convolutional start-end detector module is also proposed to produce moment predictions in a late fusion manner.
sent8: Zhang et al. [77] present a hierarchical multi-modal encoder (HAMMER) to capture both coarse-and fine-grained semantic information from the videos and train the model with three sub-tasks (i.e., video retrieval, segment temporal localization, and masked language modeling).
sent9: Zhang et al. [79] introduce contrastive learning for VCMR, designing a retrieval and localization network with contrastive learning (ReLoCLNet)."
237532483,A Survey on Temporal Sentence Grounding in Videos,Computer Science,https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453,s2,Two-stage method,"For a two-stage method, the pre-segmenting of proposal candidates is conducted separately with the model computation. It takes the pre-segmented candidates and the sentence query as inputs of a cross-modal matching module for target segment localization. The two-stage methods can be grouped into two categories based on different ways to generate proposals.

2.1.1 sliding window-based. Early methods including MCN [23], CTRL [16], ROLE [38], MCF [63], ACRN [37], SLTA [27] and ACL-K [18], adopt multi-scale sliding window sampling strategy for the generation of candidate proposals. There are two pioneering works MCN [23] and CTRL [16] to define TSGV task and construct benchmark datasets. Firstly, Hendricks et al. [23] propose MCN, which samples all the candidate moments (i.e. segments) via sliding window mechanism, and then projects the video moment representation and query representation into a common embedding space. The ℓ 2 distance between the sentence query and the corresponding target video moment in this space is minimized to supervise the model training (c.f ., Fig. 3b). Specifically, MCN encourages the sentence query to be closer to the target moment than negative moments in a shared embedding space. Since the negative moments either come from other segments within the same video (intra-video) or from different videos (inter-video), MCN devises two similar but different ranking loss functions:

where L ( , ) = max(0, − + ), is a margin. As for training sample , the intra-video ranking loss encourages sentence to be closer to the target moment at the location than the negative moments from other possible locations within the same video, while the inter-video ranking loss encourages sentence to be closer to the target one at location than the negative ones from other videos of the same location . The intra-video ranking loss is able to differentiate between subtle difference within a video while the inter-video ranking loss can differentiate between broad semantic concepts. At the same time, Gao et al. [16] propose CTRL, which is the first one to adapt R-CNN [20] methodology from object detection to the TSGV domain. Particularly, CTRL also leverages sliding window to obtain candidate segments of various lengths, and as shown in Fig. 3a, it exploits a multi-modal processing module to fuse the candidate segment representation with the sentence representation by three operators (i.e., add, multiply, and full-connected layer). Then, CTRL feeds the fused representation into another fully-connected layer to predict the alignment score and location offsets between the candidate segment and the target segment. CTRL designs a multi-task loss function to train the model, including visual-semantic alignment loss and location regression loss:

where is the visual-semantic alignment loss considering both aligned (video segment, query) pairs and misaligned pairs. , measures the alignment score between video segment and sentence . The location regression loss is only accounted for aligned pairs to predict the correct coordinates. is a smooth-L1 function.

Compared to above CTRL that treats the query as a whole, Liu et al. [38] further make some improvements by decomposing the query and adaptively get the important textual components according to the temporal video context.

Since CTRL overlooks the spatial-temporal information inside the moment and the query, Liu et al. [37] further propose an attentive cross-modal retrieval network (ACRN). With a memory attention network guided by the sentence query, ACRN adaptively assigns weights to the contextual moment representations for memorization to augment the moment representation. SLTA [27] also devises a spatial and language-temporal attention model to adaptively identify the relevant objects and interactions based on the query information.

Wu and Han [63] propose a multi-modal circulant fusion (MCF) in contrast to the simple fusion ways employed in CTRL including element-wise product, element-wise sum, or concatenation. MCF extends the visual/textual vector to the circulant matrix, which can fully exploit the interactions of the visual and textual representations. By plugging MCF into CTRL, the grounding accuracy is further improved. Previous works like CTRL, ACRN and MCF directly calculate the visual-semantic correlation without explicitly modelling the activity information within two modalities, and the candidate segments fairly sampled by sliding window may contain various meaningless noisy contents which do not contain any activity. Hence, Ge et al. [18] explicitly mine activity concepts from both visual and textual parts as prior knowledge to provide an actionness score for each candidate segment, reflecting how confident it contains activities, which enhances the localization accuracy.

Despite the simplicity and effectiveness of such two-stage sliding window-based methods, they suffer from inefficient computation since there are too many overlapped areas re-computed due to the densely sampling process with predefined multi-scale sliding windows.","[[], ['b62', 'b17', 'b37', 'b22', 'b26', 'b36', 'b15'], ['b15', 'b19'], [], ['b37'], ['b36', 'b26'], ['b17', 'b62'], []]","[[], ['b62', 'b17', 'b37', 'b22', 'b26', 'b36', 'b15'], ['b15', 'b19'], [], ['b37'], ['b36', 'b26'], ['b17', 'b62'], []]",14,"sent1: For a two-stage method, the pre-segmenting of proposal candidates is conducted separately with the model computation.
sent2: It takes the pre-segmented candidates and the sentence query as inputs of a cross-modal matching module for target segment localization.
sent3: The two-stage methods can be grouped into two categories based on different ways to generate proposals.
sent4: 2.1.1 sliding window-based. Early methods including MCN [23], CTRL [16], ROLE [38], MCF [63], ACRN [37], SLTA [27] and ACL-K [18], adopt multi-scale sliding window sampling strategy for the generation of candidate proposals.
sent5: There are two pioneering works MCN [23] and CTRL [16] to define TSGV task and construct benchmark datasets.
sent6: Firstly, Hendricks et al. [23] propose MCN, which samples all the candidate moments (i.e. segments) via sliding window mechanism, and then projects the video moment representation and query representation into a common embedding space.
sent7: The ℓ 2 distance between the sentence query and the corresponding target video moment in this space is minimized to supervise the model training (c.f ., Fig. 3b).
sent8: Specifically, MCN encourages the sentence query to be closer to the target moment than negative moments in a shared embedding space.
sent9: Since the negative moments either come from other segments within the same video (intra-video) or from different videos (inter-video), MCN devises two similar but different ranking loss functions:where L ( , ) = max(0, − + ), is a margin.
sent10: As for training sample , the intra-video ranking loss encourages sentence to be closer to the target moment at the location than the negative moments from other possible locations within the same video, while the inter-video ranking loss encourages sentence to be closer to the target one at location than the negative ones from other videos of the same location .
sent11: The intra-video ranking loss is able to differentiate between subtle difference within a video while the inter-video ranking loss can differentiate between broad semantic concepts.
sent12: At the same time, Gao et al. [16] propose CTRL, which is the first one to adapt R-CNN [20] methodology from object detection to the TSGV domain.
sent13: Particularly, CTRL also leverages sliding window to obtain candidate segments of various lengths, and as shown in Fig. 3a, it exploits a multi-modal processing module to fuse the candidate segment representation with the sentence representation by three operators (i.e., add, multiply, and full-connected layer).
sent14: Then, CTRL feeds the fused representation into another fully-connected layer to predict the alignment score and location offsets between the candidate segment and the target segment.
sent15: CTRL designs a multi-task loss function to train the model, including visual-semantic alignment loss and location regression loss:where is the visual-semantic alignment loss considering both aligned (video segment, query) pairs and misaligned pairs.
sent16: , measures the alignment score between video segment and sentence .
sent17: The location regression loss is only accounted for aligned pairs to predict the correct coordinates.
sent18: is a smooth-L1 function. Compared to above CTRL that treats the query as a whole, Liu et al. [38] further make some improvements by decomposing the query and adaptively get the important textual components according to the temporal video context.
sent19: Since CTRL overlooks the spatial-temporal information inside the moment and the query, Liu et al. [37] further propose an attentive cross-modal retrieval network (ACRN).
sent20: With a memory attention network guided by the sentence query, ACRN adaptively assigns weights to the contextual moment representations for memorization to augment the moment representation.
sent21: SLTA [27] also devises a spatial and language-temporal attention model to adaptively identify the relevant objects and interactions based on the query information.
sent22: Wu and Han [63] propose a multi-modal circulant fusion (MCF) in contrast to the simple fusion ways employed in CTRL including element-wise product, element-wise sum, or concatenation.
sent23: MCF extends the visual/textual vector to the circulant matrix, which can fully exploit the interactions of the visual and textual representations.
sent24: By plugging MCF into CTRL, the grounding accuracy is further improved.
sent25: Previous works like CTRL, ACRN and MCF directly calculate the visual-semantic correlation without explicitly modelling the activity information within two modalities, and the candidate segments fairly sampled by sliding window may contain various meaningless noisy contents which do not contain any activity.
sent26: Hence, Ge et al. [18] explicitly mine activity concepts from both visual and textual parts as prior knowledge to provide an actionness score for each candidate segment, reflecting how confident it contains activities, which enhances the localization accuracy.
sent27: Despite the simplicity and effectiveness of such two-stage sliding window-based methods, they suffer from inefficient computation since there are too many overlapped areas re-computed due to the densely sampling process with predefined multi-scale sliding windows."
237532483,A Survey on Temporal Sentence Grounding in Videos,Computer Science,https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453,s4,End-to-end method,"The end-to-end model follows one single-pass pattern. We divide it into two types, i.e., anchor-based and anchor-free, based on whether the method uses anchors (i.e., proposals) to make predictions.

2.2.1 anchor-based. The representative anchor-based works include TGN [5], CMIN [27], SCDM [73], MAN [78], CBP [60], CSMGAN [36], 2D-TAN [83], FIAN [46], SMIN [59] and Zhang et al. [82].

TGN [5] is one typical end-to-end deep architecture, which can localize the target moment in one single pass without handling heavily overlapped pre-segmented candidate moments. As shown in Fig. 5, TGN dynamically matches the sentence and video units via a sequential LSTM grounder with fine-grained frame-by-word interaction, and at each time step, the grounder would simultaneously score a group of candidate segments with different temporal scales ending at this time step.

CMIN [84] sequentially scores a set of candidate moments of multi-scale anchors like TGN but with a sequential BiGRU network, and refines the candidate moments with boundary regression. To further enhance the cross-modal matching, it devises a novel cross-modal interaction network (CMIN), which first leverages a syntactic GCN to model the syntactic structure of queries, and captures long-range temporal dependencies of video context with a multi-head self-attention, then employs the fine-grained cross-modal multi-stage interaction module to produce the cross-modal features for following sequentially scoring.

Similarly, CBP [60] builds a single-stream model with sequential LSTM, which jointly predicts temporal anchors and boundaries at each time step for yield precise localization. Furthermore, to better detect semantic boundaries, CBP devises a self attention based module to collect contextual clues instead of simply concatenating the contextual features like [16,18,23]. Based on interaction output of both language and video, it explicitly measures the contributions from different contextual elements.

CSMGAN [36] also adopts such a single-pass scheme. It builds a joint graph for modelling the cross-/self-modal relations via iterative message passing, to capture the high-order interactions between two modalities effectively. Each node of the graph aggregates the messages from its neighbor nodes in an edge-weighted manner and updates its state with both aggregated message and current state through ConvGRU.

Qu et al. [46] present a fine-grained iterative attention network (FIAN), which devises a contentoriented strategy to generate candidate moments differing from the anchor-based methods with sequential RNNs mentioned above. FIAN employs a refined cross-modal guided attention (CGA) block to capture the detailed cross-modal interactions, and further adopts a symmetrical iterative attention to generate both sentence-aware video and video-aware sentence representations, where the latter are explicitly facilitated to enhance the former and finally both parts contribute to a robust cross-modal feature.

TGN establishes the temporal grounding architecture through a sequential LSTM network, while Yuan et al. [73] propose SCDM, which exploits a hierarchical temporal convolutional network to conduct target segment localization, and couples it with a semantics-conditioned dynamic modulation to fully leverage sentence semantics to compose the sentence-related video contents over time. As shown in Fig. 6, the multimodal fusion module fuses the entire sentence and each video clip in a fine-grained manner. The fused representation is formulated as:

With such fused representations as inputs, the semantic modulated temporal convolution module further correlates sentence-related video contents in a temporal convolution procedure, dynamically modulating the temporal feature maps concerning the sentence. Specifically, for each temporal convolutional layer, the feature map is denoted as A = {a }. The feature unit a will be modulated based on the modulation vectors and :

where the modulation vectors are computed based on the sentence representation S = {s } =1 :

Finally, the position prediction module outputs the location offsets and overlap scores of candidate video segments based on the modulated features. MAN [78] also leverages temporal convolutional network to address the TSGV task, where the sentence query is integrated as dynamic filters into the convolutional process. Specifically, as shown in Fig. 7, MAN encodes the entire video stream using a hierarchical convolutional network to produce multi-scale candidate moment representations. The textual features are encoded as dynamic filters and convolved with such visual representations. Additionally, MAN exploits the graph-structured moment relation modelling adapted from Graph Convolution Network (GCN) [30] for temporal reasoning to further improve the moment representations.

Both SCDM and MAN only consider 1D temporal feature maps, while the 2D-TAN [83] network models the temporal relations of video segments via a two-dimensional map. As shown in Fig. 8, it firstly divides the video into evenly spaced video clips with duration . The ( , )-th location on the 2D temporal map represents a candidate moment (or anchor) from the time to ( + 1) . This kind of 2D temporal map covers diverse video moments with different lengths, while representing their adjacent relations. The proposed temporal adjacent network fuses the sentence representation with each of the candidate moment feature and then leverages convolutional neural network to embed the video context information, and finally predicts the confidence score of each candidate to be the final target segment. 2D-TAN adopts a binary cross entropy loss with a the scaled IoU as the supervision signal. The scaled IoU is controlled by two thresholds and as:

where is the temporal IoU between one candidate moment and the groundtruth moment. Thus, the loss function can be expressed as:

where is the predicted confidence score of a moment. Wang et al. [59] propose a structured multi-level interaction network (SMIN), which makes further modifications on the 2D temporal feature map as its proposal generation module. SMIN explores the inherent structure of moment, which can be disentangled into visual content and positional boundary parts for fine-grained cross-modal and intra-moment interaction. Zhang et al. [82] also adopts the same proposal generation approach as that of 2D-TAN, designing a visuallanguage transformer backbone followed by a multi-stage aggregation module to get discriminative moment representations for more accurate moment localization.

Despite the superior performance anchor-based methods have achieved, the performance is sensitive with the heuristic rules manually designed (i.e., the number and scales of anchors). As a result, such anchor-based methods can not adapt to the situation with variable video length. Meanwhile, although the pre-segmentation like two-stage methods is not required, it still essentially depends on the ranking of proposal candidates, which will also influence its efficiency.","[[], ['b4', 'b59', 'b82', 'b45', 'b77', 'b81', 'b35', 'b26', 'b58', 'b72'], ['b4'], ['b83'], ['b15', 'b17', 'b59', 'b22'], ['b35'], ['b45'], ['b72'], [], [], ['b29', 'b77'], ['b82'], [], ['b58', 'b81'], []]","[[], ['b4', 'b59', 'b82', 'b45', 'b77', 'b81', 'b35', 'b26', 'b58', 'b72'], ['b4'], ['b83'], ['b15', 'b17', 'b59', 'b22'], ['b35'], ['b45'], ['b72'], [], [], ['b29', 'b77'], ['b82'], [], ['b58', 'b81'], []]",24,"sent1: The end-to-end model follows one single-pass pattern.
sent2: We divide it into two types, i.e., anchor-based and anchor-free, based on whether the method uses anchors (i.e., proposals) to make predictions.
sent3: 2.2.1 anchor-based. The representative anchor-based works include TGN [5], CMIN [27], SCDM [73], MAN [78], CBP [60], CSMGAN [36], 2D-TAN [83], FIAN [46], SMIN [59] and Zhang et al. [82].
sent4: TGN [5] is one typical end-to-end deep architecture, which can localize the target moment in one single pass without handling heavily overlapped pre-segmented candidate moments.
sent5: As shown in Fig. 5, TGN dynamically matches the sentence and video units via a sequential LSTM grounder with fine-grained frame-by-word interaction, and at each time step, the grounder would simultaneously score a group of candidate segments with different temporal scales ending at this time step.
sent6: CMIN [84] sequentially scores a set of candidate moments of multi-scale anchors like TGN but with a sequential BiGRU network, and refines the candidate moments with boundary regression.
sent7: To further enhance the cross-modal matching, it devises a novel cross-modal interaction network (CMIN), which first leverages a syntactic GCN to model the syntactic structure of queries, and captures long-range temporal dependencies of video context with a multi-head self-attention, then employs the fine-grained cross-modal multi-stage interaction module to produce the cross-modal features for following sequentially scoring.
sent8: Similarly, CBP [60] builds a single-stream model with sequential LSTM, which jointly predicts temporal anchors and boundaries at each time step for yield precise localization.
sent9: Furthermore, to better detect semantic boundaries, CBP devises a self attention based module to collect contextual clues instead of simply concatenating the contextual features like [16,18,23].
sent10: Based on interaction output of both language and video, it explicitly measures the contributions from different contextual elements.
sent11: CSMGAN [36] also adopts such a single-pass scheme.
sent12: It builds a joint graph for modelling the cross-/self-modal relations via iterative message passing, to capture the high-order interactions between two modalities effectively.
sent13: Each node of the graph aggregates the messages from its neighbor nodes in an edge-weighted manner and updates its state with both aggregated message and current state through ConvGRU.
sent14: Qu et al. [46] present a fine-grained iterative attention network (FIAN), which devises a contentoriented strategy to generate candidate moments differing from the anchor-based methods with sequential RNNs mentioned above.
sent15: FIAN employs a refined cross-modal guided attention (CGA) block to capture the detailed cross-modal interactions, and further adopts a symmetrical iterative attention to generate both sentence-aware video and video-aware sentence representations, where the latter are explicitly facilitated to enhance the former and finally both parts contribute to a robust cross-modal feature.
sent16: TGN establishes the temporal grounding architecture through a sequential LSTM network, while Yuan et al. [73] propose SCDM, which exploits a hierarchical temporal convolutional network to conduct target segment localization, and couples it with a semantics-conditioned dynamic modulation to fully leverage sentence semantics to compose the sentence-related video contents over time.
sent17: As shown in Fig. 6, the multimodal fusion module fuses the entire sentence and each video clip in a fine-grained manner.
sent18: The fused representation is formulated as:With such fused representations as inputs, the semantic modulated temporal convolution module further correlates sentence-related video contents in a temporal convolution procedure, dynamically modulating the temporal feature maps concerning the sentence.
sent19: Specifically, for each temporal convolutional layer, the feature map is denoted as A = {a }.
sent20: The feature unit a will be modulated based on the modulation vectors and :where the modulation vectors are computed based on the sentence representation S = {s } =1 :Finally, the position prediction module outputs the location offsets and overlap scores of candidate video segments based on the modulated features.
sent21: MAN [78] also leverages temporal convolutional network to address the TSGV task, where the sentence query is integrated as dynamic filters into the convolutional process.
sent22: Specifically, as shown in Fig. 7, MAN encodes the entire video stream using a hierarchical convolutional network to produce multi-scale candidate moment representations.
sent23: The textual features are encoded as dynamic filters and convolved with such visual representations.
sent24: Additionally, MAN exploits the graph-structured moment relation modelling adapted from Graph Convolution Network (GCN) [30] for temporal reasoning to further improve the moment representations.
sent25: Both SCDM and MAN only consider 1D temporal feature maps, while the 2D-TAN [83] network models the temporal relations of video segments via a two-dimensional map.
sent26: As shown in Fig. 8, it firstly divides the video into evenly spaced video clips with duration .
sent27: The ( , )-th location on the 2D temporal map represents a candidate moment (or anchor) from the time to ( + 1) .
sent28: This kind of 2D temporal map covers diverse video moments with different lengths, while representing their adjacent relations.
sent29: The proposed temporal adjacent network fuses the sentence representation with each of the candidate moment feature and then leverages convolutional neural network to embed the video context information, and finally predicts the confidence score of each candidate to be the final target segment.
sent30: 2D-TAN adopts a binary cross entropy loss with a the scaled IoU as the supervision signal.
sent31: The scaled IoU is controlled by two thresholds and as:where is the temporal IoU between one candidate moment and the groundtruth moment.
sent32: Thus, the loss function can be expressed as:where is the predicted confidence score of a moment.
sent33: Wang et al. [59] propose a structured multi-level interaction network (SMIN), which makes further modifications on the 2D temporal feature map as its proposal generation module.
sent34: SMIN explores the inherent structure of moment, which can be disentangled into visual content and positional boundary parts for fine-grained cross-modal and intra-moment interaction.
sent35: Zhang et al. [82] also adopts the same proposal generation approach as that of 2D-TAN, designing a visuallanguage transformer backbone followed by a multi-stage aggregation module to get discriminative moment representations for more accurate moment localization.
sent36: Despite the superior performance anchor-based methods have achieved, the performance is sensitive with the heuristic rules manually designed (i.e., the number and scales of anchors).
sent37: As a result, such anchor-based methods can not adapt to the situation with variable video length.
sent38: Meanwhile, although the pre-segmentation like two-stage methods is not required, it still essentially depends on the ranking of proposal candidates, which will also influence its efficiency."
237532483,A Survey on Temporal Sentence Grounding in Videos,Computer Science,https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453,s7,Weakly supervised method,"For the annotation of groundtruth data in TSGV, the annotators should read the query and watch the video first, and then determine the start and end points of the query-indicated segment in the video. Such a human-labored process is very time-consuming. Therefore, due to the labor-intensive groundtruth annotation procedure, some works start to extend TSGV to a weakly supervised scenario where the locations of groundtruth segments (i.e., the start and end timestamps) are unavailable in the training stage. This is formally named as weakly supervised TSGV. The typical methods include WSDEC [14], TGA [43], WSLLN [17], SCN [34], Chen et al. [12], VLANet [40], MARN [54], BAR [64], RTBPN [85], CCL [86], EC-SL [11], LoGAN [55] and CRM [26]. In general, weakly supervised methods for TSGV can be grouped into two categories (i.e., MIL-based and reconstruction-based). One representative work will be illustrated in detail for each category, after which we will introduce the remaining.

Some works [12,17,43,55] adopt multi-instance learning (MIL) to address the weakly TSGV task. When temporal annotations are not available, the whole video is treated as a bag of instances with bag-level annotations, and the predictions for instances (video segment proposals) are aggregated as the bag-level prediction.

TGA [43] is a typical MIL-based method which learns the visual-text alignment in the video level by maximizing the matching scores of the videos and their corresponding descriptions while minimizing the matching scores of the videos and the descriptions of others. It presents text-guided attention (TGA) to get text-specific global video representations, learning the joint representation of both the video and the video-level description. As illustrated in Fig. 14, TGA first employs a GRU for sentence embedding and a pretrained image encoder for extracting frame-level features. The similarity between ℎ sentence and the ℎ temporal feature within the ℎ video denoted as is computed and a softmax opration is applied to get the text-guided attention weights for each temporal unit denoted as :

Thus we could get the sentence-wise global video feature f :

WSLLN [17] is another MIL-based end-to-end weakly supervised language localization network conducting clip-sentence alignment and segment selection simultaneously. Huang et al. [26] present a cross-sentence relations mining (CRM) method exploring the cross-sentence relations within paragraph-level scope to improve the per-sentence localization accuracy. A video-language alignment network (VLANet) proposed by Ma et al. [40] prunes the irrelevant moment candidates with the Surrogate Proposal Module and utilizes multi-directional attention to get a sharper attention map for better multimodal alignment. It considers the multi-directional interactions between each surrogate proposal and query, devising the cascaded cross-modal attention (CCA) module performing both intra-and inter-modality attention. VLANet also adopts a contrastive loss for clustering the videos and queries of the similar semantics. Wu et al. [64] attempts to apply a RL-based model for weakly TSGV, which proposes a boundary adaptive refinement framework (BAR) for achieving boundary-flexible and content-aware grounding results. Chen et al. [12] propose a novel coarse-to-fine model based on MIL. First, the coarse stage selects a rough segment from a set of predefined sliding windows, which semantically corresponds to the given sentence. Afterwards, the fine stage mines the fine-grained matching relationship between each frame in the coarse segment and the sentence. It thereby refines the boundary of the coarse segment by grouping the frames and get a more precise grounding result. Tan et al. [55] propose a Latent Graph Co-Attention Network (LoGAN), a novel co-attention model that performs fine-grained semantic reasoning over an entire video. LoGAN is also a MIL-based method, which performs a similar frame-by-word interaction with the supervised method TGN [5] and adapts the graph-based method from another supervised method MAN [78] for iterative frame representation update.

Since MIL-based methods typically learn the visual-text alignment with a triplet loss, these methods heavily depend on the quality of randomly-selected negative samples, which are often easy to distinguish from the positive ones and cannot provide strong supervision signals.

The reconstruction-based methods [11,14,34,54] attempt to reconstruct the given sentence query based on the selected video segments and use the intermediate results for sentence localization. Unlike MIL-based methods, the reconstruction-based methods learn the visual-textual alignment in an indirect way. As depicted in Fig. 15, Lin et al. [34] propose a semantic completion network (SCN) to predict the masked important words within the query according to the visual context of generated and selected video proposals. Specifically, for each proposal , denoted byv = {v } = , with the masked query representation^, the energy word distribution e at ℎ time step can be computed as: where f = {f } =1 are the cross-modal semantic representations. Dec and Enc are respectively the textual decoder and visual encoder based on bi-directional Transformer [57]. Afterwards, the reconstruction loss can be computed by adding up all negative log-likelihood of masked words:

Song et al. [54] present a Multi-Level Attentional Reconstruction Network (MARN), which leverages the idea of attentional reconstruction. MARN uses proposal-level attentions to rank the segment candidates and refine them with clip-level attentions.

Duan et al. [14] formulate and address the problem of weakly supervised dense event captioning in videos (i.e., to detect and describe all events of interest in a video), which is a dual problem of weakly supervised TSGV. It presents a cycle system to train the model which can solve such a pair of dual problems at the same time. In other words, weakly supervised TSGV can be regarded as an intermediate task in such a cycle system. Similar to [14], Chen and Jiang [11] also employ a loop system for dense event captioning. They adopt a concept learner to construct an induced set of concept features to enhance the information passing between the sentence localizer and event captioner.

Besides, instead of proposing a reconstruction-based or MIL-based method, Zhang et al. [86] design a counterfactual contrastive learning paradigm to improve the visual-and-language grounding tasks. A regularized two-branch proposal network (RTBPN) [85] is also presented to explore sufficient intra-sample confrontment with sharable two-branch proposal module for distinguishing the target moment from plausible negative moments.","[['b85', 'b10', 'b16', 'b84', 'b25', 'b63', 'b42', 'b54', 'b53', 'b39', 'b11', 'b13', 'b33'], ['b54', 'b11', 'b16', 'b42'], ['b42'], [], ['b4', 'b16', 'b25', 'b63', 'b54', 'b39', 'b77', 'b11'], [], ['b10', 'b56', 'b53', 'b13', 'b33'], ['b53'], ['b10', 'b13'], ['b84', 'b85']]","[['b85', 'b10', 'b16', 'b84', 'b25', 'b63', 'b42', 'b54', 'b53', 'b39', 'b11', 'b13', 'b33'], ['b54', 'b11', 'b16', 'b42'], ['b42'], [], ['b4', 'b16', 'b25', 'b63', 'b54', 'b39', 'b77', 'b11'], [], ['b10', 'b56', 'b53', 'b13', 'b33'], ['b53'], ['b10', 'b13'], ['b84', 'b85']]",36,"sent1: For the annotation of groundtruth data in TSGV, the annotators should read the query and watch the video first, and then determine the start and end points of the query-indicated segment in the video.
sent2: Such a human-labored process is very time-consuming.
sent3: Therefore, due to the labor-intensive groundtruth annotation procedure, some works start to extend TSGV to a weakly supervised scenario where the locations of groundtruth segments (i.e., the start and end timestamps) are unavailable in the training stage.
sent4: This is formally named as weakly supervised TSGV.
sent5: The typical methods include WSDEC [14], TGA [43], WSLLN [17], SCN [34], Chen et al. [12], VLANet [40], MARN [54], BAR [64], RTBPN [85], CCL [86], EC-SL [11], LoGAN [55] and CRM [26].
sent6: In general, weakly supervised methods for TSGV can be grouped into two categories (i.e., MIL-based and reconstruction-based).
sent7: One representative work will be illustrated in detail for each category, after which we will introduce the remaining.
sent8: Some works [12,17,43,55] adopt multi-instance learning (MIL) to address the weakly TSGV task.
sent9: When temporal annotations are not available, the whole video is treated as a bag of instances with bag-level annotations, and the predictions for instances (video segment proposals) are aggregated as the bag-level prediction.
sent10: TGA [43] is a typical MIL-based method which learns the visual-text alignment in the video level by maximizing the matching scores of the videos and their corresponding descriptions while minimizing the matching scores of the videos and the descriptions of others.
sent11: It presents text-guided attention (TGA) to get text-specific global video representations, learning the joint representation of both the video and the video-level description.
sent12: As illustrated in Fig. 14, TGA first employs a GRU for sentence embedding and a pretrained image encoder for extracting frame-level features.
sent13: The similarity between ℎ sentence and the ℎ temporal feature within the ℎ video denoted as is computed and a softmax opration is applied to get the text-guided attention weights for each temporal unit denoted as :Thus we could get the sentence-wise global video feature f :WSLLN [17] is another MIL-based end-to-end weakly supervised language localization network conducting clip-sentence alignment and segment selection simultaneously.
sent14: Huang et al. [26] present a cross-sentence relations mining (CRM) method exploring the cross-sentence relations within paragraph-level scope to improve the per-sentence localization accuracy.
sent15: A video-language alignment network (VLANet) proposed by Ma et al. [40] prunes the irrelevant moment candidates with the Surrogate Proposal Module and utilizes multi-directional attention to get a sharper attention map for better multimodal alignment.
sent16: It considers the multi-directional interactions between each surrogate proposal and query, devising the cascaded cross-modal attention (CCA) module performing both intra-and inter-modality attention.
sent17: VLANet also adopts a contrastive loss for clustering the videos and queries of the similar semantics.
sent18: Wu et al. [64] attempts to apply a RL-based model for weakly TSGV, which proposes a boundary adaptive refinement framework (BAR) for achieving boundary-flexible and content-aware grounding results.
sent19: Chen et al. [12] propose a novel coarse-to-fine model based on MIL.
sent20: First, the coarse stage selects a rough segment from a set of predefined sliding windows, which semantically corresponds to the given sentence.
sent21: Afterwards, the fine stage mines the fine-grained matching relationship between each frame in the coarse segment and the sentence.
sent22: It thereby refines the boundary of the coarse segment by grouping the frames and get a more precise grounding result.
sent23: Tan et al. [55] propose a Latent Graph Co-Attention Network (LoGAN), a novel co-attention model that performs fine-grained semantic reasoning over an entire video.
sent24: LoGAN is also a MIL-based method, which performs a similar frame-by-word interaction with the supervised method TGN [5] and adapts the graph-based method from another supervised method MAN [78] for iterative frame representation update.
sent25: Since MIL-based methods typically learn the visual-text alignment with a triplet loss, these methods heavily depend on the quality of randomly-selected negative samples, which are often easy to distinguish from the positive ones and cannot provide strong supervision signals.
sent26: The reconstruction-based methods [11,14,34,54] attempt to reconstruct the given sentence query based on the selected video segments and use the intermediate results for sentence localization.
sent27: Unlike MIL-based methods, the reconstruction-based methods learn the visual-textual alignment in an indirect way.
sent28: As depicted in Fig. 15, Lin et al. [34] propose a semantic completion network (SCN) to predict the masked important words within the query according to the visual context of generated and selected video proposals.
sent29: Specifically, for each proposal , denoted byv = {v } = , with the masked query representation^, the energy word distribution e at ℎ time step can be computed as: where f = {f } =1 are the cross-modal semantic representations.
sent30: Dec and Enc are respectively the textual decoder and visual encoder based on bi-directional Transformer [57].
sent31: Afterwards, the reconstruction loss can be computed by adding up all negative log-likelihood of masked words:Song et al. [54] present a Multi-Level Attentional Reconstruction Network (MARN), which leverages the idea of attentional reconstruction.
sent32: MARN uses proposal-level attentions to rank the segment candidates and refine them with clip-level attentions.
sent33: Duan et al. [14] formulate and address the problem of weakly supervised dense event captioning in videos (i.e., to detect and describe all events of interest in a video), which is a dual problem of weakly supervised TSGV.
sent34: It presents a cycle system to train the model which can solve such a pair of dual problems at the same time.
sent35: In other words, weakly supervised TSGV can be regarded as an intermediate task in such a cycle system.
sent36: Similar to [14], Chen and Jiang [11] also employ a loop system for dense event captioning.
sent37: They adopt a concept learner to construct an induced set of concept features to enhance the information passing between the sentence localizer and event captioner.
sent38: Besides, instead of proposing a reconstruction-based or MIL-based method, Zhang et al. [86] design a counterfactual contrastive learning paradigm to improve the visual-and-language grounding tasks.
sent39: A regularized two-branch proposal network (RTBPN) [85] is also presented to explore sufficient intra-sample confrontment with sharable two-branch proposal module for distinguishing the target moment from plausible negative moments."
237532483,A Survey on Temporal Sentence Grounding in Videos,Computer Science,https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453,s6,Reinforcement learning-based method,"As another kind of anchor-free approach, RL-based frameworks view such a task as a sequential decision process. The action space for each step is a set of handcraft-designed temporal transformations (e.g., shifting, scaling). The typical methods include R-W-M [22], SM-RL [62], TripNet [21], STRONG [2], TSP-PRL [65] and AVMR [3].

He et al. [22] first introduce deep reinforcement learning techniques to address the task of TSGV, which formulates TSGV as a sequential decision making problem. As depicted in Fig. 12, at each time step, the observation network outputs the current state of the environment for the actor-critic module to generate an action policy (i.e., the probabilistic distribution of all the actions predefined in the action space), based on which the agent will perform an action to adjust the temporal boundaries. This iterative process will be ended when encountering the STOP action or reaching the maximum number of steps (i.e., ). Specifically, at each step, the current state vector is computed as:

where ( ) is generated by a FC layer whose inputs are the concatenated features including the segment-specific features (i.e., the normalized boundary pair ( −1) = [ ( −1) , ( −1) ] and local segment C3D feature ( −1) ) and global features (i.e., the sentence embedding and entire video C3D feature ). Then the actor-critic module employs GRU to model the sequential decision making process. At each time step, GRU takes ( ) as input and the hidden state is used for policy (denoted as ( ( ) | ( ) , )) generation and state-value (denoted as ( ( ) | )) estimation. The reward for each step is designed to encourage a higher tIoU compared to that of the last step. The accumulated reward function is then defined as ( is a constant discount factor):

Then they introduce the advantage function as objective which is approximated by the Mente Carlo sampling to get the policy gradient:

They further leverage two supervised tasks (i.e., tIoU regression and location regression) so the parameters can be updated from both policy gradient and supervised gradient to help the agent obtain more accurate information about the environment. Wang et al. [62] propose an RNN-based RL model which sequentially observes a selective set of video frames and finally obtains the temporal boundaries given the query. Cao et al. [2] firstly leverage the spatial scene tracking task, which utilizes a spatial-level RL for filtering out the information that is not relevant to the text query. The spatial-level RL can enhance the temporallevel RL for adjusting the temporal boundaries of the video. TripNet [21] uses gated attention to align textual and visual features, leading to improved accuracy. It incorporates a policy network for efficient search, which selects a fixed temporal bounding box moving around without watching the entire video.

TSP-PRL [65] adopts a tree-structured policy that is different from conventional RL-based methods, inspired by a human's coarse-to-fine decision-making paradigm. As shown in Fig. 13, the agent receives the state from the environment (video clips) and estimates a primitive action via tree-structured policy, including root policy and leaf policy. The action selection is depicted by a switch over the interface in the tree-structured policy. The alignment network will predict a confidence score to determine when to stop. Meanwhile, AVMR [3] addresses TSGV under the adversarial learning paradigm, which designs a RL-based proposal generator to generate proposal candidates and employs Bayesian Personalized Ranking as a discriminator to rank these generated moment proposals in a pairwise manner.","[['b64', 'b20', 'b21', 'b61', 'b1', 'b2'], ['b21'], [], [], ['b20', 'b1', 'b61'], ['b64', 'b2']]","[['b64', 'b20', 'b21', 'b61', 'b1', 'b2'], ['b21'], [], [], ['b20', 'b1', 'b61'], ['b64', 'b2']]",12,"sent1: As another kind of anchor-free approach, RL-based frameworks view such a task as a sequential decision process.
sent2: The action space for each step is a set of handcraft-designed temporal transformations (e.g., shifting, scaling).
sent3: The typical methods include R-W-M [22], SM-RL [62], TripNet [21], STRONG [2], TSP-PRL [65] and AVMR [3].He et al. [22] first introduce deep reinforcement learning techniques to address the task of TSGV, which formulates TSGV as a sequential decision making problem.
sent4: As depicted in Fig. 12, at each time step, the observation network outputs the current state of the environment for the actor-critic module to generate an action policy (i.e., the probabilistic distribution of all the actions predefined in the action space), based on which the agent will perform an action to adjust the temporal boundaries.
sent5: This iterative process will be ended when encountering the STOP action or reaching the maximum number of steps (i.e., ).
sent6: Specifically, at each step, the current state vector is computed as:where ( ) is generated by a FC layer whose inputs are the concatenated features including the segment-specific features (i.e., the normalized boundary pair ( −1) = [ ( −1) , ( −1) ] and local segment C3D feature ( −1) ) and global features (i.e., the sentence embedding and entire video C3D feature ).
sent7: Then the actor-critic module employs GRU to model the sequential decision making process.
sent8: At each time step, GRU takes ( ) as input and the hidden state is used for policy (denoted as ( ( ) | ( ) , ))
sent9: generation and state-value (denoted as ( ( ) | ))
sent10: estimation. The reward for each step is designed to encourage a higher tIoU compared to that of the last step.
sent11: The accumulated reward function is then defined as ( is a constant discount factor):
sent12: Then they introduce the advantage function as objective which is approximated by the Mente Carlo sampling to get the policy gradient:They further leverage two supervised tasks (i.e., tIoU regression and location regression) so the parameters can be updated from both policy gradient and supervised gradient to help the agent obtain more accurate information about the environment.
sent13: Wang et al. [62] propose an RNN-based RL model which sequentially observes a selective set of video frames and finally obtains the temporal boundaries given the query.
sent14: Cao et al. [2] firstly leverage the spatial scene tracking task, which utilizes a spatial-level RL for filtering out the information that is not relevant to the text query.
sent15: The spatial-level RL can enhance the temporallevel RL for adjusting the temporal boundaries of the video.
sent16: TripNet [21] uses gated attention to align textual and visual features, leading to improved accuracy.
sent17: It incorporates a policy network for efficient search, which selects a fixed temporal bounding box moving around without watching the entire video.
sent18: TSP-PRL [65] adopts a tree-structured policy that is different from conventional RL-based methods, inspired by a human's coarse-to-fine decision-making paradigm.
sent19: As shown in Fig. 13, the agent receives the state from the environment (video clips) and estimates a primitive action via tree-structured policy, including root policy and leaf policy.
sent20: The action selection is depicted by a switch over the interface in the tree-structured policy.
sent21: The alignment network will predict a confidence score to determine when to stop.
sent22: Meanwhile, AVMR [3] addresses TSGV under the adversarial learning paradigm, which designs a RL-based proposal generator to generate proposal candidates and employs Bayesian Personalized Ranking as a discriminator to rank these generated moment proposals in a pairwise manner."
237532483,A Survey on Temporal Sentence Grounding in Videos,Computer Science,https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453,s12,Performance Comparison,"In this section, we give a thorough performance comparison of the aforementioned approaches based on four benchmark datasets. For convenience and fairness, we uniformly adopt = 1 and ∈ {0.3, 0.5, 0.7} for the metric of R@ ,IoU@ . Table 2 reports the experimental results of twostage methods, Table 3 is presented for end-to-end methods, Table 4 compares the performance of both RL-based and weakly supervised methods, and Table 5 separately reports the experimental results on DiDeMo dataset with MCN-specific metrics.

Two-stage method. As shown in Table 2, the overall performance of two-stage methods seems poorer than other approaches. The possible reasons lie in three folds: (1) Firstly, most of the two-stage methods combine video and sentence features coarsely, and neglect the fine-grained visual and textual interactions for accurate temporal sentence grounding in videos. (2) Secondly, separating the candidate segment generation and sentence-segment matching procedures will make the model unable to be globally optimized, which can also influence the overall performance.

(3) Thirdly, establishing matching relationships between sentence queries and individual segments will make the local video content separate with the global video context, which may also hurt the temporal grounding accuracy.

Specifically, for the sliding window-based methods, all the methods achieve the lowest grounding accuracy on the TACoS dataset compared to the other three datasets. The reason is that the cooking activities in TACoS take place in the same kitchen scene with only some slightly varied cooking objects (e.g. chopping board, knife, and bread). Thus, it is hard to do temporal location predictions for such fine-grained activities. Meanwhile, the lengths of videos in TACoS are also longer, which will greatly increase the target segment searching space and bring more difficulties. ACL-K outperforms the other sliding window-based methods by a large margin on the TACoS and Charades-STA datasets, proving the effectiveness of aligning the activity concepts mined from both textual and visual parts. MCN gets the most inferior results on the Charades-STA dataset, which shows that its simple multimodal matching and ranking strategy for candidate segments cannot deal well with the segments of various and flexible locations. However, CTRL, ACRN, ROLE, SLTA and ACL-K can adjust the candidate segment boundaries based on the model location offsets prediction, which can therefore improve the performances. All of the sliding window-based methods have not conducted experiments on the large-scale ActivityNet Captions dataset, which may due to the extremely expensive computation for multi-scale sliding window sampling.

The proposal-generated methods achieve even better performance than the sliding window-based methods though the number of proposal candidates decreases. QSPN with query-guided segment proposal network and auxiliary captioning loss significantly outperforms other two-stage methods on the Charades dataset, verifying that unlike sliding window-based sampling, the presented query-guided proposal network is able to provide more effective candidate moments with finer temporal granularity. QSPN also conducts experiments on ActivityNet Captions that is comprised of richer scenes and achieves competitive results, which also proves the effectiveness of captioning supervision and query-guided proposals. Since the videos in Charades-STA dataset are of shorter lengths and contain less diverse activities, it is necessary to focus more on the metrics with higher IoU thresholds. SAP consistently outperforms other sliding-window based methods on Charades-STA with a higher IoU threshold, which attributes to its discriminative generated proposals and additional refinement process.

End-to-end method. For anchor-based methods, TGN achieves the lowest performance on TACoS and ActivityNet Captions datasets. CMIN also performs poorly on TACoS. The common inferior accuracy achieved by TGN, CMIN and CBP may attribute to their single-stream anchorbased localization framework. With sequential RNNs, they fail to reason complex cross-modal relations. Instead of employing RNN-based frameworks, both SCDM and MAN use convolutional neural networks to better capture fine-grained interactions and diverse video contents of different temporal granularities, which consistently achieve better performance. To make further improvement, 2D-TAN extends it to 2D feature maps to model the adjacent relations of various candidate moments of multi-anchors. SMIN and Zhang et al. [82] that adopt such a similar 2D structure modelling the relationships of candidate moments, also achieve superior results out of anchor-based methods. Specifically, Zhang et al. [82] performs the best on TACoS while SMIN has surpassed other methods on Charades-STA, which also prove the effectiveness of 2D moment relationship modelling. Furthermore, CSMGAN, FIAN, SMIN and Zhang et al. [82] all achieve superior results on ActivityNet Captions dataset. It is noted that although CSMGAN adopts the similar sequential RNN like TGN but it builds a joint graph for modeling the cross-/self-modal relations which can capture the high-order interactions between two modalities effectively, and FIAN employs a symmetrical iterative attention to obtain more robust cross-modal features for more accurate localization.

For anchor-free methods, reading comprehension-inspired methods including ExCL, VSLNet and Rodriguez et al. [48] outperform other anchor-free methods with a significant gap. Specifically, ExCL performs the best on TACoS and ActivityNet Captions dataset while VSLNet achieves the best performance on Charades-STA dataset, which proves that adopting such mature techniques in Table 3. The performance comparison of end-to-end frameworks (AB:anchor-based,AF:anchor-free,OT:others). reading comprehension area for TSGV is available and effective. However, Rodriguez et al. [48] achieves the lowest performance on ActivityNet Captions. One possible reason is that the subjectivity of annotation is hardest to model for this challenging dataset. The dense anchor-free methods including DRN, GDP and DEBUG outperform the early sparse regression network ABLR, justifying the importance of increasing the number of positive training samples. However, the additional regression-based methods including PMI, HVTG and LGI achieve superior performance on ActivityNet Captions dataset and LGI even performs best on Charades-STA dataset, which may result from more effective interaction between visual and textual contents. It is noted that L-Net has not been included in the table since the original paper [6] did not report the specific experimental values. Additionally, other methods like BPNet, DPIN and CBLN which adopt neither anchor-based nor anchor-free achieve comparable results on three datasets. It is noted that CBLN achieves the best results out of all end-to-end methods on Charades-STA and ActivityNet Captions datasets, which quite highlights the superiority of combining the advances of both anchor-based and anchor-free and its special biaffine-based architecture.

RL-based method. The upper part of Table 4 reports the performance of RL-based methods for TSGV. As we can see, TSP-PRL achieves promising performance on ActivityNet Captions, proving the effectiveness of borrowing the idea of the coarse-to-fine human-decision-making process. STRONG and AVMR achieves the best performance out of the RL-based frameworks on both TACoS and Charades-STA datasets, which proves the effectiveness of spatial RL for scene tracking and the employment of adversarial learning, respectively. R-W-M, TripNet and SM-RL achieve relative inferior performance. Specifically, SM-RL achieves lowest performance on Charades-STA. TripNet keeps the lowest performance on ActivityNet Captions. Although RL-based methods can not reach the performance of end-to-end state-of-the-art methods, they offer brand-new thoughts to address the TSGV task and enhance the ability of interpretability.

Weakly supervised method. The experimental results of Charades-STA and ActivityNet Captions datasets for weakly supervised methods are shown at the bottom part of Table 4. The performance of DiDeMo for weakly supervised methods will be presented later. We cannot tell which framework (i.e., MIL-based or reconstruction-based) has absolute advances according to the overall performance. Specifically, CRM achieves the best performance on Charades-STA and ActivityNet Captions datasets out of all weakly supervised methods. The results are also competitive compared with those of other fully supervised methods DiDeMo evaluation results with particular metrics. As aforementioned, MCN [23] measures the results with the IoU threshold = 1. Some works [5,40,78] also followed MCN using such metrics. We supplementally list the evaluation results (i.e., R@1,m@1 and R@5,m@1) on DiDeMo at Table 5 grouped by whether the method belongs to fully-supervised or weakly supervised.

Specifically, LoGAN achieves the best performance among the weakly supervised methods while TGA [43] achieves the worst. As for fully supervised methods, the performance achieved by MCN and MAN is inferior to that of TGN.","[[], [], [], [], [], ['b81'], ['b47', 'b5'], [], ['b4', 'b39', 'b77', 'b22'], ['b42']]","[[], [], [], [], [], ['b81'], ['b47', 'b5'], [], ['b4', 'b39', 'b77', 'b22'], ['b42']]",8,"sent1: In this section, we give a thorough performance comparison of the aforementioned approaches based on four benchmark datasets.
sent2: For convenience and fairness, we uniformly adopt = 1 and ∈ {0.3, 0.5, 0.7} for the metric of R@ ,IoU@ .
sent3: Table 2 reports the experimental results of twostage methods, Table 3 is presented for end-to-end methods, Table 4 compares the performance of both RL-based and weakly supervised methods, and Table 5 separately reports the experimental results on DiDeMo dataset with MCN-specific metrics.
sent4: Two-stage method. As shown in Table 2, the overall performance of two-stage methods seems poorer than other approaches.
sent5: The possible reasons lie in three folds: (1) Firstly, most of the two-stage methods combine video and sentence features coarsely, and neglect the fine-grained visual and textual interactions for accurate temporal sentence grounding in videos.
sent6: (2) Secondly, separating the candidate segment generation and sentence-segment matching procedures will make the model unable to be globally optimized, which can also influence the overall performance.
sent7: (3) Thirdly, establishing matching relationships between sentence queries and individual segments will make the local video content separate with the global video context, which may also hurt the temporal grounding accuracy.
sent8: Specifically, for the sliding window-based methods, all the methods achieve the lowest grounding accuracy on the TACoS dataset compared to the other three datasets.
sent9: The reason is that the cooking activities in TACoS take place in the same kitchen scene with only some slightly varied cooking objects (e.g. chopping board, knife, and bread).
sent10: Thus, it is hard to do temporal location predictions for such fine-grained activities.
sent11: Meanwhile, the lengths of videos in TACoS are also longer, which will greatly increase the target segment searching space and bring more difficulties.
sent12: ACL-K outperforms the other sliding window-based methods by a large margin on the TACoS and Charades-STA datasets, proving the effectiveness of aligning the activity concepts mined from both textual and visual parts.
sent13: MCN gets the most inferior results on the Charades-STA dataset, which shows that its simple multimodal matching and ranking strategy for candidate segments cannot deal well with the segments of various and flexible locations.
sent14: However, CTRL, ACRN, ROLE, SLTA and ACL-K can adjust the candidate segment boundaries based on the model location offsets prediction, which can therefore improve the performances.
sent15: All of the sliding window-based methods have not conducted experiments on the large-scale ActivityNet Captions dataset, which may due to the extremely expensive computation for multi-scale sliding window sampling.
sent16: The proposal-generated methods achieve even better performance than the sliding window-based methods though the number of proposal candidates decreases.
sent17: QSPN with query-guided segment proposal network and auxiliary captioning loss significantly outperforms other two-stage methods on the Charades dataset, verifying that unlike sliding window-based sampling, the presented query-guided proposal network is able to provide more effective candidate moments with finer temporal granularity.
sent18: QSPN also conducts experiments on ActivityNet Captions that is comprised of richer scenes and achieves competitive results, which also proves the effectiveness of captioning supervision and query-guided proposals.
sent19: Since the videos in Charades-STA dataset are of shorter lengths and contain less diverse activities, it is necessary to focus more on the metrics with higher IoU thresholds.
sent20: SAP consistently outperforms other sliding-window based methods on Charades-STA with a higher IoU threshold, which attributes to its discriminative generated proposals and additional refinement process.
sent21: End-to-end method. For anchor-based methods, TGN achieves the lowest performance on TACoS and ActivityNet Captions datasets.
sent22: CMIN also performs poorly on TACoS. The common inferior accuracy achieved by TGN, CMIN and CBP may attribute to their single-stream anchorbased localization framework.
sent23: With sequential RNNs, they fail to reason complex cross-modal relations.
sent24: Instead of employing RNN-based frameworks, both SCDM and MAN use convolutional neural networks to better capture fine-grained interactions and diverse video contents of different temporal granularities, which consistently achieve better performance.
sent25: To make further improvement, 2D-TAN extends it to 2D feature maps to model the adjacent relations of various candidate moments of multi-anchors.
sent26: SMIN and Zhang et al. [82] that adopt such a similar 2D structure modelling the relationships of candidate moments, also achieve superior results out of anchor-based methods.
sent27: Specifically, Zhang et al. [82] performs the best on TACoS while SMIN has surpassed other methods on Charades-STA, which also prove the effectiveness of 2D moment relationship modelling.
sent28: Furthermore, CSMGAN, FIAN, SMIN and Zhang et al. [82] all achieve superior results on ActivityNet Captions dataset.
sent29: It is noted that although CSMGAN adopts the similar sequential RNN like TGN but it builds a joint graph for modeling the cross-/self-modal relations which can capture the high-order interactions between two modalities effectively, and FIAN employs a symmetrical iterative attention to obtain more robust cross-modal features for more accurate localization.
sent30: For anchor-free methods, reading comprehension-inspired methods including ExCL, VSLNet and Rodriguez et al. [48] outperform other anchor-free methods with a significant gap.
sent31: Specifically, ExCL performs the best on TACoS and ActivityNet Captions dataset while VSLNet achieves the best performance on Charades-STA dataset, which proves that adopting such mature techniques in Table 3.
sent32: The performance comparison of end-to-end frameworks (AB:anchor-based,AF:anchor-free,OT:others). reading comprehension area for TSGV is available and effective.
sent33: However, Rodriguez et al. [48] achieves the lowest performance on ActivityNet Captions.
sent34: One possible reason is that the subjectivity of annotation is hardest to model for this challenging dataset.
sent35: The dense anchor-free methods including DRN, GDP and DEBUG outperform the early sparse regression network ABLR, justifying the importance of increasing the number of positive training samples.
sent36: However, the additional regression-based methods including PMI, HVTG and LGI achieve superior performance on ActivityNet Captions dataset and LGI even performs best on Charades-STA dataset, which may result from more effective interaction between visual and textual contents.
sent37: It is noted that L-Net has not been included in the table since the original paper [6] did not report the specific experimental values.
sent38: Additionally, other methods like BPNet, DPIN and CBLN which adopt neither anchor-based nor anchor-free achieve comparable results on three datasets.
sent39: It is noted that CBLN achieves the best results out of all end-to-end methods on Charades-STA and ActivityNet Captions datasets, which quite highlights the superiority of combining the advances of both anchor-based and anchor-free and its special biaffine-based architecture.
sent40: RL-based method. The upper part of Table 4 reports the performance of RL-based methods for TSGV.
sent41: As we can see, TSP-PRL achieves promising performance on ActivityNet Captions, proving the effectiveness of borrowing the idea of the coarse-to-fine human-decision-making process.
sent42: STRONG and AVMR achieves the best performance out of the RL-based frameworks on both TACoS and Charades-STA datasets, which proves the effectiveness of spatial RL for scene tracking and the employment of adversarial learning, respectively.
sent43: R-W-M, TripNet and SM-RL achieve relative inferior performance.
sent44: Specifically, SM-RL achieves lowest performance on Charades-STA.
sent45: TripNet keeps the lowest performance on ActivityNet Captions.
sent46: Although RL-based methods can not reach the performance of end-to-end state-of-the-art methods, they offer brand-new thoughts to address the TSGV task and enhance the ability of interpretability.
sent47: Weakly supervised method. The experimental results of Charades-STA and ActivityNet Captions datasets for weakly supervised methods are shown at the bottom part of Table 4.
sent48: The performance of DiDeMo for weakly supervised methods will be presented later.
sent49: We cannot tell which framework (i.e., MIL-based or reconstruction-based) has absolute advances according to the overall performance.
sent50: Specifically, CRM achieves the best performance on Charades-STA and ActivityNet Captions datasets out of all weakly supervised methods.
sent51: The results are also competitive compared with those of other fully supervised methods DiDeMo evaluation results with particular metrics.
sent52: As aforementioned, MCN [23] measures the results with the IoU threshold = 1.
sent53: Some works [5,40,78] also followed MCN using such metrics.
sent54: We supplementally list the evaluation results (i.e., R@1,m@1 and R@5,m@1) on DiDeMo at Table 5 grouped by whether the method belongs to fully-supervised or weakly supervised.
sent55: Specifically, LoGAN achieves the best performance among the weakly supervised methods while TGA [43] achieves the worst.
sent56: As for fully supervised methods, the performance achieved by MCN and MAN is inferior to that of TGN."
237532483,A Survey on Temporal Sentence Grounding in Videos,Computer Science,https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453,s9,Datasets,"Several datasets for TSGV from different scenarios with their distinct characteristics have been proposed in the past few years. There is no doubt that the effort of creating these datasets and designing corresponding evaluation metrics do promote the development of TSGV. Table 1 provides an overview about the statistics of public datasets, indicating the trend of involving more complicated activities and not being constrained in a narrow and specific scene (e.g., kitchen). We will introduce them more concretely in the following.

DiDeMo [23]. This dataset is collected from Flickr, and consists of various human activities uploaded by personal users. Hendricks et al. [23] split and label video segments from original untrimmed videos by aggregating five-second clip units, which means the lengths of groundtruth segments are times of five seconds. They claim that this trick is for avoiding ambiguity of labeling and accelerating the validation process. However, such a length-fixed issue makes the retrieval task easier since it compresses the searching space into a set with limited candidates. The data split is also provided by [23], with 33008, 4180, and 4022 video-sentence pairs for training, validation, and test, respectively.

TACoS [47]. TACoS is built based on MPII-Compositive dataset [49]. It contains 127 complex videos featuring cooking activities, and each video has several segments being annotated by sentence descriptions illustrating people's cooking actions. The average length of videos in TACoS is around 300s, which is much longer than that of other benchmark datasets. The total amount of sentence-segment pairs is 17,344 in this dataset, and 50%, 25%, 25% of which are used for training, validation, and test, respectively.

Charades-STA [16]. Charades-STA is built upon Charades [52], which is originally collected for video activity recognition, and consists of 9848 videos depicting human daily indoor activities. Specifically, Charades contains 157 activity categories and 27,847 video-level sentence descriptions. Based on Charades, Gao et al. [16] construct Charades-STA with a semi-automatic pipeline, which parses the activity label out of the video description first and aligns the description with the original label-indicated temporal intervals. As such, the yielded (description, interval) pairs can be seen as the (sentence query, target segment) pairs for TSGV. Since the length of original description in Charades-STA is quite short, Gao et al. [16] further enhance the complexity of the description by combining consecutive descriptions into a more complex sentence for test. As a result, Charades-STA contains 13,898 sentence-segment pairs for training, 4,233 simple sentence-segment pairs (6.3 words per sentence), and 1,378 complex sentence-segment pairs for test (12.4 ","[[], ['b22'], ['b46', 'b48'], ['b51', None, 'b15']]","[[], ['b22'], ['b46', 'b48'], ['b51', None, 'b15']]",6,"sent1: Several datasets for TSGV from different scenarios with their distinct characteristics have been proposed in the past few years.
sent2: There is no doubt that the effort of creating these datasets and designing corresponding evaluation metrics do promote the development of TSGV.
sent3: Table 1 provides an overview about the statistics of public datasets, indicating the trend of involving more complicated activities and not being constrained in a narrow and specific scene (e.g., kitchen).
sent4: We will introduce them more concretely in the following.
sent5: DiDeMo [23]. This dataset is collected from Flickr, and consists of various human activities uploaded by personal users.
sent6: Hendricks et al. [23] split and label video segments from original untrimmed videos by aggregating five-second clip units, which means the lengths of groundtruth segments are times of five seconds.
sent7: They claim that this trick is for avoiding ambiguity of labeling and accelerating the validation process.
sent8: However, such a length-fixed issue makes the retrieval task easier since it compresses the searching space into a set with limited candidates.
sent9: The data split is also provided by [23], with 33008, 4180, and 4022 video-sentence pairs for training, validation, and test, respectively.
sent10: TACoS [47]. TACoS is built based on MPII-Compositive dataset [49].
sent11: It contains 127 complex videos featuring cooking activities, and each video has several segments being annotated by sentence descriptions illustrating people's cooking actions.
sent12: The average length of videos in TACoS is around 300s, which is much longer than that of other benchmark datasets.
sent13: The total amount of sentence-segment pairs is 17,344 in this dataset, and 50%, 25%, 25% of which are used for training, validation, and test, respectively.
sent14: Charades-STA [16]. Charades-STA is built upon Charades [52], which is originally collected for video activity recognition, and consists of 9848 videos depicting human daily indoor activities.
sent15: Specifically, Charades contains 157 activity categories and 27,847 video-level sentence descriptions.
sent16: Based on Charades, Gao et al. [16] construct Charades-STA with a semi-automatic pipeline, which parses the activity label out of the video description first and aligns the description with the original label-indicated temporal intervals.
sent17: As such, the yielded (description, interval) pairs can be seen as the (sentence query, target segment) pairs for TSGV.
sent18: Since the length of original description in Charades-STA is quite short, Gao et al. [16] further enhance the complexity of the description by combining consecutive descriptions into a more complex sentence for test.
sent19: As a result, Charades-STA contains 13,898 sentence-segment pairs for training, 4,233 simple sentence-segment pairs (6.3 words per sentence), and 1,378 complex sentence-segment pairs for test (12.4"
237532483,A Survey on Temporal Sentence Grounding in Videos,Computer Science,https://www.semanticscholar.org/paper/191a8f11cbeaaeb2b21f1b0ef9d867d60a32d453,s5,anchor-free.,"Instead of ranking a vast number of proposal candidates, the anchor-free methods start from more fine-grained video units such as frames or clips, and aim to predict the probability for each frame/clip being the start and end point of the target segment, or directly regress the start and end points from the global view. The typical methods include ABLR [75], L-Net [6], LGI [44], PMI [8], Rodriguez et al. [48], DEBUG [39], GDP [7], HVTG [10], DRN [76], ExCL [19], and VSLNet [80].

Yuan et al. propose ABLR [75], which solves TSGV from a global perspective without generating anchors. Specifically, as shown in Fig. 9, to preserve the context information, ABLR first encodes both video and sentence via bidirectional LSTM networks. Then, a multi-modal co-attention mechanism is introduced to generate not only video attention which reflects the global video structure, but also sentence attention which highlights the crucial details for temporal localization. Finally, an attention-based coordinates prediction module is designed to regress the temporal coordinates (i.e. the starting timestamp and the ending timestamp ) of sentence query from the former output attentions. Meanwhile, there are two different regression strategies (i.e., attention weight-based regression and attended feature-based regression) with the location regression loss :

where is a smooth L1 function. Besides the location regression loss that aims to minimize the distance between the temporal coordinates of the predicted and the groundtruth segments, ABLR also designs an attention calibration loss to get the video attentions more accurately:

Here, encourages the attention weights of the video clips within the groundtruth segment to be higher.

LGI [44] formulates the TSGV task as the attention-based location regression like ABLR. It further presents a more effective local-global video-text interaction module, which models the multi-level interactions between semantic phrases and video segments.

Chen et al. [8] propose pairwise modality interaction (PMI) via a channel-gated modality interaction model to explicitly model the channel-level and sequence-level interactions in a pairwise fashion, which also directly predicts the boundaries. Specifically, a light-weight convolutional network is applied as the localization head to process the feature sequence and output the video-text relevance score and boundary prediction. HVTG [10] also computes the frame-level relevance scores and make boundary prediction based on these scores. To perform the fine-grained interaction among the visual objects and between the visual object and the language query, HVTG devises a hierarchical visual-textual graph to encode the features. Objects in each video frame and words in the sentence query are considered as the graph nodes.

Unlike ABLR that regresses the coordinates of target moment directly, ExCL [19] borrows the idea from the Reading Comprehension task [4] in natural language processing area. The process of retrieving a video segment from the video is analogous to extract a text span from the passage. Specifically, as shown in Fig. 10, ExCL employs three different variants of start-end frame predictor networks (i.e., MLP, Tied-LSTM and Conditioned-LSTM) to predict start and end probabilities for each frame. The text sentence encoder (noted in orange) and video encoder (noted in blue) both use bidirectional LSTMs for feature encoding. ExCL has two modes which depend on what the training objective is. ExCL-clf uses a classification loss, which is trained using negative log-likelihood loss:

while ExCL-reg uses a regression loss for training, formulating start and end time prediction by computing an expectation over the probability distribution given by SoftMax outputs:

VSLNet [80] also employs a standard span-based Question Answering framework. VSLNet further distinguishes the differences between video sequence and text passage for better adaption to TSGV task. To address the differences, it designs a query-guided highlighting strategy to narrow down the search space to a smaller coarse highlight region. L-Net [6] introduces a boundary model to predict the start and end boundaries, semantically localizing the video segment given the language query. It devises a cross-gated attended recurrent network to emphasize the relevant video parts while the irrelevant ones are gated out, and a cross-modal interactor for fine-grained interactions between two modalities.

Rodriguez et al. [48] also predicts start and end probabilities for each video unit. But they further model the uncertainty of boundary labels, using two Gaussian distributions as groundtruth probability distributions. The uncertainty of boundary labels results from the subjectivity of annotating process. Before the final localization, this model also adopts a dynamic filter-based guided attention mechanism to dynamically generate filters applied over video features given the sentence query, focusing on most relevant video part.

Lu et al. [39] propose a dense bottom-up grounding framework (DEBUG), which localizes the target segment by predicting the distances to bidirectional temporal boundaries for all frames inside the groundtruth segment. In this way, all frames inside the groundtruth segment can be seen as positive samples, alleviating the severe imbalance issue caused by only regarding the groundtruth segment boundaries as positive samples. As shown in Fig. 11, a typical dense anchor-free model usually contains a backbone framework for multimodal feature encoding and a head network for frame-level predictions. Specifically, DEBUG adopts QANet as its backbone network which models the interaction between videos and queries, and designs three branches as head networks which aim to separately predict the classification score, boundary distances, and confidence score for each frame.

Similarly, DRN [76] and GDP [7] also adopt such a dense anchor-free framework. For backbone, DRN uses a video-query interaction module to obtain fused hierarchical feature maps. For head network, DRN densely predicts the distances to boundaries, matching score and estimated IoU for each frame within the groundtruth segment. Meanwhile, for backbone, GDP leverages a Graph-FPN layer which conducts graph convolution over all nodes in the scene space to enhance the integrated frame features. For head network, GDP predicts the distances from its location to the boundaries of target moment and a confidence score to rank its boundary prediction for each frame.

Compared with anchor-based methods, the anchor-free methods are obviously computationefficient and robust to variable video duration. Despite these significant advantages, it is difficult for anchor-free methods to capture segment-level features for multimodal interactions.

Different from the aforementioned end-to-end methods which either samples from multi-scale anchors or directly regresses the final coordinates, some methods out of these patterns have emerged. The boundary proposal network (BPNet) [66] keeps the advantages of both anchorbased and anchor-free methods and avoids the defects, which generates proposals by anchor-free methods and then matches them with the sentence query in an anchor-based manner. Wang et al. [58] propose a dual path interaction network (DPIN) containing two branches (i.e., a boundary prediction pathway for frame-level features and an alignment pathway for segment-level features) to complementarily localize the target moment. Inspired from the dependency tree parsing task in natural language processing community, a biaffine-based architecture named context-aware biaffine localizing network (CBLN) [35] has been proposed which can simultaneously score all possible pairs of start and end indices.","[['b43', 'b74', 'b47', 'b5', 'b7', 'b79', 'b38', 'b9', 'b18', 'b75', 'b6'], ['b74'], [], [], ['b43'], ['b7', 'b9'], ['b3', 'b18'], [], ['b79', 'b5'], ['b47'], ['b38'], ['b6', 'b75'], [], ['b34', 'b57', 'b65']]","[['b43', 'b74', 'b47', 'b5', 'b7', 'b79', 'b38', 'b9', 'b18', 'b75', 'b6'], ['b74'], [], [], ['b43'], ['b7', 'b9'], ['b3', 'b18'], [], ['b79', 'b5'], ['b47'], ['b38'], ['b6', 'b75'], [], ['b34', 'b57', 'b65']]",26,"sent1: Instead of ranking a vast number of proposal candidates, the anchor-free methods start from more fine-grained video units such as frames or clips, and aim to predict the probability for each frame/clip being the start and end point of the target segment, or directly regress the start and end points from the global view.
sent2: The typical methods include ABLR [75], L-Net [6], LGI [44], PMI [8], Rodriguez et al. [48], DEBUG [39], GDP [7], HVTG [10], DRN [76], ExCL [19], and VSLNet [80].
sent3: Yuan et al. propose ABLR [75], which solves TSGV from a global perspective without generating anchors.
sent4: Specifically, as shown in Fig. 9, to preserve the context information, ABLR first encodes both video and sentence via bidirectional LSTM networks.
sent5: Then, a multi-modal co-attention mechanism is introduced to generate not only video attention which reflects the global video structure, but also sentence attention which highlights the crucial details for temporal localization.
sent6: Finally, an attention-based coordinates prediction module is designed to regress the temporal coordinates (i.e. the starting timestamp and the ending timestamp ) of sentence query from the former output attentions.
sent7: Meanwhile, there are two different regression strategies (i.e., attention weight-based regression and attended feature-based regression) with the location regression loss :where is a smooth L1 function.
sent8: Besides the location regression loss that aims to minimize the distance between the temporal coordinates of the predicted and the groundtruth segments, ABLR also designs an attention calibration loss to get the video attentions more accurately:Here, encourages the attention weights of the video clips within the groundtruth segment to be higher.
sent9: LGI [44] formulates the TSGV task as the attention-based location regression like ABLR.
sent10: It further presents a more effective local-global video-text interaction module, which models the multi-level interactions between semantic phrases and video segments.
sent11: Chen et al. [8] propose pairwise modality interaction (PMI) via a channel-gated modality interaction model to explicitly model the channel-level and sequence-level interactions in a pairwise fashion, which also directly predicts the boundaries.
sent12: Specifically, a light-weight convolutional network is applied as the localization head to process the feature sequence and output the video-text relevance score and boundary prediction.
sent13: HVTG [10] also computes the frame-level relevance scores and make boundary prediction based on these scores.
sent14: To perform the fine-grained interaction among the visual objects and between the visual object and the language query, HVTG devises a hierarchical visual-textual graph to encode the features.
sent15: Objects in each video frame and words in the sentence query are considered as the graph nodes.
sent16: Unlike ABLR that regresses the coordinates of target moment directly, ExCL [19] borrows the idea from the Reading Comprehension task [4] in natural language processing area.
sent17: The process of retrieving a video segment from the video is analogous to extract a text span from the passage.
sent18: Specifically, as shown in Fig. 10, ExCL employs three different variants of start-end frame predictor networks (i.e., MLP, Tied-LSTM and Conditioned-LSTM) to predict start and end probabilities for each frame.
sent19: The text sentence encoder (noted in orange) and video encoder (noted in blue) both use bidirectional LSTMs for feature encoding.
sent20: ExCL has two modes which depend on what the training objective is.
sent21: ExCL-clf uses a classification loss, which is trained using negative log-likelihood loss:while ExCL-reg uses a regression loss for training, formulating start and end time prediction by computing an expectation over the probability distribution given by SoftMax outputs:
sent22: VSLNet [80] also employs a standard span-based Question Answering framework.
sent23: VSLNet further distinguishes the differences between video sequence and text passage for better adaption to TSGV task.
sent24: To address the differences, it designs a query-guided highlighting strategy to narrow down the search space to a smaller coarse highlight region.
sent25: L-Net [6] introduces a boundary model to predict the start and end boundaries, semantically localizing the video segment given the language query.
sent26: It devises a cross-gated attended recurrent network to emphasize the relevant video parts while the irrelevant ones are gated out, and a cross-modal interactor for fine-grained interactions between two modalities.
sent27: Rodriguez et al. [48] also predicts start and end probabilities for each video unit.
sent28: But they further model the uncertainty of boundary labels, using two Gaussian distributions as groundtruth probability distributions.
sent29: The uncertainty of boundary labels results from the subjectivity of annotating process.
sent30: Before the final localization, this model also adopts a dynamic filter-based guided attention mechanism to dynamically generate filters applied over video features given the sentence query, focusing on most relevant video part.
sent31: Lu et al. [39] propose a dense bottom-up grounding framework (DEBUG), which localizes the target segment by predicting the distances to bidirectional temporal boundaries for all frames inside the groundtruth segment.
sent32: In this way, all frames inside the groundtruth segment can be seen as positive samples, alleviating the severe imbalance issue caused by only regarding the groundtruth segment boundaries as positive samples.
sent33: As shown in Fig. 11, a typical dense anchor-free model usually contains a backbone framework for multimodal feature encoding and a head network for frame-level predictions.
sent34: Specifically, DEBUG adopts QANet as its backbone network which models the interaction between videos and queries, and designs three branches as head networks which aim to separately predict the classification score, boundary distances, and confidence score for each frame.
sent35: Similarly, DRN [76] and GDP [7] also adopt such a dense anchor-free framework.
sent36: For backbone, DRN uses a video-query interaction module to obtain fused hierarchical feature maps.
sent37: For head network, DRN densely predicts the distances to boundaries, matching score and estimated IoU for each frame within the groundtruth segment.
sent38: Meanwhile, for backbone, GDP leverages a Graph-FPN layer which conducts graph convolution over all nodes in the scene space to enhance the integrated frame features.
sent39: For head network, GDP predicts the distances from its location to the boundaries of target moment and a confidence score to rank its boundary prediction for each frame.
sent40: Compared with anchor-based methods, the anchor-free methods are obviously computationefficient and robust to variable video duration.
sent41: Despite these significant advantages, it is difficult for anchor-free methods to capture segment-level features for multimodal interactions.
sent42: Different from the aforementioned end-to-end methods which either samples from multi-scale anchors or directly regresses the final coordinates, some methods out of these patterns have emerged.
sent43: The boundary proposal network (BPNet) [66] keeps the advantages of both anchorbased and anchor-free methods and avoids the defects, which generates proposals by anchor-free methods and then matches them with the sentence query in an anchor-based manner.
sent44: Wang et al. [58] propose a dual path interaction network (DPIN) containing two branches (i.e., a boundary prediction pathway for frame-level features and an alignment pathway for segment-level features) to complementarily localize the target moment.
sent45: Inspired from the dependency tree parsing task in natural language processing community, a biaffine-based architecture named context-aware biaffine localizing network (CBLN) [35] has been proposed which can simultaneously score all possible pairs of start and end indices."
237642905,"Aishna Gupta, Anuska Rakshit. A Technical Survey on the Modeling of Topical Bot","Computer Science, Linguistics",https://www.semanticscholar.org/paper/2846dd9c7578f680675b0d70b5f5cffe8fb8612c,s4,3) An intermediate representation -A method has been,"proposed to use more flexible intermediate representations rather than encoding X ⊕ C using a fixed-size vector. This enhances the representation capability to address the one-to-many issue in the dialog system and to improve the interpretability of the representation to more readily control the response generation. 4) An encoder that encodes user input and dialog context -It encodes richer information and generates more informative responses. Moreover, it can be used to extract topic words using LDA and encodes such words in a topic-aware model. Dialog Models based on grounded knowledge: Data stored in Wikipedia or Freebase are referred to as Knowledge facts.

A knowledge grounded open-domain dialog system can identify the entities and topics mentioned in user input further linking them into real-world facts, retrieve related background information, and thereby respond to users in a proactive way that is by recommending new, related topics to discuss [4].

A knowledge-grounded model assists to generate a response by integrating few retrieved posts that are relevant to the input. The generation model (one of the systems in which the knowledge-grounded model is used) can generate a word in response from the context or the knowledge base. The DAN performs competitively with more complicated neural networks that explicitly model semantic and syntactic compositionality. [7] Then decoding is done which includes a graph attention mechanism in which the model initially takes care of a knowledge graph and then the decoder chooses a word to generate from either the graph or the common lexis. The plan is to provide the model with appropriate long-form text as a input source of external knowledge. A reading comprehension is performed on this text in response to each conversational turn, thereby resulting in a more focused integration of external knowledge than prior approaches. Most of the studies focus on two challenges: 1) Knowledge-aware generation -providing the required knowledge into a generated response. 2) Knowledge selection -selecting appropriate knowledge that can be incorporated in the next response given in the dialog context and previously-selected knowledge. Consistency: While searching for blogs on a specific topic, it has been observed that information seekers prefer blogs that place a central focus on that topic rather than mentioning the topic in a diffused format [4]. Therefore, to be able to focus on a particular topic consistency is needed. A topical dialog system needs to encapsulate consistent behaviours so that it can gain the user's confidence and trust. However, there are three major consistency issues which include.

Persona Consistency: Can be grouped into two categories that address the dialog models:

1) Implicit personalization -the persona is implicitly represented by a persona vector-like proposing a ranking-based approach to integrate a personal knowledge base and user interests in a dialogue system. It utilizes learned user persona features to capture userlevel consistency implicitly. 2) Explicit personalization-to generate personalitycoherent responses given a pre-specified user profile. This explicit persona model controls the conversation generation using explicitly defined user profiles. The chatbot's persona is defined by a key-value table which consists of name, gender, age, occupation, and many more. During generation, firstly the model chooses a keyvalue from the profile and then a response is decoded from the chosen key-value pair forward and backward.

Stylistic Response Generation: It is a form of personalization in conversation. It is closely related to domain adaptation and transfer learning. There are two main challenges:

1) To construct training data having pairs of responses that are of the same content but are usually in different approach. 2) To extract content and style in representation. In order to solve the above problems an idea was proposed to train a general conversation model on a large corpus in the source domain then to transfer the model to a new speaker or target domain using small amounts of personalized (or stylistic) data in the target domain [4]. Some of the other proposed models include:

1) A two-phase transfer learning approach i.e., initialization then adaptation which can be used to generate personalized responses. Furthermore, a quasi-Turing test method can be implemented to evaluate the performance of the generated responses. 2) A multi-task learning approach where the response generation and utterance representation are treated as two sub-tasks for speaker role adaptation. Contextual Consistency: Earlier work focused on representing better dialog contexts using hierarchical models, this was viewed as implicit modelling of contextual consistency. Recently, contextual consistency is noticed as a natural language inference (NLI) problem.

Interactiveness: Interactiveness refers to the system's capacity to execute complex social objectives such as entertainment and conforming by optimizing its behaviours and applying dialog strategies in multi-turn conversation. From a technical viewpoint, interactiveness mainly involves sentiment and emotion detection, dialog state tracking, topic detection and recommendation, dialog policy learning, and controllable response generation. However, optimizing the behaviours and strategies of a dialog system to maximize long-term user engagement and accomplish long-term, complex goals is still a major issue. To overcome this and improve interactiveness, it is important to understand the user's emotion and optimize the system's behaviour and interaction strategy in multi-turn conversations.

User Emotion Model: Emotion perception and expression is an important factor in building a human-like machine. To generate emotional responses, we have Emotional Chatting Machine (ECM) ECM consists of three components: 1) While decoding the internal emotional state gradually decays and finally reach zero. 2) Each decoding position is fed with an emotion category embedding. 3) An external memory that permits the model to select emotional or generic words. Patterns have been observed in human-human conversations such as empathy and comfort, which would inspire a more delicate but firm design of emotional communication between humans and machines.

Another method of affective response generation was developed which consists of three components:

1) For searching effective responses -the effective beam search algorithm. 2) To increase or decrease the affective consistency between a post and a response -the effective loss functions. 3) To supplement word vectors -the effective vectors based on Dominance dimensions. To generate the reviews of a particular polarity, a multiclass generative adversarial network was proposed which consists of generators for multiple polarities and discriminators. A challenging issue in this is emotion representation.

With the massive increase in social interactions on online social networks, there has also been an increase of hateful activities that exploit such infrastructure. Detecting such hateful speech is important for analyzing public sentiment of a group of users towards another group, and for discouraging associated wrongful activities. [6] Emotional representation is also a very major issue in the existing model. One of the simple approaches is to project implicit subtle emotion label to a vector. However, this approach failed to explicitly model the user's emotional changeover during a conversation. it would try to cheer the user up through e.g., shifting to new discussion that are more comfortable for both parties.

This model is crucial for a dialog system to establish a long-term connection with a user because the user is more interested to engage with the system if the system can always detect a negative transformation in her emotion during the conversation it would try to cheer the user up through e.g., shifting to new discussion that are more comfortable for both parties. Strategy of Conversation Behaviour: A framework was built to capture the abstract emotions such as politeness strategies freedom concepts that are used to start a conversation and examined their relation. When is framework is applied in a controlled environment it is possible to detect early warning signs of antisocial behaviour in online conversation. Firstly, to detect signs of deadlock a retrieval-based method is proposed and then the response is received containing the entities related to the input.

A proactive suggestion method was proposed for the user that would provide a look-ahead post in addition to the system response, circumstances, and prior generated response. The user can use the generated post directly or type a new one during the conversation. However, asking upright questions in conversation can be shown as an important proactive behaviour. Thus, a typed decoder is proposed to generate meaningful questions by predicting a type distribution over topic words at each decoding position. The final output distribution was modelled by the type distribution, leading to a strong control over the question to be generated. In addition, a dataset of clarification questions was evaluated and a neural network model was built for ranking clarification questions. The most important drift for future research on this field are:

1) The comprehensive investigation of conversation behaviours in the human-human dialog.

2) The second is to create a more sophisticated real-world dialog setting for system development and evaluation [4]. J. SPEAKER GENDER ANALYSIS Gender detection systems based on Gaussian Mixture Models, i-vectors and Convolutional Neural Networks (CNN) were trained using an internal database of 2,284 French speakers and evaluated using REPERE challenge corpus out of which the CNN system obtained the best performance with a frame-level gender detection F-measure of 96.52 and a hourly women speaking time percentage error below 0.6% [15]. The data is extracted from SwDA and analysis is done on the speaker gender information to check whether latent modes of unsupervised learning in the dynamic speaker model could pick up some gender language variations. The process involves gathering the latent mode association scores for every 32 modes i.e. the no of modes differs according to the input data, which was computed in Latent Model Analyzer.

Then to test the associate score distributions of male vs female utterances group mean tests for individual modes are carried out. The strength of the difference is measured using the most effective way to measure effect size -The cohen-d score.

Cohen's D Formula is computed as:

D= (M1-M2) / SP Where, M1 and M2 denotes the sample means for groups 1 and 2 SP denotes the pooled estimated population standard deviation.

Lastly, the p-value is computed using the Mann-Whitney U test. The Mann-Whitney U test is generally used to work around the underlying assumption of normality in parametric tests. However, this method is used when the validity of the assumptions of the t-test is not certain thus this test has wider applicability. Cohen's d relies on the pooled standard deviation (the denominator of equation) to standardize the measure of the ES; it assumes the groups having (roughly) equal size and variance [16].

Therefore, the group mean tests are carried out on the following three sets: 1) All conversations, 2) Conversations involving either males or females, and 3) Conversations involving both genders. The Cohen's D scores for the group mean tests are shown below graphs.

Previously in Telephone Conversations, numerous researches were done to analyse the difference between genders. The main motive in analysing the gender linguistic differences in genders was of two folds.

1) From the scientific perspective, it can increase the understanding of language production. 2) From the engineering perspective, the performance of a number of natural language processing tasks, such as text classification, machine translation, or automatic speech recognition by training better language models can be improved. The methods which are used for characterizing the differences between genders and gender pairs are similar to the processes used for text classification. Studies have shown that the most effective ways for characterizing the differences between gender categories. Firstly, the transcript of each speaker has to be classified i.e., based on the appropriate gender category. And the second approach involves the application of feature selection methods, this would reveal the most characteristic features for each gender.

From each set, the most female-like mode (with the most positive Cohen-d score) and the most male-like mode (with the most negative Cohen's-d score) are identified. The significant advantage of using gender information for automatic speech recognition is that it can be robustly detected using acoustic features.","[[], ['b3'], ['b3', 'b6'], [], [], [], ['b3'], [], [], [], [], [], ['b5'], [], [], [], ['b3', 'b16'], [], [], [], ['b18'], [], [], [], []]","[[], ['b3'], ['b3', 'b6'], [], [], [], ['b3'], [], [], [], [], [], ['b5'], [], [], [], ['b3', 'b16'], [], [], [], ['b18'], [], [], [], []]",8,"sent1: proposed to use more flexible intermediate representations rather than encoding X ⊕ C using a fixed-size vector.
sent2: This enhances the representation capability to address the one-to-many issue in the dialog system and to improve the interpretability of the representation to more readily control the response generation.
sent3: 4) An encoder that encodes user input and dialog context -It encodes richer information and generates more informative responses.
sent4: Moreover, it can be used to extract topic words using LDA and encodes such words in a topic-aware model.
sent5: Dialog Models based on grounded knowledge: Data stored in Wikipedia or Freebase are referred to as Knowledge facts.
sent6: A knowledge grounded open-domain dialog system can identify the entities and topics mentioned in user input further linking them into real-world facts, retrieve related background information, and thereby respond to users in a proactive way that is by recommending new, related topics to discuss [4].
sent7: A knowledge-grounded model assists to generate a response by integrating few retrieved posts that are relevant to the input.
sent8: The generation model (one of the systems in which the knowledge-grounded model is used) can generate a word in response from the context or the knowledge base.
sent9: The DAN performs competitively with more complicated neural networks that explicitly model semantic and syntactic compositionality.
sent10: [7] Then decoding is done which includes a graph attention mechanism in which the model initially takes care of a knowledge graph and then the decoder chooses a word to generate from either the graph or the common lexis.
sent11: The plan is to provide the model with appropriate long-form text as a input source of external knowledge.
sent12: A reading comprehension is performed on this text in response to each conversational turn, thereby resulting in a more focused integration of external knowledge than prior approaches.
sent13: Most of the studies focus on two challenges: 1) Knowledge-aware generation -providing the required knowledge into a generated response.
sent14: 2) Knowledge selection -selecting appropriate knowledge that can be incorporated in the next response given in the dialog context and previously-selected knowledge.
sent15: Consistency: While searching for blogs on a specific topic, it has been observed that information seekers prefer blogs that place a central focus on that topic rather than mentioning the topic in a diffused format [4].
sent16: Therefore, to be able to focus on a particular topic consistency is needed.
sent17: A topical dialog system needs to encapsulate consistent behaviours so that it can gain the user's confidence and trust.
sent18: However, there are three major consistency issues which include.
sent19: Persona Consistency: Can be grouped into two categories that address the dialog models:1) Implicit personalization -the persona is implicitly represented by a persona vector-like proposing a ranking-based approach to integrate a personal knowledge base and user interests in a dialogue system.
sent20: It utilizes learned user persona features to capture userlevel consistency implicitly.
sent21: 2) Explicit personalization-to generate personalitycoherent responses given a pre-specified user profile.
sent22: This explicit persona model controls the conversation generation using explicitly defined user profiles.
sent23: The chatbot's persona is defined by a key-value table which consists of name, gender, age, occupation, and many more.
sent24: During generation, firstly the model chooses a keyvalue from the profile and then a response is decoded from the chosen key-value pair forward and backward.
sent25: Stylistic Response Generation: It is a form of personalization in conversation.
sent26: It is closely related to domain adaptation and transfer learning.
sent27: There are two main challenges:1) To construct training data having pairs of responses that are of the same content but are usually in different approach.
sent28: 2) To extract content and style in representation.
sent29: In order to solve the above problems an idea was proposed to train a general conversation model on a large corpus in the source domain then to transfer the model to a new speaker or target domain using small amounts of personalized (or stylistic) data in the target domain [4].
sent30: Some of the other proposed models include:1)
sent31: A two-phase transfer learning approach i.e., initialization then adaptation which can be used to generate personalized responses.
sent32: Furthermore, a quasi-Turing test method can be implemented to evaluate the performance of the generated responses.
sent33: 2) A multi-task learning approach where the response generation and utterance representation are treated as two sub-tasks for speaker role adaptation.
sent34: Contextual Consistency: Earlier work focused on representing better dialog contexts using hierarchical models, this was viewed as implicit modelling of contextual consistency.
sent35: Recently, contextual consistency is noticed as a natural language inference (NLI) problem.
sent36: Interactiveness: Interactiveness refers to the system's capacity to execute complex social objectives such as entertainment and conforming by optimizing its behaviours and applying dialog strategies in multi-turn conversation.
sent37: From a technical viewpoint, interactiveness mainly involves sentiment and emotion detection, dialog state tracking, topic detection and recommendation, dialog policy learning, and controllable response generation.
sent38: However, optimizing the behaviours and strategies of a dialog system to maximize long-term user engagement and accomplish long-term, complex goals is still a major issue.
sent39: To overcome this and improve interactiveness, it is important to understand the user's emotion and optimize the system's behaviour and interaction strategy in multi-turn conversations.
sent40: User Emotion Model: Emotion perception and expression is an important factor in building a human-like machine.
sent41: To generate emotional responses, we have Emotional Chatting Machine (ECM) ECM consists of three components: 1) While decoding the internal emotional state gradually decays and finally reach zero.
sent42: 2) Each decoding position is fed with an emotion category embedding.
sent43: 3) An external memory that permits the model to select emotional or generic words.
sent44: Patterns have been observed in human-human conversations such as empathy and comfort, which would inspire a more delicate but firm design of emotional communication between humans and machines.
sent45: Another method of affective response generation was developed which consists of three components:1) For searching effective responses -the effective beam search algorithm.
sent46: 2) To increase or decrease the affective consistency between a post and a response -the effective loss functions.
sent47: 3) To supplement word vectors -the effective vectors based on Dominance dimensions.
sent48: To generate the reviews of a particular polarity, a multiclass generative adversarial network was proposed which consists of generators for multiple polarities and discriminators.
sent49: A challenging issue in this is emotion representation.
sent50: With the massive increase in social interactions on online social networks, there has also been an increase of hateful activities that exploit such infrastructure.
sent51: Detecting such hateful speech is important for analyzing public sentiment of a group of users towards another group, and for discouraging associated wrongful activities.
sent52: [6] Emotional representation is also a very major issue in the existing model.
sent53: One of the simple approaches is to project implicit subtle emotion label to a vector.
sent54: However, this approach failed to explicitly model the user's emotional changeover during a conversation.
sent55: it would try to cheer the user up through e.g., shifting to new discussion that are more comfortable for both parties.
sent56: This model is crucial for a dialog system to establish a long-term connection with a user because the user is more interested to engage with the system if the system can always detect a negative transformation in her emotion during the conversation it would try to cheer the user up through e.g., shifting to new discussion that are more comfortable for both parties.
sent57: Strategy of Conversation Behaviour: A framework was built to capture the abstract emotions such as politeness strategies freedom concepts that are used to start a conversation and examined their relation.
sent58: When is framework is applied in a controlled environment it is possible to detect early warning signs of antisocial behaviour in online conversation.
sent59: Firstly, to detect signs of deadlock a retrieval-based method is proposed and then the response is received containing the entities related to the input.
sent60: A proactive suggestion method was proposed for the user that would provide a look-ahead post in addition to the system response, circumstances, and prior generated response.
sent61: The user can use the generated post directly or type a new one during the conversation.
sent62: However, asking upright questions in conversation can be shown as an important proactive behaviour.
sent63: Thus, a typed decoder is proposed to generate meaningful questions by predicting a type distribution over topic words at each decoding position.
sent64: The final output distribution was modelled by the type distribution, leading to a strong control over the question to be generated.
sent65: In addition, a dataset of clarification questions was evaluated and a neural network model was built for ranking clarification questions.
sent66: The most important drift for future research on this field are:1)
sent67: The comprehensive investigation of conversation behaviours in the human-human dialog.2) The second is to create a more sophisticated real-world dialog setting for system development and evaluation [4].
sent68: J. SPEAKER GENDER ANALYSIS Gender detection systems based on Gaussian Mixture Models, i-vectors and Convolutional Neural Networks (CNN) were trained using an internal database of 2,284 French speakers and evaluated using REPERE challenge corpus out of which the CNN system obtained the best performance with a frame-level gender detection F-measure of 96.52 and a hourly women speaking time percentage error below 0.6% [15].
sent69: The data is extracted from SwDA and analysis is done on the speaker gender information to check whether latent modes of unsupervised learning in the dynamic speaker model could pick up some gender language variations.
sent70: The process involves gathering the latent mode association scores for every 32 modes i.e. the no of modes differs according to the input data, which was computed in Latent Model Analyzer.
sent71: Then to test the associate score distributions of male vs female utterances group mean tests for individual modes are carried out.
sent72: The strength of the difference is measured using the most effective way to measure effect size -The cohen-d score.
sent73: Cohen's D Formula is computed as:D= (M1-M2) / SP Where, M1 and M2 denotes the sample means for groups 1 and 2 SP denotes the pooled estimated population standard deviation.
sent74: Lastly, the p-value is computed using the Mann-Whitney U test.
sent75: The Mann-Whitney U test is generally used to work around the underlying assumption of normality in parametric tests.
sent76: However, this method is used when the validity of the assumptions of the t-test is not certain thus this test has wider applicability.
sent77: Cohen's d relies on the pooled standard deviation (the denominator of equation) to standardize the measure of the ES; it assumes the groups having (roughly) equal size and variance [16].
sent78: Therefore, the group mean tests are carried out on the following three sets: 1) All conversations, 2) Conversations involving either males or females, and 3) Conversations involving both genders.
sent79: The Cohen's D scores for the group mean tests are shown below graphs.
sent80: Previously in Telephone Conversations, numerous researches were done to analyse the difference between genders.
sent81: The main motive in analysing the gender linguistic differences in genders was of two folds.
sent82: 1) From the scientific perspective, it can increase the understanding of language production.
sent83: 2) From the engineering perspective, the performance of a number of natural language processing tasks, such as text classification, machine translation, or automatic speech recognition by training better language models can be improved.
sent84: The methods which are used for characterizing the differences between genders and gender pairs are similar to the processes used for text classification.
sent85: Studies have shown that the most effective ways for characterizing the differences between gender categories.
sent86: Firstly, the transcript of each speaker has to be classified i.e., based on the appropriate gender category.
sent87: And the second approach involves the application of feature selection methods, this would reveal the most characteristic features for each gender.
sent88: From each set, the most female-like mode (with the most positive Cohen-d score) and the most male-like mode (with the most negative Cohen's-d score) are identified.
sent89: The significant advantage of using gender information for automatic speech recognition is that it can be robustly detected using acoustic features."
237642905,"Aishna Gupta, Anuska Rakshit. A Technical Survey on the Modeling of Topical Bot","Computer Science, Linguistics",https://www.semanticscholar.org/paper/2846dd9c7578f680675b0d70b5f5cffe8fb8612c,s2,Science,"A. NLU NLU is one of the most important segments in topical bots. It helps in selecting and producing system responses.

Segmentation of sentences: This is performed first as punctuation marks, double quotes, commas are not available in results of ASR and it helps in obtaining the appropriate units. ASR is responsible for identifying the human voice commands which include street names, landmarks, point of interests, distances, etc. Speech recognition does the speech to text conversion where the final output is the text corresponding to the recognized speech. [11] Sentiment and Emotion Recognition: To provide highquality responses, recognizing the sentiments and emotions of the user is one of the most important keys. This will lead to a meaningful and interesting conversation between the user and the device. There are 3 different types of sentiment in general; those are positive, negative, and neutral. Detecting these sentiments requires the ability to understand the difference in the tone and pitch of the user. The device should have built-in features that differentiate the normal/neutral pitch from positive, negative ones. This ability of understanding and simulating prosody is one of the most important parts of topical chat. Sentiment analysis is done by using different methods like lexicon-based methods, deep learning, or a combination of both.

Emotion plays an important role in human life. Interpersonal human communication includes not only language that is spoken, but also non-verbal cues as hand and body gestures, tone of the voice, which are used to express feeling and give feedback and most importantly through facial expression. [10] Emotion recognition includes recognizing fear, happiness, anger, sadness, disgust, surprise, and many more human emotions which can be detected through a change in tone. It is a difficult task to recognize all these emotions. And that is why the perfect topical chat device still doesn't exist. There are times when Alexa or Siri or Google assistants say that they aren't able to get us. These devices can't understand human jokes, sarcasm, doublemeaning words, slang, etc. It is difficult for AI to understand the meaning of words behind sarcasm. They give results on the basis of the literal meanings of words. Understanding these deep-down human emotions, sentiment and language is still impossible for AI and so as for topical chat devices. Even after providing the data of thousands of words, accents, jokes still people face problems in communicating with these devices efficiently.

Conversation Intent: Conversation intent was introduced in the default NLU model so that it can recognize the common natural ways of initiating the conversation like ""Hello"", ""How are you"", ""Let's talk"", ""Tell me a joke"" etc Response generation: For generating a response we have four different approaches: 1) Rule-based 2) Retrieval 3) Generative 4) Hybrid For a topical chat to give the best possible response, generally a huge amount of data from Wikipedia, online articles, and Reddit are extracted. Knowledge of adjectives, adverbs, pronouns, and slang are added to make the conversation more fun.

MELD is a multimodal multi-party conversational emotion recognition dataset. MELD contains raw videos, audio segments, and transcripts for multimodal processing. We believe this dataset will also be useful as a training corpus for both conversational emotion recognition and multimodal empathetic response generation. [5] A functional system can be a combination of these techniques, or follow a waterfall structure or use a hybrid approach with complementary modules retrieval-based modules that try to identify an appropriate response from the dataset of dialogs available. Retrieval can be performed using techniques such as entity matching, N-gram matching, or similarity-based on vectors such as TF-IDF, word or sentence embeddings, skip-thought vectors dual-encoder system, etc.

Hybrid approaches utilizing retrieval in combination with generative models are genuinely new and have shown promising outcomes in recent years, typically with sequenceto-sequence approaches with some variants. [2] Dialog Management: This is responsible for making the conversation engaging and finding the results related to the conversation. Once response generation is done, the dialog manager works in selecting the right response and the context. It also helps in smoothly changing or transiting the topics which the user can find interesting Dialog Management: This is responsible for making the conversation engaging and finding the results related to the conversation. Once response generation is done, the dialog manager works in selecting the right response and the context. It also helps in smoothly changing or transiting the topics which the user can find interesting.

B. SPEECH RECOGNITION BY CONVERSATIONAL AUTOMATIC Speech recognition systems can be separated in several different classes by describing what types of utterances they have the ability to recognize, these classes are classified as the following: Isolated Words, Connected Words, Continuous Speech, Spontaneous Speech [13]. Automatic Speech Recognition (ASR) is very important for voice-based assistants like google assistant. It's the advanced technology that helps us (humans) in talking to computer-based interface and to an extent the device recognize human conversations. ASR is based on or revolves around NLP (Natural Language Processing). NLP's work is to comprehend human speech and reply accordingly. Although having an accuracy of more than 95%, it still can't have a smooth interactive conversation between the device and the human. One of the majors is, even after having thousands of words, dictionaries, informal words, slangs it still is not enough when comes to having a comparison with humans. NLP enables computers to perform a wide range of natural language related tasks at all levels, ranging from parsing and part-of-speech (POS) tagging, to machine translation and dialogue systems. [9] C. CONVERSATIONAL DATASETS AND COMMON-SENSE REASONING Open source and pre-processed datasets can be used to get the information about trending topics, culture, preparing long and short-term memory, and integrating them within their dialog manager to make the responses seem as natural as possible. The term ""Social Bot"" is a superordinate concept which summarizes different types of (semi-) automatic agents. These agents are designed to fulfill a specific purpose by means of one-or many-sided communication in online media [17]. Moreover, to complement common sense reasoning, user satisfaction modules can be included to improve engagement along with the coherence of conversations.

D. CONTEXT AND DIALOG MODELLING The most powerful component of a conversational agent is a robust system that can handle dialogs effectively. Tasks that can be accomplished by the system include:

1) Help break down the complexity of the open domain problem to a manageable set of interaction modes, and 2) Be able to scale the topic based on its diversity and breadth differs. A common strategy is using a hierarchical architecture dialog modeling with the main Dialog Manager (DM) and multiple smaller DMs corresponding to specific tasks, topics, or contexts. The focus should be made not just on response generation but also on customer experience, and conversational strategies to increase engagement. Social bots received an obfuscated user hash code to enable personalization for repeat users.

E. CONVERSATIONAL TOPIC TRACKER For detecting the conversation topics, Deep Average Networks (DAN) can be adopted and train a topic classifier on interaction data categorized into multiple matters. The conversational topic tracker was identified for various purposes such as sentiment analysis, conversation evaluation, entity extraction, response generation, and many more. A novel extension was made by adding topic-word attention to formulate an attention-based DAN (ADAN) that allows the system to jointly capture topic keywords in an utterance and perform topic classification. It was observed that a user's satisfaction correlates well with long and coherent on-topic conversations, while metrics of topic breadth may provide complementary information to user ratings, as the repetitiveness of topics is hardly captured in user ratings due to the intrinsic limitations of live user data collection [12].

F. SELECTION AND RANKING TECHNIQUES In the case of rule-based rankers, the ranker chooses a response from the candidate responses obtained from submodules based on some logic. For model-based strategies, a supervised or reinforcement learning approach can be applied, trained on user ratings or on pre-defined large-scale dialog datasets such as Yahoo Answers, Reddit commentaries, OpenSubtitles, and much more. Higher scores are provided to the ranker to correct responses (e.g., followup comments on Quora are considered correct responses) while ignoring the incorrect or non-coherent responses obtained by sampling. To categorize responses and choose the best one, social bots need mechanisms to achieve the goal of having coherent and engaging conversations. When a reinforcement learning approach is used developed frameworks where the agent is a ranker, the actions are the candidate responses obtained from sub-modules, and the agent is trying to maximize the trade-off between satisfying the customer immediately versus taking into account the long-term reward of selecting a certain response. [3] G","[[], ['b10'], ['b9'], [], ['b4'], ['b1'], ['b8', 'b14', 'b19'], [], [], ['b13'], ['b2']]","[[], ['b10'], ['b9'], [], ['b4'], ['b1'], ['b8', 'b14', 'b19'], [], [], ['b13'], ['b2']]",9,"sent1: A. NLU NLU is one of the most important segments in topical bots.
sent2: It helps in selecting and producing system responses.
sent3: Segmentation of sentences: This is performed first as punctuation marks, double quotes, commas are not available in results of ASR and it helps in obtaining the appropriate units.
sent4: ASR is responsible for identifying the human voice commands which include street names, landmarks, point of interests, distances, etc.
sent5: Speech recognition does the speech to text conversion where the final output is the text corresponding to the recognized speech.
sent6: [11] Sentiment and Emotion Recognition: To provide highquality responses, recognizing the sentiments and emotions of the user is one of the most important keys.
sent7: This will lead to a meaningful and interesting conversation between the user and the device.
sent8: There are 3 different types of sentiment in general; those are positive, negative, and neutral.
sent9: Detecting these sentiments requires the ability to understand the difference in the tone and pitch of the user.
sent10: The device should have built-in features that differentiate the normal/neutral pitch from positive, negative ones.
sent11: This ability of understanding and simulating prosody is one of the most important parts of topical chat.
sent12: Sentiment analysis is done by using different methods like lexicon-based methods, deep learning, or a combination of both.
sent13: Emotion plays an important role in human life.
sent14: Interpersonal human communication includes not only language that is spoken, but also non-verbal cues as hand and body gestures, tone of the voice, which are used to express feeling and give feedback and most importantly through facial expression.
sent15: [10] Emotion recognition includes recognizing fear, happiness, anger, sadness, disgust, surprise, and many more human emotions which can be detected through a change in tone.
sent16: It is a difficult task to recognize all these emotions.
sent17: And that is why the perfect topical chat device still doesn't exist.
sent18: There are times when Alexa or Siri or Google assistants say that they aren't able to get us.
sent19: These devices can't understand human jokes, sarcasm, doublemeaning words, slang, etc.
sent20: It is difficult for AI to understand the meaning of words behind sarcasm.
sent21: They give results on the basis of the literal meanings of words.
sent22: Understanding these deep-down human emotions, sentiment and language is still impossible for AI and so as for topical chat devices.
sent23: Even after providing the data of thousands of words, accents, jokes still people face problems in communicating with these devices efficiently.
sent24: Conversation Intent: Conversation intent was introduced in the default NLU model so that it can recognize the common natural ways of initiating the conversation like ""Hello"", ""How are you"", ""Let's talk"", ""Tell me a joke"" etc Response generation: For generating a response we have four different approaches: 1) Rule-based 2) Retrieval 3) Generative 4) Hybrid For a topical chat to give the best possible response, generally a huge amount of data from Wikipedia, online articles, and Reddit are extracted.
sent25: Knowledge of adjectives, adverbs, pronouns, and slang are added to make the conversation more fun.
sent26: MELD is a multimodal multi-party conversational emotion recognition dataset.
sent27: MELD contains raw videos, audio segments, and transcripts for multimodal processing.
sent28: We believe this dataset will also be useful as a training corpus for both conversational emotion recognition and multimodal empathetic response generation.
sent29: [5] A functional system can be a combination of these techniques, or follow a waterfall structure or use a hybrid approach with complementary modules retrieval-based modules that try to identify an appropriate response from the dataset of dialogs available.
sent30: Retrieval can be performed using techniques such as entity matching, N-gram matching, or similarity-based on vectors such as TF-IDF, word or sentence embeddings, skip-thought vectors dual-encoder system, etc.
sent31: Hybrid approaches utilizing retrieval in combination with generative models are genuinely new and have shown promising outcomes in recent years, typically with sequenceto-sequence approaches with some variants.
sent32: [2] Dialog Management: This is responsible for making the conversation engaging and finding the results related to the conversation.
sent33: Once response generation is done, the dialog manager works in selecting the right response and the context.
sent34: It also helps in smoothly changing or transiting the topics which the user can find interesting Dialog Management: This is responsible for making the conversation engaging and finding the results related to the conversation.
sent35: Once response generation is done, the dialog manager works in selecting the right response and the context.
sent36: It also helps in smoothly changing or transiting the topics which the user can find interesting.
sent37: B. SPEECH RECOGNITION BY CONVERSATIONAL AUTOMATIC Speech recognition systems can be separated in several different classes by describing what types of utterances they have the ability to recognize, these classes are classified as the following: Isolated Words, Connected Words, Continuous Speech, Spontaneous Speech [13].
sent38: Automatic Speech Recognition (ASR) is very important for voice-based assistants like google assistant.
sent39: It's the advanced technology that helps us (humans) in talking to computer-based interface and to an extent the device recognize human conversations.
sent40: ASR is based on or revolves around NLP (Natural Language Processing).
sent41: NLP's work is to comprehend human speech and reply accordingly.
sent42: Although having an accuracy of more than 95%, it still can't have a smooth interactive conversation between the device and the human.
sent43: One of the majors is, even after having thousands of words, dictionaries, informal words, slangs it still is not enough when comes to having a comparison with humans.
sent44: NLP enables computers to perform a wide range of natural language related tasks at all levels, ranging from parsing and part-of-speech (POS) tagging, to machine translation and dialogue systems.
sent45: [9] C. CONVERSATIONAL DATASETS AND COMMON-SENSE REASONING Open source and pre-processed datasets can be used to get the information about trending topics, culture, preparing long and short-term memory, and integrating them within their dialog manager to make the responses seem as natural as possible.
sent46: The term ""Social Bot"" is a superordinate concept which summarizes different types of (semi-) automatic agents.
sent47: These agents are designed to fulfill a specific purpose by means of one-or many-sided communication in online media [17].
sent48: Moreover, to complement common sense reasoning, user satisfaction modules can be included to improve engagement along with the coherence of conversations.
sent49: D. CONTEXT AND DIALOG MODELLING The most powerful component of a conversational agent is a robust system that can handle dialogs effectively.
sent50: Tasks that can be accomplished by the system include:1) Help break down the complexity of the open domain problem to a manageable set of interaction modes, and 2) Be able to scale the topic based on its diversity and breadth differs.
sent51: A common strategy is using a hierarchical architecture dialog modeling with the main Dialog Manager (DM) and multiple smaller DMs corresponding to specific tasks, topics, or contexts.
sent52: The focus should be made not just on response generation but also on customer experience, and conversational strategies to increase engagement.
sent53: Social bots received an obfuscated user hash code to enable personalization for repeat users.
sent54: E. CONVERSATIONAL TOPIC TRACKER For detecting the conversation topics, Deep Average Networks (DAN) can be adopted and train a topic classifier on interaction data categorized into multiple matters.
sent55: The conversational topic tracker was identified for various purposes such as sentiment analysis, conversation evaluation, entity extraction, response generation, and many more.
sent56: A novel extension was made by adding topic-word attention to formulate an attention-based DAN (ADAN) that allows the system to jointly capture topic keywords in an utterance and perform topic classification.
sent57: It was observed that a user's satisfaction correlates well with long and coherent on-topic conversations, while metrics of topic breadth may provide complementary information to user ratings, as the repetitiveness of topics is hardly captured in user ratings due to the intrinsic limitations of live user data collection [12].
sent58: F. SELECTION AND RANKING TECHNIQUES
sent59: In the case of rule-based rankers, the ranker chooses a response from the candidate responses obtained from submodules based on some logic.
sent60: For model-based strategies, a supervised or reinforcement learning approach can be applied, trained on user ratings or on pre-defined large-scale dialog datasets such as Yahoo Answers, Reddit commentaries, OpenSubtitles, and much more.
sent61: Higher scores are provided to the ranker to correct responses (e.g., followup comments on Quora are considered correct responses) while ignoring the incorrect or non-coherent responses obtained by sampling.
sent62: To categorize responses and choose the best one, social bots need mechanisms to achieve the goal of having coherent and engaging conversations.
sent63: When a reinforcement learning approach is used developed frameworks where the agent is a ranker, the actions are the candidate responses obtained from sub-modules, and the agent is trying to maximize the trade-off between satisfying the customer immediately versus taking into account the long-term reward of selecting a certain response.
sent64: [3] G"
243886770,A Survey on Sentiment Analysis Approaches in e-Commerce,"Computer Science, Business",https://www.semanticscholar.org/paper/51cbfd00189b029802981df08872dded5a34c7c2,s14,B. Theory of Sentiments Analysis in E-Commerce,"This section discussed previous work related sentiment analysis in customers"" reviews e-commerce. Amazon dataset on product""s reviews has been selected by few writers for sentiment analysis. Sentiment analysis of unstructured data in Amazon dataset helps to measure and evaluate information in sentiment in reviews using natural language processing techniques [4] [7] [34]. Sentiment analysis was implemented for analysis of e-commerce product reviews to categorize negative and positive comments and visualize it in charts [4]. The model is developed with unigram and bigram and evaluate with classifier such as linear support vector machine, Multinomial Naïve Bayes, Stochastic Gradient, Random Forest, Logistic regression and Decision tree by product category cellphone, musical and electronics [4] [27]. The result measure using accuracy, precision, recall and F-measure whereby linear support machine shows highest accuracy 93.57 better results compared to other papers. Text mining techniques Apriori and Term Frequency-Inverse Document Frequency (TF-IDF) were applied for identifying text features in proper way [23] [34]. Table I shows implementation of sentiment analysis on review by some researchers based on Amazon dataset. The table presents many research and different sentiment analysis approaches toward resolving problems in e-commerce customers"" reviews. Process of identifying sentiment from unstructured dataset provides different results as different methods are applied. The challenging part of sentiment analysis is to discover what customers like and dislike as written expression [15] [27]. The researchers used sentiment analysis method for identifying sentiment scores in online reviews and overall result as presented in Table I. Different researchers have conducted different approaches for feature extraction and sentiment classification, hence, some future improvement in method application is needed to attain greater accuracy.  Table I, most of the method in sentiment analysis implement feature extraction and sentiment classification in their process flow for get better accuracy results. Many researchers have implemented text mining method for extract most frequent information from dataset like TF-IDF and Apriori algorithm. Machine learning supervised method is used in most of the papers for classification of information. The experimental results from frequent pattern mining and supervised machine learning methods are able to provide more than 90% accuracy result. For future investigation, lexicon method and result analysis needed for compare accuracy result with machine learning method in sentiment analysis of customers"" reviews.","[['b29', 'b10', 'b21', 'b40', 'b33']]","[['b29', 'b10', 'b21', 'b40', 'b33']]",5,"sent1: This section discussed previous work related sentiment analysis in customers"" reviews e-commerce.
sent2: Amazon dataset on product""s reviews has been selected by few writers for sentiment analysis.
sent3: Sentiment analysis of unstructured data in Amazon dataset helps to measure and evaluate information in sentiment in reviews using natural language processing techniques
sent4: [4] [7] [34]. Sentiment analysis was implemented for analysis of e-commerce product reviews to categorize negative and positive comments and visualize it in charts [4].
sent5: The model is developed with unigram and bigram and evaluate with classifier such as linear support vector machine, Multinomial Naïve Bayes, Stochastic Gradient, Random Forest, Logistic regression and Decision tree by product category cellphone, musical and electronics [4] [27].
sent6: The result measure using accuracy, precision, recall and F-measure whereby linear support machine shows highest accuracy 93.57 better results compared to other papers.
sent7: Text mining techniques Apriori and Term Frequency-Inverse Document Frequency (TF-IDF) were applied for identifying text features in proper way [23] [34].
sent8: Table I shows implementation of sentiment analysis on review by some researchers based on Amazon dataset.
sent9: The table presents many research and different sentiment analysis approaches toward resolving problems in e-commerce customers"" reviews.
sent10: Process of identifying sentiment from unstructured dataset provides different results as different methods are applied.
sent11: The challenging part of sentiment analysis is to discover what customers like and dislike as written expression [15] [27].
sent12: The researchers used sentiment analysis method for identifying sentiment scores in online reviews and overall result as presented in Table I.
sent13: Different researchers have conducted different approaches for feature extraction and sentiment classification, hence, some future improvement in method application is needed to attain greater accuracy.
sent14: Table I, most of the method in sentiment analysis implement feature extraction and sentiment classification in their process flow for get better accuracy results.
sent15: Many researchers have implemented text mining method for extract most frequent information from dataset like TF-IDF and Apriori algorithm.
sent16: Machine learning supervised method is used in most of the papers for classification of information.
sent17: The experimental results from frequent pattern mining and supervised machine learning methods are able to provide more than 90% accuracy result.
sent18: For future investigation, lexicon method and result analysis needed for compare accuracy result with machine learning method in sentiment analysis of customers"" reviews."
243886770,A Survey on Sentiment Analysis Approaches in e-Commerce,"Computer Science, Business",https://www.semanticscholar.org/paper/51cbfd00189b029802981df08872dded5a34c7c2,s11,Recall = (2),"F-score is calculated based on recall and precision as) [ 

Mean absolute error (MAE) and Root Mean Square Error (RMSE) measure the closeness between fitted line to the data points [18] [29] [30].

Confusion matrix helps to show data difference between two classes [29]. ","[[None], ['b36', 'b24'], ['b35']]","[[None], ['b36', 'b24'], ['b35']]",4,"sent1: F-score is calculated based on recall and precision as) [ Mean absolute error (MAE) and Root Mean Square Error (RMSE) measure the closeness between fitted line to the data points [18]
sent2: [29] [30]. Confusion matrix helps to show data difference between two classes [29]."
243886770,A Survey on Sentiment Analysis Approaches in e-Commerce,"Computer Science, Business",https://www.semanticscholar.org/paper/51cbfd00189b029802981df08872dded5a34c7c2,s10,D. Evaluation Score,"Based on feature extraction and sentiment classification on online reviews is rank the result using statical method [2]. The overall evaluation result is very important to judge subjective online reviews for customers. The result can predict or measure with mean squared error (MSA), confusion matrix, accuracy, precision, recall and F1-score [9] [15] [28] [29].

Equation of precision is presented as true positive (high quality reviews) divide by true positive (high positive reviews) + false positive (low quality reviews) [5] [18] [29].

Recall represented as true positive (high quality reviews) divide by true positive (high quality reviews) + False negative (low quality reviews) [5] [18] [29].","[['b8', 'b35', 'b15'], ['b11', 'b35'], ['b11', 'b35']]","[['b8', 'b35', 'b15'], ['b11', 'b35'], ['b11', 'b35']]",7,"sent1: Based on feature extraction and sentiment classification on online reviews is rank the result using statical method [2].
sent2: The overall evaluation result is very important to judge subjective online reviews for customers.
sent3: The result can predict or measure with mean squared error (MSA), confusion matrix, accuracy, precision, recall and F1-score [9]
sent4: [15] [28] [29]. Equation of precision is presented as true positive (high quality reviews) divide by true positive (high positive reviews) + false positive (low quality reviews) [5] [18] [29].
sent5: Recall represented as true positive (high quality reviews) divide by true positive (high quality reviews) + False negative (low quality reviews) [5] [18] [29]."
243886770,A Survey on Sentiment Analysis Approaches in e-Commerce,"Computer Science, Business",https://www.semanticscholar.org/paper/51cbfd00189b029802981df08872dded5a34c7c2,s8,C. Sentiment Classification,"Sentiment refers to feeling, emotions or responses of an individual by words for expressing human behavior and character [11] [25]. Hence, in this area explicit and implicit features that extract and identify hidden sentiment in measurable format. Whereby there are few methods to polarize the aspect in review theoretically: lexicon and machine learning classifier [2] [11] [13]. There are dictionary-based and corpus-based approaches for lexicon based such as Senti, HowNet, and Wordnet [8]. It is WorldNet dictionary which is stored with polarity positive, negative and neutral. Whereby automatically it is able to score the words in documents by www.ijacsa.thesai.org counting number of positive and negative words in review [21]. If the review has more positive words than negative words, it is polarized as positive reviews. Some machine learning classifier for supervised learning are Naïve Bayes, Support vector machines (SVM), Maximum Entropy and Random forest [2] [9] [10] [26] [27] [31]. Fig. 2 shows summary overall sentiment classification based machine learning and lexicon approaches. Supervised learning required training labeled data to process output result based on input data [21], whereas unsupervised learning requires unlabeled training data to identify pattern of data output. Many researches used Naïve Bayes and SVM machine learning method for sentiment classification [15] [21].","[['b27', 'b14', 'b8', 'b17', 'b37', 'b21', 'b31', 'b32', 'b19']]","[['b27', 'b14', 'b8', 'b17', 'b37', 'b21', 'b31', 'b32', 'b19']]",9,"sent1: Sentiment refers to feeling, emotions or responses of an individual by words for expressing human behavior and character [11] [25].
sent2: Hence, in this area explicit and implicit features that extract and identify hidden sentiment in measurable format.
sent3: Whereby there are few methods to polarize the aspect in review theoretically: lexicon and machine learning classifier [2]
sent4: [11] [13]. There are dictionary-based and corpus-based approaches for lexicon based such as Senti, HowNet, and Wordnet [8].
sent5: It is WorldNet dictionary which is stored with polarity positive, negative and neutral.
sent6: Whereby automatically it is able to score the words in documents by www.ijacsa.thesai.org counting number of positive and negative words in review [21].
sent7: If the review has more positive words than negative words, it is polarized as positive reviews.
sent8: Some machine learning classifier for supervised learning are Naïve Bayes, Support vector machines (SVM), Maximum Entropy and Random forest [2] [9] [10] [26] [27] [31].
sent9: Fig. 2 shows summary overall sentiment classification based machine learning and lexicon approaches.
sent10: Supervised learning required training labeled data to process output result based on input data [21], whereas unsupervised learning requires unlabeled training data to identify pattern of data output.
sent11: Many researches used Naïve Bayes and SVM machine learning method for sentiment classification [15] [21]."
243886770,A Survey on Sentiment Analysis Approaches in e-Commerce,"Computer Science, Business",https://www.semanticscholar.org/paper/51cbfd00189b029802981df08872dded5a34c7c2,s7,B. Feature Extraction,"Aspect extraction from unstructured data helps extract all relevant information from dataset, reduces or removes irrelevant features of data for sentiment classification whereby the method is known as feature extraction [2] [19]. Feature extraction also helps extract implicit information from reviews other than explicit opinion to give more effective and better performance. There are few methods used for feature extraction :-frequent pattern mining with association rule mining, term document matrix (TDM), parts-of-speech (POS) tagging, Maximum entropy (ME), N-gram and lexicon [2] [8] [10] [16]. Those methods have advantages and disadvantages while applying for extract features in reviews. Frequent pattern mining is itemset, subsequence or substructures which helps find sequence database [20]. Apriori algorithm with association rules is one of the approach is in many fields. Other than that, according to U.A.Chauhan with other researchers has implemented Part-of-speech to find differences between noun, adjective, verb and adverb [5]. By extracting the term, in sentences reveals the hidden story and emotions of customers to be classified positive or negative. Furthermore, TDM is implemented to compute frequency of each word using method like bag of words and term frequency-inverse document frequency (TF-IDF) [5] [21] [22] [23]. TF-IDF helps to calculate number of times the word occurs and focuses on the importance term. By extracting most frequent words, researchers can ignore words with least scores. Some implement N-gram features for extracting the features as unigram (One word), bigram (2 words) and trigram (3 words) whereby N represent number of words [22] [24]. Based on researchers, unigrams features commit to increase accuracy result in classification method. N-grams helps to avoid semantic scores, the score calculation creates domain independent sentiment dictionary and computes to eliminate human annotators. These are some options by researchers for extract features from dataset before classifying the sentiment into positive, negative or neutral.","[['b8', 'b14', 'b29', 'b22', 'b25', 'b28', 'b26', None, 'b30', 'b11']]","[['b8', 'b14', 'b29', 'b22', 'b25', 'b28', 'b26', None, 'b30', 'b11']]",10,"sent1: Aspect extraction from unstructured data helps extract all relevant information from dataset, reduces or removes irrelevant features of data for sentiment classification whereby the method is known as feature extraction [2] [19].
sent2: Feature extraction also helps extract implicit information from reviews other than explicit opinion to give more effective and better performance.
sent3: There are few methods used for feature extraction :-frequent pattern mining with association rule mining, term document matrix (TDM), parts-of-speech (POS) tagging, Maximum entropy (ME), N-gram and lexicon [2] [8]
sent4: [10] [16]. Those methods have advantages and disadvantages while applying for extract features in reviews.
sent5: Frequent pattern mining is itemset, subsequence or substructures which helps find sequence database [20].
sent6: Apriori algorithm with association rules is one of the approach is in many fields.
sent7: Other than that, according to U.A.Chauhan with other researchers has implemented Part-of-speech to find differences between noun, adjective, verb and adverb [5].
sent8: By extracting the term, in sentences reveals the hidden story and emotions of customers to be classified positive or negative.
sent9: Furthermore, TDM is implemented to compute frequency of each word using method like bag of words and term frequency-inverse document frequency (TF-IDF) [5] [21] [22] [23]. TF-IDF helps to calculate number of times the word occurs and focuses on the importance term.
sent10: By extracting most frequent words, researchers can ignore words with least scores.
sent11: Some implement N-gram features for extracting the features as unigram (One word), bigram (2 words) and trigram (3 words) whereby N represent number of words [22] [24].
sent12: Based on researchers, unigrams features commit to increase accuracy result in classification method.
sent13: N-grams helps to avoid semantic scores, the score calculation creates domain independent sentiment dictionary and computes to eliminate human annotators.
sent14: These are some options by researchers for extract features from dataset before classifying the sentiment into positive, negative or neutral."
243886770,A Survey on Sentiment Analysis Approaches in e-Commerce,"Computer Science, Business",https://www.semanticscholar.org/paper/51cbfd00189b029802981df08872dded5a34c7c2,s6,A. Preprocessed Texts,"As first step, data cleaning exhibit to clean unnecessary reviews from selected dataset [17] [18]. Data preprocessing perform to remove all missing values, remove stop words, tokenization, unwanted symbols, digits and URL tags [31]. Tokenization helps divide sentences into words, phrase or symbol and remove all stop words such as ""the"", ""is"", ""are"" and ""a"" [9]. The words required to convert to lower case as preparation for next step.","[['b23', 'b37', 'b15', 'b24']]","[['b23', 'b37', 'b15', 'b24']]",4,"sent1: As first step, data cleaning exhibit to clean unnecessary reviews from selected dataset [17] [18].
sent2: Data preprocessing perform to remove all missing values, remove stop words, tokenization, unwanted symbols, digits and URL tags [31].
sent3: Tokenization helps divide sentences into words, phrase or symbol and remove all stop words such as ""the"", ""is"", ""are"" and ""a"" [9].
sent4: The words required to convert to lower case as preparation for next step."
243886770,A Survey on Sentiment Analysis Approaches in e-Commerce,"Computer Science, Business",https://www.semanticscholar.org/paper/51cbfd00189b029802981df08872dded5a34c7c2,s4,C. Aspect-Level Sentiment Analysis (ASA),"ASA is opinion that classifies by identifying entities and their properties by classification and extraction [13] [15]. Whereby, it is interested on opinion words only from the reviews such as ""Love the Amazon show"", it is clearly mentioned using the word love [12]. The aspect ""love"" from the text is important feature extraction phase that needed for sentiment analysis method. At aspect level classification, researcher presents hybrid model for the analysis of finegrained product""s features [14]. It also expressed out sentiment polarity for further prediction process [16].  ","[['b22', 'b20', 'b21', 'b18', 'b19']]","[['b22', 'b20', 'b21', 'b18', 'b19']]",5,"sent1: ASA is opinion that classifies by identifying entities and their properties by classification and extraction [13] [15].
sent2: Whereby, it is interested on opinion words only from the reviews such as ""Love the Amazon show"", it is clearly mentioned using the word love [12].
sent3: The aspect ""love"" from the text is important feature extraction phase that needed for sentiment analysis method.
sent4: At aspect level classification, researcher presents hybrid model for the analysis of finegrained product""s features [14].
sent5: It also expressed out sentiment polarity for further prediction process [16]."
243886770,A Survey on Sentiment Analysis Approaches in e-Commerce,"Computer Science, Business",https://www.semanticscholar.org/paper/51cbfd00189b029802981df08872dded5a34c7c2,s1,II. SENTIMENT ANALSIS LEVEL,"Nowadays, the huge number of reviews requires efficient method for analyzing [4]. Customers and retailers reading thousands of reviews manually take plenty of time to classify the reviews in e-commerce using sentiment analysis method. The volume of reviews stored like mountain which requires some effective classifier to identify valuable information from text. Sentiment analysis or opinion mining is useful to extract customer""s behavior by analyzing and exploring customer""s reviews in E-commerce [7] [8] [9]. Customers express their emotions by writing subjective judgement about the products in E-commerce [40]. Sentiment analysis also helps to categorize the unstructured text as positive, negative and neutral whereby summarizes judgement by customers in order to understand other customer""s expression and strength better about product and retailer [7] [10] [11]. Unstructured sentiments refer to detailed opinion by customer about the product [8]. Some information is explicit and others are implicit features. There are three levels of sentiment analysis: www.ijacsa.thesai.org Document-level Sentiment Analysis (DSA), Sentence-level Sentiment Analysis (SSA) and Aspect-level Sentiment Analysis (ASA).","[['b14', 'b10', 'b17', 'b46', 'b13', 'b15']]","[['b14', 'b10', 'b17', 'b46', 'b13', 'b15']]",6,"sent1: Nowadays, the huge number of reviews requires efficient method for analyzing [4].
sent2: Customers and retailers reading thousands of reviews manually take plenty of time to classify the reviews in e-commerce using sentiment analysis method.
sent3: The volume of reviews stored like mountain which requires some effective classifier to identify valuable information from text.
sent4: Sentiment analysis or opinion mining is useful to extract customer""s behavior by analyzing and exploring customer""s reviews in E-
sent5: commerce [7] [8] [9]. Customers express their emotions by writing subjective judgement about the products in E-commerce [40].
sent6: Sentiment analysis also helps to categorize the unstructured text as positive, negative and neutral whereby summarizes judgement by customers in order to understand other customer""s expression and strength better about product and retailer [7] [10][11].
sent7: Unstructured sentiments refer to detailed opinion by customer about the product [8].
sent8: Some information is explicit and others are implicit features.
sent9: There are three levels of sentiment analysis: www.ijacsa.thesai.org Document-level Sentiment Analysis (DSA), Sentence-level Sentiment Analysis (SSA) and Aspect-level Sentiment Analysis (ASA)."
244786380,A Review of Factors Affecting the Effectiveness of Phishing,Computer Science,https://www.semanticscholar.org/paper/b83c7598f874602facb4d8b13cc494960dbceb50,s4,Literature Review,"Jampen et al. conducted a survey to analyze the effectiveness and sustainability of organizational training programs, aimed at giving anti-phishing awareness to its employees, in reducing their employees"" vulnerability to the phishing attacks. They categorized works using a well-formed methodology that took into consideration various parts of the training programs. They noted important results in the technical literature. They found out that, overall, researchers found a consensus to most research questions that gave regard to the convenience of training programs for anti-phishing. A mixture of findings came from how age affects the likelihood of the accomplishment of anti-phishing programs to train employees. Jampen et al. gave a description based on their comprehensive analysis on the design of a properly structured training program for anti-phishing and a framework with a set of recommended directives for research. (Jampen et al., 2020) Their study reveals how phishing awareness training can reduce the attacker""s chances of success, which will need them to go the extra miles in convincing users who have been trained already. Panum et al. performed an exploratory work on the efficiency of modern and popular and solutions that detect phishing, by analyzing the techniques of detection that they shared in common. They presented sample mechanisms capable of avoiding detection by causing unnoticeable perturbations. They proposed steps and measures to take in the design to improve the evaluation of the robustness of adversaries in the future. They brought to light a terminology, for respective methods, that does not depend on the application or environment to elucidate the conditions for the setting of the adversaries. Three axioms that should be accounted by any solution that detects phishing attacks were presented by them, based on an agreed upon phishing definition. This aided the solutions not to use the wrong inference attributes. They disintegrated the inference methods from the detection solutions into a collection of strategies. They evaluated the capability of absconding capture and cases of perturbations that allowed it. Their findings allow the definition of guidelines to the design of solutions for phishing detection for the community. (Panum et al., 2020) This means that hackers are sure to face impedance as they design common phishing websites, as phishing solutions equipped with such guidelines shall be detected.

Bitaab et al. carried out an extended study of measurements on the attacks related to phishing that occurred during the premature stages of the coronavirus pandemic between January 2020 and May 2020. They used their dataset to perform tracking of the trends and the reason for the growth in the phishing attacks. They analyzed and collected records of Domain Name Systems (DNS), certificates of Transport Layer Security (TLS), the phishing websites"" source codes, web traffic, and Uniform Resource Locators (URLs), emails of phishing attacks, news, and announcements of the government. They found out that the traffic of phishing attacks increased by a fraction of 220% in comparison to the rate before the COVID-19 pandemic, which exceeded previous seasonal trends. The attackers would orchestrate various hacks to manipulate the victims"" uncertainty and fear about the pandemic. The attackers used modern ways to which the existing defense systems could not handle. The analysis of Bitaab et al. displayed the ability for new defenses of ecosystems and upgraded teamwork among parties to promote quicker and efficient plans for ecosystems to tackle the uprising volume of phishing. (Bitaab et al., 2020) Their study provides quicker mutations to phishing defenses, which may be able to tackle unplanned scenarios such as a pandemic. This may reduce the hacker""s ability to take advantage of uprising tragedies to perform their cybercrime.

Steves et al. proposed a rating scale to solve the issue faced by organizations investing in training programs for phishing awareness. Since Chief Information Security Officers (CISOs) are highly dissatisfied if phishing click rates that result from the training exercises are very high. The training budget must be justified to the board officials explaining the necessity of the training as the click rates are not reducing. Their study gave rise to a debate that the level of difficulty of the phishing attack email targeted at an individual should be a factor in measuring the variance of the click rates. Based on previous studies, a phishing email forged to align with the context of a user""s job is much harder for the users to detect malice. This made Steves et al. come up with a Phish Scale to aid CISOs and the implementers of the phishing tests to give a rating of how difficult their attack is. The scale justifies any associated clicks from the tests. Cues of phishing and the context of users from former researches devised the base of their scale. They applied the scale to recent and old published results from the phishing attack exercises performed by enterprises. Their Phish scale had decent results with their selected datasets and revealed large potential as a scaling tool and a catalyst for sharing information regarding the clicks observed during a phishing experiment. (Steves et al., 2020) Their tool provides disambiguation to the top management as phishing training clicks rise in number, which helps them be able to make more effective decisions. This ensures that phishing training may have a stable budget and hackers shall more likely have to face highly trained staff, which again may reduce their chances for success.

Wang et al. attempted to detect phishing using Bidirectional Long Short-Term Memory (BLSTM) and Random Forest classifiers. The results of their experiments were satisfying in regards to the detection of phishing and their study contributed to applying the algorithms that they proposed to the field of information security. The Bidirectional Long Short-Term Memory model produced a rate of recognition of 95.47% in comparison to the Random Forest model that produced 87.53%. The results of Wang et al. reveal that the BLSTM detection method is more reliable in guaranteeing security in the network and uncovering the relevance of the model that they proposed to detect phishing. (Wang et al., 2020) Their study revealed efficient means to tackle phishing, and future solutions may be adapted to fill gaps in weaknesses of solutions of the past.

Mohith Gowda et al. devised a novel mechanism to identify websites used for phishing with the use of a novel architecture for a browser on the client""s side. They applied the extraction framework rule to draw out the websites"" properties using only the URL. A list consisting of 30 various features of a URL is populated. Sharma & Bashir performed a study where they looked into the attackers"" emails that were available to the public in repository databases for phishing attacks. They analyzed the characteristics and contents of those phishing emails. In order for them to understand the language and techniques that attackers used to be able to lure their targets, they considered many variables. Their findings showed that the words of the attackers used in their emails would aim at exploiting emotional triggers in humans such as anticipation and fear. The role played by their findings centered on a human study is a major step directed to improving the programs for training and enhancing the detection of phishing attacks, similarly, human factors may take part in the security of systems. (Sharma & Bashir, 2020) Broadhurst et al. performed a study based on explorations and observations on 138 recruits from the orientation week of a university for several months in 2017. The aim of their study was to find out cybercrime risks. They ran social engineering attacks, observed the responses, and compared how the participants took the risks to cybercrime before and after the phishing campaign. Their quasi-experimental survey exposed the test subjects to fake emails and phishing attacks. The intention of the emails was to steal confidential data from the victims or convince them to navigate to poisoned sites by clicking on the malicious links in the mail. Their techniques varied in terms of individualization. The phishing categories involved targeted or spear, tailored and generic. They classified the subjects based on the awareness of cybercrime in two groups, viz. Hunter and Passive condition. Those in the Hunter class, throughout the experiment, were aware of all forms of swindles and the ones in the Passive class did not get any warning. Broadhurst et al. analyzed the effects of the type of scam, awareness of cybercrime, competence in the field of information technology (IT), gender of the subject, and perception of safety on the internet to how susceptible their email scams were. They found out that spear phishing had a better chance of being engaged to than a generic phishing attempt. Their analysis also pointed out that there was a higher probability that fresh men and international students would face deception from the phishing than the senior and domestic students. For the further exploration of all their variables and the results, they performed a generalized linear model (GLM) analysis. (Broadhurst et al., 2020) Williams & Joinson conducted theoretical based research to investigate the methods in which the present and future phishing interactions may target the users along with the effect they play on the susceptibility of phishing. They developed and validated a survey measure centered on the protection motivation theory constructs across two studies. Such constructs include perceived vulnerability, efficacy to response, severity, and self-efficacy. They assessed the features contributing to the decision that people make on whether they shall stay updated by phishing awareness to protect themselves or not. They analyzed what role each construct played in the intentions of the user to know the latest phishing techniques that will evolve and the capability of phishing discrimination via a phishing quiz assessment. Williams & Joinson observed that larger intentions came from a greater perception of the threats"" response efficacy, severity, and self-efficacy while low intentions to know about the phishing techniques came from a high perception of vulnerability. They did not manage to find a relationship with the ability to discriminate against phishing. With the knowledge on the causes of users"" intentions in maintaining education and pursuing updates about phishing dangers, the assurance is available, that efficient interventions come, and maximum potential effects exist. (Williams & Joinson, 2020) Frauenstein & Flowerday presented a model based on theory to counter the susceptibility of phishing on social network sites (SSNs). They collected data from 215 subjects and observed the contribution of the processing of information to phishing on social networks. They regarded how users are vulnerable to the sites based on their personalities to identify characteristics of users that may be more susceptible to phishing on these social media sites. They performed a Structural Equation Modeling (SEM) analysis, and the results showed that heuristic processing faced a negative impact from the conscious users, and thus were less vulnerable to phishing on SNSs. Their analysis supported and confirmed previous studies that the susceptibility to phishing increases with heuristic processing. The research of Frauenstein & Flowerday contributed to the discipline of information security as being one of the first studies to analyze how the relationship between the Big Five Personality Traits: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Openness, and the systematic heuristic model is affected. (Frauenstein & Flowerday, 2020) ","[[None, 'b7'], [None], [], [None], [None, 'b2', 'b15']]","[[None, 'b7'], [None], [], [None], [None, 'b2', 'b15']]",7,"sent1: Jampen et al. conducted a survey to analyze the effectiveness and sustainability of organizational training programs, aimed at giving anti-phishing awareness to its employees, in reducing their employees"" vulnerability to the phishing attacks.
sent2: They categorized works using a well-formed methodology that took into consideration various parts of the training programs.
sent3: They noted important results in the technical literature.
sent4: They found out that, overall, researchers found a consensus to most research questions that gave regard to the convenience of training programs for anti-phishing.
sent5: A mixture of findings came from how age affects the likelihood of the accomplishment of anti-phishing programs to train employees.
sent6: Jampen et al. gave a description based on their comprehensive analysis on the design of a properly structured training program for anti-phishing and a framework with a set of recommended directives for research.
sent7: (Jampen et al., 2020) Their study reveals how phishing awareness training can reduce the attacker""s chances of success, which will need them to go the extra miles in convincing users who have been trained already.
sent8: Panum et al. performed an exploratory work on the efficiency of modern and popular and solutions that detect phishing, by analyzing the techniques of detection that they shared in common.
sent9: They presented sample mechanisms capable of avoiding detection by causing unnoticeable perturbations.
sent10: They proposed steps and measures to take in the design to improve the evaluation of the robustness of adversaries in the future.
sent11: They brought to light a terminology, for respective methods, that does not depend on the application or environment to elucidate the conditions for the setting of the adversaries.
sent12: Three axioms that should be accounted by any solution that detects phishing attacks were presented by them, based on an agreed upon phishing definition.
sent13: This aided the solutions not to use the wrong inference attributes.
sent14: They disintegrated the inference methods from the detection solutions into a collection of strategies.
sent15: They evaluated the capability of absconding capture and cases of perturbations that allowed it.
sent16: Their findings allow the definition of guidelines to the design of solutions for phishing detection for the community.
sent17: (Panum et al., 2020) This means that hackers are sure to face impedance as they design common phishing websites, as phishing solutions equipped with such guidelines shall be detected.
sent18: Bitaab et al. carried out an extended study of measurements on the attacks related to phishing that occurred during the premature stages of the coronavirus pandemic between January 2020 and May 2020.
sent19: They used their dataset to perform tracking of the trends and the reason for the growth in the phishing attacks.
sent20: They analyzed and collected records of Domain Name Systems (DNS), certificates of Transport Layer Security (TLS), the phishing websites"" source codes, web traffic, and Uniform Resource Locators (URLs), emails of phishing attacks, news, and announcements of the government.
sent21: They found out that the traffic of phishing attacks increased by a fraction of 220% in comparison to the rate before the COVID-19 pandemic, which exceeded previous seasonal trends.
sent22: The attackers would orchestrate various hacks to manipulate the victims"" uncertainty and fear about the pandemic.
sent23: The attackers used modern ways to which the existing defense systems could not handle.
sent24: The analysis of Bitaab et al. displayed the ability for new defenses of ecosystems and upgraded teamwork among parties to promote quicker and efficient plans for ecosystems to tackle the uprising volume of phishing.
sent25: (Bitaab et al., 2020) Their study provides quicker mutations to phishing defenses, which may be able to tackle unplanned scenarios such as a pandemic.
sent26: This may reduce the hacker""s ability to take advantage of uprising tragedies to perform their cybercrime.Steves et al. proposed a rating scale to solve the issue faced by organizations investing in training programs for phishing awareness.
sent27: Since Chief Information Security Officers (CISOs) are highly dissatisfied if phishing click rates that result from the training exercises are very high.
sent28: The training budget must be justified to the board officials explaining the necessity of the training as the click rates are not reducing.
sent29: Their study gave rise to a debate that the level of difficulty of the phishing attack email targeted at an individual should be a factor in measuring the variance of the click rates.
sent30: Based on previous studies, a phishing email forged to align with the context of a user""s job is much harder for the users to detect malice.
sent31: This made Steves et al. come up with a Phish Scale to aid CISOs and the implementers of the phishing tests to give a rating of how difficult their attack is.
sent32: The scale justifies any associated clicks from the tests.
sent33: Cues of phishing and the context of users from former researches devised the base of their scale.
sent34: They applied the scale to recent and old published results from the phishing attack exercises performed by enterprises.
sent35: Their Phish scale had decent results with their selected datasets and revealed large potential as a scaling tool and a catalyst for sharing information regarding the clicks observed during a phishing experiment.
sent36: (Steves et al., 2020) Their tool provides disambiguation to the top management as phishing training clicks rise in number, which helps them be able to make more effective decisions.
sent37: This ensures that phishing training may have a stable budget and hackers shall more likely have to face highly trained staff, which again may reduce their chances for success.
sent38: Wang et al. attempted to detect phishing using Bidirectional Long Short-Term Memory (BLSTM) and Random Forest classifiers.
sent39: The results of their experiments were satisfying in regards to the detection of phishing and their study contributed to applying the algorithms that they proposed to the field of information security.
sent40: The Bidirectional Long Short-Term Memory model produced a rate of recognition of 95.47% in comparison to the Random Forest model that produced 87.53%.
sent41: The results of Wang et al. reveal that the BLSTM detection method is more reliable in guaranteeing security in the network and uncovering the relevance of the model that they proposed to detect phishing.
sent42: (Wang et al., 2020) Their study revealed efficient means to tackle phishing, and future solutions may be adapted to fill gaps in weaknesses of solutions of the past.
sent43: Mohith Gowda et al. devised a novel mechanism to identify websites used for phishing with the use of a novel architecture for a browser on the client""s side.
sent44: They applied the extraction framework rule to draw out the websites"" properties using only the URL.
sent45: A list consisting of 30 various features of a URL is populated.
sent46: Sharma & Bashir performed a study where they looked into the attackers"" emails that were available to the public in repository databases for phishing attacks.
sent47: They analyzed the characteristics and contents of those phishing emails.
sent48: In order for them to understand the language and techniques that attackers used to be able to lure their targets, they considered many variables.
sent49: Their findings showed that the words of the attackers used in their emails would aim at exploiting emotional triggers in humans such as anticipation and fear.
sent50: The role played by their findings centered on a human study is a major step directed to improving the programs for training and enhancing the detection of phishing attacks, similarly, human factors may take part in the security of systems.
sent51: (Sharma & Bashir, 2020) Broadhurst et al. performed a study based on explorations and observations on 138 recruits from the orientation week of a university for several months in 2017.
sent52: The aim of their study was to find out cybercrime risks.
sent53: They ran social engineering attacks, observed the responses, and compared how the participants took the risks to cybercrime before and after the phishing campaign.
sent54: Their quasi-experimental survey exposed the test subjects to fake emails and phishing attacks.
sent55: The intention of the emails was to steal confidential data from the victims or convince them to navigate to poisoned sites by clicking on the malicious links in the mail.
sent56: Their techniques varied in terms of individualization.
sent57: The phishing categories involved targeted or spear, tailored and generic.
sent58: They classified the subjects based on the awareness of cybercrime in two groups, viz.
sent59: Hunter and Passive condition. Those in the Hunter class, throughout the experiment, were aware of all forms of swindles and the ones in the Passive class did not get any warning.
sent60: Broadhurst et al. analyzed the effects of the type of scam, awareness of cybercrime, competence in the field of information technology (IT), gender of the subject, and perception of safety on the internet to how susceptible their email scams were.
sent61: They found out that spear phishing had a better chance of being engaged to than a generic phishing attempt.
sent62: Their analysis also pointed out that there was a higher probability that fresh men and international students would face deception from the phishing than the senior and domestic students.
sent63: For the further exploration of all their variables and the results, they performed a generalized linear model (GLM) analysis.
sent64: (Broadhurst et al., 2020) Williams & Joinson conducted theoretical based research to investigate the methods in which the present and future phishing interactions may target the users along with the effect they play on the susceptibility of phishing.
sent65: They developed and validated a survey measure centered on the protection motivation theory constructs across two studies.
sent66: Such constructs include perceived vulnerability, efficacy to response, severity, and self-efficacy.
sent67: They assessed the features contributing to the decision that people make on whether they shall stay updated by phishing awareness to protect themselves or not.
sent68: They analyzed what role each construct played in the intentions of the user to know the latest phishing techniques that will evolve and the capability of phishing discrimination via a phishing quiz assessment.
sent69: Williams & Joinson observed that larger intentions came from a greater perception of the threats"" response efficacy, severity, and self-efficacy while low intentions to know about the phishing techniques came from a high perception of vulnerability.
sent70: They did not manage to find a relationship with the ability to discriminate against phishing.
sent71: With the knowledge on the causes of users"" intentions in maintaining education and pursuing updates about phishing dangers, the assurance is available, that efficient interventions come, and maximum potential effects exist.
sent72: (Williams & Joinson, 2020) Frauenstein & Flowerday presented a model based on theory to counter the susceptibility of phishing on social network sites (SSNs).
sent73: They collected data from 215 subjects and observed the contribution of the processing of information to phishing on social networks.
sent74: They regarded how users are vulnerable to the sites based on their personalities to identify characteristics of users that may be more susceptible to phishing on these social media sites.
sent75: They performed a Structural Equation Modeling (SEM) analysis, and the results showed that heuristic processing faced a negative impact from the conscious users, and thus were less vulnerable to phishing on SNSs.
sent76: Their analysis supported and confirmed previous studies that the susceptibility to phishing increases with heuristic processing.
sent77: The research of Frauenstein & Flowerday contributed to the discipline of information security as being one of the first studies to analyze how the relationship between the Big Five Personality Traits: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Openness, and the systematic heuristic model is affected.
sent78: (Frauenstein & Flowerday, 2020)"
244786380,A Review of Factors Affecting the Effectiveness of Phishing,Computer Science,https://www.semanticscholar.org/paper/b83c7598f874602facb4d8b13cc494960dbceb50,s1,Spear Phishing,"Spear phishing is a form of phishing attack that targets a single primary individual. The attacker focuses their attention on a single person and lures them into surrendering their secret data without them having a clue what is going on. The email consists of information concerning the target directly such as their home address, the company they work for, their job title etc. Social media sites such as LinkedIn have become a useful tool for hackers to plot their spear phishing hacks due to the professionalism aimed by users in submitting true information relating to their academic history and professional careers. (FireEye, 2018) (Bullee et al., 2017) Vishing Voice phishing or Vishing is another form of phishing where by the attacker makes a phone call to the victim and then tries to manipulate them over the phone in giving up confidential data. Hackers find vishing convenient as they may avoid detection by bouncing off several cellphone towers making them hard to trace. Reports have revealed that most victims of such attacks do not file reports to the police or responsible authorities. (Maseno, 2017) Whaling Whaling is a form of phishing that targets the high-profile individuals such as chief executive officers (CEOs), presidents, kings or queens etc. The attackers perform intensive profiling for extended periods before initiating an attack. This is a relatively serious type of threat as top executives have access to the most sensitive and critical data. A loss of such information is taken to have grave consequences. (Gupta et al., 2018) Keylogger A keylogger is a form of spyware that can be either hardware or software based. Its sole purpose is to record all the keystrokes that are typed in by the user, without them knowing. It does this behind the scenes so the user cannot suspect anything. The strokes of each key are compiled altogether into a log type of file. The hacker makes it so the logs may be sent to them secretly via email or any other covert mechanism. (Parekh et al., 2020) Backdoor A backdoor is a form of malware that functions to allow hackers to gain access to a machine without any approval, authorization, or authentication. The attackers normally use them to circumvent authentication systems and gain unapproved remote-control sessions to a machine or device. Once in a machine, they can hide themselves in many ways and go on further to steal confidential data or cause a denial of service. (Loi, 2017) ","[['b4', 'b3', 'b14', 'b10', 'b5', 'b12']]","[['b4', 'b3', 'b14', 'b10', 'b5', 'b12']]",6,"sent1: Spear phishing is a form of phishing attack that targets a single primary individual.
sent2: The attacker focuses their attention on a single person and lures them into surrendering their secret data without them having a clue what is going on.
sent3: The email consists of information concerning the target directly such as their home address, the company they work for, their job title etc.
sent4: Social media sites such as LinkedIn have become a useful tool for hackers to plot their spear phishing hacks due to the professionalism aimed by users in submitting true information relating to their academic history and professional careers.
sent5: (FireEye, 2018) (Bullee et al., 2017)
sent6: Vishing Voice phishing or Vishing is another form of phishing where by the attacker makes a phone call to the victim and then tries to manipulate them over the phone in giving up confidential data.
sent7: Hackers find vishing convenient as they may avoid detection by bouncing off several cellphone towers making them hard to trace.
sent8: Reports have revealed that most victims of such attacks do not file reports to the police or responsible authorities.
sent9: (Maseno, 2017) Whaling Whaling is a form of phishing that targets the high-profile individuals such as chief executive officers (CEOs), presidents, kings or queens etc.
sent10: The attackers perform intensive profiling for extended periods before initiating an attack.
sent11: This is a relatively serious type of threat as top executives have access to the most sensitive and critical data.
sent12: A loss of such information is taken to have grave consequences.
sent13: (Gupta et al., 2018) Keylogger A keylogger is a form of spyware that can be either hardware or software based.
sent14: Its sole purpose is to record all the keystrokes that are typed in by the user, without them knowing.
sent15: It does this behind the scenes so the user cannot suspect anything.
sent16: The strokes of each key are compiled altogether into a log type of file.
sent17: The hacker makes it so the logs may be sent to them secretly via email or any other covert mechanism.
sent18: (Parekh et al., 2020) Backdoor A backdoor is a form of malware that functions to allow hackers to gain access to a machine without any approval, authorization, or authentication.
sent19: The attackers normally use them to circumvent authentication systems and gain unapproved remote-control sessions to a machine or device.
sent20: Once in a machine, they can hide themselves in many ways and go on further to steal confidential data or cause a denial of service. (Loi, 2017)"
244786380,A Review of Factors Affecting the Effectiveness of Phishing,Computer Science,https://www.semanticscholar.org/paper/b83c7598f874602facb4d8b13cc494960dbceb50,s3,Reverse Shell,"A reverse shell is a connection shell that is virtual and open to the attacker""s machine that initiates from the victims"" device. With an open connection, an attacker is capable of running scripts and executing various evil commands on the victim""s machine. This mechanism is normally used by many common RATs and backdoors. The hacker gains the privileges of the user who was logged in when the session was initiated and may work to elevate privileges to gain further access. Most reverse shells use the Transmission Control Protocol (TCP) for the establishment, but the Internet Control Message Protocol (ICMP) has also been observed in use. Any port may be used to create the communication link. When hiding under allowed ports, the firewall and other intrusion detection systems face difficulty in identifying an incident because the ports are approved to communicate. When the shell is run under port 443 Secure Sockets Layer (SSL), content inspection is very hard as the traffic is encrypted. (Lu, 2019) Virus A computer virus is a dangerous piece of code that self-replicates by infecting other programs within its reach by injecting malicious code in them. Many viruses are dependent on some kind of executable, of which they are hosted by, before they may be able to start running their code. They may become very severe, to the point where they may completely ruin all of your hardware and software. A virus cannot propagate without human intervention. This is what makes it different from a worm. A virus needs some kind of host program to keep it in action. Viruses may be spread very rapidly and unintentionally as victims may tend to share poisoned files or send attachments in the email that have been infected. (Kumar & Dey, 2019) Worm A computer worm is a malicious program that replicates on its own without the need of interacting with any other file, and spreads across machines in a network. It transmits copies of its code throughout the network without attaching to any programs like in the case of a virus. They normally consume a large amount of the network bandwidth and cause a lack of service availability to users on the network. The first case of a worm propagating over the internet was the Morris worm released by Robert Tappan Morris on November 2, 1988. (Jajoo, 2017) Rootkit A rootkit is a program written primarily for evading detection while maintaining privileged access to the system. Host Intrusion Detection Systems (HIDS), Host Intrusion Prevention Systems (HIPS), Network Intrusion Detection Systems (NIDS), Network Intrusion Prevention Systems (NIPS), firewalls, Security Information and Event Management (SIEM) solutions all find it very hard to detect rootkits as they can modify their underlying operating system kernel code that they utilized for detection and thus usurping the control of security. (Nadim et al., 2021) Despite the various technical attacks which a hacker may deploy on a target, psychological attacks are the most dangerous. Victims"" emotions are the most vulnerable asset in a phishing operation and it is the attacker's main priority. To efficiently be able to reduce the effects of such an attack, it is essential to ask ourselves, what emotion are they trying to break? Our study focuses on applying machine learning to train a model to be able to classify the emotion present from a group of words. The model is trained using a large dataset of social sentences and phrases with differing emotions. We match positive emotions to positive sentiments, and vice versa. Likewise, neutral emotions are matched to neutral sentiments. The model is trained against the resulting dataset of emotions and related text.

Section 2 is a literature review section where we shall look at various contributions to detect, improve awareness, or counteract phishing. Section 3 is the methodology section. Insights of the attacking methods hackers use to achieve their phishing attempts are discussed here. The implementation methodology of our emotion detection scheme to capture the emotion attackers exploit in their phishing email is explained. Section 4 is the results and discussions section. We talk of the findings of what factors motivate hackers to perform phishing and make victims more susceptible to phishing attacks. We also look into how our emotion detection scheme performs. Section 5 is the conclusion section where we conclude our exploratory research review and propose future studies to enhance phishing detection schemes.","[['b11', 'b6', 'b13', 'b9'], []]","[['b11', 'b6', 'b13', 'b9'], []]",4,"sent1: A reverse shell is a connection shell that is virtual and open to the attacker""s machine that initiates from the victims"" device.
sent2: With an open connection, an attacker is capable of running scripts and executing various evil commands on the victim""s machine.
sent3: This mechanism is normally used by many common RATs and backdoors.
sent4: The hacker gains the privileges of the user who was logged in when the session was initiated and may work to elevate privileges to gain further access.
sent5: Most reverse shells use the Transmission Control Protocol (TCP) for the establishment, but the Internet Control Message Protocol (ICMP) has also been observed in use.
sent6: Any port may be used to create the communication link.
sent7: When hiding under allowed ports, the firewall and other intrusion detection systems face difficulty in identifying an incident because the ports are approved to communicate.
sent8: When the shell is run under port 443 Secure Sockets Layer (SSL), content inspection is very hard as the traffic is encrypted.
sent9: (Lu, 2019) Virus A computer virus is a dangerous piece of code that self-replicates by infecting other programs within its reach by injecting malicious code in them.
sent10: Many viruses are dependent on some kind of executable, of which they are hosted by, before they may be able to start running their code.
sent11: They may become very severe, to the point where they may completely ruin all of your hardware and software.
sent12: A virus cannot propagate without human intervention.
sent13: This is what makes it different from a worm.
sent14: A virus needs some kind of host program to keep it in action.
sent15: Viruses may be spread very rapidly and unintentionally as victims may tend to share poisoned files or send attachments in the email that have been infected.
sent16: (Kumar & Dey, 2019) Worm A computer worm is a malicious program that replicates on its own without the need of interacting with any other file, and spreads across machines in a network.
sent17: It transmits copies of its code throughout the network without attaching to any programs like in the case of a virus.
sent18: They normally consume a large amount of the network bandwidth and cause a lack of service availability to users on the network.
sent19: The first case of a worm propagating over the internet was the Morris worm released by Robert Tappan Morris on November 2, 1988.
sent20: (Jajoo, 2017) Rootkit A rootkit is a program written primarily for evading detection while maintaining privileged access to the system.
sent21: Host Intrusion Detection Systems (HIDS), Host Intrusion Prevention Systems (HIPS), Network Intrusion Detection Systems (NIDS), Network Intrusion Prevention Systems (NIPS), firewalls, Security Information and Event Management (SIEM) solutions all find it very hard to detect rootkits as they can modify their underlying operating system kernel code that they utilized for detection and thus usurping the control of security.
sent22: (Nadim et al., 2021) Despite the various technical attacks which a hacker may deploy on a target, psychological attacks are the most dangerous.
sent23: Victims"" emotions are the most vulnerable asset in a phishing operation and it is the attacker's main priority.
sent24: To efficiently be able to reduce the effects of such an attack, it is essential to ask ourselves, what emotion are they trying to break?
sent25: Our study focuses on applying machine learning to train a model to be able to classify the emotion present from a group of words.
sent26: The model is trained using a large dataset of social sentences and phrases with differing emotions.
sent27: We match positive emotions to positive sentiments, and vice versa.
sent28: Likewise, neutral emotions are matched to neutral sentiments.
sent29: The model is trained against the resulting dataset of emotions and related text.
sent30: Section 2 is a literature review section where we shall look at various contributions to detect, improve awareness, or counteract phishing.
sent31: Section 3 is the methodology section.
sent32: Insights of the attacking methods hackers use to achieve their phishing attempts are discussed here.
sent33: The implementation methodology of our emotion detection scheme to capture the emotion attackers exploit in their phishing email is explained.
sent34: Section 4 is the results and discussions section.
sent35: We talk of the findings of what factors motivate hackers to perform phishing and make victims more susceptible to phishing attacks.
sent36: We also look into how our emotion detection scheme performs.
sent37: Section 5 is the conclusion section where we conclude our exploratory research review and propose future studies to enhance phishing detection schemes."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s19,Text heatmap.,"The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].

The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].","[['b60'], ['b133'], ['b60'], ['b133']]","[['b60'], ['b133'], ['b60'], ['b133']]",4,"sent1: The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient.
sent2: In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output.
sent3: With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.
sent4: Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example.
sent5: However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3.
sent6: Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works.
sent7: Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].
sent8: The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient.
sent9: In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output.
sent10: With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.
sent11: Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example.
sent12: However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3.
sent13: Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works.
sent14: Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134]."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s21,Text quality metrics.,"The methods in this category measure general quality aspects of the generated text, and are originated from translation, summarizing or captioning tasks. The most widely used metrics in the papers reviewed are BLEU [102], ROUGE-L [90], METEOR [12,82] and CIDEr [140], which measure the similarity of a target text (also referred to as candidate), against one or more reference texts (ground truth). These metrics are mainly based on counting n-gram matchings between the candidate and the ground truth. BLEU is precision-oriented, ROUGE-L and METEOR are F1-scores that can be biased towards precision or recall with a given parameter, and CIDEr attempts to capture both precision and recall through a TF-IDF score. Most of these metrics have variants and parameters for their calculation: ROUGE is a set of multiple metrics, being ROUGE-L the only one used in this task; METEOR has variants presented by the same authors [30][31][32]; and CIDEr was presented with the CIDEr-D variant to prevent gameability effects. SPICE [9] is a metric designed for the image captioning task, and evaluates the underlying meaning of the sentences describing the image scene, partially disregarding fluency or grammatical aspects. Specifically, the text is parsed as a graph, capturing the objects, their described characteristics and relations, which are then measured against the ground truth using an F1-score. Even though SPICE attempts to assess the semantic information in a caption, we believe it is not suitable for medical reports, as the graph parsing is designed for general domain objects. Nonetheless, Zhang et al. [162] presented the medical correctness metric MIRQI, applying a similar idea in a specific medical domain, which we will discuss in the next subsection (5.4.2).

Besides standard captioning metrics, we identified two other approaches to measure text quality. First, Alsharid et al. [7] used Grammar Bot 3 , a rule and statistics based automated system that counts the grammatical errors in sentences. Second, Harzig et al. 2019a [48] measured the sentence variability, by counting the different sentences in a set of reports. They argue that the sentences indicating abnormalities occur very rarely in the dataset, while the ones indicating normality are the most frequent. Hence, a certain level of variability is desired, and a system generating reports with low variability may indicate that not all medical conditions are being captured.

Lastly, both works from Li et al. [85,86] performed human evaluation with non-expert users via Amazon Mechanical Turk (AMT), following the same procedure. The authors presented two reports to the AMT participants, one generated with the proposed model and one generated with the CoAtt model [68] as baseline, and asked them to choose the most similar with the ground truth in terms of fluency, abnormalities correctness and content coverage. The results shown that their report was preferred around 50-60% of the cases, while the baseline around 20-30% (for the rest, none or both were preferred). We categorize this evaluation as a text quality metric, as the participants are not experts, and their answers are not fine-grained (i.e., did not specify what failed: fluency, correctness or coverage; or by how much they failed). 5.4.2 Medical correctness metrics. While the most common purpose of the text quality metrics is to measure the similarity between the generated report and a ground truth, they do not necessarily capture the medical facts in the reports [11,17,92,107,108,161]. For example, the sentences ""effusion is observed"" and ""effusion is not observed"" are very similar, thus may present a very high score for any metric based on n-gram matching, though the medical facts are the exact opposite. Therefore, an evaluation directly measuring the reports correctness is required, not necessarily taking into account fluency, grammatical rules or text quality in general. From the literature reviewed, in ten works [7,16,61,67,85,92,98,148,154,162] the authors presented an automatic metric to address this issue, four works [7,38,126,131] did a formal expert evaluation, and multiple works [49, 76, 86, 89, 94, 120, 126, 131, 144, 155-158, 162, 163] evaluated medical correctness indirectly from auxiliary tasks. The methods are listed in Table 6 and are further discussed next.

In several works the authors presented a method that detects concepts in the generated and ground truth reports, and compare the results using common classification metrics, such as accuracy, F1-score, and more. The main difference between these methods lies in how the concepts are automatically detected in the reports. The simplest approaches are keyword-based, which consists in reporting the ratio of a set of keywords found between the generated report and ground truth, like MeSH Accuracy [61] that uses MeSH terms, and Keyword Accuracy that uses 438 MTI terms (presented by Xue et al. [154] and used in A3FN [150]). Similarly, Medical Abnormality Terminology Detection [85] calculates precision and false positive rate of the 10 most frequent abnormalityrelated terms in the dataset; and Wu et al. [148] calculated accuracy, sensitivity and specificity for a set of keywords.

Other approaches are abnormality-based, which attempt to directly classify abnormalities from the report by different means: Abnormality Detection [67] uses manually designed patterns; Medical Abnormality Detection [92] uses the CheXpert labeler tool [63]; Biswal et al. [16] used a character-level CNN [160] that classifies multiple CheXpert labels [63]; and Moradi et al. [98] used a proprietary software to extract semantic descriptors. Lastly, Anatomical Relevance Score (ARS) [7] is a body-part-based approach, which detects the anatomical elements mentioned in a report considering the vocabulary used. Though these methods may be useful for measuring medical correctness to a certain degree, there is no consensus or standard, and there is no formal evaluation of the correlation with expert judgement. From the discussed techniques, Alsharid et al. [7] are the only authors that also performed an expert evaluation of the generated reports, though they did not conduct a correlation or similar analysis to validate the ARS method.

Zhang et al. [162] went further with the concept extraction and presented Medical Image Report Quality Index (MIRQI), which works in a similar fashion as the SPICE [9] metric presented in the text quality subsection (5.4.1). MIRQI applies ideas from NegBio [106] and the CheXpert labeler [63] to identify diseases or medical conditions in the reports, considering synonyms and negations, and uses the Stanford parser [23] to obtain semantic dependencies and finer-grained attributes from each sentence, such as severity, size, shape, body parts, etc. With this information, an abnormality graph is built for each report, where each node is a disease with its attributes, and the nodes are connected if they belong to the same organ or tissue. Lastly, the graphs from the ground truth and generated reports are matched node-wise, and MIRQI-p (precision), MIRQI-r (recall) and MIRQI-F1 (F1-score) are computed. Compared to the formerly discussed correctness metrics, we believe this approach seems more robust to assess the medical facts in the reports, as it attempts to capture the attributes and relations, opposed to the concepts only. However, the authors did not present an evaluation against expert judgement, so we cannot determine if this metric is sufficient.

Considering human evaluation, only a few works [7,38,126,131] present a formal expert medical correctness assessment. In the work by Alsharid et al. [7] a medical professional assessed the reports on a Likert Scale from 0 to 2 in four different aspects: accurately describes the image, presents no incorrect information, is grammatically correct and is relevant for the image; the results were further separated for samples from different body parts, showing averages between 0.5 and 1. Gale et al. [38] asked a radiologist to evaluate the correctness of the hip fractures description, finding that the fracture's character was properly described 98% of the cases, while the fracture location only for 90%. In the work by Tian et al. [131] a medical expert evaluated 30 randomly selected reports with a rating from 1 (definite accept) to 5 (definite reject), scoring an average of 2.33. Lastly, Spinks and Moens [126] asked four questions to three experts regarding the generated reports, where the third and fourth questions measured correctness: ""Do you agree with the proposed diagnosis?"", answering 0 (no) or 1 (yes) and ""How certain are you about your final diagnosis?"", from 1 (not sure) to 4 (very sure). The average scores were high (0.88 and 3.75), showing agreement with the model's diagnosis, and certainty on the experts' diagnoses. The other questions concerned explainability aspects, and are detailed in the next subsection (5.4.3). So far, there is no standard approach to perform an expert evaluation, though we believe the first two approaches provide finer-grained information than the latter two, and hence should be more useful for determining in which cases the models are failing and for designing improvements. The certainty question should also be very useful, as diagnoses may be susceptible to human judgement.

Lastly, multiple papers [49,86,89,94,120,131,144,155,156,158,162,163] evaluated the performance of the auxiliary tasks with ROC-AUC, accuracy and other typical classification or segmentation metrics, as shown in Table 6. Note that in any of these cases, the task is a previous or intermediary step of the process and is not derived from the report. In consequence, even if the classification has great performance, the language component could be performing poorly, and the generated reports still may be inaccurate. Accordingly, we believe this type of measure should not be used as the primary report correctness evaluation, unless it can be proven that the report reproduces exactly the classification made (e.g. by a template-filling process).

The methods in this category measure general quality aspects of the generated text, and are originated from translation, summarizing or captioning tasks. The most widely used metrics in the papers reviewed are BLEU [102], ROUGE-L [90], METEOR [12,82] and CIDEr [140], which measure the similarity of a target text (also referred to as candidate), against one or more reference texts (ground truth). These metrics are mainly based on counting n-gram matchings between the candidate and the ground truth. BLEU is precision-oriented, ROUGE-L and METEOR are F1-scores that can be biased towards precision or recall with a given parameter, and CIDEr attempts to capture both precision and recall through a TF-IDF score. Most of these metrics have variants and parameters for their calculation: ROUGE is a set of multiple metrics, being ROUGE-L the only one used in this task; METEOR has variants presented by the same authors [30][31][32]; and CIDEr was presented with the CIDEr-D variant to prevent gameability effects. SPICE [9] is a metric designed for the image captioning task, and evaluates the underlying meaning of the sentences describing the image scene, partially disregarding fluency or grammatical aspects. Specifically, the text is parsed as a graph, capturing the objects, their described characteristics and relations, which are then measured against the ground truth using an F1-score. Even though SPICE attempts to assess the semantic information in a caption, we believe it is not suitable for medical reports, as the graph parsing is designed for general domain objects. Nonetheless, Zhang et al. [162] presented the medical correctness metric MIRQI, applying a similar idea in a specific medical domain, which we will discuss in the next subsection (5.4.2).

Besides standard captioning metrics, we identified two other approaches to measure text quality. First, Alsharid et al. [7] used Grammar Bot 3 , a rule and statistics based automated system that counts the grammatical errors in sentences. Second, Harzig et al. 2019a [48] measured the sentence variability, by counting the different sentences in a set of reports. They argue that the sentences indicating abnormalities occur very rarely in the dataset, while the ones indicating normality are the most frequent. Hence, a certain level of variability is desired, and a system generating reports with low variability may indicate that not all medical conditions are being captured.

Lastly, both works from Li et al. [85,86] performed human evaluation with non-expert users via Amazon Mechanical Turk (AMT), following the same procedure. The authors presented two reports to the AMT participants, one generated with the proposed model and one generated with the CoAtt model [68] as baseline, and asked them to choose the most similar with the ground truth in terms of fluency, abnormalities correctness and content coverage. The results shown that their report was preferred around 50-60% of the cases, while the baseline around 20-30% (for the rest, none or both were preferred). We categorize this evaluation as a text quality metric, as the participants are not experts, and their answers are not fine-grained (i.e., did not specify what failed: fluency, correctness or coverage; or by how much they failed). 5.4.2 Medical correctness metrics. While the most common purpose of the text quality metrics is to measure the similarity between the generated report and a ground truth, they do not necessarily capture the medical facts in the reports [11,17,92,107,108,161]. For example, the sentences ""effusion is observed"" and ""effusion is not observed"" are very similar, thus may present a very high score for any metric based on n-gram matching, though the medical facts are the exact opposite. Therefore, an evaluation directly measuring the reports correctness is required, not necessarily taking into account fluency, grammatical rules or text quality in general. From the literature reviewed, in ten works [7,16,61,67,85,92,98,148,154,162] the authors presented an automatic metric to address this issue, four works [7,38,126,131] did a formal expert evaluation, and multiple works [49, 76, 86, 89, 94, 120, 126, 131, 144, 155-158, 162, 163] evaluated medical correctness indirectly from auxiliary tasks. The methods are listed in Table 6 and are further discussed next.

In several works the authors presented a method that detects concepts in the generated and ground truth reports, and compare the results using common classification metrics, such as accuracy, F1-score, and more. The main difference between these methods lies in how the concepts are automatically detected in the reports. The simplest approaches are keyword-based, which consists in reporting the ratio of a set of keywords found between the generated report and ground truth, like MeSH Accuracy [61] that uses MeSH terms, and Keyword Accuracy that uses 438 MTI terms (presented by Xue et al. [154] and used in A3FN [150]). Similarly, Medical Abnormality Terminology Detection [85] calculates precision and false positive rate of the 10 most frequent abnormalityrelated terms in the dataset; and Wu et al. [148] calculated accuracy, sensitivity and specificity for a set of keywords.

Other approaches are abnormality-based, which attempt to directly classify abnormalities from the report by different means: Abnormality Detection [67] uses manually designed patterns; Medical Abnormality Detection [92] uses the CheXpert labeler tool [63]; Biswal et al. [16] used a character-level CNN [160] that classifies multiple CheXpert labels [63]; and Moradi et al. [98] used a proprietary software to extract semantic descriptors. Lastly, Anatomical Relevance Score (ARS) [7] is a body-part-based approach, which detects the anatomical elements mentioned in a report considering the vocabulary used. Though these methods may be useful for measuring medical correctness to a certain degree, there is no consensus or standard, and there is no formal evaluation of the correlation with expert judgement. From the discussed techniques, Alsharid et al. [7] are the only authors that also performed an expert evaluation of the generated reports, though they did not conduct a correlation or similar analysis to validate the ARS method.

Zhang et al. [162] went further with the concept extraction and presented Medical Image Report Quality Index (MIRQI), which works in a similar fashion as the SPICE [9] metric presented in the text quality subsection (5.4.1). MIRQI applies ideas from NegBio [106] and the CheXpert labeler [63] to identify diseases or medical conditions in the reports, considering synonyms and negations, and uses the Stanford parser [23] to obtain semantic dependencies and finer-grained attributes from each sentence, such as severity, size, shape, body parts, etc. With this information, an abnormality graph is built for each report, where each node is a disease with its attributes, and the nodes are connected if they belong to the same organ or tissue. Lastly, the graphs from the ground truth and generated reports are matched node-wise, and MIRQI-p (precision), MIRQI-r (recall) and MIRQI-F1 (F1-score) are computed. Compared to the formerly discussed correctness metrics, we believe this approach seems more robust to assess the medical facts in the reports, as it attempts to capture the attributes and relations, opposed to the concepts only. However, the authors did not present an evaluation against expert judgement, so we cannot determine if this metric is sufficient.

Considering human evaluation, only a few works [7,38,126,131] present a formal expert medical correctness assessment. In the work by Alsharid et al. [7] a medical professional assessed the reports on a Likert Scale from 0 to 2 in four different aspects: accurately describes the image, presents no incorrect information, is grammatically correct and is relevant for the image; the results were further separated for samples from different body parts, showing averages between 0.5 and 1. Gale et al. [38] asked a radiologist to evaluate the correctness of the hip fractures description, finding that the fracture's character was properly described 98% of the cases, while the fracture location only for 90%. In the work by Tian et al. [131] a medical expert evaluated 30 randomly selected reports with a rating from 1 (definite accept) to 5 (definite reject), scoring an average of 2.33. Lastly, Spinks and Moens [126] asked four questions to three experts regarding the generated reports, where the third and fourth questions measured correctness: ""Do you agree with the proposed diagnosis?"", answering 0 (no) or 1 (yes) and ""How certain are you about your final diagnosis?"", from 1 (not sure) to 4 (very sure). The average scores were high (0.88 and 3.75), showing agreement with the model's diagnosis, and certainty on the experts' diagnoses. The other questions concerned explainability aspects, and are detailed in the next subsection (5.4.3). So far, there is no standard approach to perform an expert evaluation, though we believe the first two approaches provide finer-grained information than the latter two, and hence should be more useful for determining in which cases the models are failing and for designing improvements. The certainty question should also be very useful, as diagnoses may be susceptible to human judgement.

Lastly, multiple papers [49,86,89,94,120,131,144,155,156,158,162,163] evaluated the performance of the auxiliary tasks with ROC-AUC, accuracy and other typical classification or segmentation metrics, as shown in Table 6. Note that in any of these cases, the task is a previous or intermediary step of the process and is not derived from the report. In consequence, even if the classification has great performance, the language component could be performing poorly, and the generated reports still may be inaccurate. Accordingly, we believe this type of measure should not be used as the primary report correctness evaluation, unless it can be proven that the report reproduces exactly the classification made (e.g. by a template-filling process).","[['b29', 'b8', 'b161', 'b101', 'b81', 'b89', None, 'b31', 'b30', 'b11', 'b139'], ['b47', 'b6'], ['b67', 'b153', 'b85', 'b161', 'b97', 'b91', 'b147', 'b130', 'b84', 'b66', 'b106', 'b6', 'b16', 'b60', 'b37', 'b107', 'b15', 'b10', 'b125', None, 'b160'], ['b153', 'b60', 'b84', 'b147', 'b149'], ['b62', 'b159', 'b66', 'b97', 'b91', 'b15', 'b6'], ['b8', 'b105', 'b62', 'b161', 'b22'], ['b125', 'b37', None, 'b130', 'b6'], ['b88', 'b85', 'b119', 'b157', 'b161', 'b155', 'b143', 'b162', 'b48', 'b93', 'b130', 'b154'], ['b29', 'b8', 'b161', 'b101', 'b81', 'b89', None, 'b31', 'b30', 'b11', 'b139'], ['b47', 'b6'], ['b67', 'b153', 'b85', 'b161', 'b97', 'b91', 'b147', 'b130', 'b84', 'b66', 'b106', 'b6', 'b16', 'b60', 'b37', 'b107', 'b15', 'b10', 'b125', None, 'b160'], ['b153', 'b60', 'b84', 'b147', 'b149'], ['b62', 'b159', 'b66', 'b97', 'b91', 'b15', 'b6'], ['b8', 'b105', 'b62', 'b161', 'b22'], ['b125', 'b37', None, 'b130', 'b6'], ['b88', 'b85', 'b119', 'b157', 'b161', 'b155', 'b143', 'b162', 'b48', 'b93', 'b130', 'b154']]","[['b29', 'b8', 'b161', 'b101', 'b81', 'b89', None, 'b31', 'b30', 'b11', 'b139'], ['b47', 'b6'], ['b67', 'b153', 'b85', 'b161', 'b97', 'b91', 'b147', 'b130', 'b84', 'b66', 'b106', 'b6', 'b16', 'b60', 'b37', 'b107', 'b15', 'b10', 'b125', None, 'b160'], ['b153', 'b60', 'b84', 'b147', 'b149'], ['b62', 'b159', 'b66', 'b97', 'b91', 'b15', 'b6'], ['b8', 'b105', 'b62', 'b161', 'b22'], ['b125', 'b37', None, 'b130', 'b6'], ['b88', 'b85', 'b119', 'b157', 'b161', 'b155', 'b143', 'b162', 'b48', 'b93', 'b130', 'b154'], ['b29', 'b8', 'b161', 'b101', 'b81', 'b89', None, 'b31', 'b30', 'b11', 'b139'], ['b47', 'b6'], ['b67', 'b153', 'b85', 'b161', 'b97', 'b91', 'b147', 'b130', 'b84', 'b66', 'b106', 'b6', 'b16', 'b60', 'b37', 'b107', 'b15', 'b10', 'b125', None, 'b160'], ['b153', 'b60', 'b84', 'b147', 'b149'], ['b62', 'b159', 'b66', 'b97', 'b91', 'b15', 'b6'], ['b8', 'b105', 'b62', 'b161', 'b22'], ['b125', 'b37', None, 'b130', 'b6'], ['b88', 'b85', 'b119', 'b157', 'b161', 'b155', 'b143', 'b162', 'b48', 'b93', 'b130', 'b154']]",136,"sent1: The methods in this category measure general quality aspects of the generated text, and are originated from translation, summarizing or captioning tasks.
sent2: The most widely used metrics in the papers reviewed are BLEU [102], ROUGE-L [90], METEOR [12,82] and CIDEr [140], which measure the similarity of a target text (also referred to as candidate), against one or more reference texts (ground truth).
sent3: These metrics are mainly based on counting n-gram matchings between the candidate and the ground truth.
sent4: BLEU is precision-oriented, ROUGE-L and METEOR are F1-scores that can be biased towards precision or recall with a given parameter, and CIDEr attempts to capture both precision and recall through a TF-IDF score.
sent5: Most of these metrics have variants and parameters for their calculation: ROUGE is a set of multiple metrics, being ROUGE-L the only one used in this task; METEOR has variants presented by the same authors [30][31][32]; and CIDEr was presented with the CIDEr-D variant to prevent gameability effects.
sent6: SPICE [9] is a metric designed for the image captioning task, and evaluates the underlying meaning of the sentences describing the image scene, partially disregarding fluency or grammatical aspects.
sent7: Specifically, the text is parsed as a graph, capturing the objects, their described characteristics and relations, which are then measured against the ground truth using an F1-score.
sent8: Even though SPICE attempts to assess the semantic information in a caption, we believe it is not suitable for medical reports, as the graph parsing is designed for general domain objects.
sent9: Nonetheless, Zhang et al. [162] presented the medical correctness metric MIRQI, applying a similar idea in a specific medical domain, which we will discuss in the next subsection (5.4.2).
sent10: Besides standard captioning metrics, we identified two other approaches to measure text quality.
sent11: First, Alsharid et al. [7] used Grammar Bot 3 , a rule and statistics based automated system that counts the grammatical errors in sentences.
sent12: Second, Harzig et al. 2019a [48] measured the sentence variability, by counting the different sentences in a set of reports.
sent13: They argue that the sentences indicating abnormalities occur very rarely in the dataset, while the ones indicating normality are the most frequent.
sent14: Hence, a certain level of variability is desired, and a system generating reports with low variability may indicate that not all medical conditions are being captured.
sent15: Lastly, both works from Li et al. [85,86] performed human evaluation with non-expert users via Amazon Mechanical Turk (AMT), following the same procedure.
sent16: The authors presented two reports to the AMT participants, one generated with the proposed model and one generated with the CoAtt model [68] as baseline, and asked them to choose the most similar with the ground truth in terms of fluency, abnormalities correctness and content coverage.
sent17: The results shown that their report was preferred around 50-60% of the cases, while the baseline around 20-30% (for the rest, none or both were preferred).
sent18: We categorize this evaluation as a text quality metric, as the participants are not experts, and their answers are not fine-grained (i.e., did not specify what failed: fluency, correctness or coverage; or by how much they failed).
sent19: 5.4.2 Medical correctness metrics.
sent20: While the most common purpose of the text quality metrics is to measure the similarity between the generated report and a ground truth, they do not necessarily capture the medical facts in the reports [11,17,92,107,108,161].
sent21: For example, the sentences ""effusion is observed"" and ""effusion is not observed"" are very similar, thus may present a very high score for any metric based on n-gram matching, though the medical facts are the exact opposite.
sent22: Therefore, an evaluation directly measuring the reports correctness is required, not necessarily taking into account fluency, grammatical rules or text quality in general.
sent23: From the literature reviewed, in ten works [7,16,61,67,85,92,98,148,154,162] the authors presented an automatic metric to address this issue, four works [7,38,126,131] did a formal expert evaluation, and multiple works [49, 76, 86, 89, 94, 120, 126, 131, 144, 155-158, 162, 163] evaluated medical correctness indirectly from auxiliary tasks.
sent24: The methods are listed in Table 6 and are further discussed next.
sent25: In several works the authors presented a method that detects concepts in the generated and ground truth reports, and compare the results using common classification metrics, such as accuracy, F1-score, and more.
sent26: The main difference between these methods lies in how the concepts are automatically detected in the reports.
sent27: The simplest approaches are keyword-based, which consists in reporting the ratio of a set of keywords found between the generated report and ground truth, like MeSH Accuracy [61] that uses MeSH terms, and Keyword Accuracy that uses 438 MTI terms (presented by Xue et al. [154] and used in A3FN [150]).
sent28: Similarly, Medical Abnormality Terminology Detection [85] calculates precision and false positive rate of the 10 most frequent abnormalityrelated terms in the dataset; and Wu et al. [148] calculated accuracy, sensitivity and specificity for a set of keywords.
sent29: Other approaches are abnormality-based, which attempt to directly classify abnormalities from the report by different means: Abnormality Detection [67] uses manually designed patterns; Medical Abnormality Detection [92] uses the CheXpert labeler tool [63]; Biswal et al. [16] used a character-level CNN [160] that classifies multiple CheXpert labels [63]; and Moradi et al. [98] used a proprietary software to extract semantic descriptors.
sent30: Lastly, Anatomical Relevance Score (ARS) [7] is a body-part-based approach, which detects the anatomical elements mentioned in a report considering the vocabulary used.
sent31: Though these methods may be useful for measuring medical correctness to a certain degree, there is no consensus or standard, and there is no formal evaluation of the correlation with expert judgement.
sent32: From the discussed techniques, Alsharid et al. [7] are the only authors that also performed an expert evaluation of the generated reports, though they did not conduct a correlation or similar analysis to validate the ARS method.
sent33: Zhang et al. [162] went further with the concept extraction and presented Medical Image Report Quality Index (MIRQI), which works in a similar fashion as the SPICE [9] metric presented in the text quality subsection (5.4.1).
sent34: MIRQI applies ideas from NegBio [106] and the CheXpert labeler [63] to identify diseases or medical conditions in the reports, considering synonyms and negations, and uses the Stanford parser [23] to obtain semantic dependencies and finer-grained attributes from each sentence, such as severity, size, shape, body parts, etc.
sent35: With this information, an abnormality graph is built for each report, where each node is a disease with its attributes, and the nodes are connected if they belong to the same organ or tissue.
sent36: Lastly, the graphs from the ground truth and generated reports are matched node-wise, and MIRQI-p (precision), MIRQI-r (recall) and MIRQI-F1 (F1-score) are computed.
sent37: Compared to the formerly discussed correctness metrics, we believe this approach seems more robust to assess the medical facts in the reports, as it attempts to capture the attributes and relations, opposed to the concepts only.
sent38: However, the authors did not present an evaluation against expert judgement, so we cannot determine if this metric is sufficient.
sent39: Considering human evaluation, only a few works [7,38,126,131] present a formal expert medical correctness assessment.
sent40: In the work by Alsharid et al. [7] a medical professional assessed the reports on a Likert Scale from 0 to 2 in four different aspects: accurately describes the image, presents no incorrect information, is grammatically correct and is relevant for the image; the results were further separated for samples from different body parts, showing averages between 0.5 and 1.
sent41: Gale et al. [38] asked a radiologist to evaluate the correctness of the hip fractures description, finding that the fracture's character was properly described 98% of the cases, while the fracture location only for 90%.
sent42: In the work by Tian et al. [131] a medical expert evaluated 30 randomly selected reports with a rating from 1 (definite accept) to 5 (definite reject), scoring an average of 2.33.
sent43: Lastly, Spinks and Moens [126] asked four questions to three experts regarding the generated reports, where the third and fourth questions measured correctness: ""Do you agree with the proposed diagnosis?"", answering 0 (no) or 1 (yes) and ""How certain are you about your final diagnosis?"", from 1 (not sure) to 4 (very sure).
sent44: The average scores were high (0.88 and 3.75), showing agreement with the model's diagnosis, and certainty on the experts' diagnoses.
sent45: The other questions concerned explainability aspects, and are detailed in the next subsection (5.4.3).
sent46: So far, there is no standard approach to perform an expert evaluation, though we believe the first two approaches provide finer-grained information than the latter two, and hence should be more useful for determining in which cases the models are failing and for designing improvements.
sent47: The certainty question should also be very useful, as diagnoses may be susceptible to human judgement.
sent48: Lastly, multiple papers [49,86,89,94,120,131,144,155,156,158,162,163] evaluated the performance of the auxiliary tasks with ROC-AUC, accuracy and other typical classification or segmentation metrics, as shown in Table 6.
sent49: Note that in any of these cases, the task is a previous or intermediary step of the process and is not derived from the report.
sent50: In consequence, even if the classification has great performance, the language component could be performing poorly, and the generated reports still may be inaccurate.
sent51: Accordingly, we believe this type of measure should not be used as the primary report correctness evaluation, unless it can be proven that the report reproduces exactly the classification made (e.g. by a template-filling process).
sent52: The methods in this category measure general quality aspects of the generated text, and are originated from translation, summarizing or captioning tasks.
sent53: The most widely used metrics in the papers reviewed are BLEU [102], ROUGE-L [90], METEOR [12,82] and CIDEr [140], which measure the similarity of a target text (also referred to as candidate), against one or more reference texts (ground truth).
sent54: These metrics are mainly based on counting n-gram matchings between the candidate and the ground truth.
sent55: BLEU is precision-oriented, ROUGE-L and METEOR are F1-scores that can be biased towards precision or recall with a given parameter, and CIDEr attempts to capture both precision and recall through a TF-IDF score.
sent56: Most of these metrics have variants and parameters for their calculation: ROUGE is a set of multiple metrics, being ROUGE-L the only one used in this task; METEOR has variants presented by the same authors [30][31][32]; and CIDEr was presented with the CIDEr-D variant to prevent gameability effects.
sent57: SPICE [9] is a metric designed for the image captioning task, and evaluates the underlying meaning of the sentences describing the image scene, partially disregarding fluency or grammatical aspects.
sent58: Specifically, the text is parsed as a graph, capturing the objects, their described characteristics and relations, which are then measured against the ground truth using an F1-score.
sent59: Even though SPICE attempts to assess the semantic information in a caption, we believe it is not suitable for medical reports, as the graph parsing is designed for general domain objects.
sent60: Nonetheless, Zhang et al. [162] presented the medical correctness metric MIRQI, applying a similar idea in a specific medical domain, which we will discuss in the next subsection (5.4.2).
sent61: Besides standard captioning metrics, we identified two other approaches to measure text quality.
sent62: First, Alsharid et al. [7] used Grammar Bot 3 , a rule and statistics based automated system that counts the grammatical errors in sentences.
sent63: Second, Harzig et al. 2019a [48] measured the sentence variability, by counting the different sentences in a set of reports.
sent64: They argue that the sentences indicating abnormalities occur very rarely in the dataset, while the ones indicating normality are the most frequent.
sent65: Hence, a certain level of variability is desired, and a system generating reports with low variability may indicate that not all medical conditions are being captured.
sent66: Lastly, both works from Li et al. [85,86] performed human evaluation with non-expert users via Amazon Mechanical Turk (AMT), following the same procedure.
sent67: The authors presented two reports to the AMT participants, one generated with the proposed model and one generated with the CoAtt model [68] as baseline, and asked them to choose the most similar with the ground truth in terms of fluency, abnormalities correctness and content coverage.
sent68: The results shown that their report was preferred around 50-60% of the cases, while the baseline around 20-30% (for the rest, none or both were preferred).
sent69: We categorize this evaluation as a text quality metric, as the participants are not experts, and their answers are not fine-grained (i.e., did not specify what failed: fluency, correctness or coverage; or by how much they failed).
sent70: 5.4.2 Medical correctness metrics.
sent71: While the most common purpose of the text quality metrics is to measure the similarity between the generated report and a ground truth, they do not necessarily capture the medical facts in the reports [11,17,92,107,108,161].
sent72: For example, the sentences ""effusion is observed"" and ""effusion is not observed"" are very similar, thus may present a very high score for any metric based on n-gram matching, though the medical facts are the exact opposite.
sent73: Therefore, an evaluation directly measuring the reports correctness is required, not necessarily taking into account fluency, grammatical rules or text quality in general.
sent74: From the literature reviewed, in ten works [7,16,61,67,85,92,98,148,154,162] the authors presented an automatic metric to address this issue, four works [7,38,126,131] did a formal expert evaluation, and multiple works [49, 76, 86, 89, 94, 120, 126, 131, 144, 155-158, 162, 163] evaluated medical correctness indirectly from auxiliary tasks.
sent75: The methods are listed in Table 6 and are further discussed next.
sent76: In several works the authors presented a method that detects concepts in the generated and ground truth reports, and compare the results using common classification metrics, such as accuracy, F1-score, and more.
sent77: The main difference between these methods lies in how the concepts are automatically detected in the reports.
sent78: The simplest approaches are keyword-based, which consists in reporting the ratio of a set of keywords found between the generated report and ground truth, like MeSH Accuracy [61] that uses MeSH terms, and Keyword Accuracy that uses 438 MTI terms (presented by Xue et al. [154] and used in A3FN [150]).
sent79: Similarly, Medical Abnormality Terminology Detection [85] calculates precision and false positive rate of the 10 most frequent abnormalityrelated terms in the dataset; and Wu et al. [148] calculated accuracy, sensitivity and specificity for a set of keywords.
sent80: Other approaches are abnormality-based, which attempt to directly classify abnormalities from the report by different means: Abnormality Detection [67] uses manually designed patterns; Medical Abnormality Detection [92] uses the CheXpert labeler tool [63]; Biswal et al. [16] used a character-level CNN [160] that classifies multiple CheXpert labels [63]; and Moradi et al. [98] used a proprietary software to extract semantic descriptors.
sent81: Lastly, Anatomical Relevance Score (ARS) [7] is a body-part-based approach, which detects the anatomical elements mentioned in a report considering the vocabulary used.
sent82: Though these methods may be useful for measuring medical correctness to a certain degree, there is no consensus or standard, and there is no formal evaluation of the correlation with expert judgement.
sent83: From the discussed techniques, Alsharid et al. [7] are the only authors that also performed an expert evaluation of the generated reports, though they did not conduct a correlation or similar analysis to validate the ARS method.
sent84: Zhang et al. [162] went further with the concept extraction and presented Medical Image Report Quality Index (MIRQI), which works in a similar fashion as the SPICE [9] metric presented in the text quality subsection (5.4.1).
sent85: MIRQI applies ideas from NegBio [106] and the CheXpert labeler [63] to identify diseases or medical conditions in the reports, considering synonyms and negations, and uses the Stanford parser [23] to obtain semantic dependencies and finer-grained attributes from each sentence, such as severity, size, shape, body parts, etc.
sent86: With this information, an abnormality graph is built for each report, where each node is a disease with its attributes, and the nodes are connected if they belong to the same organ or tissue.
sent87: Lastly, the graphs from the ground truth and generated reports are matched node-wise, and MIRQI-p (precision), MIRQI-r (recall) and MIRQI-F1 (F1-score) are computed.
sent88: Compared to the formerly discussed correctness metrics, we believe this approach seems more robust to assess the medical facts in the reports, as it attempts to capture the attributes and relations, opposed to the concepts only.
sent89: However, the authors did not present an evaluation against expert judgement, so we cannot determine if this metric is sufficient.
sent90: Considering human evaluation, only a few works [7,38,126,131] present a formal expert medical correctness assessment.
sent91: In the work by Alsharid et al. [7] a medical professional assessed the reports on a Likert Scale from 0 to 2 in four different aspects: accurately describes the image, presents no incorrect information, is grammatically correct and is relevant for the image; the results were further separated for samples from different body parts, showing averages between 0.5 and 1.
sent92: Gale et al. [38] asked a radiologist to evaluate the correctness of the hip fractures description, finding that the fracture's character was properly described 98% of the cases, while the fracture location only for 90%.
sent93: In the work by Tian et al. [131] a medical expert evaluated 30 randomly selected reports with a rating from 1 (definite accept) to 5 (definite reject), scoring an average of 2.33.
sent94: Lastly, Spinks and Moens [126] asked four questions to three experts regarding the generated reports, where the third and fourth questions measured correctness: ""Do you agree with the proposed diagnosis?"", answering 0 (no) or 1 (yes) and ""How certain are you about your final diagnosis?"", from 1 (not sure) to 4 (very sure).
sent95: The average scores were high (0.88 and 3.75), showing agreement with the model's diagnosis, and certainty on the experts' diagnoses.
sent96: The other questions concerned explainability aspects, and are detailed in the next subsection (5.4.3).
sent97: So far, there is no standard approach to perform an expert evaluation, though we believe the first two approaches provide finer-grained information than the latter two, and hence should be more useful for determining in which cases the models are failing and for designing improvements.
sent98: The certainty question should also be very useful, as diagnoses may be susceptible to human judgement.
sent99: Lastly, multiple papers [49,86,89,94,120,131,144,155,156,158,162,163] evaluated the performance of the auxiliary tasks with ROC-AUC, accuracy and other typical classification or segmentation metrics, as shown in Table 6.
sent100: Note that in any of these cases, the task is a previous or intermediary step of the process and is not derived from the report.
sent101: In consequence, even if the classification has great performance, the language component could be performing poorly, and the generated reports still may be inaccurate.
sent102: Accordingly, we believe this type of measure should not be used as the primary report correctness evaluation, unless it can be proven that the report reproduces exactly the classification made (e.g. by a template-filling process)."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s67,Dataset,"Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9. Additional data contained in each dataset. MeSH [115] and RadLex [81] are sets of medical concepts. MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools. Manual means manually annotated by experts. In all cases, the localization information was manually annotated by experts.    Table 11. Summary of optimization strategies used in the literature.

Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9. Additional data contained in each dataset. MeSH [115] and RadLex [81] are sets of medical concepts. MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools. Manual means manually annotated by experts. In all cases, the localization information was manually annotated by experts.    Table 11. Summary of optimization strategies used in the literature.","[['b27', 'b62', 'b105', 'b124', 'b99', 'b80', 'b114', 'b9', 'b83'], ['b27', 'b62', 'b105', 'b124', 'b99', 'b80', 'b114', 'b9', 'b83']]","[['b27', 'b62', 'b105', 'b124', 'b99', 'b80', 'b114', 'b9', 'b83'], ['b27', 'b62', 'b105', 'b124', 'b99', 'b80', 'b114', 'b9', 'b83']]",18,"sent1: Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9.
sent2: Additional data contained in each dataset.
sent3: MeSH [115] and RadLex [81] are sets of medical concepts.
sent4: MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools.
sent5: Manual means manually annotated by experts.
sent6: In all cases, the localization information was manually annotated by experts.
sent7: Table 11. Summary of optimization strategies used in the literature.
sent8: Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9.
sent9: Additional data contained in each dataset.
sent10: MeSH [115] and RadLex [81] are sets of medical concepts.
sent11: MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools.
sent12: Manual means manually annotated by experts.
sent13: In all cases, the localization information was manually annotated by experts.
sent14: Table 11. Summary of optimization strategies used in the literature."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s18,Image heatmap.,"In the papers reviewed, there are three different approaches to generating heatmaps over the input image, each of them with a different interpretation. First, many architectures employ an attention mechanism over the image spatial features during the report generation, as it was discussed in the Language Component section (5.2.3). These mechanisms can be leveraged to produce a heatmap indicating the image regions that were most important to generate the report. In particular, some models provide a heatmap for each word [92,144,163], for each sentence [61,68,153,154], or for the whole report [86]. By showing these feature importance maps, an expert should be able to determine if the model is focusing on the correct regions of the image, which could improve their trust on the system. Second, some works use particular deep learning architectures to perform image segmentation, i.e. classification and localization at the same time. The model by Ma et al. [94] uses a CNN to classify the severity of four different key characteristics of cervical cancer, and then uses an attention mechanism over the visual spatial features to generate heatmaps indicating the position of each relevant property. Tian et al. [131] used an FCN to classify each pixel of an image with the presence of a liver or tumor, and the result is averaged with an attention map to further improve localization. Han et al. [47] proposed the ACAE module (see section 5.2.3 for details), which is used to classify at pixel level different parts of the spine (vertebrae, discs or neural foramina), and if they show an abnormality or not. Kisilev et al. [76] and Spinks and Moens [126] used a Faster R-CNN [112] architecture to detect image regions with lesion and body parts of interest.

Lastly, some works use gradient-or activation-based methods for CNNs to generate a saliency map indicating the regions of most importance for a classification, such as CAM [165], Grad-CAM [118], SmoothGrad [124], or the one proposed by Zagoruyko and Komodakis [77]. Refer to Table 3 in the Input and Output section (5.2.1) for a list of the papers using each technique. To determine Cardiomegaly Pneumothorax Fig. 4. Examples from the ChestX-ray14 dataset [143] classified with a CNN based on ResNet-50 [52], and using CAM [165] to provide a heatmap indicating the the spatial regions of most importance as local explanation. The left example presents Cardiomegaly and the right Pneumothorax, and both samples were correctly classified by the CNN. Red boxes represent a localization of the condition annotated by an expert.

which of these methods performs better in a general setting, Adebayo et al. [3] performed multiple evaluations (""sanity checks"") over Grad-CAM, SmoothGrad, and other similar methods, and showed that Grad-CAM should be more reliable in terms of correlation with the input images and the classification made. As an example of these techniques, Figure 4 shows two chest X-rays from the ChestX-ray14 dataset [143] with a heatmap generated with CAM, plus an expert-annotated bounding box locating the abnormality (provided with the dataset).

In both segmentation and saliency map methods, the heatmap information provides much richer information than classification alone, as it also includes the location of an specific concept, such as an abnormality or a body part. Providing this type of explanation should allow an expert to assess the localization capabilities of the model and the system accuracy, thus improving the model's transparency throughout its process.

In the papers reviewed, there are three different approaches to generating heatmaps over the input image, each of them with a different interpretation. First, many architectures employ an attention mechanism over the image spatial features during the report generation, as it was discussed in the Language Component section (5.2.3). These mechanisms can be leveraged to produce a heatmap indicating the image regions that were most important to generate the report. In particular, some models provide a heatmap for each word [92,144,163], for each sentence [61,68,153,154], or for the whole report [86]. By showing these feature importance maps, an expert should be able to determine if the model is focusing on the correct regions of the image, which could improve their trust on the system. Second, some works use particular deep learning architectures to perform image segmentation, i.e. classification and localization at the same time. The model by Ma et al. [94] uses a CNN to classify the severity of four different key characteristics of cervical cancer, and then uses an attention mechanism over the visual spatial features to generate heatmaps indicating the position of each relevant property. Tian et al. [131] used an FCN to classify each pixel of an image with the presence of a liver or tumor, and the result is averaged with an attention map to further improve localization. Han et al. [47] proposed the ACAE module (see section 5.2.3 for details), which is used to classify at pixel level different parts of the spine (vertebrae, discs or neural foramina), and if they show an abnormality or not. Kisilev et al. [76] and Spinks and Moens [126] used a Faster R-CNN [112] architecture to detect image regions with lesion and body parts of interest.

Lastly, some works use gradient-or activation-based methods for CNNs to generate a saliency map indicating the regions of most importance for a classification, such as CAM [165], Grad-CAM [118], SmoothGrad [124], or the one proposed by Zagoruyko and Komodakis [77]. Refer to Table 3 in the Input and Output section (5.2.1) for a list of the papers using each technique. To determine Cardiomegaly Pneumothorax Fig. 4. Examples from the ChestX-ray14 dataset [143] classified with a CNN based on ResNet-50 [52], and using CAM [165] to provide a heatmap indicating the the spatial regions of most importance as local explanation. The left example presents Cardiomegaly and the right Pneumothorax, and both samples were correctly classified by the CNN. Red boxes represent a localization of the condition annotated by an expert.

which of these methods performs better in a general setting, Adebayo et al. [3] performed multiple evaluations (""sanity checks"") over Grad-CAM, SmoothGrad, and other similar methods, and showed that Grad-CAM should be more reliable in terms of correlation with the input images and the classification made. As an example of these techniques, Figure 4 shows two chest X-rays from the ChestX-ray14 dataset [143] with a heatmap generated with CAM, plus an expert-annotated bounding box locating the abnormality (provided with the dataset).

In both segmentation and saliency map methods, the heatmap information provides much richer information than classification alone, as it also includes the location of an specific concept, such as an abnormality or a body part. Providing this type of explanation should allow an expert to assess the localization capabilities of the model and the system accuracy, thus improving the model's transparency throughout its process.","[['b67', 'b153', 'b85', 'b111', 'b125', 'b60', 'b143', 'b46', 'b75', 'b91', 'b162', 'b152', 'b93', 'b130'], ['b142', 'b51', 'b76', 'b123', 'b117', 'b164'], ['b142', 'b2'], [], ['b67', 'b153', 'b85', 'b111', 'b125', 'b60', 'b143', 'b46', 'b75', 'b91', 'b162', 'b152', 'b93', 'b130'], ['b142', 'b51', 'b76', 'b123', 'b117', 'b164'], ['b142', 'b2'], []]","[['b67', 'b153', 'b85', 'b111', 'b125', 'b60', 'b143', 'b46', 'b75', 'b91', 'b162', 'b152', 'b93', 'b130'], ['b142', 'b51', 'b76', 'b123', 'b117', 'b164'], ['b142', 'b2'], [], ['b67', 'b153', 'b85', 'b111', 'b125', 'b60', 'b143', 'b46', 'b75', 'b91', 'b162', 'b152', 'b93', 'b130'], ['b142', 'b51', 'b76', 'b123', 'b117', 'b164'], ['b142', 'b2'], []]",44,"sent1: In the papers reviewed, there are three different approaches to generating heatmaps over the input image, each of them with a different interpretation.
sent2: First, many architectures employ an attention mechanism over the image spatial features during the report generation, as it was discussed in the Language Component section (5.2.3).
sent3: These mechanisms can be leveraged to produce a heatmap indicating the image regions that were most important to generate the report.
sent4: In particular, some models provide a heatmap for each word [92,144,163], for each sentence [61,68,153,154], or for the whole report [86].
sent5: By showing these feature importance maps, an expert should be able to determine if the model is focusing on the correct regions of the image, which could improve their trust on the system.
sent6: Second, some works use particular deep learning architectures to perform image segmentation, i.e. classification and localization at the same time.
sent7: The model by Ma et al. [94] uses a CNN to classify the severity of four different key characteristics of cervical cancer, and then uses an attention mechanism over the visual spatial features to generate heatmaps indicating the position of each relevant property.
sent8: Tian et al. [131] used an FCN to classify each pixel of an image with the presence of a liver or tumor, and the result is averaged with an attention map to further improve localization.
sent9: Han et al. [47] proposed the ACAE module (see section 5.2.3 for details), which is used to classify at pixel level different parts of the spine (vertebrae, discs or neural foramina), and if they show an abnormality or not.
sent10: Kisilev et al. [76] and Spinks and Moens [126] used a Faster R-CNN [112] architecture to detect image regions with lesion and body parts of interest.
sent11: Lastly, some works use gradient-or activation-based methods for CNNs to generate a saliency map indicating the regions of most importance for a classification, such as CAM [165], Grad-CAM [118], SmoothGrad [124], or the one proposed by Zagoruyko and Komodakis [77].
sent12: Refer to Table 3 in the Input and Output section (5.2.1) for a list of the papers using each technique.
sent13: To determine Cardiomegaly Pneumothorax Fig. 4.
sent14: Examples from the ChestX-ray14 dataset [143] classified with a CNN based on ResNet-50 [52], and using CAM [165] to provide a heatmap indicating the the spatial regions of most importance as local explanation.
sent15: The left example presents Cardiomegaly and the right Pneumothorax, and both samples were correctly classified by the CNN.
sent16: Red boxes represent a localization of the condition annotated by an expert.which of these methods performs better in a general setting, Adebayo et al. [3] performed multiple evaluations (""sanity checks"") over Grad-CAM, SmoothGrad, and other similar methods, and showed that Grad-CAM should be more reliable in terms of correlation with the input images and the classification made.
sent17: As an example of these techniques, Figure 4 shows two chest X-rays from the ChestX-ray14 dataset [143] with a heatmap generated with CAM, plus an expert-annotated bounding box locating the abnormality (provided with the dataset).
sent18: In both segmentation and saliency map methods, the heatmap information provides much richer information than classification alone, as it also includes the location of an specific concept, such as an abnormality or a body part.
sent19: Providing this type of explanation should allow an expert to assess the localization capabilities of the model and the system accuracy, thus improving the model's transparency throughout its process.
sent20: In the papers reviewed, there are three different approaches to generating heatmaps over the input image, each of them with a different interpretation.
sent21: First, many architectures employ an attention mechanism over the image spatial features during the report generation, as it was discussed in the Language Component section (5.2.3).
sent22: These mechanisms can be leveraged to produce a heatmap indicating the image regions that were most important to generate the report.
sent23: In particular, some models provide a heatmap for each word [92,144,163], for each sentence [61,68,153,154], or for the whole report [86].
sent24: By showing these feature importance maps, an expert should be able to determine if the model is focusing on the correct regions of the image, which could improve their trust on the system.
sent25: Second, some works use particular deep learning architectures to perform image segmentation, i.e. classification and localization at the same time.
sent26: The model by Ma et al. [94] uses a CNN to classify the severity of four different key characteristics of cervical cancer, and then uses an attention mechanism over the visual spatial features to generate heatmaps indicating the position of each relevant property.
sent27: Tian et al. [131] used an FCN to classify each pixel of an image with the presence of a liver or tumor, and the result is averaged with an attention map to further improve localization.
sent28: Han et al. [47] proposed the ACAE module (see section 5.2.3 for details), which is used to classify at pixel level different parts of the spine (vertebrae, discs or neural foramina), and if they show an abnormality or not.
sent29: Kisilev et al. [76] and Spinks and Moens [126] used a Faster R-CNN [112] architecture to detect image regions with lesion and body parts of interest.
sent30: Lastly, some works use gradient-or activation-based methods for CNNs to generate a saliency map indicating the regions of most importance for a classification, such as CAM [165], Grad-CAM [118], SmoothGrad [124], or the one proposed by Zagoruyko and Komodakis [77].
sent31: Refer to Table 3 in the Input and Output section (5.2.1) for a list of the papers using each technique.
sent32: To determine Cardiomegaly Pneumothorax Fig. 4.
sent33: Examples from the ChestX-ray14 dataset [143] classified with a CNN based on ResNet-50 [52], and using CAM [165] to provide a heatmap indicating the the spatial regions of most importance as local explanation.
sent34: The left example presents Cardiomegaly and the right Pneumothorax, and both samples were correctly classified by the CNN.
sent35: Red boxes represent a localization of the condition annotated by an expert.which of these methods performs better in a general setting, Adebayo et al. [3] performed multiple evaluations (""sanity checks"") over Grad-CAM, SmoothGrad, and other similar methods, and showed that Grad-CAM should be more reliable in terms of correlation with the input images and the classification made.
sent36: As an example of these techniques, Figure 4 shows two chest X-rays from the ChestX-ray14 dataset [143] with a heatmap generated with CAM, plus an expert-annotated bounding box locating the abnormality (provided with the dataset).
sent37: In both segmentation and saliency map methods, the heatmap information provides much richer information than classification alone, as it also includes the location of an specific concept, such as an abnormality or a body part.
sent38: Providing this type of explanation should allow an expert to assess the localization capabilities of the model and the system accuracy, thus improving the model's transparency throughout its process."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s17,Classification.,"As explained in the Auxiliary Tasks section (5.2.5), many deep learning architectures include multi-label classification to improve performance, providing a set of classified concepts as secondary output. Even though in most papers this kind of output is not presented as an explanation of the report, we consider that its nature could improve the transparency of the model, which is an important way of improving the interpretability in a medical context [134]. By providing this detection information from an intermediate step of the model's process, an expert could further understand the internal process, validate the decision with their domain knowledge and calibrate their trust in the system.

As shown in Table 3 from section 5.2.1, the terms classified are very diverse. Some works classify very broad concepts, such as body parts or organs [7,98,157,158], or image modality [51]. Other works perform a more specific classification, such as diseases or abnormalities [16,76,86,126,144,157,158,162], or a normal or abnormal status at sentence level [150]. Lastly, several works [44,48,68,120,128,132,155,156] classify over a subset of MeSH terms or similar, which may contain a mix of general broad medical concepts and specific abnormalities or conditions. We believe that this additional output would be useful for an expert, though the specific concepts should provide much richer information. If the classification is more specific, the user will be able to validate on a much narrower scope the system's performance.

As explained in the Auxiliary Tasks section (5.2.5), many deep learning architectures include multi-label classification to improve performance, providing a set of classified concepts as secondary output. Even though in most papers this kind of output is not presented as an explanation of the report, we consider that its nature could improve the transparency of the model, which is an important way of improving the interpretability in a medical context [134]. By providing this detection information from an intermediate step of the model's process, an expert could further understand the internal process, validate the decision with their domain knowledge and calibrate their trust in the system.

As shown in Table 3 from section 5.2.1, the terms classified are very diverse. Some works classify very broad concepts, such as body parts or organs [7,98,157,158], or image modality [51]. Other works perform a more specific classification, such as diseases or abnormalities [16,76,86,126,144,157,158,162], or a normal or abnormal status at sentence level [150]. Lastly, several works [44,48,68,120,128,132,155,156] classify over a subset of MeSH terms or similar, which may contain a mix of general broad medical concepts and specific abnormalities or conditions. We believe that this additional output would be useful for an expert, though the specific concepts should provide much richer information. If the classification is more specific, the user will be able to validate on a much narrower scope the system's performance.","[['b133'], ['b67', 'b85', 'b161', 'b156', 'b143', 'b97', 'b157', 'b155', 'b149', 'b6', 'b43', 'b50', 'b131', 'b75', 'b154', 'b15', 'b119', 'b125', 'b47', 'b127'], ['b133'], ['b67', 'b85', 'b161', 'b156', 'b143', 'b97', 'b157', 'b155', 'b149', 'b6', 'b43', 'b50', 'b131', 'b75', 'b154', 'b15', 'b119', 'b125', 'b47', 'b127']]","[['b133'], ['b67', 'b85', 'b161', 'b156', 'b143', 'b97', 'b157', 'b155', 'b149', 'b6', 'b43', 'b50', 'b131', 'b75', 'b154', 'b15', 'b119', 'b125', 'b47', 'b127'], ['b133'], ['b67', 'b85', 'b161', 'b156', 'b143', 'b97', 'b157', 'b155', 'b149', 'b6', 'b43', 'b50', 'b131', 'b75', 'b154', 'b15', 'b119', 'b125', 'b47', 'b127']]",42,"sent1: As explained in the Auxiliary Tasks section (5.2.5), many deep learning architectures include multi-label classification to improve performance, providing a set of classified concepts as secondary output.
sent2: Even though in most papers this kind of output is not presented as an explanation of the report, we consider that its nature could improve the transparency of the model, which is an important way of improving the interpretability in a medical context [134].
sent3: By providing this detection information from an intermediate step of the model's process, an expert could further understand the internal process, validate the decision with their domain knowledge and calibrate their trust in the system.
sent4: As shown in Table 3 from section 5.2.1, the terms classified are very diverse.
sent5: Some works classify very broad concepts, such as body parts or organs [7,98,157,158], or image modality [51].
sent6: Other works perform a more specific classification, such as diseases or abnormalities [16,76,86,126,144,157,158,162], or a normal or abnormal status at sentence level [150].
sent7: Lastly, several works [44,48,68,120,128,132,155,156] classify over a subset of MeSH terms or similar, which may contain a mix of general broad medical concepts and specific abnormalities or conditions.
sent8: We believe that this additional output would be useful for an expert, though the specific concepts should provide much richer information.
sent9: If the classification is more specific, the user will be able to validate on a much narrower scope the system's performance.
sent10: As explained in the Auxiliary Tasks section (5.2.5), many deep learning architectures include multi-label classification to improve performance, providing a set of classified concepts as secondary output.
sent11: Even though in most papers this kind of output is not presented as an explanation of the report, we consider that its nature could improve the transparency of the model, which is an important way of improving the interpretability in a medical context [134].
sent12: By providing this detection information from an intermediate step of the model's process, an expert could further understand the internal process, validate the decision with their domain knowledge and calibrate their trust in the system.
sent13: As shown in Table 3 from section 5.2.1, the terms classified are very diverse.
sent14: Some works classify very broad concepts, such as body parts or organs [7,98,157,158], or image modality [51].
sent15: Other works perform a more specific classification, such as diseases or abnormalities [16,76,86,126,144,157,158,162], or a normal or abnormal status at sentence level [150].
sent16: Lastly, several works [44,48,68,120,128,132,155,156] classify over a subset of MeSH terms or similar, which may contain a mix of general broad medical concepts and specific abnormalities or conditions.
sent17: We believe that this additional output would be useful for an expert, though the specific concepts should provide much richer information.
sent18: If the classification is more specific, the user will be able to validate on a much narrower scope the system's performance."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s15,Explainability,"There have been multiple attempts on providing a definition for explainability in the Explainable Artificial Intelligence (XAI) area [33,91,113]. For the task of report generation from medical images, we use a similar definition by Doshi-Velez and Kim [33]: the ability to justify an outcome in understandable terms for a human, and we use it interchangeably with the term interpretability. In this medical context, an automated system requires high explainability levels as two main facts hold: the decisions derived from the system will probably have direct consequences for patients, and the diagnosis task is not trivial and susceptible to human judgement [33,113]. Furthermore, the explanation methods employed in this medical task should attempt to solve several related aspects: align with clinicians' expectations and acquire their trust, increase system transparency, assess results quality, and allow addressing accountability, fairness and ethical concerns [4,113,134].

There are many ways to address the explainability aspect of AI systems in the medical domain, as listed in the recent survey on interpretable AI for radiology by Reyes et al. [113]. Multiple categories can be identified, starting with global vs local, the former refers to explanations regarding the whole system's operation, and the latter to explanations for one sample. For local explanations, there are different kinds of approaches, such as feature importance, concept-based, example-based, and uncertainty, to mention a few. Feature importance methods attempt to compute a level of importance for each input value, to understand which characteristics were most relevant to make a decision; for example, gradient-based methods for CNNs such as Grad-CAM [118], Guided Backpropagation [127] or DeepLIFT [121]; and other techniques such as LIME [114]. In concept-based methods, like TCAV [75] or RCV [43], the contributions to the prediction from multiple concepts are quantified, so the user can check if the concepts used by the model are correct. Example-based approaches present additional examples with the output, either with a similar outcome, so the user can look for a common pattern, or with an opposite outcome (counter-factual). Uncertainty methods provide the level of confidence of the model for a given prediction. For global explanations, there are sample-based approaches, such as SP-LIME [114], or methods to directly increase the transparency of the system.

Despite the importance of explainability in this area, only two reviewed works focused explicitly on this topic. Gale et al. [38] proposed the automatic generation of a natural language report as an explanation for a classification task; however, their approach does not include an explanation for the report. Spinks and Moens [126] present a counter-factual local explanation, as will be detailed in subsection 5.3.1. Additionally, in 29 works the model architecture generates a secondary output that can also be presented as a local explanation. We distinguish three types of outputs: classification (section 5.3.2), heatmap over the input image (section 5.3.3), and heatmap over the input text (section 5.3.4). These were already summarized in Table 3 in the Input and Output section (5.2.1). Next, the explanaibility aspects of the outputs are discussed. [126] proposed an architecture to both classify a disease and generate a caption from a chest X-ray, based on GANs and autoencoders, as detailed in the Model Design section (5.2). Thus, to provide a local explanation, at inference time the input image is encoded into a latent vector, which is used to generate a new chest X-ray and a new report, both of them subject to result in the nearest alternative classification, i.e., the nearest diagnosis. With this information, a user could compare the original X-ray with the generated image, and attempt to understand why the model has reached its decision.

There have been multiple attempts on providing a definition for explainability in the Explainable Artificial Intelligence (XAI) area [33,91,113]. For the task of report generation from medical images, we use a similar definition by Doshi-Velez and Kim [33]: the ability to justify an outcome in understandable terms for a human, and we use it interchangeably with the term interpretability. In this medical context, an automated system requires high explainability levels as two main facts hold: the decisions derived from the system will probably have direct consequences for patients, and the diagnosis task is not trivial and susceptible to human judgement [33,113]. Furthermore, the explanation methods employed in this medical task should attempt to solve several related aspects: align with clinicians' expectations and acquire their trust, increase system transparency, assess results quality, and allow addressing accountability, fairness and ethical concerns [4,113,134].

There are many ways to address the explainability aspect of AI systems in the medical domain, as listed in the recent survey on interpretable AI for radiology by Reyes et al. [113]. Multiple categories can be identified, starting with global vs local, the former refers to explanations regarding the whole system's operation, and the latter to explanations for one sample. For local explanations, there are different kinds of approaches, such as feature importance, concept-based, example-based, and uncertainty, to mention a few. Feature importance methods attempt to compute a level of importance for each input value, to understand which characteristics were most relevant to make a decision; for example, gradient-based methods for CNNs such as Grad-CAM [118], Guided Backpropagation [127] or DeepLIFT [121]; and other techniques such as LIME [114]. In concept-based methods, like TCAV [75] or RCV [43], the contributions to the prediction from multiple concepts are quantified, so the user can check if the concepts used by the model are correct. Example-based approaches present additional examples with the output, either with a similar outcome, so the user can look for a common pattern, or with an opposite outcome (counter-factual). Uncertainty methods provide the level of confidence of the model for a given prediction. For global explanations, there are sample-based approaches, such as SP-LIME [114], or methods to directly increase the transparency of the system.

Despite the importance of explainability in this area, only two reviewed works focused explicitly on this topic. Gale et al. [38] proposed the automatic generation of a natural language report as an explanation for a classification task; however, their approach does not include an explanation for the report. Spinks and Moens [126] present a counter-factual local explanation, as will be detailed in subsection 5.3.1. Additionally, in 29 works the model architecture generates a secondary output that can also be presented as a local explanation. We distinguish three types of outputs: classification (section 5.3.2), heatmap over the input image (section 5.3.3), and heatmap over the input text (section 5.3.4). These were already summarized in Table 3 in the Input and Output section (5.2.1). Next, the explanaibility aspects of the outputs are discussed. [126] proposed an architecture to both classify a disease and generate a caption from a chest X-ray, based on GANs and autoencoders, as detailed in the Model Design section (5.2). Thus, to provide a local explanation, at inference time the input image is encoded into a latent vector, which is used to generate a new chest X-ray and a new report, both of them subject to result in the nearest alternative classification, i.e., the nearest diagnosis. With this information, a user could compare the original X-ray with the generated image, and attempt to understand why the model has reached its decision.","[['b3', 'b133', 'b112', 'b90', 'b32'], ['b113', 'b126', 'b74', 'b112', 'b42', 'b120', 'b117'], ['b125', 'b37'], ['b3', 'b133', 'b112', 'b90', 'b32'], ['b113', 'b126', 'b74', 'b112', 'b42', 'b120', 'b117'], ['b125', 'b37']]","[['b3', 'b133', 'b112', 'b90', 'b32'], ['b113', 'b126', 'b74', 'b112', 'b42', 'b120', 'b117'], ['b125', 'b37'], ['b3', 'b133', 'b112', 'b90', 'b32'], ['b113', 'b126', 'b74', 'b112', 'b42', 'b120', 'b117'], ['b125', 'b37']]",28,"sent1: There have been multiple attempts on providing a definition for explainability in the Explainable Artificial Intelligence (XAI) area [33,91,113].
sent2: For the task of report generation from medical images, we use a similar definition by Doshi-Velez and Kim [33]: the ability to justify an outcome in understandable terms for a human, and we use it interchangeably with the term interpretability.
sent3: In this medical context, an automated system requires high explainability levels as two main facts hold: the decisions derived from the system will probably have direct consequences for patients, and the diagnosis task is not trivial and susceptible to human judgement [33,113].
sent4: Furthermore, the explanation methods employed in this medical task should attempt to solve several related aspects: align with clinicians' expectations and acquire their trust, increase system transparency, assess results quality, and allow addressing accountability, fairness and ethical concerns [4,113,134].
sent5: There are many ways to address the explainability aspect of AI systems in the medical domain, as listed in the recent survey on interpretable AI for radiology by Reyes et al. [113].
sent6: Multiple categories can be identified, starting with global vs local, the former refers to explanations regarding the whole system's operation, and the latter to explanations for one sample.
sent7: For local explanations, there are different kinds of approaches, such as feature importance, concept-based, example-based, and uncertainty, to mention a few.
sent8: Feature importance methods attempt to compute a level of importance for each input value, to understand which characteristics were most relevant to make a decision; for example, gradient-based methods for CNNs such as Grad-CAM [118], Guided Backpropagation [127] or DeepLIFT [121]; and other techniques such as LIME [114].
sent9: In concept-based methods, like TCAV [75] or RCV [43], the contributions to the prediction from multiple concepts are quantified, so the user can check if the concepts used by the model are correct.
sent10: Example-based approaches present additional examples with the output, either with a similar outcome, so the user can look for a common pattern, or with an opposite outcome (counter-factual).
sent11: Uncertainty methods provide the level of confidence of the model for a given prediction.
sent12: For global explanations, there are sample-based approaches, such as SP-LIME [114], or methods to directly increase the transparency of the system.
sent13: Despite the importance of explainability in this area, only two reviewed works focused explicitly on this topic.
sent14: Gale et al. [38] proposed the automatic generation of a natural language report as an explanation for a classification task; however, their approach does not include an explanation for the report.
sent15: Spinks and Moens [126] present a counter-factual local explanation, as will be detailed in subsection 5.3.1.
sent16: Additionally, in 29 works the model architecture generates a secondary output that can also be presented as a local explanation.
sent17: We distinguish three types of outputs: classification (section 5.3.2), heatmap over the input image (section 5.3.3), and heatmap over the input text (section 5.3.4).
sent18: These were already summarized in Table 3 in the Input and Output section (5.2.1).
sent19: Next, the explanaibility aspects of the outputs are discussed.
sent20: [126] proposed an architecture to both classify a disease and generate a caption from a chest X-ray, based on GANs and autoencoders, as detailed in the Model Design section (5.2).
sent21: Thus, to provide a local explanation, at inference time the input image is encoded into a latent vector, which is used to generate a new chest X-ray and a new report, both of them subject to result in the nearest alternative classification, i.e., the nearest diagnosis.
sent22: With this information, a user could compare the original X-ray with the generated image, and attempt to understand why the model has reached its decision.
sent23: There have been multiple attempts on providing a definition for explainability in the Explainable Artificial Intelligence (XAI) area [33,91,113].
sent24: For the task of report generation from medical images, we use a similar definition by Doshi-Velez and Kim [33]: the ability to justify an outcome in understandable terms for a human, and we use it interchangeably with the term interpretability.
sent25: In this medical context, an automated system requires high explainability levels as two main facts hold: the decisions derived from the system will probably have direct consequences for patients, and the diagnosis task is not trivial and susceptible to human judgement [33,113].
sent26: Furthermore, the explanation methods employed in this medical task should attempt to solve several related aspects: align with clinicians' expectations and acquire their trust, increase system transparency, assess results quality, and allow addressing accountability, fairness and ethical concerns [4,113,134].
sent27: There are many ways to address the explainability aspect of AI systems in the medical domain, as listed in the recent survey on interpretable AI for radiology by Reyes et al. [113].
sent28: Multiple categories can be identified, starting with global vs local, the former refers to explanations regarding the whole system's operation, and the latter to explanations for one sample.
sent29: For local explanations, there are different kinds of approaches, such as feature importance, concept-based, example-based, and uncertainty, to mention a few.
sent30: Feature importance methods attempt to compute a level of importance for each input value, to understand which characteristics were most relevant to make a decision; for example, gradient-based methods for CNNs such as Grad-CAM [118], Guided Backpropagation [127] or DeepLIFT [121]; and other techniques such as LIME [114].
sent31: In concept-based methods, like TCAV [75] or RCV [43], the contributions to the prediction from multiple concepts are quantified, so the user can check if the concepts used by the model are correct.
sent32: Example-based approaches present additional examples with the output, either with a similar outcome, so the user can look for a common pattern, or with an opposite outcome (counter-factual).
sent33: Uncertainty methods provide the level of confidence of the model for a given prediction.
sent34: For global explanations, there are sample-based approaches, such as SP-LIME [114], or methods to directly increase the transparency of the system.
sent35: Despite the importance of explainability in this area, only two reviewed works focused explicitly on this topic.
sent36: Gale et al. [38] proposed the automatic generation of a natural language report as an explanation for a classification task; however, their approach does not include an explanation for the report.
sent37: Spinks and Moens [126] present a counter-factual local explanation, as will be detailed in subsection 5.3.1.
sent38: Additionally, in 29 works the model architecture generates a secondary output that can also be presented as a local explanation.
sent39: We distinguish three types of outputs: classification (section 5.3.2), heatmap over the input image (section 5.3.3), and heatmap over the input text (section 5.3.4).
sent40: These were already summarized in Table 3 in the Input and Output section (5.2.1).
sent41: Next, the explanaibility aspects of the outputs are discussed.
sent42: [126] proposed an architecture to both classify a disease and generate a caption from a chest X-ray, based on GANs and autoencoders, as detailed in the Model Design section (5.2).
sent43: Thus, to provide a local explanation, at inference time the input image is encoded into a latent vector, which is used to generate a new chest X-ray and a new report, both of them subject to result in the nearest alternative classification, i.e., the nearest diagnosis.
sent44: With this information, a user could compare the original X-ray with the generated image, and attempt to understand why the model has reached its decision."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s14,Optimization,"Strategies. In addition to the architecture and the tasks a model can perform, a very important aspect is the optimization strategy used to learn the model's parameters. In this section we present an analysis of the optimization strategies used in the literature. A summary of this section is presented in Table 11 in appendix 9.3.

Visual Component. We first analyze the visual component optimization, identifying three general optimization decisions. The first one is whether to use a CNN from the literature with its weights pretrained in ImageNet [29]. This is a very common transfer learning practice from the computer vision literature in general [78], so it is natural to see it used in the medical domain too. However, it has been shown that ImageNet pretraining may not transfer as well to medical image tasks as they normally do to other domains, due to very dissimilar image distributions [109]. Therefore, a very common second decision is whether or not to train/fine-tune the visual component with auxiliary medical image tasks, such as most of the classification and segmentation tasks discussed in the previous section (5.2.5). The third decision is whether to freeze the visual component weights during report generation training or continue updating them in an end-to-end manner.

Report Generation. We identify two general optimization strategies in the literature: Teacherforcing (TF) and Reinforcement Learning (RL). Teacher-forcing [146] is by far the most common, as it is adopted by 32 papers [7, 16, 36, 38, 40, 44, 48, 51, 61, 67, 68, 86, 87, 89, 95, 120, 123, 126, 128, 131, 132, 144, 148, 150, 153-158, 162, 163]. The basic idea in teacher-forcing is to train a model to predict each word of the report conditioned on the previous words, therefore learning to imitate the ground truth word by word. The model typically has a softmax layer that predicts the next word, and cross entropy is the loss function of choice to measure the error and compute gradients for backpropagation. We think teacher-forcing is so widespread in the literature because of its simplicity and general applicability, as it is agnostic to the application domain (whether it be report generation in medicine or captioning of everyday images).

In contrast, 5 works [67,85,87,92,151] explored the use of reinforcement learning (RL) [71]. The main reason to use RL is the flexibility it offers to optimize non-differentiable reward functions, allowing researchers to be more creative and explore new rewards that may guide the model's learning toward domain-specific goals of interest. For example, Liu et al. [92] used RL to train their model to optimize the weighted sum of two rewards: (1) a natural language reward (CIDEr [140]) and (2) a Clinically Coherent Reward (CCR), where the latter was proposed to measure the clinical accuracy of a generated report compared to a ground-truth reference using the CheXpert labeler tool [63]. Their goal was to equip their model with two skills: natural language fluency (encouraged by CIDEr) and clinical accuracy (encouraged by CCR). Other examples of the use of RL are: the direct optimization of CIDEr [85,151], particularly in the training of a complicated hybrid template-retrieval and text generation model [85]; directly optimizing BLEU-4 after a previous teacher-forcing warmup phase [67]; and the training of the generator network of a GAN used for report generation, where the reward is provided by the discriminator network [87].

As a side note, we would like to highlight the work by Zhang et al. [161] on medical report summarization (a related task where the report is the input and with no images), illustrating how RL can be used in this setting to optimize both fluency and factual correctness. As rewards they used ROUGE [90] and a Factual Correctness reward based on the CheXpert labeler tool [63] (very similar to the CCR proposed by Liu et al. [92]). This work is a good example of the benefits of RL over teacher-forcing for text generation in a medical domain. The paper presents the results of a human evaluation with two board-certified radiologists and the model trained with RL achieved better results than the same model trained with teacher-forcing, and even slightly better results than the human baseline.

Other Losses or Training Strategies. This category encompasses the remaining optimization strategies found in the literature. The most important one is multitask learning [21], adopted by 14 papers [48,67,68,76,86,94,95,126,131,132,144,155,157,163]. The main idea is to jointly train a model in multiple complementary tasks, so that the model can learn robust parameters that perform well in all of them. Some works [48,68,131,132,144,155,163] trained the visual and language components simultaneously in multiple tasks in an end-to-end manner, i.e. report generation plus other auxiliary tasks. Other examples are the simultaneous training of object detection and attribute classification [76], diagnostic classification and cycle-consistency tasks [126], among others. Most of these papers report benefits from training in this way.

As already discussed in section 5.2.5, two works [95,155] used auxiliary supervision on the attention weights of their models. These auxiliary losses were jointly optimized with the rest of the model in report generation, effectively having a regularizer effect. Yin et al. [155] are also the only ones that included an auxiliary contrastive loss [24] to provide a direct supervision to the Sentence LSTM, thus improving their model's performance. Notice that all these works are examples of multitask learning too. Three papers [76,98,157] used regression losses. Two of them [76,157] included a bounding box regression loss as part of Faster R-CNN [112] training, and Moradi et al. [98] included a regression loss to minimize the Euclidean distance between VGG and doc2vec embeddings. As previously discussed in section 5.2.5, another optimization strategy is the use of autoencoders for the self-supervised learning of text representations. In MTMA [132] an autoencoder was used to provide an auxiliary supervision over the BiLSTM and was jointly trained with the rest of the model in a multitask learning fashion. Spinks and Moens [126] instead trained an ARAE in a first stage, then froze its weights and used the learned text embedding to support the subsequent training of a GAN.

Lastly, three works used GANs [47,87,126]. As mentioned when discussing RL, Li et al. [47] used a GAN strategy to train their model for report generation, where the generative module generates a report and the discriminator determines whether it is real or fake. Similarly, Han et al. [87] proposed RGAN, where the generator outputs segmentation maps from spine radiographs and the discriminator determines if a given segmentation map is real or fake. Spinks and Moens [126] implemented a modified version of a StackGAN [159] to generate chest X-ray images from input text representations. In their case, they trained the GAN using two cycle-consistency [167] losses: (1) image − → embedding − → image and (2) embedding − → image − → embedding. In both cases, an auxiliary inverse mapping CNN was used to close the cycle.

Synthesis. Overall, we can observe that designing a model for report generation from medical images is a complex task that involves engineering decisions at multiple levels: inputs and outputs, visual component, language component, domain knowledge, auxiliary tasks and optimization strategies. In each of these dimensions there are different approaches adopted in the reviewed literature, and the current state of research does not allow us to recommend an ""optimal model design"", mainly for reasons we will discuss in the Metrics and Performance Comparison sections (5.4 and 5.5). Nevertheless, there are valuable insights in the literature that may lead to better results, and thus are worth having in mind. For example, the use of CNNs (such as DenseNet or ResNet) as visual component and training in auxiliary medical image tasks; the use of input text alongside the images; providing the language component with tag information in addition to the visual features (e.g. medical concepts identified in the image); leveraging template databases curated with domain knowledge; or the use of multitask learning combining multiple sources of supervision. Lastly, to improve report quality from a medical perspective, the use of reinforcement learning with adequate reward functions appears as the most promising approach.

Strategies. In addition to the architecture and the tasks a model can perform, a very important aspect is the optimization strategy used to learn the model's parameters. In this section we present an analysis of the optimization strategies used in the literature. A summary of this section is presented in Table 11 in appendix 9.3.

Visual Component. We first analyze the visual component optimization, identifying three general optimization decisions. The first one is whether to use a CNN from the literature with its weights pretrained in ImageNet [29]. This is a very common transfer learning practice from the computer vision literature in general [78], so it is natural to see it used in the medical domain too. However, it has been shown that ImageNet pretraining may not transfer as well to medical image tasks as they normally do to other domains, due to very dissimilar image distributions [109]. Therefore, a very common second decision is whether or not to train/fine-tune the visual component with auxiliary medical image tasks, such as most of the classification and segmentation tasks discussed in the previous section (5.2.5). The third decision is whether to freeze the visual component weights during report generation training or continue updating them in an end-to-end manner.

Report Generation. We identify two general optimization strategies in the literature: Teacherforcing (TF) and Reinforcement Learning (RL). Teacher-forcing [146] is by far the most common, as it is adopted by 32 papers [7, 16, 36, 38, 40, 44, 48, 51, 61, 67, 68, 86, 87, 89, 95, 120, 123, 126, 128, 131, 132, 144, 148, 150, 153-158, 162, 163]. The basic idea in teacher-forcing is to train a model to predict each word of the report conditioned on the previous words, therefore learning to imitate the ground truth word by word. The model typically has a softmax layer that predicts the next word, and cross entropy is the loss function of choice to measure the error and compute gradients for backpropagation. We think teacher-forcing is so widespread in the literature because of its simplicity and general applicability, as it is agnostic to the application domain (whether it be report generation in medicine or captioning of everyday images).

In contrast, 5 works [67,85,87,92,151] explored the use of reinforcement learning (RL) [71]. The main reason to use RL is the flexibility it offers to optimize non-differentiable reward functions, allowing researchers to be more creative and explore new rewards that may guide the model's learning toward domain-specific goals of interest. For example, Liu et al. [92] used RL to train their model to optimize the weighted sum of two rewards: (1) a natural language reward (CIDEr [140]) and (2) a Clinically Coherent Reward (CCR), where the latter was proposed to measure the clinical accuracy of a generated report compared to a ground-truth reference using the CheXpert labeler tool [63]. Their goal was to equip their model with two skills: natural language fluency (encouraged by CIDEr) and clinical accuracy (encouraged by CCR). Other examples of the use of RL are: the direct optimization of CIDEr [85,151], particularly in the training of a complicated hybrid template-retrieval and text generation model [85]; directly optimizing BLEU-4 after a previous teacher-forcing warmup phase [67]; and the training of the generator network of a GAN used for report generation, where the reward is provided by the discriminator network [87].

As a side note, we would like to highlight the work by Zhang et al. [161] on medical report summarization (a related task where the report is the input and with no images), illustrating how RL can be used in this setting to optimize both fluency and factual correctness. As rewards they used ROUGE [90] and a Factual Correctness reward based on the CheXpert labeler tool [63] (very similar to the CCR proposed by Liu et al. [92]). This work is a good example of the benefits of RL over teacher-forcing for text generation in a medical domain. The paper presents the results of a human evaluation with two board-certified radiologists and the model trained with RL achieved better results than the same model trained with teacher-forcing, and even slightly better results than the human baseline.

Other Losses or Training Strategies. This category encompasses the remaining optimization strategies found in the literature. The most important one is multitask learning [21], adopted by 14 papers [48,67,68,76,86,94,95,126,131,132,144,155,157,163]. The main idea is to jointly train a model in multiple complementary tasks, so that the model can learn robust parameters that perform well in all of them. Some works [48,68,131,132,144,155,163] trained the visual and language components simultaneously in multiple tasks in an end-to-end manner, i.e. report generation plus other auxiliary tasks. Other examples are the simultaneous training of object detection and attribute classification [76], diagnostic classification and cycle-consistency tasks [126], among others. Most of these papers report benefits from training in this way.

As already discussed in section 5.2.5, two works [95,155] used auxiliary supervision on the attention weights of their models. These auxiliary losses were jointly optimized with the rest of the model in report generation, effectively having a regularizer effect. Yin et al. [155] are also the only ones that included an auxiliary contrastive loss [24] to provide a direct supervision to the Sentence LSTM, thus improving their model's performance. Notice that all these works are examples of multitask learning too. Three papers [76,98,157] used regression losses. Two of them [76,157] included a bounding box regression loss as part of Faster R-CNN [112] training, and Moradi et al. [98] included a regression loss to minimize the Euclidean distance between VGG and doc2vec embeddings. As previously discussed in section 5.2.5, another optimization strategy is the use of autoencoders for the self-supervised learning of text representations. In MTMA [132] an autoencoder was used to provide an auxiliary supervision over the BiLSTM and was jointly trained with the rest of the model in a multitask learning fashion. Spinks and Moens [126] instead trained an ARAE in a first stage, then froze its weights and used the learned text embedding to support the subsequent training of a GAN.

Lastly, three works used GANs [47,87,126]. As mentioned when discussing RL, Li et al. [47] used a GAN strategy to train their model for report generation, where the generative module generates a report and the discriminator determines whether it is real or fake. Similarly, Han et al. [87] proposed RGAN, where the generator outputs segmentation maps from spine radiographs and the discriminator determines if a given segmentation map is real or fake. Spinks and Moens [126] implemented a modified version of a StackGAN [159] to generate chest X-ray images from input text representations. In their case, they trained the GAN using two cycle-consistency [167] losses: (1) image − → embedding − → image and (2) embedding − → image − → embedding. In both cases, an auxiliary inverse mapping CNN was used to close the cycle.

Synthesis. Overall, we can observe that designing a model for report generation from medical images is a complex task that involves engineering decisions at multiple levels: inputs and outputs, visual component, language component, domain knowledge, auxiliary tasks and optimization strategies. In each of these dimensions there are different approaches adopted in the reviewed literature, and the current state of research does not allow us to recommend an ""optimal model design"", mainly for reasons we will discuss in the Metrics and Performance Comparison sections (5.4 and 5.5). Nevertheless, there are valuable insights in the literature that may lead to better results, and thus are worth having in mind. For example, the use of CNNs (such as DenseNet or ResNet) as visual component and training in auxiliary medical image tasks; the use of input text alongside the images; providing the language component with tag information in addition to the visual features (e.g. medical concepts identified in the image); leveraging template databases curated with domain knowledge; or the use of multitask learning combining multiple sources of supervision. Lastly, to improve report quality from a medical perspective, the use of reinforcement learning with adequate reward functions appears as the most promising approach.","[[], ['b77', 'b108', 'b28'], [None, 'b145'], ['b70', 'b62', 'b84', 'b66', 'b91', 'b150', 'b86', 'b139'], ['b89', 'b62', 'b160', 'b91'], ['b67', 'b85', 'b125', 'b131', 'b156', 'b20', 'b47', 'b143', 'b66', 'b162', 'b75', 'b93', 'b130', 'b154', 'b94'], ['b111', 'b125', 'b131', 'b156', 'b97', 'b23', 'b75', 'b154', 'b94'], ['b158', 'b125', 'b46', 'b166', 'b86'], [], [], ['b77', 'b108', 'b28'], [None, 'b145'], ['b70', 'b62', 'b84', 'b66', 'b91', 'b150', 'b86', 'b139'], ['b89', 'b62', 'b160', 'b91'], ['b67', 'b85', 'b125', 'b131', 'b156', 'b20', 'b47', 'b143', 'b66', 'b162', 'b75', 'b93', 'b130', 'b154', 'b94'], ['b111', 'b125', 'b131', 'b156', 'b97', 'b23', 'b75', 'b154', 'b94'], ['b158', 'b125', 'b46', 'b166', 'b86'], []]","[[], ['b77', 'b108', 'b28'], [None, 'b145'], ['b70', 'b62', 'b84', 'b66', 'b91', 'b150', 'b86', 'b139'], ['b89', 'b62', 'b160', 'b91'], ['b67', 'b85', 'b125', 'b131', 'b156', 'b20', 'b47', 'b143', 'b66', 'b162', 'b75', 'b93', 'b130', 'b154', 'b94'], ['b111', 'b125', 'b131', 'b156', 'b97', 'b23', 'b75', 'b154', 'b94'], ['b158', 'b125', 'b46', 'b166', 'b86'], [], [], ['b77', 'b108', 'b28'], [None, 'b145'], ['b70', 'b62', 'b84', 'b66', 'b91', 'b150', 'b86', 'b139'], ['b89', 'b62', 'b160', 'b91'], ['b67', 'b85', 'b125', 'b131', 'b156', 'b20', 'b47', 'b143', 'b66', 'b162', 'b75', 'b93', 'b130', 'b154', 'b94'], ['b111', 'b125', 'b131', 'b156', 'b97', 'b23', 'b75', 'b154', 'b94'], ['b158', 'b125', 'b46', 'b166', 'b86'], []]",92,"sent1: Strategies. In addition to the architecture and the tasks a model can perform, a very important aspect is the optimization strategy used to learn the model's parameters.
sent2: In this section we present an analysis of the optimization strategies used in the literature.
sent3: A summary of this section is presented in Table 11 in appendix 9.3.
sent4: Visual Component. We first analyze the visual component optimization, identifying three general optimization decisions.
sent5: The first one is whether to use a CNN from the literature with its weights pretrained in ImageNet [29].
sent6: This is a very common transfer learning practice from the computer vision literature in general [78], so it is natural to see it used in the medical domain too.
sent7: However, it has been shown that ImageNet pretraining may not transfer as well to medical image tasks as they normally do to other domains, due to very dissimilar image distributions [109].
sent8: Therefore, a very common second decision is whether or not to train/fine-tune the visual component with auxiliary medical image tasks, such as most of the classification and segmentation tasks discussed in the previous section (5.2.5).
sent9: The third decision is whether to freeze the visual component weights during report generation training or continue updating them in an end-to-end manner.
sent10: Report Generation. We identify two general optimization strategies in the literature: Teacherforcing (TF) and Reinforcement Learning (RL).
sent11: Teacher-forcing [146] is by far the most common, as it is adopted by 32 papers [7, 16, 36, 38, 40, 44, 48, 51, 61, 67, 68, 86, 87, 89, 95, 120, 123, 126, 128, 131, 132, 144, 148, 150, 153-158, 162, 163].
sent12: The basic idea in teacher-forcing is to train a model to predict each word of the report conditioned on the previous words, therefore learning to imitate the ground truth word by word.
sent13: The model typically has a softmax layer that predicts the next word, and cross entropy is the loss function of choice to measure the error and compute gradients for backpropagation.
sent14: We think teacher-forcing is so widespread in the literature because of its simplicity and general applicability, as it is agnostic to the application domain (whether it be report generation in medicine or captioning of everyday images).
sent15: In contrast, 5 works [67,85,87,92,151] explored the use of reinforcement learning (RL) [71].
sent16: The main reason to use RL is the flexibility it offers to optimize non-differentiable reward functions, allowing researchers to be more creative and explore new rewards that may guide the model's learning toward domain-specific goals of interest.
sent17: For example, Liu et al. [92] used RL to train their model to optimize the weighted sum of two rewards: (1) a natural language reward (CIDEr [140]) and (2) a Clinically Coherent Reward (CCR), where the latter was proposed to measure the clinical accuracy of a generated report compared to a ground-truth reference using the CheXpert labeler tool [63].
sent18: Their goal was to equip their model with two skills: natural language fluency (encouraged by CIDEr) and clinical accuracy (encouraged by CCR).
sent19: Other examples of the use of RL are: the direct optimization of CIDEr [85,151], particularly in the training of a complicated hybrid template-retrieval and text generation model [85]; directly optimizing BLEU-4 after a previous teacher-forcing warmup phase [67]; and the training of the generator network of a GAN used for report generation, where the reward is provided by the discriminator network [87].
sent20: As a side note, we would like to highlight the work by Zhang et al. [161] on medical report summarization (a related task where the report is the input and with no images), illustrating how RL can be used in this setting to optimize both fluency and factual correctness.
sent21: As rewards they used ROUGE [90] and a Factual Correctness reward based on the CheXpert labeler tool [63] (very similar to the CCR proposed by Liu et al. [92]).
sent22: This work is a good example of the benefits of RL over teacher-forcing for text generation in a medical domain.
sent23: The paper presents the results of a human evaluation with two board-certified radiologists and the model trained with RL achieved better results than the same model trained with teacher-forcing, and even slightly better results than the human baseline.
sent24: Other Losses or Training Strategies.
sent25: This category encompasses the remaining optimization strategies found in the literature.
sent26: The most important one is multitask learning [21], adopted by 14 papers [48,67,68,76,86,94,95,126,131,132,144,155,157,163].
sent27: The main idea is to jointly train a model in multiple complementary tasks, so that the model can learn robust parameters that perform well in all of them.
sent28: Some works [48,68,131,132,144,155,163] trained the visual and language components simultaneously in multiple tasks in an end-to-end manner, i.e. report generation plus other auxiliary tasks.
sent29: Other examples are the simultaneous training of object detection and attribute classification [76], diagnostic classification and cycle-consistency tasks [126], among others.
sent30: Most of these papers report benefits from training in this way.
sent31: As already discussed in section 5.2.5, two works [95,155] used auxiliary supervision on the attention weights of their models.
sent32: These auxiliary losses were jointly optimized with the rest of the model in report generation, effectively having a regularizer effect.
sent33: Yin et al. [155] are also the only ones that included an auxiliary contrastive loss [24] to provide a direct supervision to the Sentence LSTM, thus improving their model's performance.
sent34: Notice that all these works are examples of multitask learning too.
sent35: Three papers [76,98,157] used regression losses.
sent36: Two of them [76,157] included a bounding box regression loss as part of Faster R-CNN [112] training, and Moradi et al. [98] included a regression loss to minimize the Euclidean distance between VGG and doc2vec embeddings.
sent37: As previously discussed in section 5.2.5, another optimization strategy is the use of autoencoders for the self-supervised learning of text representations.
sent38: In MTMA [132] an autoencoder was used to provide an auxiliary supervision over the BiLSTM and was jointly trained with the rest of the model in a multitask learning fashion.
sent39: Spinks and Moens [126] instead trained an ARAE in a first stage, then froze its weights and used the learned text embedding to support the subsequent training of a GAN.
sent40: Lastly, three works used GANs [47,87,126].
sent41: As mentioned when discussing RL, Li et al. [47] used a GAN strategy to train their model for report generation, where the generative module generates a report and the discriminator determines whether it is real or fake.
sent42: Similarly, Han et al. [87] proposed RGAN, where the generator outputs segmentation maps from spine radiographs and the discriminator determines if a given segmentation map is real or fake.
sent43: Spinks and Moens [126] implemented a modified version of a StackGAN [159] to generate chest X-ray images from input text representations.
sent44: In their case, they trained the GAN using two cycle-consistency [167] losses: (1) image − → embedding − → image and (2) embedding − → image − → embedding.
sent45: In both cases, an auxiliary inverse mapping CNN was used to close the cycle.
sent46: Synthesis. Overall, we can observe that designing a model for report generation from medical images is a complex task that involves engineering decisions at multiple levels: inputs and outputs, visual component, language component, domain knowledge, auxiliary tasks and optimization strategies.
sent47: In each of these dimensions there are different approaches adopted in the reviewed literature, and the current state of research does not allow us to recommend an ""optimal model design"", mainly for reasons we will discuss in the Metrics and Performance Comparison sections (5.4 and 5.5).
sent48: Nevertheless, there are valuable insights in the literature that may lead to better results, and thus are worth having in mind.
sent49: For example, the use of CNNs (such as DenseNet or ResNet) as visual component and training in auxiliary medical image tasks; the use of input text alongside the images; providing the language component with tag information in addition to the visual features (e.g. medical concepts identified in the image); leveraging template databases curated with domain knowledge; or the use of multitask learning combining multiple sources of supervision.
sent50: Lastly, to improve report quality from a medical perspective, the use of reinforcement learning with adequate reward functions appears as the most promising approach.
sent51: Strategies. In addition to the architecture and the tasks a model can perform, a very important aspect is the optimization strategy used to learn the model's parameters.
sent52: In this section we present an analysis of the optimization strategies used in the literature.
sent53: A summary of this section is presented in Table 11 in appendix 9.3.
sent54: Visual Component. We first analyze the visual component optimization, identifying three general optimization decisions.
sent55: The first one is whether to use a CNN from the literature with its weights pretrained in ImageNet [29].
sent56: This is a very common transfer learning practice from the computer vision literature in general [78], so it is natural to see it used in the medical domain too.
sent57: However, it has been shown that ImageNet pretraining may not transfer as well to medical image tasks as they normally do to other domains, due to very dissimilar image distributions [109].
sent58: Therefore, a very common second decision is whether or not to train/fine-tune the visual component with auxiliary medical image tasks, such as most of the classification and segmentation tasks discussed in the previous section (5.2.5).
sent59: The third decision is whether to freeze the visual component weights during report generation training or continue updating them in an end-to-end manner.
sent60: Report Generation. We identify two general optimization strategies in the literature: Teacherforcing (TF) and Reinforcement Learning (RL).
sent61: Teacher-forcing [146] is by far the most common, as it is adopted by 32 papers [7, 16, 36, 38, 40, 44, 48, 51, 61, 67, 68, 86, 87, 89, 95, 120, 123, 126, 128, 131, 132, 144, 148, 150, 153-158, 162, 163].
sent62: The basic idea in teacher-forcing is to train a model to predict each word of the report conditioned on the previous words, therefore learning to imitate the ground truth word by word.
sent63: The model typically has a softmax layer that predicts the next word, and cross entropy is the loss function of choice to measure the error and compute gradients for backpropagation.
sent64: We think teacher-forcing is so widespread in the literature because of its simplicity and general applicability, as it is agnostic to the application domain (whether it be report generation in medicine or captioning of everyday images).
sent65: In contrast, 5 works [67,85,87,92,151] explored the use of reinforcement learning (RL) [71].
sent66: The main reason to use RL is the flexibility it offers to optimize non-differentiable reward functions, allowing researchers to be more creative and explore new rewards that may guide the model's learning toward domain-specific goals of interest.
sent67: For example, Liu et al. [92] used RL to train their model to optimize the weighted sum of two rewards: (1) a natural language reward (CIDEr [140]) and (2) a Clinically Coherent Reward (CCR), where the latter was proposed to measure the clinical accuracy of a generated report compared to a ground-truth reference using the CheXpert labeler tool [63].
sent68: Their goal was to equip their model with two skills: natural language fluency (encouraged by CIDEr) and clinical accuracy (encouraged by CCR).
sent69: Other examples of the use of RL are: the direct optimization of CIDEr [85,151], particularly in the training of a complicated hybrid template-retrieval and text generation model [85]; directly optimizing BLEU-4 after a previous teacher-forcing warmup phase [67]; and the training of the generator network of a GAN used for report generation, where the reward is provided by the discriminator network [87].
sent70: As a side note, we would like to highlight the work by Zhang et al. [161] on medical report summarization (a related task where the report is the input and with no images), illustrating how RL can be used in this setting to optimize both fluency and factual correctness.
sent71: As rewards they used ROUGE [90] and a Factual Correctness reward based on the CheXpert labeler tool [63] (very similar to the CCR proposed by Liu et al. [92]).
sent72: This work is a good example of the benefits of RL over teacher-forcing for text generation in a medical domain.
sent73: The paper presents the results of a human evaluation with two board-certified radiologists and the model trained with RL achieved better results than the same model trained with teacher-forcing, and even slightly better results than the human baseline.
sent74: Other Losses or Training Strategies.
sent75: This category encompasses the remaining optimization strategies found in the literature.
sent76: The most important one is multitask learning [21], adopted by 14 papers [48,67,68,76,86,94,95,126,131,132,144,155,157,163].
sent77: The main idea is to jointly train a model in multiple complementary tasks, so that the model can learn robust parameters that perform well in all of them.
sent78: Some works [48,68,131,132,144,155,163] trained the visual and language components simultaneously in multiple tasks in an end-to-end manner, i.e. report generation plus other auxiliary tasks.
sent79: Other examples are the simultaneous training of object detection and attribute classification [76], diagnostic classification and cycle-consistency tasks [126], among others.
sent80: Most of these papers report benefits from training in this way.
sent81: As already discussed in section 5.2.5, two works [95,155] used auxiliary supervision on the attention weights of their models.
sent82: These auxiliary losses were jointly optimized with the rest of the model in report generation, effectively having a regularizer effect.
sent83: Yin et al. [155] are also the only ones that included an auxiliary contrastive loss [24] to provide a direct supervision to the Sentence LSTM, thus improving their model's performance.
sent84: Notice that all these works are examples of multitask learning too.
sent85: Three papers [76,98,157] used regression losses.
sent86: Two of them [76,157] included a bounding box regression loss as part of Faster R-CNN [112] training, and Moradi et al. [98] included a regression loss to minimize the Euclidean distance between VGG and doc2vec embeddings.
sent87: As previously discussed in section 5.2.5, another optimization strategy is the use of autoencoders for the self-supervised learning of text representations.
sent88: In MTMA [132] an autoencoder was used to provide an auxiliary supervision over the BiLSTM and was jointly trained with the rest of the model in a multitask learning fashion.
sent89: Spinks and Moens [126] instead trained an ARAE in a first stage, then froze its weights and used the learned text embedding to support the subsequent training of a GAN.
sent90: Lastly, three works used GANs [47,87,126].
sent91: As mentioned when discussing RL, Li et al. [47] used a GAN strategy to train their model for report generation, where the generative module generates a report and the discriminator determines whether it is real or fake.
sent92: Similarly, Han et al. [87] proposed RGAN, where the generator outputs segmentation maps from spine radiographs and the discriminator determines if a given segmentation map is real or fake.
sent93: Spinks and Moens [126] implemented a modified version of a StackGAN [159] to generate chest X-ray images from input text representations.
sent94: In their case, they trained the GAN using two cycle-consistency [167] losses: (1) image − → embedding − → image and (2) embedding − → image − → embedding.
sent95: In both cases, an auxiliary inverse mapping CNN was used to close the cycle.
sent96: Synthesis. Overall, we can observe that designing a model for report generation from medical images is a complex task that involves engineering decisions at multiple levels: inputs and outputs, visual component, language component, domain knowledge, auxiliary tasks and optimization strategies.
sent97: In each of these dimensions there are different approaches adopted in the reviewed literature, and the current state of research does not allow us to recommend an ""optimal model design"", mainly for reasons we will discuss in the Metrics and Performance Comparison sections (5.4 and 5.5).
sent98: Nevertheless, there are valuable insights in the literature that may lead to better results, and thus are worth having in mind.
sent99: For example, the use of CNNs (such as DenseNet or ResNet) as visual component and training in auxiliary medical image tasks; the use of input text alongside the images; providing the language component with tag information in addition to the visual features (e.g. medical concepts identified in the image); leveraging template databases curated with domain knowledge; or the use of multitask learning combining multiple sources of supervision.
sent100: Lastly, to improve report quality from a medical perspective, the use of reinforcement learning with adequate reward functions appears as the most promising approach."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s22,Explainability metrics.,"Providing interpretable justifications for the model's outcome is essential in this medical domain, and furthermore, we should be able to evaluate them to answer questions such as, does the method justify the model's decision?, which method provides a better explanation? However, there is no consensus on evaluation methods for AI explainability, and in many cases the definition of a better explanation remains subjective [22,33,113].

Consequently, none of the papers reviewed used an automatic metric to assess explainability, and only two works [38,126] conduct a formal human expert evaluation. Gale et al. [38] presented the report generation as an explanation of a medical image classification task, and evaluated it by comparing three methods: (a) SmoothGrad [124] to highlight the most important pixels used, (b) a generated report in natural language, and (c) both placed side by side. Five experts assessed 30 images, rating each explanation in a scale from 1 (unsatisfactory) to 10 (perfect); achieving average scores of (a) 4.4, (b) 7 and (c) 8.8 for each method. Though the authors emphasize the importance of the natural language explanations, their approach does not include an explanation for the report itself, so it cannot be directly used for the report generation task.

The model proposed by Spinks and Moens [126] generates a chest X-ray as a counter-factual example, and they compared this explanation method against a feature importance heatmap generated with the Zagoruyko and Komodakis saliency map technique [77]. Three experts evaluated 150 samples answering four questions, the first two regarding explainability aspects: ""Does the explanation justify the diagnosis?"" ""Does the model appear to understand the important parts of the X-ray?"" The answers were in a scale from 1 (no) to 4 (yes), and their method achieved a higher score than the saliency map (2.39 vs 1.31 for the first question, and 2.45 vs 1.81 for the second), showing their counter-factual approach should be better in this setting. The other two questions relate more to medical correctness, and are discussed in the previous section (5.4.2).

We believe the explanation evaluations should be very important in this area, and as there is no consensus, we outline some possible guidelines. Following ideas from Tonekaboni et al. [134], we believe three aspects from the explanations should be assessed: (1) consistency, (2) alignment with domain knowledge, and (3) user impact. First, the consistency across the data should be assessed, answering questions such as: do explanations change with variations to the input data?; or to the prediction?; or to the model design?; or with different images from the same patient? As pointed out by Tonekaboni et al. [134], inconsistent explanations may negatively affect the clinicians' trust, and an interpretability method laying them out should be reviewed. Examples of consistency or robustness evaluations can be found in the work by Adebayo et al. [3] for image saliency maps, and in the work by Jain and Wallace [66] for attention in recurrent neural networks.

Second, the alignment with domain knowledge should evaluate if the explanation is consistent with an expert's knowledge: would they provide the same explanation for that decision? For instance, given a feature importance method, is the model focusing on the correct features? As an example, consider the second and third questions employed by Spinks and Moens [126] detailed earlier. To mention other examples, Wang et al. [143] evaluated CAM [165] generated heatmaps for disease classification against expert provided bounding-boxes locating the diseases, using intersection-overunion like metrics; Kim et al. [75] proposed a model to classify Diabetic Retinopathy from retina fundus images, and they compared the TCAV [75] extracted concepts against expert knowledge. Notice many works reviewed in this survey used classification or segmentation as an auxiliary task, which can be used as local explanations, and evaluated them with common metrics (such as accuracy, precision, etc.), as discussed in the previous sections (5.3 and 5.4.2). As the authors did not mention the secondary outputs as local explanations, we categorized the said evaluations as medical correctness metrics, but they are also measuring alignment with domain knowledge for the interpretability methods, and as such may be very useful.

Lastly, the user impact should attempt to answer questions like, is it a good explanation? Does it provide useful or novel information? Does it justify the model's decision? Is it provided with an appropriate representation for the experts? As examples, the assessment proposed by Gale et al. [38] and the first question used by Spinks and Moens [126] measure user impact. Notice that most of these concepts are very subjective, and the definitions, the questions and assessments will vary for different sub-domains and target experts. We believe more specific definitions and fine-grained aspects should arise in the future, as research in this topic grows. For reference, this category includes the domain appropriate representation and potential actionability concepts presented by Tonekaboni et al. [134]. Synthesis. Almost all the works include text quality metrics, though these are not able to capture the medical facts in a report [11,17,92,107,108,161]. Several works proposed medical correctness assessments over the reports, but unfortunately none of the proposals was evaluated against expert judgement. The auxiliary tasks can be evaluated to measure correctness indirectly from the process, but often it will not be sufficient for the report's correctness. Only two works evaluate explainability directly with experts, and the auxiliary tasks' assessments could be useful to measure alignment between the explanations and domain knowledge. Overall, we believe that medical correctness should be the primary aspect to evaluate in the generated reports, using one or more automatic metrics. For now, and even though none of the metrics proposed has been evaluated against expert judgement, MIRQI [162] seems like the most promising approach to fulfill this purpose, as it should be able to capture richer information from the reports. Additionally, text quality metrics can be used as a secondary evaluation, since they may be useful for measuring fluency, grammar or variability, and to compare with previous baselines. Lastly, explainability evaluation methods should arise to assess multiple key aspects, such as its consistency, alignment with domain knowledge, and the user impact.

Providing interpretable justifications for the model's outcome is essential in this medical domain, and furthermore, we should be able to evaluate them to answer questions such as, does the method justify the model's decision?, which method provides a better explanation? However, there is no consensus on evaluation methods for AI explainability, and in many cases the definition of a better explanation remains subjective [22,33,113].

Consequently, none of the papers reviewed used an automatic metric to assess explainability, and only two works [38,126] conduct a formal human expert evaluation. Gale et al. [38] presented the report generation as an explanation of a medical image classification task, and evaluated it by comparing three methods: (a) SmoothGrad [124] to highlight the most important pixels used, (b) a generated report in natural language, and (c) both placed side by side. Five experts assessed 30 images, rating each explanation in a scale from 1 (unsatisfactory) to 10 (perfect); achieving average scores of (a) 4.4, (b) 7 and (c) 8.8 for each method. Though the authors emphasize the importance of the natural language explanations, their approach does not include an explanation for the report itself, so it cannot be directly used for the report generation task.

The model proposed by Spinks and Moens [126] generates a chest X-ray as a counter-factual example, and they compared this explanation method against a feature importance heatmap generated with the Zagoruyko and Komodakis saliency map technique [77]. Three experts evaluated 150 samples answering four questions, the first two regarding explainability aspects: ""Does the explanation justify the diagnosis?"" ""Does the model appear to understand the important parts of the X-ray?"" The answers were in a scale from 1 (no) to 4 (yes), and their method achieved a higher score than the saliency map (2.39 vs 1.31 for the first question, and 2.45 vs 1.81 for the second), showing their counter-factual approach should be better in this setting. The other two questions relate more to medical correctness, and are discussed in the previous section (5.4.2).

We believe the explanation evaluations should be very important in this area, and as there is no consensus, we outline some possible guidelines. Following ideas from Tonekaboni et al. [134], we believe three aspects from the explanations should be assessed: (1) consistency, (2) alignment with domain knowledge, and (3) user impact. First, the consistency across the data should be assessed, answering questions such as: do explanations change with variations to the input data?; or to the prediction?; or to the model design?; or with different images from the same patient? As pointed out by Tonekaboni et al. [134], inconsistent explanations may negatively affect the clinicians' trust, and an interpretability method laying them out should be reviewed. Examples of consistency or robustness evaluations can be found in the work by Adebayo et al. [3] for image saliency maps, and in the work by Jain and Wallace [66] for attention in recurrent neural networks.

Second, the alignment with domain knowledge should evaluate if the explanation is consistent with an expert's knowledge: would they provide the same explanation for that decision? For instance, given a feature importance method, is the model focusing on the correct features? As an example, consider the second and third questions employed by Spinks and Moens [126] detailed earlier. To mention other examples, Wang et al. [143] evaluated CAM [165] generated heatmaps for disease classification against expert provided bounding-boxes locating the diseases, using intersection-overunion like metrics; Kim et al. [75] proposed a model to classify Diabetic Retinopathy from retina fundus images, and they compared the TCAV [75] extracted concepts against expert knowledge. Notice many works reviewed in this survey used classification or segmentation as an auxiliary task, which can be used as local explanations, and evaluated them with common metrics (such as accuracy, precision, etc.), as discussed in the previous sections (5.3 and 5.4.2). As the authors did not mention the secondary outputs as local explanations, we categorized the said evaluations as medical correctness metrics, but they are also measuring alignment with domain knowledge for the interpretability methods, and as such may be very useful.

Lastly, the user impact should attempt to answer questions like, is it a good explanation? Does it provide useful or novel information? Does it justify the model's decision? Is it provided with an appropriate representation for the experts? As examples, the assessment proposed by Gale et al. [38] and the first question used by Spinks and Moens [126] measure user impact. Notice that most of these concepts are very subjective, and the definitions, the questions and assessments will vary for different sub-domains and target experts. We believe more specific definitions and fine-grained aspects should arise in the future, as research in this topic grows. For reference, this category includes the domain appropriate representation and potential actionability concepts presented by Tonekaboni et al. [134]. Synthesis. Almost all the works include text quality metrics, though these are not able to capture the medical facts in a report [11,17,92,107,108,161]. Several works proposed medical correctness assessments over the reports, but unfortunately none of the proposals was evaluated against expert judgement. The auxiliary tasks can be evaluated to measure correctness indirectly from the process, but often it will not be sufficient for the report's correctness. Only two works evaluate explainability directly with experts, and the auxiliary tasks' assessments could be useful to measure alignment between the explanations and domain knowledge. Overall, we believe that medical correctness should be the primary aspect to evaluate in the generated reports, using one or more automatic metrics. For now, and even though none of the metrics proposed has been evaluated against expert judgement, MIRQI [162] seems like the most promising approach to fulfill this purpose, as it should be able to capture richer information from the reports. Additionally, text quality metrics can be used as a secondary evaluation, since they may be useful for measuring fluency, grammar or variability, and to compare with previous baselines. Lastly, explainability evaluation methods should arise to assess multiple key aspects, such as its consistency, alignment with domain knowledge, and the user impact.","[['b21', 'b32', 'b112'], ['b123', 'b125', 'b37'], ['b76', 'b125', None], ['b65', 'b2', 'b133'], ['b142', 'b74', 'b125', 'b164'], ['b10', 'b16', 'b125', 'b133', 'b37', 'b161', 'b91', 'b107', 'b106', 'b160'], ['b21', 'b32', 'b112'], ['b123', 'b125', 'b37'], ['b76', 'b125', None], ['b65', 'b2', 'b133'], ['b142', 'b74', 'b125', 'b164'], ['b10', 'b16', 'b125', 'b133', 'b37', 'b161', 'b91', 'b107', 'b106', 'b160']]","[['b21', 'b32', 'b112'], ['b123', 'b125', 'b37'], ['b76', 'b125', None], ['b65', 'b2', 'b133'], ['b142', 'b74', 'b125', 'b164'], ['b10', 'b16', 'b125', 'b133', 'b37', 'b161', 'b91', 'b107', 'b106', 'b160'], ['b21', 'b32', 'b112'], ['b123', 'b125', 'b37'], ['b76', 'b125', None], ['b65', 'b2', 'b133'], ['b142', 'b74', 'b125', 'b164'], ['b10', 'b16', 'b125', 'b133', 'b37', 'b161', 'b91', 'b107', 'b106', 'b160']]",52,"sent1: Providing interpretable justifications for the model's outcome is essential in this medical domain, and furthermore, we should be able to evaluate them to answer questions such as, does the method justify the model's decision?, which method provides a better explanation?
sent2: However, there is no consensus on evaluation methods for AI explainability, and in many cases the definition of a better explanation remains subjective [22,33,113].
sent3: Consequently, none of the papers reviewed used an automatic metric to assess explainability, and only two works [38,126] conduct a formal human expert evaluation.
sent4: Gale et al. [38] presented the report generation as an explanation of a medical image classification task, and evaluated it by comparing three methods: (a) SmoothGrad [124] to highlight the most important pixels used, (b) a generated report in natural language, and (c) both placed side by side.
sent5: Five experts assessed 30 images, rating each explanation in a scale from 1 (unsatisfactory) to 10 (perfect); achieving average scores of (a) 4.4, (b) 7 and (c) 8.8 for each method.
sent6: Though the authors emphasize the importance of the natural language explanations, their approach does not include an explanation for the report itself, so it cannot be directly used for the report generation task.
sent7: The model proposed by Spinks and Moens [126] generates a chest X-ray as a counter-factual example, and they compared this explanation method against a feature importance heatmap generated with the Zagoruyko and Komodakis saliency map technique [77].
sent8: Three experts evaluated 150 samples answering four questions, the first two regarding explainability aspects: ""Does the explanation justify the diagnosis?""
sent9: ""Does the model appear to understand the important parts of the X-ray?""
sent10: The answers were in a scale from 1 (no) to 4 (yes), and their method achieved a higher score than the saliency map (2.39 vs 1.31 for the first question, and 2.45 vs 1.81 for the second), showing their counter-factual approach should be better in this setting.
sent11: The other two questions relate more to medical correctness, and are discussed in the previous section (5.4.2).
sent12: We believe the explanation evaluations should be very important in this area, and as there is no consensus, we outline some possible guidelines.
sent13: Following ideas from Tonekaboni et al. [134], we believe three aspects from the explanations should be assessed: (1) consistency, (2) alignment with domain knowledge, and (3) user impact.
sent14: First, the consistency across the data should be assessed, answering questions such as: do explanations change with variations to the input data?; or to the prediction?; or to the model design?; or with different images from the same patient?
sent15: As pointed out by Tonekaboni et al. [134], inconsistent explanations may negatively affect the clinicians' trust, and an interpretability method laying them out should be reviewed.
sent16: Examples of consistency or robustness evaluations can be found in the work by Adebayo et al. [3] for image saliency maps, and in the work by Jain and Wallace [66] for attention in recurrent neural networks.
sent17: Second, the alignment with domain knowledge should evaluate if the explanation is consistent with an expert's knowledge: would they provide the same explanation for that decision?
sent18: For instance, given a feature importance method, is the model focusing on the correct features?
sent19: As an example, consider the second and third questions employed by Spinks and Moens [126] detailed earlier.
sent20: To mention other examples, Wang et al. [143] evaluated CAM [165] generated heatmaps for disease classification against expert provided bounding-boxes locating the diseases, using intersection-overunion like metrics; Kim et al. [75] proposed a model to classify Diabetic Retinopathy from retina fundus images, and they compared the TCAV [75] extracted concepts against expert knowledge.
sent21: Notice many works reviewed in this survey used classification or segmentation as an auxiliary task, which can be used as local explanations, and evaluated them with common metrics (such as accuracy, precision, etc.), as discussed in the previous sections (5.3 and 5.4.2).
sent22: As the authors did not mention the secondary outputs as local explanations, we categorized the said evaluations as medical correctness metrics, but they are also measuring alignment with domain knowledge for the interpretability methods, and as such may be very useful.
sent23: Lastly, the user impact should attempt to answer questions like, is it a good explanation?
sent24: Does it provide useful or novel information?
sent25: Does it justify the model's decision?
sent26: Is it provided with an appropriate representation for the experts?
sent27: As examples, the assessment proposed by Gale et al. [38] and the first question used by Spinks and Moens [126] measure user impact.
sent28: Notice that most of these concepts are very subjective, and the definitions, the questions and assessments will vary for different sub-domains and target experts.
sent29: We believe more specific definitions and fine-grained aspects should arise in the future, as research in this topic grows.
sent30: For reference, this category includes the domain appropriate representation and potential actionability concepts presented by Tonekaboni et al. [134].
sent31: Synthesis. Almost all the works include text quality metrics, though these are not able to capture the medical facts in a report [11,17,92,107,108,161].
sent32: Several works proposed medical correctness assessments over the reports, but unfortunately none of the proposals was evaluated against expert judgement.
sent33: The auxiliary tasks can be evaluated to measure correctness indirectly from the process, but often it will not be sufficient for the report's correctness.
sent34: Only two works evaluate explainability directly with experts, and the auxiliary tasks' assessments could be useful to measure alignment between the explanations and domain knowledge.
sent35: Overall, we believe that medical correctness should be the primary aspect to evaluate in the generated reports, using one or more automatic metrics.
sent36: For now, and even though none of the metrics proposed has been evaluated against expert judgement, MIRQI [162] seems like the most promising approach to fulfill this purpose, as it should be able to capture richer information from the reports.
sent37: Additionally, text quality metrics can be used as a secondary evaluation, since they may be useful for measuring fluency, grammar or variability, and to compare with previous baselines.
sent38: Lastly, explainability evaluation methods should arise to assess multiple key aspects, such as its consistency, alignment with domain knowledge, and the user impact.
sent39: Providing interpretable justifications for the model's outcome is essential in this medical domain, and furthermore, we should be able to evaluate them to answer questions such as, does the method justify the model's decision?, which method provides a better explanation?
sent40: However, there is no consensus on evaluation methods for AI explainability, and in many cases the definition of a better explanation remains subjective [22,33,113].
sent41: Consequently, none of the papers reviewed used an automatic metric to assess explainability, and only two works [38,126] conduct a formal human expert evaluation.
sent42: Gale et al. [38] presented the report generation as an explanation of a medical image classification task, and evaluated it by comparing three methods: (a) SmoothGrad [124] to highlight the most important pixels used, (b) a generated report in natural language, and (c) both placed side by side.
sent43: Five experts assessed 30 images, rating each explanation in a scale from 1 (unsatisfactory) to 10 (perfect); achieving average scores of (a) 4.4, (b) 7 and (c) 8.8 for each method.
sent44: Though the authors emphasize the importance of the natural language explanations, their approach does not include an explanation for the report itself, so it cannot be directly used for the report generation task.
sent45: The model proposed by Spinks and Moens [126] generates a chest X-ray as a counter-factual example, and they compared this explanation method against a feature importance heatmap generated with the Zagoruyko and Komodakis saliency map technique [77].
sent46: Three experts evaluated 150 samples answering four questions, the first two regarding explainability aspects: ""Does the explanation justify the diagnosis?""
sent47: ""Does the model appear to understand the important parts of the X-ray?""
sent48: The answers were in a scale from 1 (no) to 4 (yes), and their method achieved a higher score than the saliency map (2.39 vs 1.31 for the first question, and 2.45 vs 1.81 for the second), showing their counter-factual approach should be better in this setting.
sent49: The other two questions relate more to medical correctness, and are discussed in the previous section (5.4.2).
sent50: We believe the explanation evaluations should be very important in this area, and as there is no consensus, we outline some possible guidelines.
sent51: Following ideas from Tonekaboni et al. [134], we believe three aspects from the explanations should be assessed: (1) consistency, (2) alignment with domain knowledge, and (3) user impact.
sent52: First, the consistency across the data should be assessed, answering questions such as: do explanations change with variations to the input data?; or to the prediction?; or to the model design?; or with different images from the same patient?
sent53: As pointed out by Tonekaboni et al. [134], inconsistent explanations may negatively affect the clinicians' trust, and an interpretability method laying them out should be reviewed.
sent54: Examples of consistency or robustness evaluations can be found in the work by Adebayo et al. [3] for image saliency maps, and in the work by Jain and Wallace [66] for attention in recurrent neural networks.
sent55: Second, the alignment with domain knowledge should evaluate if the explanation is consistent with an expert's knowledge: would they provide the same explanation for that decision?
sent56: For instance, given a feature importance method, is the model focusing on the correct features?
sent57: As an example, consider the second and third questions employed by Spinks and Moens [126] detailed earlier.
sent58: To mention other examples, Wang et al. [143] evaluated CAM [165] generated heatmaps for disease classification against expert provided bounding-boxes locating the diseases, using intersection-overunion like metrics; Kim et al. [75] proposed a model to classify Diabetic Retinopathy from retina fundus images, and they compared the TCAV [75] extracted concepts against expert knowledge.
sent59: Notice many works reviewed in this survey used classification or segmentation as an auxiliary task, which can be used as local explanations, and evaluated them with common metrics (such as accuracy, precision, etc.), as discussed in the previous sections (5.3 and 5.4.2).
sent60: As the authors did not mention the secondary outputs as local explanations, we categorized the said evaluations as medical correctness metrics, but they are also measuring alignment with domain knowledge for the interpretability methods, and as such may be very useful.
sent61: Lastly, the user impact should attempt to answer questions like, is it a good explanation?
sent62: Does it provide useful or novel information?
sent63: Does it justify the model's decision?
sent64: Is it provided with an appropriate representation for the experts?
sent65: As examples, the assessment proposed by Gale et al. [38] and the first question used by Spinks and Moens [126] measure user impact.
sent66: Notice that most of these concepts are very subjective, and the definitions, the questions and assessments will vary for different sub-domains and target experts.
sent67: We believe more specific definitions and fine-grained aspects should arise in the future, as research in this topic grows.
sent68: For reference, this category includes the domain appropriate representation and potential actionability concepts presented by Tonekaboni et al. [134].
sent69: Synthesis. Almost all the works include text quality metrics, though these are not able to capture the medical facts in a report [11,17,92,107,108,161].
sent70: Several works proposed medical correctness assessments over the reports, but unfortunately none of the proposals was evaluated against expert judgement.
sent71: The auxiliary tasks can be evaluated to measure correctness indirectly from the process, but often it will not be sufficient for the report's correctness.
sent72: Only two works evaluate explainability directly with experts, and the auxiliary tasks' assessments could be useful to measure alignment between the explanations and domain knowledge.
sent73: Overall, we believe that medical correctness should be the primary aspect to evaluate in the generated reports, using one or more automatic metrics.
sent74: For now, and even though none of the metrics proposed has been evaluated against expert judgement, MIRQI [162] seems like the most promising approach to fulfill this purpose, as it should be able to capture richer information from the reports.
sent75: Additionally, text quality metrics can be used as a secondary evaluation, since they may be useful for measuring fluency, grammar or variability, and to compare with previous baselines.
sent76: Lastly, explainability evaluation methods should arise to assess multiple key aspects, such as its consistency, alignment with domain knowledge, and the user impact."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s13,Auxiliary Tasks.,"Although the main objective in most papers is to learn a model for report generation from medical images, many works also include and optimize auxiliary tasks to boost their performance. A summary of these tasks is presented in Table 10 in appendix 9.2. The most common auxiliary tasks are multi-label (16 papers) and single-label (11 papers) classification. These tasks are generally intended to provide additional supervision to the model's visual component, in order to improve the CNN's capabilities to extract quality visual features. Some common tasks are identifying the presence or absence of different abnormalities, diseases, organs, body parts, medical concepts, detecting image modality, etc. Datasets often used for this purpose are ChestX-ray14 [143] and CheXpert [63], where the common practice is to pretrain the CNN in those datasets before moving on to report generation. Many papers report better performance in report generation thanks to these auxiliary classification tasks. The three works [48,67,150] following the hierarchical approach with Dual Word LSTM used a classification task to supervise the gating mechanism that chooses between generating a normal sentence, an abnormal sentence or stopping. Two models [47,131] perform a segmentation task. Tian et al. [131] trained a fully convolutional network (FCN) with segmentation masks of a liver and tumor, and Han et al. [47] trained an RGAN for pixel level classification. Similarly, two models [76,157] use a Faster-RCNN [112] trained for detection and classification of bounding boxes enclosing lesions or other regions of interest in the images.

Two works [95,155] used regularization supervision on attention weights. CORAL8 [95] receives regularization supervision on its visual attention weights to prevent them from degrading into uniform distribution, which would offer no advantage over average pooling. Similarly, Yin et al. [155] added two regularizations to their model's attention weights: one on the weights over spatial visual features and another on the weights over tag embedding vectors. In both works the attention supervision provided a significant contribution to the performance.

Two works [98,155] included a task to enforce a matching between embeddings from two different sources. Yin et al. [155] projected the topic vectors from the Sentence LSTM and the word embeddings from the respective ground-truth sentence into a common semantic space, and enforced a matching via contrastive loss [24]. This task significantly improved the Sentence LSTM's training and the model's overall performance. Moradi et al. [98] trained a MLP for mapping image visual encodings (obtained by a VGG network) to the vector representation of its corresponding groundtruth report (obtained via doc2vec [83], which in itself was another auxiliary task), by minimizing the Euclidean distance. The trained MLP was then used to predict doc2vec representations for unseen images and retrieve the report with the closest representation. Two works [126,132] used text autoencoders, which allow learning compact representations of unlabeled data in a self-supervised manner: an encoder network maps the input into a latent representation, and a decoder network has to recover the original input back. MTMA [132] uses a BiLSTM to encode the sentences of the indication and findings sections of a report (input text), in order to generate the impression section (output). To improve the encoding quality of the BiLSTM, the authors trained the decoder branch of a hierarchical autoencoder [88] to recover the original sentence from the BiLSTM encoding. The experimental results showed that the autoencoder supervision provided a significant boost to the model's performance. Spinks and Moens [126] trained an ARAE [164] (1) to learn compact representations of reports (serving as input to a GAN that generates chest X-ray images) and (2) to recover a report given an arbitrary compact representation (used in inference mode for report generation).

Lastly, Spinks and Moens [126] were the only ones to also implement cycle-consistency tasks [167] to train a GAN and an inverse mapping CNN together, to make both chest X-ray image generation and encoding more robust. These tasks will be further detailed in the next section.

Although the main objective in most papers is to learn a model for report generation from medical images, many works also include and optimize auxiliary tasks to boost their performance. A summary of these tasks is presented in Table 10 in appendix 9.2. The most common auxiliary tasks are multi-label (16 papers) and single-label (11 papers) classification. These tasks are generally intended to provide additional supervision to the model's visual component, in order to improve the CNN's capabilities to extract quality visual features. Some common tasks are identifying the presence or absence of different abnormalities, diseases, organs, body parts, medical concepts, detecting image modality, etc. Datasets often used for this purpose are ChestX-ray14 [143] and CheXpert [63], where the common practice is to pretrain the CNN in those datasets before moving on to report generation. Many papers report better performance in report generation thanks to these auxiliary classification tasks. The three works [48,67,150] following the hierarchical approach with Dual Word LSTM used a classification task to supervise the gating mechanism that chooses between generating a normal sentence, an abnormal sentence or stopping. Two models [47,131] perform a segmentation task. Tian et al. [131] trained a fully convolutional network (FCN) with segmentation masks of a liver and tumor, and Han et al. [47] trained an RGAN for pixel level classification. Similarly, two models [76,157] use a Faster-RCNN [112] trained for detection and classification of bounding boxes enclosing lesions or other regions of interest in the images.

Two works [95,155] used regularization supervision on attention weights. CORAL8 [95] receives regularization supervision on its visual attention weights to prevent them from degrading into uniform distribution, which would offer no advantage over average pooling. Similarly, Yin et al. [155] added two regularizations to their model's attention weights: one on the weights over spatial visual features and another on the weights over tag embedding vectors. In both works the attention supervision provided a significant contribution to the performance.

Two works [98,155] included a task to enforce a matching between embeddings from two different sources. Yin et al. [155] projected the topic vectors from the Sentence LSTM and the word embeddings from the respective ground-truth sentence into a common semantic space, and enforced a matching via contrastive loss [24]. This task significantly improved the Sentence LSTM's training and the model's overall performance. Moradi et al. [98] trained a MLP for mapping image visual encodings (obtained by a VGG network) to the vector representation of its corresponding groundtruth report (obtained via doc2vec [83], which in itself was another auxiliary task), by minimizing the Euclidean distance. The trained MLP was then used to predict doc2vec representations for unseen images and retrieve the report with the closest representation. Two works [126,132] used text autoencoders, which allow learning compact representations of unlabeled data in a self-supervised manner: an encoder network maps the input into a latent representation, and a decoder network has to recover the original input back. MTMA [132] uses a BiLSTM to encode the sentences of the indication and findings sections of a report (input text), in order to generate the impression section (output). To improve the encoding quality of the BiLSTM, the authors trained the decoder branch of a hierarchical autoencoder [88] to recover the original sentence from the BiLSTM encoding. The experimental results showed that the autoencoder supervision provided a significant boost to the model's performance. Spinks and Moens [126] trained an ARAE [164] (1) to learn compact representations of reports (serving as input to a GAN that generates chest X-ray images) and (2) to recover a report given an arbitrary compact representation (used in inference mode for report generation).

Lastly, Spinks and Moens [126] were the only ones to also implement cycle-consistency tasks [167] to train a GAN and an inverse mapping CNN together, to make both chest X-ray image generation and encoding more robust. These tasks will be further detailed in the next section.","[['b111', 'b62', 'b156', 'b142', 'b47', 'b66', 'b46', 'b75', 'b149', 'b130'], ['b154', 'b94'], ['b125', 'b82', 'b131', 'b163', 'b97', 'b23', 'b87', 'b154'], ['b166', 'b125'], ['b111', 'b62', 'b156', 'b142', 'b47', 'b66', 'b46', 'b75', 'b149', 'b130'], ['b154', 'b94'], ['b125', 'b82', 'b131', 'b163', 'b97', 'b23', 'b87', 'b154'], ['b166', 'b125']]","[['b111', 'b62', 'b156', 'b142', 'b47', 'b66', 'b46', 'b75', 'b149', 'b130'], ['b154', 'b94'], ['b125', 'b82', 'b131', 'b163', 'b97', 'b23', 'b87', 'b154'], ['b166', 'b125'], ['b111', 'b62', 'b156', 'b142', 'b47', 'b66', 'b46', 'b75', 'b149', 'b130'], ['b154', 'b94'], ['b125', 'b82', 'b131', 'b163', 'b97', 'b23', 'b87', 'b154'], ['b166', 'b125']]",44,"sent1: Although the main objective in most papers is to learn a model for report generation from medical images, many works also include and optimize auxiliary tasks to boost their performance.
sent2: A summary of these tasks is presented in Table 10 in appendix 9.2.
sent3: The most common auxiliary tasks are multi-label (16 papers) and single-label (11 papers) classification.
sent4: These tasks are generally intended to provide additional supervision to the model's visual component, in order to improve the CNN's capabilities to extract quality visual features.
sent5: Some common tasks are identifying the presence or absence of different abnormalities, diseases, organs, body parts, medical concepts, detecting image modality, etc.
sent6: Datasets often used for this purpose are ChestX-ray14 [143] and CheXpert [63], where the common practice is to pretrain the CNN in those datasets before moving on to report generation.
sent7: Many papers report better performance in report generation thanks to these auxiliary classification tasks.
sent8: The three works [48,67,150] following the hierarchical approach with Dual Word LSTM used a classification task to supervise the gating mechanism that chooses between generating a normal sentence, an abnormal sentence or stopping.
sent9: Two models [47,131] perform a segmentation task.
sent10: Tian et al. [131] trained a fully convolutional network (FCN) with segmentation masks of a liver and tumor, and Han et al. [47] trained an RGAN for pixel level classification.
sent11: Similarly, two models [76,157] use a Faster-RCNN [112] trained for detection and classification of bounding boxes enclosing lesions or other regions of interest in the images.
sent12: Two works [95,155] used regularization supervision on attention weights.
sent13: CORAL8 [95] receives regularization supervision on its visual attention weights to prevent them from degrading into uniform distribution, which would offer no advantage over average pooling.
sent14: Similarly, Yin et al. [155] added two regularizations to their model's attention weights: one on the weights over spatial visual features and another on the weights over tag embedding vectors.
sent15: In both works the attention supervision provided a significant contribution to the performance.
sent16: Two works [98,155] included a task to enforce a matching between embeddings from two different sources.
sent17: Yin et al. [155] projected the topic vectors from the Sentence LSTM and the word embeddings from the respective ground-truth sentence into a common semantic space, and enforced a matching via contrastive loss [24].
sent18: This task significantly improved the Sentence LSTM's training and the model's overall performance.
sent19: Moradi et al. [98] trained a MLP for mapping image visual encodings (obtained by a VGG network) to the vector representation of its corresponding groundtruth report (obtained via doc2vec [83], which in itself was another auxiliary task), by minimizing the Euclidean distance.
sent20: The trained MLP was then used to predict doc2vec representations for unseen images and retrieve the report with the closest representation.
sent21: Two works [126,132] used text autoencoders, which allow learning compact representations of unlabeled data in a self-supervised manner: an encoder network maps the input into a latent representation, and a decoder network has to recover the original input back.
sent22: MTMA [132] uses a BiLSTM to encode the sentences of the indication and findings sections of a report (input text), in order to generate the impression section (output).
sent23: To improve the encoding quality of the BiLSTM, the authors trained the decoder branch of a hierarchical autoencoder [88] to recover the original sentence from the BiLSTM encoding.
sent24: The experimental results showed that the autoencoder supervision provided a significant boost to the model's performance.
sent25: Spinks and Moens [126] trained an ARAE [164] (1) to learn compact representations of reports (serving as input to a GAN that generates chest X-ray images) and (2) to recover a report given an arbitrary compact representation (used in inference mode for report generation).
sent26: Lastly, Spinks and Moens [126] were the only ones to also implement cycle-consistency tasks [167] to train a GAN and an inverse mapping CNN together, to make both chest X-ray image generation and encoding more robust.
sent27: These tasks will be further detailed in the next section.
sent28: Although the main objective in most papers is to learn a model for report generation from medical images, many works also include and optimize auxiliary tasks to boost their performance.
sent29: A summary of these tasks is presented in Table 10 in appendix 9.2.
sent30: The most common auxiliary tasks are multi-label (16 papers) and single-label (11 papers) classification.
sent31: These tasks are generally intended to provide additional supervision to the model's visual component, in order to improve the CNN's capabilities to extract quality visual features.
sent32: Some common tasks are identifying the presence or absence of different abnormalities, diseases, organs, body parts, medical concepts, detecting image modality, etc.
sent33: Datasets often used for this purpose are ChestX-ray14 [143] and CheXpert [63], where the common practice is to pretrain the CNN in those datasets before moving on to report generation.
sent34: Many papers report better performance in report generation thanks to these auxiliary classification tasks.
sent35: The three works [48,67,150] following the hierarchical approach with Dual Word LSTM used a classification task to supervise the gating mechanism that chooses between generating a normal sentence, an abnormal sentence or stopping.
sent36: Two models [47,131] perform a segmentation task.
sent37: Tian et al. [131] trained a fully convolutional network (FCN) with segmentation masks of a liver and tumor, and Han et al. [47] trained an RGAN for pixel level classification.
sent38: Similarly, two models [76,157] use a Faster-RCNN [112] trained for detection and classification of bounding boxes enclosing lesions or other regions of interest in the images.
sent39: Two works [95,155] used regularization supervision on attention weights.
sent40: CORAL8 [95] receives regularization supervision on its visual attention weights to prevent them from degrading into uniform distribution, which would offer no advantage over average pooling.
sent41: Similarly, Yin et al. [155] added two regularizations to their model's attention weights: one on the weights over spatial visual features and another on the weights over tag embedding vectors.
sent42: In both works the attention supervision provided a significant contribution to the performance.
sent43: Two works [98,155] included a task to enforce a matching between embeddings from two different sources.
sent44: Yin et al. [155] projected the topic vectors from the Sentence LSTM and the word embeddings from the respective ground-truth sentence into a common semantic space, and enforced a matching via contrastive loss [24].
sent45: This task significantly improved the Sentence LSTM's training and the model's overall performance.
sent46: Moradi et al. [98] trained a MLP for mapping image visual encodings (obtained by a VGG network) to the vector representation of its corresponding groundtruth report (obtained via doc2vec [83], which in itself was another auxiliary task), by minimizing the Euclidean distance.
sent47: The trained MLP was then used to predict doc2vec representations for unseen images and retrieve the report with the closest representation.
sent48: Two works [126,132] used text autoencoders, which allow learning compact representations of unlabeled data in a self-supervised manner: an encoder network maps the input into a latent representation, and a decoder network has to recover the original input back.
sent49: MTMA [132] uses a BiLSTM to encode the sentences of the indication and findings sections of a report (input text), in order to generate the impression section (output).
sent50: To improve the encoding quality of the BiLSTM, the authors trained the decoder branch of a hierarchical autoencoder [88] to recover the original sentence from the BiLSTM encoding.
sent51: The experimental results showed that the autoencoder supervision provided a significant boost to the model's performance.
sent52: Spinks and Moens [126] trained an ARAE [164] (1) to learn compact representations of reports (serving as input to a GAN that generates chest X-ray images) and (2) to recover a report given an arbitrary compact representation (used in inference mode for report generation).
sent53: Lastly, Spinks and Moens [126] were the only ones to also implement cycle-consistency tasks [167] to train a GAN and an inverse mapping CNN together, to make both chest X-ray image generation and encoding more robust.
sent54: These tasks will be further detailed in the next section."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s11,Language Component.,"The job of the language component is to generate the report. In contrast to the visual component, in the literature we find a greater variety of approaches and creative ideas applied to this component. Table 5 presents a high-level summary of this analysis. The simplest approach is the use of a recurrent neural network, such as LSTM or GRU, to generate the full report word by word. Nine works [40,44,51,87,123,128,148,157,158] used LSTM and one work [120] tried both GRU and LSTM. All these works have in common that the GRU/LSTM receives an encoding vector from the visual component at the beginning and the full report is decoded from it. This encoding vector is typically a vector of global features output by the CNN. However, two of these works [44,128] compute a weighted sum of tag embedding vectors and provide that as input to the LSTM. Five works [38,89,131,144,163] used LSTM enhanced with an attention mechanism. In addition to the initial input, the LSTM equipped with attention can selectively attend to visual features from the visual component at each recurrent step. This typically leads to improved performance in all papers.

A known problem for recurrent networks such as LSTM is that they are not very good at generating very long texts [103]. This is not a worrying issue when reports are short, however, it can become one for long multi-sentence reports. Two papers [131,163] worked around this by generating each sentence independently with a single LSTM and then concatenating these sentences together. They accomplished this by providing the LSTM with a vector that indicates the sentence type as first input. This worked well in their case because the models were designed for structured reports, i.e., a fixed number of sentences per report and a fixed topic per sentence. Vispi [89] adopts a similar strategy: for each disease a dedicated LSTM generates the corresponding sentence, and the final report is the concatenation of them.

To tackle the generation of unstructured multi-sentence reports, a group of papers followed what we call the Hierarchical LSTM with attention approach: a Sentence LSTM generates a sequence of topic vectors, and a Word LSTM receives a topic vector and generates a sentence word by word. In this setting, the attention mechanism can be present at the sentence level, the word level or both. Figure 3 shows an illustrative example. Seven works [61,68,92,132,155,156,162] followed this approach. A common result in these papers is that a Hierarchical LSTM yields better performance in multi-sentence report generation than a single, flat LSTM. A few papers [48,67,150] went one step further and replaced the normal Word LSTM with a Dual Word LSTM: the model has a gating mechanism at the sentence level that decides if the sentence will describe an abnormality (e.g., a detected cardiomegaly) or a healthy case. Thus, there are two Word LSTMs, one for normal and one for abnormal sentences. The goal is to improve the generation of abnormal sentences by having a Word LSTM that specializes in generating them. In contrast, a single Word LSTM for everything can lead to overlearning of normal sentences and underlearning of abnormal ones, as the latter are typically less frequent due to class imbalances in datasets. The ablation analyses of these works show performance gains, thanks to this approach.

Another approach for multi-sentence report generation is the Recurrent BiLSTM-attention-LSTM approach. The basic idea is to have a LSTM generate one sentence at a time, each time conditioned on a BiLSTM based encoding of the previous sentence and the output of an attention mechanism. The process is repeated recurrently sentence by sentence until the full report is generated. Three papers used this approach [95,153,154].

Two works [7,36] approached report generation as simply learning to predict the next word given a partial report and an image. The models have dedicated components, such as LSTM and BiLSTM, for encoding the partial report and the image, and the next word is predicted by an FC layer. This approach simplifies the task (i.e., predict the next word given everything that comes before), but in practice requires that the model be applied recurrently one word at a time to produce a full report, which has quadratic instead of linear complexity.

Only one work, RTMIC [151], has explored the use of the Transformer [139] architecture for report generation. In RTMIC multiple image crops are obtained using Grad-CAM, then from each crop a feature vector is obtained, and finally a Transformer converts these vectors into a report. The paper's results show some performance gains in CIDEr and BLEU with respect to some baselines that do not use the Transformer. Likewise, Spinks and Moens [126] were the only ones to use an adversarially regularized autoencoder (ARAE) [164] to generate reports. Their model combines an ARAE with a StackGAN and a normal CNN, achieving better performance than a convolutional caption generation baseline in several NLP metrics.

We also identify a group of papers [47,49,76,94,98] following a Template based approach. The language component in these works operates programmatically by following if-then rules or other heuristics in order to retrieve, fill and/or combine templates from a database in order to generate a report. The visual component typically outputs discrete classification labels that the language component processes programmatically. In the case of Harzig et al. 2019b [49], image localizations per class are also recovered using CAM [165], and in the case of Han et al. [47] the visual component outputs an image segmentation. In both cases the language component includes special localization-based rules or templates, thus incorporating location information in the generated report. Kisilev et al. [76] followed a different approach: a multi-layer perceptron learns to map image encodings to doc2vec [83] representations of corresponding reports. During inference, the ground-truth report with the closest doc2vec representation is retrieved.

Lastly, we identify three papers [16,85,86] following the Hybrid template retrieval + generation/edition approach. These works seek to combine the benefits of templates with the flexibility of a generative module to either generate sentences from scratch or paraphrase templates as needed on a case-by-case basis. KERP [86] uses Graph Transformers (GTR) to map the visual input into a sequence of templates from a curated database. A Paraphrase GTR then maps each template to its paraphrased version. HRGR [85] follows the hierarchical LSTM approach with a twist-it replaces the Word LSTM with a gate module that chooses between two options: retrieving a template or generating a sentence from scratch (via a Word LSTM). Lastly, CLARA [16] is somewhat different, as it was designed as an interactive tool to assist a human to write reports. A human introduces anchor words and the prefix of a sentence, and Lucene [18] processes them as a query to retrieve sentence templates from a database. A sequence-to-sequence network then reads and paraphrases each sentence template to get the final report. CLARA can also operate fully automatically by receiving an empty prefix and predicting the anchor words itself. According to reported results, the model consistently achieved better performance than many baselines.

The job of the language component is to generate the report. In contrast to the visual component, in the literature we find a greater variety of approaches and creative ideas applied to this component. Table 5 presents a high-level summary of this analysis. The simplest approach is the use of a recurrent neural network, such as LSTM or GRU, to generate the full report word by word. Nine works [40,44,51,87,123,128,148,157,158] used LSTM and one work [120] tried both GRU and LSTM. All these works have in common that the GRU/LSTM receives an encoding vector from the visual component at the beginning and the full report is decoded from it. This encoding vector is typically a vector of global features output by the CNN. However, two of these works [44,128] compute a weighted sum of tag embedding vectors and provide that as input to the LSTM. Five works [38,89,131,144,163] used LSTM enhanced with an attention mechanism. In addition to the initial input, the LSTM equipped with attention can selectively attend to visual features from the visual component at each recurrent step. This typically leads to improved performance in all papers.

A known problem for recurrent networks such as LSTM is that they are not very good at generating very long texts [103]. This is not a worrying issue when reports are short, however, it can become one for long multi-sentence reports. Two papers [131,163] worked around this by generating each sentence independently with a single LSTM and then concatenating these sentences together. They accomplished this by providing the LSTM with a vector that indicates the sentence type as first input. This worked well in their case because the models were designed for structured reports, i.e., a fixed number of sentences per report and a fixed topic per sentence. Vispi [89] adopts a similar strategy: for each disease a dedicated LSTM generates the corresponding sentence, and the final report is the concatenation of them.

To tackle the generation of unstructured multi-sentence reports, a group of papers followed what we call the Hierarchical LSTM with attention approach: a Sentence LSTM generates a sequence of topic vectors, and a Word LSTM receives a topic vector and generates a sentence word by word. In this setting, the attention mechanism can be present at the sentence level, the word level or both. Figure 3 shows an illustrative example. Seven works [61,68,92,132,155,156,162] followed this approach. A common result in these papers is that a Hierarchical LSTM yields better performance in multi-sentence report generation than a single, flat LSTM. A few papers [48,67,150] went one step further and replaced the normal Word LSTM with a Dual Word LSTM: the model has a gating mechanism at the sentence level that decides if the sentence will describe an abnormality (e.g., a detected cardiomegaly) or a healthy case. Thus, there are two Word LSTMs, one for normal and one for abnormal sentences. The goal is to improve the generation of abnormal sentences by having a Word LSTM that specializes in generating them. In contrast, a single Word LSTM for everything can lead to overlearning of normal sentences and underlearning of abnormal ones, as the latter are typically less frequent due to class imbalances in datasets. The ablation analyses of these works show performance gains, thanks to this approach.

Another approach for multi-sentence report generation is the Recurrent BiLSTM-attention-LSTM approach. The basic idea is to have a LSTM generate one sentence at a time, each time conditioned on a BiLSTM based encoding of the previous sentence and the output of an attention mechanism. The process is repeated recurrently sentence by sentence until the full report is generated. Three papers used this approach [95,153,154].

Two works [7,36] approached report generation as simply learning to predict the next word given a partial report and an image. The models have dedicated components, such as LSTM and BiLSTM, for encoding the partial report and the image, and the next word is predicted by an FC layer. This approach simplifies the task (i.e., predict the next word given everything that comes before), but in practice requires that the model be applied recurrently one word at a time to produce a full report, which has quadratic instead of linear complexity.

Only one work, RTMIC [151], has explored the use of the Transformer [139] architecture for report generation. In RTMIC multiple image crops are obtained using Grad-CAM, then from each crop a feature vector is obtained, and finally a Transformer converts these vectors into a report. The paper's results show some performance gains in CIDEr and BLEU with respect to some baselines that do not use the Transformer. Likewise, Spinks and Moens [126] were the only ones to use an adversarially regularized autoencoder (ARAE) [164] to generate reports. Their model combines an ARAE with a StackGAN and a normal CNN, achieving better performance than a convolutional caption generation baseline in several NLP metrics.

We also identify a group of papers [47,49,76,94,98] following a Template based approach. The language component in these works operates programmatically by following if-then rules or other heuristics in order to retrieve, fill and/or combine templates from a database in order to generate a report. The visual component typically outputs discrete classification labels that the language component processes programmatically. In the case of Harzig et al. 2019b [49], image localizations per class are also recovered using CAM [165], and in the case of Han et al. [47] the visual component outputs an image segmentation. In both cases the language component includes special localization-based rules or templates, thus incorporating location information in the generated report. Kisilev et al. [76] followed a different approach: a multi-layer perceptron learns to map image encodings to doc2vec [83] representations of corresponding reports. During inference, the ground-truth report with the closest doc2vec representation is retrieved.

Lastly, we identify three papers [16,85,86] following the Hybrid template retrieval + generation/edition approach. These works seek to combine the benefits of templates with the flexibility of a generative module to either generate sentences from scratch or paraphrase templates as needed on a case-by-case basis. KERP [86] uses Graph Transformers (GTR) to map the visual input into a sequence of templates from a curated database. A Paraphrase GTR then maps each template to its paraphrased version. HRGR [85] follows the hierarchical LSTM approach with a twist-it replaces the Word LSTM with a gate module that chooses between two options: retrieving a template or generating a sentence from scratch (via a Word LSTM). Lastly, CLARA [16] is somewhat different, as it was designed as an interactive tool to assist a human to write reports. A human introduces anchor words and the prefix of a sentence, and Lucene [18] processes them as a query to retrieve sentence templates from a database. A sequence-to-sequence network then reads and paraphrases each sentence template to get the final report. CLARA can also operate fully automatically by receiving an empty prefix and predicting the anchor words itself. According to reported results, the model consistently achieved better performance than many baselines.","[['b88', 'b43', 'b157', 'b119', 'b50', 'b156', 'b37', 'b143', 'b147', 'b162', 'b39', 'b130', 'b86', 'b122', 'b127'], ['b102', 'b162', 'b130', 'b88'], ['b67', 'b161', 'b131', 'b60', 'b155', 'b47', 'b66', 'b91', 'b149', 'b154'], ['b153', 'b94', 'b152'], ['b35', 'b6'], ['b163', 'b150', 'b138', 'b125'], ['b82', 'b46', 'b97', 'b48', 'b75', 'b93', 'b164'], ['b84', 'b17', 'b85', 'b15'], ['b88', 'b43', 'b157', 'b119', 'b50', 'b156', 'b37', 'b143', 'b147', 'b162', 'b39', 'b130', 'b86', 'b122', 'b127'], ['b102', 'b162', 'b130', 'b88'], ['b67', 'b161', 'b131', 'b60', 'b155', 'b47', 'b66', 'b91', 'b149', 'b154'], ['b153', 'b94', 'b152'], ['b35', 'b6'], ['b163', 'b150', 'b138', 'b125'], ['b82', 'b46', 'b97', 'b48', 'b75', 'b93', 'b164'], ['b84', 'b17', 'b85', 'b15']]","[['b88', 'b43', 'b157', 'b119', 'b50', 'b156', 'b37', 'b143', 'b147', 'b162', 'b39', 'b130', 'b86', 'b122', 'b127'], ['b102', 'b162', 'b130', 'b88'], ['b67', 'b161', 'b131', 'b60', 'b155', 'b47', 'b66', 'b91', 'b149', 'b154'], ['b153', 'b94', 'b152'], ['b35', 'b6'], ['b163', 'b150', 'b138', 'b125'], ['b82', 'b46', 'b97', 'b48', 'b75', 'b93', 'b164'], ['b84', 'b17', 'b85', 'b15'], ['b88', 'b43', 'b157', 'b119', 'b50', 'b156', 'b37', 'b143', 'b147', 'b162', 'b39', 'b130', 'b86', 'b122', 'b127'], ['b102', 'b162', 'b130', 'b88'], ['b67', 'b161', 'b131', 'b60', 'b155', 'b47', 'b66', 'b91', 'b149', 'b154'], ['b153', 'b94', 'b152'], ['b35', 'b6'], ['b163', 'b150', 'b138', 'b125'], ['b82', 'b46', 'b97', 'b48', 'b75', 'b93', 'b164'], ['b84', 'b17', 'b85', 'b15']]",98,"sent1: The job of the language component is to generate the report.
sent2: In contrast to the visual component, in the literature we find a greater variety of approaches and creative ideas applied to this component.
sent3: Table 5 presents a high-level summary of this analysis.
sent4: The simplest approach is the use of a recurrent neural network, such as LSTM or GRU, to generate the full report word by word.
sent5: Nine works [40,44,51,87,123,128,148,157,158] used LSTM and one work [120] tried both GRU and LSTM.
sent6: All these works have in common that the GRU/LSTM receives an encoding vector from the visual component at the beginning and the full report is decoded from it.
sent7: This encoding vector is typically a vector of global features output by the CNN.
sent8: However, two of these works [44,128] compute a weighted sum of tag embedding vectors and provide that as input to the LSTM.
sent9: Five works [38,89,131,144,163] used LSTM enhanced with an attention mechanism.
sent10: In addition to the initial input, the LSTM equipped with attention can selectively attend to visual features from the visual component at each recurrent step.
sent11: This typically leads to improved performance in all papers.
sent12: A known problem for recurrent networks such as LSTM is that they are not very good at generating very long texts [103].
sent13: This is not a worrying issue when reports are short, however, it can become one for long multi-sentence reports.
sent14: Two papers [131,163] worked around this by generating each sentence independently with a single LSTM and then concatenating these sentences together.
sent15: They accomplished this by providing the LSTM with a vector that indicates the sentence type as first input.
sent16: This worked well in their case because the models were designed for structured reports, i.e., a fixed number of sentences per report and a fixed topic per sentence.
sent17: Vispi [89] adopts a similar strategy: for each disease a dedicated LSTM generates the corresponding sentence, and the final report is the concatenation of them.
sent18: To tackle the generation of unstructured multi-sentence reports, a group of papers followed what we call the Hierarchical LSTM with attention approach: a Sentence LSTM generates a sequence of topic vectors, and a Word LSTM receives a topic vector and generates a sentence word by word.
sent19: In this setting, the attention mechanism can be present at the sentence level, the word level or both.
sent20: Figure 3 shows an illustrative example.
sent21: Seven works [61,68,92,132,155,156,162] followed this approach.
sent22: A common result in these papers is that a Hierarchical LSTM yields better performance in multi-sentence report generation than a single, flat LSTM.
sent23: A few papers [48,67,150] went one step further and replaced the normal Word LSTM with a Dual Word LSTM: the model has a gating mechanism at the sentence level that decides if the sentence will describe an abnormality (e.g., a detected cardiomegaly) or a healthy case.
sent24: Thus, there are two Word LSTMs, one for normal and one for abnormal sentences.
sent25: The goal is to improve the generation of abnormal sentences by having a Word LSTM that specializes in generating them.
sent26: In contrast, a single Word LSTM for everything can lead to overlearning of normal sentences and underlearning of abnormal ones, as the latter are typically less frequent due to class imbalances in datasets.
sent27: The ablation analyses of these works show performance gains, thanks to this approach.
sent28: Another approach for multi-sentence report generation is the Recurrent BiLSTM-attention-LSTM approach.
sent29: The basic idea is to have a LSTM generate one sentence at a time, each time conditioned on a BiLSTM based encoding of the previous sentence and the output of an attention mechanism.
sent30: The process is repeated recurrently sentence by sentence until the full report is generated.
sent31: Three papers used this approach [95,153,154].
sent32: Two works [7,36] approached report generation as simply learning to predict the next word given a partial report and an image.
sent33: The models have dedicated components, such as LSTM and BiLSTM, for encoding the partial report and the image, and the next word is predicted by an FC layer.
sent34: This approach simplifies the task (i.e., predict the next word given everything that comes before), but in practice requires that the model be applied recurrently one word at a time to produce a full report, which has quadratic instead of linear complexity.
sent35: Only one work, RTMIC [151], has explored the use of the Transformer [139] architecture for report generation.
sent36: In RTMIC multiple image crops are obtained using Grad-CAM, then from each crop a feature vector is obtained, and finally a Transformer converts these vectors into a report.
sent37: The paper's results show some performance gains in CIDEr and BLEU with respect to some baselines that do not use the Transformer.
sent38: Likewise, Spinks and Moens [126] were the only ones to use an adversarially regularized autoencoder (ARAE) [164] to generate reports.
sent39: Their model combines an ARAE with a StackGAN and a normal CNN, achieving better performance than a convolutional caption generation baseline in several NLP metrics.
sent40: We also identify a group of papers [47,49,76,94,98] following a Template based approach.
sent41: The language component in these works operates programmatically by following if-then rules or other heuristics in order to retrieve, fill and/or combine templates from a database in order to generate a report.
sent42: The visual component typically outputs discrete classification labels that the language component processes programmatically.
sent43: In the case of Harzig et al. 2019b [49], image localizations per class are also recovered using CAM [165], and in the case of Han et al. [47] the visual component outputs an image segmentation.
sent44: In both cases the language component includes special localization-based rules or templates, thus incorporating location information in the generated report.
sent45: Kisilev et al. [76] followed a different approach: a multi-layer perceptron learns to map image encodings to doc2vec [83] representations of corresponding reports.
sent46: During inference, the ground-truth report with the closest doc2vec representation is retrieved.
sent47: Lastly, we identify three papers [16,85,86] following the Hybrid template retrieval + generation/edition approach.
sent48: These works seek to combine the benefits of templates with the flexibility of a generative module to either generate sentences from scratch or paraphrase templates as needed on a case-by-case basis.
sent49: KERP [86] uses Graph Transformers (GTR) to map the visual input into a sequence of templates from a curated database.
sent50: A Paraphrase GTR then maps each template to its paraphrased version.
sent51: HRGR [85] follows the hierarchical LSTM approach with a twist-it replaces the Word LSTM with a gate module that chooses between two options: retrieving a template or generating a sentence from scratch (via a Word LSTM).
sent52: Lastly, CLARA [16] is somewhat different, as it was designed as an interactive tool to assist a human to write reports.
sent53: A human introduces anchor words and the prefix of a sentence, and Lucene [18] processes them as a query to retrieve sentence templates from a database.
sent54: A sequence-to-sequence network then reads and paraphrases each sentence template to get the final report.
sent55: CLARA can also operate fully automatically by receiving an empty prefix and predicting the anchor words itself.
sent56: According to reported results, the model consistently achieved better performance than many baselines.
sent57: The job of the language component is to generate the report.
sent58: In contrast to the visual component, in the literature we find a greater variety of approaches and creative ideas applied to this component.
sent59: Table 5 presents a high-level summary of this analysis.
sent60: The simplest approach is the use of a recurrent neural network, such as LSTM or GRU, to generate the full report word by word.
sent61: Nine works [40,44,51,87,123,128,148,157,158] used LSTM and one work [120] tried both GRU and LSTM.
sent62: All these works have in common that the GRU/LSTM receives an encoding vector from the visual component at the beginning and the full report is decoded from it.
sent63: This encoding vector is typically a vector of global features output by the CNN.
sent64: However, two of these works [44,128] compute a weighted sum of tag embedding vectors and provide that as input to the LSTM.
sent65: Five works [38,89,131,144,163] used LSTM enhanced with an attention mechanism.
sent66: In addition to the initial input, the LSTM equipped with attention can selectively attend to visual features from the visual component at each recurrent step.
sent67: This typically leads to improved performance in all papers.
sent68: A known problem for recurrent networks such as LSTM is that they are not very good at generating very long texts [103].
sent69: This is not a worrying issue when reports are short, however, it can become one for long multi-sentence reports.
sent70: Two papers [131,163] worked around this by generating each sentence independently with a single LSTM and then concatenating these sentences together.
sent71: They accomplished this by providing the LSTM with a vector that indicates the sentence type as first input.
sent72: This worked well in their case because the models were designed for structured reports, i.e., a fixed number of sentences per report and a fixed topic per sentence.
sent73: Vispi [89] adopts a similar strategy: for each disease a dedicated LSTM generates the corresponding sentence, and the final report is the concatenation of them.
sent74: To tackle the generation of unstructured multi-sentence reports, a group of papers followed what we call the Hierarchical LSTM with attention approach: a Sentence LSTM generates a sequence of topic vectors, and a Word LSTM receives a topic vector and generates a sentence word by word.
sent75: In this setting, the attention mechanism can be present at the sentence level, the word level or both.
sent76: Figure 3 shows an illustrative example.
sent77: Seven works [61,68,92,132,155,156,162] followed this approach.
sent78: A common result in these papers is that a Hierarchical LSTM yields better performance in multi-sentence report generation than a single, flat LSTM.
sent79: A few papers [48,67,150] went one step further and replaced the normal Word LSTM with a Dual Word LSTM: the model has a gating mechanism at the sentence level that decides if the sentence will describe an abnormality (e.g., a detected cardiomegaly) or a healthy case.
sent80: Thus, there are two Word LSTMs, one for normal and one for abnormal sentences.
sent81: The goal is to improve the generation of abnormal sentences by having a Word LSTM that specializes in generating them.
sent82: In contrast, a single Word LSTM for everything can lead to overlearning of normal sentences and underlearning of abnormal ones, as the latter are typically less frequent due to class imbalances in datasets.
sent83: The ablation analyses of these works show performance gains, thanks to this approach.
sent84: Another approach for multi-sentence report generation is the Recurrent BiLSTM-attention-LSTM approach.
sent85: The basic idea is to have a LSTM generate one sentence at a time, each time conditioned on a BiLSTM based encoding of the previous sentence and the output of an attention mechanism.
sent86: The process is repeated recurrently sentence by sentence until the full report is generated.
sent87: Three papers used this approach [95,153,154].
sent88: Two works [7,36] approached report generation as simply learning to predict the next word given a partial report and an image.
sent89: The models have dedicated components, such as LSTM and BiLSTM, for encoding the partial report and the image, and the next word is predicted by an FC layer.
sent90: This approach simplifies the task (i.e., predict the next word given everything that comes before), but in practice requires that the model be applied recurrently one word at a time to produce a full report, which has quadratic instead of linear complexity.
sent91: Only one work, RTMIC [151], has explored the use of the Transformer [139] architecture for report generation.
sent92: In RTMIC multiple image crops are obtained using Grad-CAM, then from each crop a feature vector is obtained, and finally a Transformer converts these vectors into a report.
sent93: The paper's results show some performance gains in CIDEr and BLEU with respect to some baselines that do not use the Transformer.
sent94: Likewise, Spinks and Moens [126] were the only ones to use an adversarially regularized autoencoder (ARAE) [164] to generate reports.
sent95: Their model combines an ARAE with a StackGAN and a normal CNN, achieving better performance than a convolutional caption generation baseline in several NLP metrics.
sent96: We also identify a group of papers [47,49,76,94,98] following a Template based approach.
sent97: The language component in these works operates programmatically by following if-then rules or other heuristics in order to retrieve, fill and/or combine templates from a database in order to generate a report.
sent98: The visual component typically outputs discrete classification labels that the language component processes programmatically.
sent99: In the case of Harzig et al. 2019b [49], image localizations per class are also recovered using CAM [165], and in the case of Han et al. [47] the visual component outputs an image segmentation.
sent100: In both cases the language component includes special localization-based rules or templates, thus incorporating location information in the generated report.
sent101: Kisilev et al. [76] followed a different approach: a multi-layer perceptron learns to map image encodings to doc2vec [83] representations of corresponding reports.
sent102: During inference, the ground-truth report with the closest doc2vec representation is retrieved.
sent103: Lastly, we identify three papers [16,85,86] following the Hybrid template retrieval + generation/edition approach.
sent104: These works seek to combine the benefits of templates with the flexibility of a generative module to either generate sentences from scratch or paraphrase templates as needed on a case-by-case basis.
sent105: KERP [86] uses Graph Transformers (GTR) to map the visual input into a sequence of templates from a curated database.
sent106: A Paraphrase GTR then maps each template to its paraphrased version.
sent107: HRGR [85] follows the hierarchical LSTM approach with a twist-it replaces the Word LSTM with a gate module that chooses between two options: retrieving a template or generating a sentence from scratch (via a Word LSTM).
sent108: Lastly, CLARA [16] is somewhat different, as it was designed as an interactive tool to assist a human to write reports.
sent109: A human introduces anchor words and the prefix of a sentence, and Lucene [18] processes them as a query to retrieve sentence templates from a database.
sent110: A sequence-to-sequence network then reads and paraphrases each sentence template to get the final report.
sent111: CLARA can also operate fully automatically by receiving an empty prefix and predicting the anchor words itself.
sent112: According to reported results, the model consistently achieved better performance than many baselines."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s10,Visual Component.,"The most important observation is that all surveyed works use CNNs to process the input images. This is not surprising since CNNs have dominated the state of the art in computer vision for several years [74]. The typical visual processing pipeline consists of a CNN that receives an input image and outputs a volume of feature maps of dimensions × × , where and denote spatial dimensions (width and height) and denotes the channel dimensions (depth or number of feature maps). These visual features are then leveraged by the language component to make decisions for report generation (e.g., which sentence to write, which template to retrieve, next word to output, etc.), typically by way of an attention mechanism.

However, some works did not strictly follow this pattern. For example, in two works [44,128] a CNN is used for multi-label classification of tags, which are then mapped to embedded vectors via embedding matrix lookup. Thus, the report generation module only has access to these tag vectors but no access to the visual features themselves. Similarly, two works [68,155] classify and look up tag embedding vectors, but unlike the previous works, the language component uses co-attention to access both tags vectors and visual features simultaneously. Their ablation analysis showed that the semantic information provided by these tags complements the visual information and improves the model's performance in report generation. Other works [86,162] used graph neural networks immediately after the CNN to encode the visual information in terms of medical concepts and their relations. Thus, the language component receives the intermediate graph representation instead of the raw visual features. The ablation analysis by Zhang et al. [162] showed some performance gains thanks to the graph neural network. Vispi [89] implements a two-stage procedure, where two distinct CNNs are used. In the first stage a DenseNet 121 [60] classifies abnormalities in the image, and then Grad-CAM [118] is used to localize and crop a region of the image for each detected class. Then, in the second stage the multiple image crops are treated as independent images and processed by a typical CNN+LSTM architecture, with ResNet 101 [52] as the CNN. A similar idea was followed in RTMIC [151], where a DenseNet 121 is pretrained for classification in ChestX-ray14 [143] and CAM is used to get image crops for each class.

We observe a wide variety of CNN architectures used in the literature, though most works employ standard designs. Table 4 presents a summary. The most common ones are ResNet (11 works), VGG (11 works), and DenseNet (9 works). Other standard architectures used are Faster R-CNN, Inception V3, GoogLeNet, MobileNet V2, Spatial Regularization Network (SRN) and U-Net. Five works used ad hoc architectures not previously published (marked with (*) in Table 4). For example, EcNet is an ad hoc architecture used in MDNet [163] and was proposed as an improvement over ResNet. However, the authors acknowledged that its design resembles DenseNet, which was published the same year (2017). RGAN, proposed by Han et al. [47], is a novel architecture that follows the generative adversarial network (GAN) [41] approach, with a generative module comprising the encoder and decoder parts of an atrous convolution autoencoder (ACAE) with a spatial LSTM between them. Similarly, Spinks and Moens [126] used a slightly modified version of a StackGAN [159] to learn the mapping from report encoding to chest X-ray images, and a custom CNN to learn the inverse mapping. Both are trained together, but only the latter is part of the report generation pipeline during inference.  [112] [76, 157] Inception V3 [130] [123] GoogLeNet [129] [120] MobileNet V2 [59] [49] SRN [166] [44] U-Net [116] [128] EcNet (*) [163] 

[47] StackGAN [159] (slightly modified version) (*) [126] CNN (*) [126,132] CNN (unspecified architecture) [148,150] 

The most important observation is that all surveyed works use CNNs to process the input images. This is not surprising since CNNs have dominated the state of the art in computer vision for several years [74]. The typical visual processing pipeline consists of a CNN that receives an input image and outputs a volume of feature maps of dimensions × × , where and denote spatial dimensions (width and height) and denotes the channel dimensions (depth or number of feature maps). These visual features are then leveraged by the language component to make decisions for report generation (e.g., which sentence to write, which template to retrieve, next word to output, etc.), typically by way of an attention mechanism.

However, some works did not strictly follow this pattern. For example, in two works [44,128] a CNN is used for multi-label classification of tags, which are then mapped to embedded vectors via embedding matrix lookup. Thus, the report generation module only has access to these tag vectors but no access to the visual features themselves. Similarly, two works [68,155] classify and look up tag embedding vectors, but unlike the previous works, the language component uses co-attention to access both tags vectors and visual features simultaneously. Their ablation analysis showed that the semantic information provided by these tags complements the visual information and improves the model's performance in report generation. Other works [86,162] used graph neural networks immediately after the CNN to encode the visual information in terms of medical concepts and their relations. Thus, the language component receives the intermediate graph representation instead of the raw visual features. The ablation analysis by Zhang et al. [162] showed some performance gains thanks to the graph neural network. Vispi [89] implements a two-stage procedure, where two distinct CNNs are used. In the first stage a DenseNet 121 [60] classifies abnormalities in the image, and then Grad-CAM [118] is used to localize and crop a region of the image for each detected class. Then, in the second stage the multiple image crops are treated as independent images and processed by a typical CNN+LSTM architecture, with ResNet 101 [52] as the CNN. A similar idea was followed in RTMIC [151], where a DenseNet 121 is pretrained for classification in ChestX-ray14 [143] and CAM is used to get image crops for each class.

We observe a wide variety of CNN architectures used in the literature, though most works employ standard designs. Table 4 presents a summary. The most common ones are ResNet (11 works), VGG (11 works), and DenseNet (9 works). Other standard architectures used are Faster R-CNN, Inception V3, GoogLeNet, MobileNet V2, Spatial Regularization Network (SRN) and U-Net. Five works used ad hoc architectures not previously published (marked with (*) in Table 4). For example, EcNet is an ad hoc architecture used in MDNet [163] and was proposed as an improvement over ResNet. However, the authors acknowledged that its design resembles DenseNet, which was published the same year (2017). RGAN, proposed by Han et al. [47], is a novel architecture that follows the generative adversarial network (GAN) [41] approach, with a generative module comprising the encoder and decoder parts of an atrous convolution autoencoder (ACAE) with a spatial LSTM between them. Similarly, Spinks and Moens [126] used a slightly modified version of a StackGAN [159] to learn the mapping from report encoding to chest X-ray images, and a custom CNN to learn the inverse mapping. Both are trained together, but only the latter is part of the report generation pipeline during inference.  [112] [76, 157] Inception V3 [130] [123] GoogLeNet [129] [120] MobileNet V2 [59] [49] SRN [166] [44] U-Net [116] [128] EcNet (*) [163] 

[47] StackGAN [159] (slightly modified version) (*) [126] CNN (*) [126,132] CNN (unspecified architecture) [148,150] ","[['b73'], ['b88', 'b67', 'b43', 'b85', 'b161', 'b59', 'b142', 'b51', 'b150', 'b117', 'b154', 'b127'], ['b158', 'b111', 'b115', 'b125', 'b129', 'b165', 'b128', 'b46', 'b58', 'b162', 'b40'], ['b158', 'b125', 'b131', 'b147', 'b149'], ['b73'], ['b88', 'b67', 'b43', 'b85', 'b161', 'b59', 'b142', 'b51', 'b150', 'b117', 'b154', 'b127'], ['b158', 'b111', 'b115', 'b125', 'b129', 'b165', 'b128', 'b46', 'b58', 'b162', 'b40'], ['b158', 'b125', 'b131', 'b147', 'b149']]","[['b73'], ['b88', 'b67', 'b43', 'b85', 'b161', 'b59', 'b142', 'b51', 'b150', 'b117', 'b154', 'b127'], ['b158', 'b111', 'b115', 'b125', 'b129', 'b165', 'b128', 'b46', 'b58', 'b162', 'b40'], ['b158', 'b125', 'b131', 'b147', 'b149'], ['b73'], ['b88', 'b67', 'b43', 'b85', 'b161', 'b59', 'b142', 'b51', 'b150', 'b117', 'b154', 'b127'], ['b158', 'b111', 'b115', 'b125', 'b129', 'b165', 'b128', 'b46', 'b58', 'b162', 'b40'], ['b158', 'b125', 'b131', 'b147', 'b149']]",58,"sent1: The most important observation is that all surveyed works use CNNs to process the input images.
sent2: This is not surprising since CNNs have dominated the state of the art in computer vision for several years [74].
sent3: The typical visual processing pipeline consists of a CNN that receives an input image and outputs a volume of feature maps of dimensions ×
sent4: × , where and denote spatial dimensions (width and height) and denotes the channel dimensions (depth or number of feature maps).
sent5: These visual features are then leveraged by the language component to make decisions for report generation (e.g., which sentence to write, which template to retrieve, next word to output, etc.), typically by way of an attention mechanism.
sent6: However, some works did not strictly follow this pattern.
sent7: For example, in two works [44,128] a CNN is used for multi-label classification of tags, which are then mapped to embedded vectors via embedding matrix lookup.
sent8: Thus, the report generation module only has access to these tag vectors but no access to the visual features themselves.
sent9: Similarly, two works [68,155] classify and look up tag embedding vectors, but unlike the previous works, the language component uses co-attention to access both tags vectors and visual features simultaneously.
sent10: Their ablation analysis showed that the semantic information provided by these tags complements the visual information and improves the model's performance in report generation.
sent11: Other works [86,162] used graph neural networks immediately after the CNN to encode the visual information in terms of medical concepts and their relations.
sent12: Thus, the language component receives the intermediate graph representation instead of the raw visual features.
sent13: The ablation analysis by Zhang et al. [162] showed some performance gains thanks to the graph neural network.
sent14: Vispi [89] implements a two-stage procedure, where two distinct CNNs are used.
sent15: In the first stage a DenseNet 121 [60] classifies abnormalities in the image, and then Grad-CAM [118] is used to localize and crop a region of the image for each detected class.
sent16: Then, in the second stage the multiple image crops are treated as independent images and processed by a typical CNN+LSTM architecture, with ResNet 101 [52] as the CNN.
sent17: A similar idea was followed in RTMIC [151], where a DenseNet 121 is pretrained for classification in ChestX-ray14 [143] and CAM is used to get image crops for each class.
sent18: We observe a wide variety of CNN architectures used in the literature, though most works employ standard designs.
sent19: Table 4 presents a summary. The most common ones are ResNet (11 works), VGG (11 works), and DenseNet (9 works).
sent20: Other standard architectures used are Faster R-CNN, Inception V3, GoogLeNet, MobileNet V2, Spatial Regularization Network (SRN) and U-Net.
sent21: Five works used ad hoc architectures not previously published (marked with (*) in Table 4).
sent22: For example, EcNet is an ad hoc architecture used in MDNet [163] and was proposed as an improvement over ResNet.
sent23: However, the authors acknowledged that its design resembles DenseNet, which was published the same year (2017).
sent24: RGAN, proposed by Han et al. [47], is a novel architecture that follows the generative adversarial network (GAN) [41] approach, with a generative module comprising the encoder and decoder parts of an atrous convolution autoencoder (ACAE) with a spatial LSTM between them.
sent25: Similarly, Spinks and Moens [126] used a slightly modified version of a StackGAN [159] to learn the mapping from report encoding to chest X-ray images, and a custom CNN to learn the inverse mapping.
sent26: Both are trained together, but only the latter is part of the report generation pipeline during inference.
sent27: [112] [76, 157] Inception V3 [130] [123] GoogLeNet [129] [120] MobileNet V2 [59] [49] SRN [166] [44]
sent28: U-Net [116] [128] EcNet (*) [163] [47] StackGAN [159] (slightly modified version) (*) [126] CNN (*) [126,132] CNN (unspecified architecture) [148,150] The most important observation is that all surveyed works use CNNs to process the input images.
sent29: This is not surprising since CNNs have dominated the state of the art in computer vision for several years [74].
sent30: The typical visual processing pipeline consists of a CNN that receives an input image and outputs a volume of feature maps of dimensions ×
sent31: × , where and denote spatial dimensions (width and height) and denotes the channel dimensions (depth or number of feature maps).
sent32: These visual features are then leveraged by the language component to make decisions for report generation (e.g., which sentence to write, which template to retrieve, next word to output, etc.), typically by way of an attention mechanism.
sent33: However, some works did not strictly follow this pattern.
sent34: For example, in two works [44,128] a CNN is used for multi-label classification of tags, which are then mapped to embedded vectors via embedding matrix lookup.
sent35: Thus, the report generation module only has access to these tag vectors but no access to the visual features themselves.
sent36: Similarly, two works [68,155] classify and look up tag embedding vectors, but unlike the previous works, the language component uses co-attention to access both tags vectors and visual features simultaneously.
sent37: Their ablation analysis showed that the semantic information provided by these tags complements the visual information and improves the model's performance in report generation.
sent38: Other works [86,162] used graph neural networks immediately after the CNN to encode the visual information in terms of medical concepts and their relations.
sent39: Thus, the language component receives the intermediate graph representation instead of the raw visual features.
sent40: The ablation analysis by Zhang et al. [162] showed some performance gains thanks to the graph neural network.
sent41: Vispi [89] implements a two-stage procedure, where two distinct CNNs are used.
sent42: In the first stage a DenseNet 121 [60] classifies abnormalities in the image, and then Grad-CAM [118] is used to localize and crop a region of the image for each detected class.
sent43: Then, in the second stage the multiple image crops are treated as independent images and processed by a typical CNN+LSTM architecture, with ResNet 101 [52] as the CNN.
sent44: A similar idea was followed in RTMIC [151], where a DenseNet 121 is pretrained for classification in ChestX-ray14 [143] and CAM is used to get image crops for each class.
sent45: We observe a wide variety of CNN architectures used in the literature, though most works employ standard designs.
sent46: Table 4 presents a summary. The most common ones are ResNet (11 works), VGG (11 works), and DenseNet (9 works).
sent47: Other standard architectures used are Faster R-CNN, Inception V3, GoogLeNet, MobileNet V2, Spatial Regularization Network (SRN) and U-Net.
sent48: Five works used ad hoc architectures not previously published (marked with (*) in Table 4).
sent49: For example, EcNet is an ad hoc architecture used in MDNet [163] and was proposed as an improvement over ResNet.
sent50: However, the authors acknowledged that its design resembles DenseNet, which was published the same year (2017).
sent51: RGAN, proposed by Han et al. [47], is a novel architecture that follows the generative adversarial network (GAN) [41] approach, with a generative module comprising the encoder and decoder parts of an atrous convolution autoencoder (ACAE) with a spatial LSTM between them.
sent52: Similarly, Spinks and Moens [126] used a slightly modified version of a StackGAN [159] to learn the mapping from report encoding to chest X-ray images, and a custom CNN to learn the inverse mapping.
sent53: Both are trained together, but only the latter is part of the report generation pipeline during inference.
sent54: [112] [76, 157] Inception V3 [130] [123] GoogLeNet [129] [120] MobileNet V2 [59] [49] SRN [166] [44]
sent55: U-Net [116] [128] EcNet (*) [163] [47] StackGAN [159] (slightly modified version) (*) [126] CNN (*) [126,132] CNN (unspecified architecture) [148,150]"
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s9,Input and Output.,"Output. All models output a natural language report. According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence. (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic. These models are designed for datasets where reports follow a rigid structure. (3) Generative single-sentence: generate a report word by word, but only output a single sentence. These models are designed for datasets with simple one-sentence  reports. (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling. This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules. And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word. This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86]. In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others. These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3. Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation). Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision. We will discuss all these outputs more in detail and their use in the explainability section (5.3).

Output. All models output a natural language report. According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence. (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic. These models are designed for datasets where reports follow a rigid structure. (3) Generative single-sentence: generate a report word by word, but only output a single sentence. These models are designed for datasets with simple one-sentence  reports. (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling. This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules. And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word. This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86]. In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others. These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3. Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation). Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision. We will discuss all these outputs more in detail and their use in the explainability section (5.3).","[['b85', 'b125', 'b60', 'b84', 'b15'], ['b85', 'b125', 'b60', 'b84', 'b15']]","[['b85', 'b125', 'b60', 'b84', 'b15'], ['b85', 'b125', 'b60', 'b84', 'b15']]",10,"sent1: Output. All models output a natural language report.
sent2: According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence.
sent3: (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic.
sent4: These models are designed for datasets where reports follow a rigid structure.
sent5: (3) Generative single-sentence: generate a report word by word, but only output a single sentence.
sent6: These models are designed for datasets with simple one-sentence  reports.
sent7: (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling.
sent8: This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules.
sent9: And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word.
sent10: This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86].
sent11: In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others.
sent12: These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3.
sent13: Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation).
sent14: Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision.
sent15: We will discuss all these outputs more in detail and their use in the explainability section (5.3).
sent16: Output. All models output a natural language report.
sent17: According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence.
sent18: (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic.
sent19: These models are designed for datasets where reports follow a rigid structure.
sent20: (3) Generative single-sentence: generate a report word by word, but only output a single sentence.
sent21: These models are designed for datasets with simple one-sentence  reports.
sent22: (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling.
sent23: This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules.
sent24: And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word.
sent25: This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86].
sent26: In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others.
sent27: These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3.
sent28: Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation).
sent29: Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision.
sent30: We will discuss all these outputs more in detail and their use in the explainability section (5.3)."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s8,Model Design,"In this section, we present an analysis of existent DL model designs, starting with a general overview of common design practices. Most models in the literature follow a standard design pattern. There is a visual component consisting at its core of a Convolutional Neural Network (CNN) [79] that processes one or more input images in order to extract visual features. Then, a language component follows, typically based on well-known NLP neural architectures (e.g., LSTM [57], BiLSTM [42], GRU [26], Transformer [139]) responsible for text processing and report generation. Also, a widespread practice for the language component is to retrieve the visual information in an adaptive manner  via an attention mechanism, as the report is written. Many papers follow variations of this pattern inspired by influential works from the image captioning domain [141,152], which are frequently cited and used as baselines. Optionally, some models receive or generate additional input or output, and a few models incorporate some form of domain knowledge explicitly in the generation process.  Table 3 presents a summary of this analysis. Input. With respect to image type, most papers (24) used chest X-rays, whereas the other papers are more or less equally distributed over other image types. A total of 32 models receive a single image (e.g. a single chest X-ray view), 6 models receive 2 images (both frontal and lateral chest X-ray views), and 2 models receive an arbitrary number of images (e.g. multi-slice abdominal CT scans). Most models in the literature only handle visual input. However, 6 works [7,16,36,61,95,132] explored the use of complementary input text, reporting performance gains in most cases. For example, two works [61,95] encode an indication paragraph with a BiLSTM. Similarly, MTMA [132] encodes the report's indication and findings sections with a BiLSTM per sentence first, and then a LSTM produces a final vector representation. Similarly, two works [7,36] use LSTM/BiLSTM to encode a partial report or caption as input, in order to predict the next word. Unlike other works, CLARA [16] uses a software package, Lucene [18], to perform text-based retrieval of report templates. The input text is processed by Lucene as a search query, and the retrieved templates are paraphrased by an encoder-decoder network to generate the final report.

In this section, we present an analysis of existent DL model designs, starting with a general overview of common design practices. Most models in the literature follow a standard design pattern. There is a visual component consisting at its core of a Convolutional Neural Network (CNN) [79] that processes one or more input images in order to extract visual features. Then, a language component follows, typically based on well-known NLP neural architectures (e.g., LSTM [57], BiLSTM [42], GRU [26], Transformer [139]) responsible for text processing and report generation. Also, a widespread practice for the language component is to retrieve the visual information in an adaptive manner  via an attention mechanism, as the report is written. Many papers follow variations of this pattern inspired by influential works from the image captioning domain [141,152], which are frequently cited and used as baselines. Optionally, some models receive or generate additional input or output, and a few models incorporate some form of domain knowledge explicitly in the generation process.  Table 3 presents a summary of this analysis. Input. With respect to image type, most papers (24) used chest X-rays, whereas the other papers are more or less equally distributed over other image types. A total of 32 models receive a single image (e.g. a single chest X-ray view), 6 models receive 2 images (both frontal and lateral chest X-ray views), and 2 models receive an arbitrary number of images (e.g. multi-slice abdominal CT scans). Most models in the literature only handle visual input. However, 6 works [7,16,36,61,95,132] explored the use of complementary input text, reporting performance gains in most cases. For example, two works [61,95] encode an indication paragraph with a BiLSTM. Similarly, MTMA [132] encodes the report's indication and findings sections with a BiLSTM per sentence first, and then a LSTM produces a final vector representation. Similarly, two works [7,36] use LSTM/BiLSTM to encode a partial report or caption as input, in order to predict the next word. Unlike other works, CLARA [16] uses a software package, Lucene [18], to perform text-based retrieval of report templates. The input text is processed by Lucene as a search query, and the retrieved templates are paraphrased by an encoder-decoder network to generate the final report.","[['b41', 'b17', 'b56', 'b151', 'b25', 'b60', 'b131', 'b35', 'b94', 'b78', 'b140', 'b138', 'b15', 'b6'], ['b41', 'b17', 'b56', 'b151', 'b25', 'b60', 'b131', 'b35', 'b94', 'b78', 'b140', 'b138', 'b15', 'b6']]","[['b41', 'b17', 'b56', 'b151', 'b25', 'b60', 'b131', 'b35', 'b94', 'b78', 'b140', 'b138', 'b15', 'b6'], ['b41', 'b17', 'b56', 'b151', 'b25', 'b60', 'b131', 'b35', 'b94', 'b78', 'b140', 'b138', 'b15', 'b6']]",28,"sent1: In this section, we present an analysis of existent DL model designs, starting with a general overview of common design practices.
sent2: Most models in the literature follow a standard design pattern.
sent3: There is a visual component consisting at its core of a Convolutional Neural Network (CNN) [79] that processes one or more input images in order to extract visual features.
sent4: Then, a language component follows, typically based on well-known NLP neural architectures (e.g., LSTM [57], BiLSTM [42], GRU [26], Transformer [139]) responsible for text processing and report generation.
sent5: Also, a widespread practice for the language component is to retrieve the visual information in an adaptive manner  via an attention mechanism, as the report is written.
sent6: Many papers follow variations of this pattern inspired by influential works from the image captioning domain [141,152], which are frequently cited and used as baselines.
sent7: Optionally, some models receive or generate additional input or output, and a few models incorporate some form of domain knowledge explicitly in the generation process.
sent8: Table 3 presents a summary of this analysis.
sent9: Input. With respect to image type, most papers (24) used chest X-rays, whereas the other papers are more or less equally distributed over other image types.
sent10: A total of 32 models receive a single image (e.g. a single chest X-ray view), 6 models receive 2 images (both frontal and lateral chest X-ray views), and 2 models receive an arbitrary number of images (e.g. multi-slice abdominal CT scans).
sent11: Most models in the literature only handle visual input.
sent12: However, 6 works [7,16,36,61,95,132] explored the use of complementary input text, reporting performance gains in most cases.
sent13: For example, two works [61,95] encode an indication paragraph with a BiLSTM.
sent14: Similarly, MTMA [132] encodes the report's indication and findings sections with a BiLSTM per sentence first, and then a LSTM produces a final vector representation.
sent15: Similarly, two works [7,36] use LSTM/BiLSTM to encode a partial report or caption as input, in order to predict the next word.
sent16: Unlike other works, CLARA [16] uses a software package, Lucene [18], to perform text-based retrieval of report templates.
sent17: The input text is processed by Lucene as a search query, and the retrieved templates are paraphrased by an encoder-decoder network to generate the final report.
sent18: In this section, we present an analysis of existent DL model designs, starting with a general overview of common design practices.
sent19: Most models in the literature follow a standard design pattern.
sent20: There is a visual component consisting at its core of a Convolutional Neural Network (CNN) [79] that processes one or more input images in order to extract visual features.
sent21: Then, a language component follows, typically based on well-known NLP neural architectures (e.g., LSTM [57], BiLSTM [42], GRU [26], Transformer [139]) responsible for text processing and report generation.
sent22: Also, a widespread practice for the language component is to retrieve the visual information in an adaptive manner  via an attention mechanism, as the report is written.
sent23: Many papers follow variations of this pattern inspired by influential works from the image captioning domain [141,152], which are frequently cited and used as baselines.
sent24: Optionally, some models receive or generate additional input or output, and a few models incorporate some form of domain knowledge explicitly in the generation process.
sent25: Table 3 presents a summary of this analysis.
sent26: Input. With respect to image type, most papers (24) used chest X-rays, whereas the other papers are more or less equally distributed over other image types.
sent27: A total of 32 models receive a single image (e.g. a single chest X-ray view), 6 models receive 2 images (both frontal and lateral chest X-ray views), and 2 models receive an arbitrary number of images (e.g. multi-slice abdominal CT scans).
sent28: Most models in the literature only handle visual input.
sent29: However, 6 works [7,16,36,61,95,132] explored the use of complementary input text, reporting performance gains in most cases.
sent30: For example, two works [61,95] encode an indication paragraph with a BiLSTM.
sent31: Similarly, MTMA [132] encodes the report's indication and findings sections with a BiLSTM per sentence first, and then a LSTM produces a final vector representation.
sent32: Similarly, two works [7,36] use LSTM/BiLSTM to encode a partial report or caption as input, in order to predict the next word.
sent33: Unlike other works, CLARA [16] uses a software package, Lucene [18], to perform text-based retrieval of report templates.
sent34: The input text is processed by Lucene as a search query, and the retrieved templates are paraphrased by an encoder-decoder network to generate the final report."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s7,Dataset,"Year Image Type # images # reports # patients Used by papers Report datasets IU X-ray [28] 2015  [19] 2019 Chest X-Ray 160,868 109,931 67,625 None (5) ImageCLEF Caption 2017 [35] 2017 Biomedical (2) 184,614 184,614 -[51] ImageCLEF Caption 2018 [39] 2018 Biomedical (2) 232,305 232,305 -None (5) ROCO [105] 2018 Multiple radiology (3) 81,825 81,825 -None (5) PEIR Gross [68] 2017 Gross lesions 7,442 7,442 - [68] INBreast (pt) [99] 2012 Mammography X-ray 410 115 115 [87,128] STARE [58] 1975 Retinal fundus 400 400 -None (5) RDIF (1) [95] 2019   (1) : the RDIF dataset is pending release. (2) : for the ImageCLEF datasets, images were extracted from PubMed Central papers and filtered automatically in order to keep only clinical images, but some unintended samples from other domains are also included. (3) : contains multiple modalities, namely CT, Ultrasound, X-Ray, Fluoroscopy, PET, Mammography, MRI, Angiography and PET-CT. (4) : the images are frames extracted from videos. (5) : none of the papers reviewed used this dataset.

The third column in Table 2 lists the image modalities for each dataset, showing chest X-rays concentrates most of the efforts in report datasets [19,28,44,69,85], though there are also datasets with biomedical images from varied types [35,39,68,105], mammography [99] and hip X-rays [37], ultrasound images [7,158], retinal images [58], doppler echocardiographies [98], cervical images [94], and kidney [95] and bladder biopsies [163]. This adds an extra challenge, since different kinds of exams may need different solutions, as the clinical conditions will be diverse. For example, a fundus retinal image may differ significantly from a chest X-ray; or a radiologist analyzing an X-ray may follow a different procedure than a pathologist reading a biopsy.

From the public report datasets, IU X-ray [28] is the most commonly used, consisting of 7,470 frontal and lateral chest X-rays and 3,955 reports. Additionally, each report was manually annotated with Medical Subject Heading (MeSH) 2 [115] and RadLex [81] terms, and automatically annotated with MeSH terms using the MTI [100] system plus the negation tool from MetaMap [10]. Figure 1 shows a sample image and report from this dataset. Note that for deep learning methods, the amount of data may seem insufficient, compared to general domain datasets with millions of samples, such as ImageNet [29]. This issue could be addressed with pre-training or data augmentation techniques. Also, this may be partially solved with the more recent datasets MIMIC-CXR [69] or PadChest [19], which contain 377,110 and 160,868 images respectively, but have not been widely used yet.

All report datasets include images and reports, and most of them also include labels for each report. Furthermore, INbreast [99] includes contours locating the labels in the images, the Ultrasound collection [157,158] includes bounding boxes locating organs, and IU X-ray [28] and RDIF [95] include additional text written by the physician who requested the exam. The complete detail of additional information is shown in Table 9 in appendix 9.1. This information can be leveraged as a supplementary context to further improve the system performance. On the one hand, the labels and image localization can be used to design auxiliary tasks (see section 5.2.5), and to further evaluate the text generation process (see section 5.4). On the other hand, the indication may contain additional information not present in the image, such as a patient's previous condition, which in some cases may be essential to address the task [95].

Lastly, many works use classification datasets, which do not provide a report for each image, but a set of clinical conditions or abnormalities present or absent in the image. In most cases, this kind of information is used to perform image classification as pre-training, an intermediate, or an auxiliary task to generate the report. One remarkable case is the CheXpert dataset [63], which contains 224,316 images, and was also presented with the CheXpert labeler, an automatic rule-based tool that annotates 14 labels (abnormalities) as present, absent or uncertain from the natural language reports. This tool was used to label the images from the dataset, is also used in MIMIC-CXR [69] to tag the reports, and in some works to evaluate the generated reports, as discussed in the Metrics section (5.4). Notice the classification dataset list is not comprehensive, as it only includes datasets that were used in at least one of the reviewed works.

Synthesis. The datasets cover multiple image modalities and body parts, though most efforts focus on chest X-rays. This opens a potential research avenue to explore other image types and diseases, using existing solutions or raising new methods. Additionally, most collections provide valuable supplementary information, such as abnormality tags and/or localization, which can be used to design auxiliary tasks and to evaluate the performance.

Year Image Type # images # reports # patients Used by papers Report datasets IU X-ray [28] 2015  [19] 2019 Chest X-Ray 160,868 109,931 67,625 None (5) ImageCLEF Caption 2017 [35] 2017 Biomedical (2) 184,614 184,614 -[51] ImageCLEF Caption 2018 [39] 2018 Biomedical (2) 232,305 232,305 -None (5) ROCO [105] 2018 Multiple radiology (3) 81,825 81,825 -None (5) PEIR Gross [68] 2017 Gross lesions 7,442 7,442 - [68] INBreast (pt) [99] 2012 Mammography X-ray 410 115 115 [87,128] STARE [58] 1975 Retinal fundus 400 400 -None (5) RDIF (1) [95] 2019   (1) : the RDIF dataset is pending release. (2) : for the ImageCLEF datasets, images were extracted from PubMed Central papers and filtered automatically in order to keep only clinical images, but some unintended samples from other domains are also included. (3) : contains multiple modalities, namely CT, Ultrasound, X-Ray, Fluoroscopy, PET, Mammography, MRI, Angiography and PET-CT. (4) : the images are frames extracted from videos. (5) : none of the papers reviewed used this dataset.

The third column in Table 2 lists the image modalities for each dataset, showing chest X-rays concentrates most of the efforts in report datasets [19,28,44,69,85], though there are also datasets with biomedical images from varied types [35,39,68,105], mammography [99] and hip X-rays [37], ultrasound images [7,158], retinal images [58], doppler echocardiographies [98], cervical images [94], and kidney [95] and bladder biopsies [163]. This adds an extra challenge, since different kinds of exams may need different solutions, as the clinical conditions will be diverse. For example, a fundus retinal image may differ significantly from a chest X-ray; or a radiologist analyzing an X-ray may follow a different procedure than a pathologist reading a biopsy.

From the public report datasets, IU X-ray [28] is the most commonly used, consisting of 7,470 frontal and lateral chest X-rays and 3,955 reports. Additionally, each report was manually annotated with Medical Subject Heading (MeSH) 2 [115] and RadLex [81] terms, and automatically annotated with MeSH terms using the MTI [100] system plus the negation tool from MetaMap [10]. Figure 1 shows a sample image and report from this dataset. Note that for deep learning methods, the amount of data may seem insufficient, compared to general domain datasets with millions of samples, such as ImageNet [29]. This issue could be addressed with pre-training or data augmentation techniques. Also, this may be partially solved with the more recent datasets MIMIC-CXR [69] or PadChest [19], which contain 377,110 and 160,868 images respectively, but have not been widely used yet.

All report datasets include images and reports, and most of them also include labels for each report. Furthermore, INbreast [99] includes contours locating the labels in the images, the Ultrasound collection [157,158] includes bounding boxes locating organs, and IU X-ray [28] and RDIF [95] include additional text written by the physician who requested the exam. The complete detail of additional information is shown in Table 9 in appendix 9.1. This information can be leveraged as a supplementary context to further improve the system performance. On the one hand, the labels and image localization can be used to design auxiliary tasks (see section 5.2.5), and to further evaluate the text generation process (see section 5.4). On the other hand, the indication may contain additional information not present in the image, such as a patient's previous condition, which in some cases may be essential to address the task [95].

Lastly, many works use classification datasets, which do not provide a report for each image, but a set of clinical conditions or abnormalities present or absent in the image. In most cases, this kind of information is used to perform image classification as pre-training, an intermediate, or an auxiliary task to generate the report. One remarkable case is the CheXpert dataset [63], which contains 224,316 images, and was also presented with the CheXpert labeler, an automatic rule-based tool that annotates 14 labels (abnormalities) as present, absent or uncertain from the natural language reports. This tool was used to label the images from the dataset, is also used in MIMIC-CXR [69] to tag the reports, and in some works to evaluate the generated reports, as discussed in the Metrics section (5.4). Notice the classification dataset list is not comprehensive, as it only includes datasets that were used in at least one of the reviewed works.

Synthesis. The datasets cover multiple image modalities and body parts, though most efforts focus on chest X-rays. This opens a potential research avenue to explore other image types and diseases, using existing solutions or raising new methods. Additionally, most collections provide valuable supplementary information, such as abnormality tags and/or localization, which can be used to design auxiliary tasks and to evaluate the performance.","[['b4', 'b27', 'b67', 'b104', 'b3', 'b57', 'b127', 'b0', 'b1', 'b38', 'b18', 'b34', 'b2', 'b98', 'b94', 'b86'], ['b27', 'b43', 'b68', 'b67', 'b104', 'b157', 'b57', 'b84', 'b97', 'b94', 'b38', 'b162', 'b18', 'b34', 'b36', 'b93', 'b98', 'b6'], ['b27', 'b68', 'b28', 'b99', 'b80', 'b114', 'b9', 'b18'], ['b27', 'b157', 'b156', 'b98', 'b94'], ['b68', 'b62', None], [], ['b4', 'b27', 'b67', 'b104', 'b3', 'b57', 'b127', 'b0', 'b1', 'b38', 'b18', 'b34', 'b2', 'b98', 'b94', 'b86'], ['b27', 'b43', 'b68', 'b67', 'b104', 'b157', 'b57', 'b84', 'b97', 'b94', 'b38', 'b162', 'b18', 'b34', 'b36', 'b93', 'b98', 'b6'], ['b27', 'b68', 'b28', 'b99', 'b80', 'b114', 'b9', 'b18'], ['b27', 'b157', 'b156', 'b98', 'b94'], ['b68', 'b62', None], []]","[['b4', 'b27', 'b67', 'b104', 'b3', 'b57', 'b127', 'b0', 'b1', 'b38', 'b18', 'b34', 'b2', 'b98', 'b94', 'b86'], ['b27', 'b43', 'b68', 'b67', 'b104', 'b157', 'b57', 'b84', 'b97', 'b94', 'b38', 'b162', 'b18', 'b34', 'b36', 'b93', 'b98', 'b6'], ['b27', 'b68', 'b28', 'b99', 'b80', 'b114', 'b9', 'b18'], ['b27', 'b157', 'b156', 'b98', 'b94'], ['b68', 'b62', None], [], ['b4', 'b27', 'b67', 'b104', 'b3', 'b57', 'b127', 'b0', 'b1', 'b38', 'b18', 'b34', 'b2', 'b98', 'b94', 'b86'], ['b27', 'b43', 'b68', 'b67', 'b104', 'b157', 'b57', 'b84', 'b97', 'b94', 'b38', 'b162', 'b18', 'b34', 'b36', 'b93', 'b98', 'b6'], ['b27', 'b68', 'b28', 'b99', 'b80', 'b114', 'b9', 'b18'], ['b27', 'b157', 'b156', 'b98', 'b94'], ['b68', 'b62', None], []]",100,"sent1: Year Image Type # images # reports # patients Used by papers Report datasets IU X-ray [28] 2015  [19] 2019 Chest X-Ray 160,868 109,931 67,625 None (5) ImageCLEF Caption 2017 [35] 2017 Biomedical (2) 184,614 184,614 -[51] ImageCLEF Caption 2018 [39] 2018 Biomedical (2) 232,305 232,305 -None (5) ROCO [105]
sent2: 2018 Multiple radiology (3) 81,825 81,825 -None (5) PEIR Gross [68] 2017 Gross lesions 7,442 7,442 - [68] INBreast (pt) [99] 2012 Mammography X-ray 410 115 115 [87,128] STARE [58] 1975
sent3: Retinal fundus 400 400 -None (5) RDIF (1) [95] 2019   (1) : the RDIF dataset is pending release.
sent4: (2) : for the ImageCLEF datasets, images were extracted from PubMed Central papers and filtered automatically in order to keep only clinical images, but some unintended samples from other domains are also included.
sent5: (3) : contains multiple modalities, namely CT, Ultrasound, X-Ray, Fluoroscopy, PET, Mammography, MRI, Angiography and PET-CT.
sent6: (4) : the images are frames extracted from videos.
sent7: (5) : none of the papers reviewed used this dataset.
sent8: The third column in Table 2 lists the image modalities for each dataset, showing chest X-rays concentrates most of the efforts in report datasets [19,28,44,69,85], though there are also datasets with biomedical images from varied types [35,39,68,105], mammography [99] and hip X-rays [37], ultrasound images [7,158], retinal images [58], doppler echocardiographies [98], cervical images [94], and kidney [95] and bladder biopsies [163].
sent9: This adds an extra challenge, since different kinds of exams may need different solutions, as the clinical conditions will be diverse.
sent10: For example, a fundus retinal image may differ significantly from a chest X-ray; or a radiologist analyzing an X-ray may follow a different procedure than a pathologist reading a biopsy.
sent11: From the public report datasets, IU X-ray [28] is the most commonly used, consisting of 7,470 frontal and lateral chest X-rays and 3,955 reports.
sent12: Additionally, each report was manually annotated with Medical Subject Heading (MeSH) 2 [115] and RadLex [81] terms, and automatically annotated with MeSH terms using the MTI [100] system plus the negation tool from MetaMap [10].
sent13: Figure 1 shows a sample image and report from this dataset.
sent14: Note that for deep learning methods, the amount of data may seem insufficient, compared to general domain datasets with millions of samples, such as ImageNet [29].
sent15: This issue could be addressed with pre-training or data augmentation techniques.
sent16: Also, this may be partially solved with the more recent datasets MIMIC-CXR [69] or PadChest [19], which contain 377,110 and 160,868 images respectively, but have not been widely used yet.
sent17: All report datasets include images and reports, and most of them also include labels for each report.
sent18: Furthermore, INbreast [99] includes contours locating the labels in the images, the Ultrasound collection [157,158] includes bounding boxes locating organs, and IU X-ray [28] and RDIF [95] include additional text written by the physician who requested the exam.
sent19: The complete detail of additional information is shown in Table 9 in appendix 9.1.
sent20: This information can be leveraged as a supplementary context to further improve the system performance.
sent21: On the one hand, the labels and image localization can be used to design auxiliary tasks (see section 5.2.5), and to further evaluate the text generation process (see section 5.4).
sent22: On the other hand, the indication may contain additional information not present in the image, such as a patient's previous condition, which in some cases may be essential to address the task [95].
sent23: Lastly, many works use classification datasets, which do not provide a report for each image, but a set of clinical conditions or abnormalities present or absent in the image.
sent24: In most cases, this kind of information is used to perform image classification as pre-training, an intermediate, or an auxiliary task to generate the report.
sent25: One remarkable case is the CheXpert dataset [63], which contains 224,316 images, and was also presented with the CheXpert labeler, an automatic rule-based tool that annotates 14 labels (abnormalities) as present, absent or uncertain from the natural language reports.
sent26: This tool was used to label the images from the dataset, is also used in MIMIC-CXR [69] to tag the reports, and in some works to evaluate the generated reports, as discussed in the Metrics section (5.4).
sent27: Notice the classification dataset list is not comprehensive, as it only includes datasets that were used in at least one of the reviewed works.
sent28: Synthesis. The datasets cover multiple image modalities and body parts, though most efforts focus on chest X-rays.
sent29: This opens a potential research avenue to explore other image types and diseases, using existing solutions or raising new methods.
sent30: Additionally, most collections provide valuable supplementary information, such as abnormality tags and/or localization, which can be used to design auxiliary tasks and to evaluate the performance.
sent31: Year Image Type # images # reports # patients Used by papers Report datasets IU X-ray [28] 2015  [19] 2019 Chest X-Ray 160,868 109,931 67,625 None (5) ImageCLEF Caption 2017 [35] 2017 Biomedical (2) 184,614 184,614 -[51] ImageCLEF Caption 2018 [39] 2018 Biomedical (2) 232,305 232,305 -None (5) ROCO [105]
sent32: 2018 Multiple radiology (3) 81,825 81,825 -None (5) PEIR Gross [68] 2017 Gross lesions 7,442 7,442 - [68] INBreast (pt) [99] 2012 Mammography X-ray 410 115 115 [87,128] STARE [58] 1975
sent33: Retinal fundus 400 400 -None (5) RDIF (1) [95] 2019   (1) : the RDIF dataset is pending release.
sent34: (2) : for the ImageCLEF datasets, images were extracted from PubMed Central papers and filtered automatically in order to keep only clinical images, but some unintended samples from other domains are also included.
sent35: (3) : contains multiple modalities, namely CT, Ultrasound, X-Ray, Fluoroscopy, PET, Mammography, MRI, Angiography and PET-CT.
sent36: (4) : the images are frames extracted from videos.
sent37: (5) : none of the papers reviewed used this dataset.
sent38: The third column in Table 2 lists the image modalities for each dataset, showing chest X-rays concentrates most of the efforts in report datasets [19,28,44,69,85], though there are also datasets with biomedical images from varied types [35,39,68,105], mammography [99] and hip X-rays [37], ultrasound images [7,158], retinal images [58], doppler echocardiographies [98], cervical images [94], and kidney [95] and bladder biopsies [163].
sent39: This adds an extra challenge, since different kinds of exams may need different solutions, as the clinical conditions will be diverse.
sent40: For example, a fundus retinal image may differ significantly from a chest X-ray; or a radiologist analyzing an X-ray may follow a different procedure than a pathologist reading a biopsy.
sent41: From the public report datasets, IU X-ray [28] is the most commonly used, consisting of 7,470 frontal and lateral chest X-rays and 3,955 reports.
sent42: Additionally, each report was manually annotated with Medical Subject Heading (MeSH) 2 [115] and RadLex [81] terms, and automatically annotated with MeSH terms using the MTI [100] system plus the negation tool from MetaMap [10].
sent43: Figure 1 shows a sample image and report from this dataset.
sent44: Note that for deep learning methods, the amount of data may seem insufficient, compared to general domain datasets with millions of samples, such as ImageNet [29].
sent45: This issue could be addressed with pre-training or data augmentation techniques.
sent46: Also, this may be partially solved with the more recent datasets MIMIC-CXR [69] or PadChest [19], which contain 377,110 and 160,868 images respectively, but have not been widely used yet.
sent47: All report datasets include images and reports, and most of them also include labels for each report.
sent48: Furthermore, INbreast [99] includes contours locating the labels in the images, the Ultrasound collection [157,158] includes bounding boxes locating organs, and IU X-ray [28] and RDIF [95] include additional text written by the physician who requested the exam.
sent49: The complete detail of additional information is shown in Table 9 in appendix 9.1.
sent50: This information can be leveraged as a supplementary context to further improve the system performance.
sent51: On the one hand, the labels and image localization can be used to design auxiliary tasks (see section 5.2.5), and to further evaluate the text generation process (see section 5.4).
sent52: On the other hand, the indication may contain additional information not present in the image, such as a patient's previous condition, which in some cases may be essential to address the task [95].
sent53: Lastly, many works use classification datasets, which do not provide a report for each image, but a set of clinical conditions or abnormalities present or absent in the image.
sent54: In most cases, this kind of information is used to perform image classification as pre-training, an intermediate, or an auxiliary task to generate the report.
sent55: One remarkable case is the CheXpert dataset [63], which contains 224,316 images, and was also presented with the CheXpert labeler, an automatic rule-based tool that annotates 14 labels (abnormalities) as present, absent or uncertain from the natural language reports.
sent56: This tool was used to label the images from the dataset, is also used in MIMIC-CXR [69] to tag the reports, and in some works to evaluate the generated reports, as discussed in the Metrics section (5.4).
sent57: Notice the classification dataset list is not comprehensive, as it only includes datasets that were used in at least one of the reviewed works.
sent58: Synthesis. The datasets cover multiple image modalities and body parts, though most efforts focus on chest X-rays.
sent59: This opens a potential research avenue to explore other image types and diseases, using existing solutions or raising new methods.
sent60: Additionally, most collections provide valuable supplementary information, such as abnormality tags and/or localization, which can be used to design auxiliary tasks and to evaluate the performance."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s1,TASK OVERVIEW,"From a purely computational perspective, the following is the main task addressed by most articles analyzed in this survey: given as input one or more medical images of a patient, a text report is output that is as similar as possible to one generated by a radiologist. From a machine learning point of view, creating a system that performs such a task would require learning a generative model from instances of reports written by radiologists. Figure 1 presents one example of such a report, Manual tags: Calcified Granuloma/lung/upper lobe/right Automatic tags: Calcified granuloma Comparison: Chest radiographs XXXX. Indication: XXXX-year-old male, chest pain. Findings: The cardiomediastinal silhouette is within normal limits for size and contour. The lungs are normally inflated without evidence of focal airspace disease, pleural effusion, or pneumothorax. Stable calcified granuloma within the right upper lung. No acute bone abnormality. Impression: No acute cardiopulmonary process. Fig. 1. Example from the IU X-ray dataset, frontal and lateral chest x-rays from a patient, alongside the natural language report and the annotated tags. XXXX is used for anonimization of the report.

taken from the IU X-ray dataset [28]. We see two input X-ray images (frontal and lateral), and below them some annotations (Tags) -some manually annotated by a radiologist and others automatically annotated-, and on the right side the report with four different sections (comparison, indication, findings, and impression). If we consider the clinical workflow of generating a medical imaging report, several aspects should be taken into account before diving into a concrete implementation.

The first aspect is considering additional patient information in the process of report generation. Most of the time, the physician asking for medical imaging is the primary care physician or a medical specialist. This implies that when radiologists write a report, they generally have patientrelevant clinical information, usually provided in the section Indication as shown in Figure 1. Also, the Comparison section can provide information of a serial follow-up procedure, to evaluate the evolution of a patient over time (e.g., aneurysm, congenital heart disease). Then, one decision can be whether or not to use these Indication and Comparison data to generate the sections Findings, Impression, or both of them. Second, the model for report generation should consider the diversity on medical images as well as body regions and conditions. There are several types of medical images, such as X-rays, CT, MRI, PET and SPECT. This implies that a model for text report generation that deals with only one type of input medical image might not solve it for other types. Also, ideally, a model should be able to generate reports from different parts of the human anatomy and diverse medical conditions. To adequately achieve this task, different body regions must have a balanced and sizable training set. Many works surveyed in this article focus on one specific part of the body and particular illnesses which limits the applicability of these methods to generalize to all possible diagnosis tasks.

Lastly, even if an AI system has perfect report generation accuracy, we might wonder if we can trust a machine in such a critical domain. One of the reasons for preferring a radiologist rather than an automated, highly accurate AI system is the chance of understanding the rationale behind the findings and impressions. In this sense, explainable AI [46] is of great importance in securing their adoption in a clinical setting.

From a purely computational perspective, the following is the main task addressed by most articles analyzed in this survey: given as input one or more medical images of a patient, a text report is output that is as similar as possible to one generated by a radiologist. From a machine learning point of view, creating a system that performs such a task would require learning a generative model from instances of reports written by radiologists. Figure 1 presents one example of such a report, Manual tags: Calcified Granuloma/lung/upper lobe/right Automatic tags: Calcified granuloma Comparison: Chest radiographs XXXX. Indication: XXXX-year-old male, chest pain. Findings: The cardiomediastinal silhouette is within normal limits for size and contour. The lungs are normally inflated without evidence of focal airspace disease, pleural effusion, or pneumothorax. Stable calcified granuloma within the right upper lung. No acute bone abnormality. Impression: No acute cardiopulmonary process. Fig. 1. Example from the IU X-ray dataset, frontal and lateral chest x-rays from a patient, alongside the natural language report and the annotated tags. XXXX is used for anonimization of the report.

taken from the IU X-ray dataset [28]. We see two input X-ray images (frontal and lateral), and below them some annotations (Tags) -some manually annotated by a radiologist and others automatically annotated-, and on the right side the report with four different sections (comparison, indication, findings, and impression). If we consider the clinical workflow of generating a medical imaging report, several aspects should be taken into account before diving into a concrete implementation.

The first aspect is considering additional patient information in the process of report generation. Most of the time, the physician asking for medical imaging is the primary care physician or a medical specialist. This implies that when radiologists write a report, they generally have patientrelevant clinical information, usually provided in the section Indication as shown in Figure 1. Also, the Comparison section can provide information of a serial follow-up procedure, to evaluate the evolution of a patient over time (e.g., aneurysm, congenital heart disease). Then, one decision can be whether or not to use these Indication and Comparison data to generate the sections Findings, Impression, or both of them. Second, the model for report generation should consider the diversity on medical images as well as body regions and conditions. There are several types of medical images, such as X-rays, CT, MRI, PET and SPECT. This implies that a model for text report generation that deals with only one type of input medical image might not solve it for other types. Also, ideally, a model should be able to generate reports from different parts of the human anatomy and diverse medical conditions. To adequately achieve this task, different body regions must have a balanced and sizable training set. Many works surveyed in this article focus on one specific part of the body and particular illnesses which limits the applicability of these methods to generalize to all possible diagnosis tasks.

Lastly, even if an AI system has perfect report generation accuracy, we might wonder if we can trust a machine in such a critical domain. One of the reasons for preferring a radiologist rather than an automated, highly accurate AI system is the chance of understanding the rationale behind the findings and impressions. In this sense, explainable AI [46] is of great importance in securing their adoption in a clinical setting.","[[], ['b27'], [], ['b45'], [], ['b27'], [], ['b45']]","[[], ['b27'], [], ['b45'], [], ['b27'], [], ['b45']]",4,"sent1: From a purely computational perspective, the following is the main task addressed by most articles analyzed in this survey: given as input one or more medical images of a patient, a text report is output that is as similar as possible to one generated by a radiologist.
sent2: From a machine learning point of view, creating a system that performs such a task would require learning a generative model from instances of reports written by radiologists.
sent3: Figure 1 presents one example of such a report, Manual tags: Calcified Granuloma/lung/upper lobe/right Automatic tags: Calcified granuloma Comparison: Chest radiographs XXXX.
sent4: Indication: XXXX-year-old male, chest pain.
sent5: Findings: The cardiomediastinal silhouette is within normal limits for size and contour.
sent6: The lungs are normally inflated without evidence of focal airspace disease, pleural effusion, or pneumothorax.
sent7: Stable calcified granuloma within the right upper lung.
sent8: No acute bone abnormality. Impression: No acute cardiopulmonary process.
sent9: Fig. 1. Example from the IU X-ray dataset, frontal and lateral chest x-rays from a patient, alongside the natural language report and the annotated tags.
sent10: XXXX is used for anonimization of the report.
sent11: taken from the IU X-ray dataset [28].
sent12: We see two input X-ray images (frontal and lateral), and below them some annotations (Tags)
sent13: -some manually annotated by a radiologist and others automatically annotated-, and on the right side the report with four different sections (comparison, indication, findings, and impression).
sent14: If we consider the clinical workflow of generating a medical imaging report, several aspects should be taken into account before diving into a concrete implementation.
sent15: The first aspect is considering additional patient information in the process of report generation.
sent16: Most of the time, the physician asking for medical imaging is the primary care physician or a medical specialist.
sent17: This implies that when radiologists write a report, they generally have patientrelevant clinical information, usually provided in the section Indication as shown in Figure 1.
sent18: Also, the Comparison section can provide information of a serial follow-up procedure, to evaluate the evolution of a patient over time (e.g., aneurysm, congenital heart disease).
sent19: Then, one decision can be whether or not to use these Indication and Comparison data to generate the sections Findings, Impression, or both of them.
sent20: Second, the model for report generation should consider the diversity on medical images as well as body regions and conditions.
sent21: There are several types of medical images, such as X-rays, CT, MRI, PET and SPECT.
sent22: This implies that a model for text report generation that deals with only one type of input medical image might not solve it for other types.
sent23: Also, ideally, a model should be able to generate reports from different parts of the human anatomy and diverse medical conditions.
sent24: To adequately achieve this task, different body regions must have a balanced and sizable training set.
sent25: Many works surveyed in this article focus on one specific part of the body and particular illnesses which limits the applicability of these methods to generalize to all possible diagnosis tasks.
sent26: Lastly, even if an AI system has perfect report generation accuracy, we might wonder if we can trust a machine in such a critical domain.
sent27: One of the reasons for preferring a radiologist rather than an automated, highly accurate AI system is the chance of understanding the rationale behind the findings and impressions.
sent28: In this sense, explainable AI [46] is of great importance in securing their adoption in a clinical setting.
sent29: From a purely computational perspective, the following is the main task addressed by most articles analyzed in this survey: given as input one or more medical images of a patient, a text report is output that is as similar as possible to one generated by a radiologist.
sent30: From a machine learning point of view, creating a system that performs such a task would require learning a generative model from instances of reports written by radiologists.
sent31: Figure 1 presents one example of such a report, Manual tags: Calcified Granuloma/lung/upper lobe/right Automatic tags: Calcified granuloma Comparison: Chest radiographs XXXX.
sent32: Indication: XXXX-year-old male, chest pain.
sent33: Findings: The cardiomediastinal silhouette is within normal limits for size and contour.
sent34: The lungs are normally inflated without evidence of focal airspace disease, pleural effusion, or pneumothorax.
sent35: Stable calcified granuloma within the right upper lung.
sent36: No acute bone abnormality. Impression: No acute cardiopulmonary process.
sent37: Fig. 1. Example from the IU X-ray dataset, frontal and lateral chest x-rays from a patient, alongside the natural language report and the annotated tags.
sent38: XXXX is used for anonimization of the report.
sent39: taken from the IU X-ray dataset [28].
sent40: We see two input X-ray images (frontal and lateral), and below them some annotations (Tags)
sent41: -some manually annotated by a radiologist and others automatically annotated-, and on the right side the report with four different sections (comparison, indication, findings, and impression).
sent42: If we consider the clinical workflow of generating a medical imaging report, several aspects should be taken into account before diving into a concrete implementation.
sent43: The first aspect is considering additional patient information in the process of report generation.
sent44: Most of the time, the physician asking for medical imaging is the primary care physician or a medical specialist.
sent45: This implies that when radiologists write a report, they generally have patientrelevant clinical information, usually provided in the section Indication as shown in Figure 1.
sent46: Also, the Comparison section can provide information of a serial follow-up procedure, to evaluate the evolution of a patient over time (e.g., aneurysm, congenital heart disease).
sent47: Then, one decision can be whether or not to use these Indication and Comparison data to generate the sections Findings, Impression, or both of them.
sent48: Second, the model for report generation should consider the diversity on medical images as well as body regions and conditions.
sent49: There are several types of medical images, such as X-rays, CT, MRI, PET and SPECT.
sent50: This implies that a model for text report generation that deals with only one type of input medical image might not solve it for other types.
sent51: Also, ideally, a model should be able to generate reports from different parts of the human anatomy and diverse medical conditions.
sent52: To adequately achieve this task, different body regions must have a balanced and sizable training set.
sent53: Many works surveyed in this article focus on one specific part of the body and particular illnesses which limits the applicability of these methods to generalize to all possible diagnosis tasks.
sent54: Lastly, even if an AI system has perfect report generation accuracy, we might wonder if we can trust a machine in such a critical domain.
sent55: One of the reasons for preferring a radiologist rather than an automated, highly accurate AI system is the chance of understanding the rationale behind the findings and impressions.
sent56: In this sense, explainable AI [46] is of great importance in securing their adoption in a clinical setting."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s12,Domain knowledge.,"Although all works used datasets from the medical domain to train their models, which can be considered a form of domain knowledge transfer, some works took special steps to explicitly incorporate additional knowledge from experts into their design. Concretely, we identify two incipient trends in the application of domain knowledge: 1) the use of graph neural networks right after the CNN, providing an architectural bias to guide the model to identify medical concepts and their relations from the images; and 2) enhancing the model's report generation with access to an external template database curated by experts.

KERP [86] incorporates knowledge at the architectural level using graph neural networks. The authors manually designed an abnormality graph and a disease graph, where each node represents an abnormality or disease, and the edges are built based on their co-occurrences in the training set. Some example abnormalities are ""low lung volumes"" and ""enlarged heart size"", whereas diseases represent a higher level of abstraction, for example ""emphysema"" or ""consolidation"". The information flows from image features (encoded by a CNN) to the abnormality graph, and then to the disease graph, via inter-node message passing. This biases the network to encode the visual information in terms of abnormalities, diseases and their relations. Similarly, Zhang et al. [162] created an observations graph, containing 20 nodes of chest abnormalities or body parts, where conditions related to the same organ or tissue are connected by edges. Their ablation analysis showed some performance gains, thanks to the graph neural network.

In seven works [16,47,49,76,85,86,98] the authors provided their models with a curated set of template sentences that are further processed in the language component to output a full report. Three works [47,49,76] used manually curated templates and if-then based programs to select and fill them. CLARA [16] uses a database indexing all sentences from the training set reports for text-based retrieval, which are then paraphrased by a generative module. Similarly, KERP [86] has access to a template database mined from the training set, which are also paraphrased later. In HRGR [85] the most common sentences in the datasets were mined and then manually grouped by meaning to further reduce repetitions. In this work the authors showed that HRGR learned to prefer templates about 80% of the time and only generate sentences from scratch the remaining 20%, suggesting that templates can be quite useful to generate most sentences in reports.

Although all works used datasets from the medical domain to train their models, which can be considered a form of domain knowledge transfer, some works took special steps to explicitly incorporate additional knowledge from experts into their design. Concretely, we identify two incipient trends in the application of domain knowledge: 1) the use of graph neural networks right after the CNN, providing an architectural bias to guide the model to identify medical concepts and their relations from the images; and 2) enhancing the model's report generation with access to an external template database curated by experts.

KERP [86] incorporates knowledge at the architectural level using graph neural networks. The authors manually designed an abnormality graph and a disease graph, where each node represents an abnormality or disease, and the edges are built based on their co-occurrences in the training set. Some example abnormalities are ""low lung volumes"" and ""enlarged heart size"", whereas diseases represent a higher level of abstraction, for example ""emphysema"" or ""consolidation"". The information flows from image features (encoded by a CNN) to the abnormality graph, and then to the disease graph, via inter-node message passing. This biases the network to encode the visual information in terms of abnormalities, diseases and their relations. Similarly, Zhang et al. [162] created an observations graph, containing 20 nodes of chest abnormalities or body parts, where conditions related to the same organ or tissue are connected by edges. Their ablation analysis showed some performance gains, thanks to the graph neural network.

In seven works [16,47,49,76,85,86,98] the authors provided their models with a curated set of template sentences that are further processed in the language component to output a full report. Three works [47,49,76] used manually curated templates and if-then based programs to select and fill them. CLARA [16] uses a database indexing all sentences from the training set reports for text-based retrieval, which are then paraphrased by a generative module. Similarly, KERP [86] has access to a template database mined from the training set, which are also paraphrased later. In HRGR [85] the most common sentences in the datasets were mined and then manually grouped by meaning to further reduce repetitions. In this work the authors showed that HRGR learned to prefer templates about 80% of the time and only generate sentences from scratch the remaining 20%, suggesting that templates can be quite useful to generate most sentences in reports.","[[], ['b85', 'b161'], ['b85', 'b84', 'b46', 'b48', 'b97', 'b75', 'b15'], [], ['b85', 'b161'], ['b85', 'b84', 'b46', 'b48', 'b97', 'b75', 'b15']]","[[], ['b85', 'b161'], ['b85', 'b84', 'b46', 'b48', 'b97', 'b75', 'b15'], [], ['b85', 'b161'], ['b85', 'b84', 'b46', 'b48', 'b97', 'b75', 'b15']]",18,"sent1: Although all works used datasets from the medical domain to train their models, which can be considered a form of domain knowledge transfer, some works took special steps to explicitly incorporate additional knowledge from experts into their design.
sent2: Concretely, we identify two incipient trends in the application of domain knowledge: 1) the use of graph neural networks right after the CNN, providing an architectural bias to guide the model to identify medical concepts and their relations from the images; and 2) enhancing the model's report generation with access to an external template database curated by experts.
sent3: KERP [86] incorporates knowledge at the architectural level using graph neural networks.
sent4: The authors manually designed an abnormality graph and a disease graph, where each node represents an abnormality or disease, and the edges are built based on their co-occurrences in the training set.
sent5: Some example abnormalities are ""low lung volumes"" and ""enlarged heart size"", whereas diseases represent a higher level of abstraction, for example ""emphysema"" or ""consolidation"".
sent6: The information flows from image features (encoded by a CNN) to the abnormality graph, and then to the disease graph, via inter-node message passing.
sent7: This biases the network to encode the visual information in terms of abnormalities, diseases and their relations.
sent8: Similarly, Zhang et al. [162] created an observations graph, containing 20 nodes of chest abnormalities or body parts, where conditions related to the same organ or tissue are connected by edges.
sent9: Their ablation analysis showed some performance gains, thanks to the graph neural network.
sent10: In seven works [16,47,49,76,85,86,98] the authors provided their models with a curated set of template sentences that are further processed in the language component to output a full report.
sent11: Three works [47,49,76] used manually curated templates and if-then based programs to select and fill them.
sent12: CLARA [16] uses a database indexing all sentences from the training set reports for text-based retrieval, which are then paraphrased by a generative module.
sent13: Similarly, KERP [86] has access to a template database mined from the training set, which are also paraphrased later.
sent14: In HRGR [85] the most common sentences in the datasets were mined and then manually grouped by meaning to further reduce repetitions.
sent15: In this work the authors showed that HRGR learned to prefer templates about 80% of the time and only generate sentences from scratch the remaining 20%, suggesting that templates can be quite useful to generate most sentences in reports.
sent16: Although all works used datasets from the medical domain to train their models, which can be considered a form of domain knowledge transfer, some works took special steps to explicitly incorporate additional knowledge from experts into their design.
sent17: Concretely, we identify two incipient trends in the application of domain knowledge: 1) the use of graph neural networks right after the CNN, providing an architectural bias to guide the model to identify medical concepts and their relations from the images; and 2) enhancing the model's report generation with access to an external template database curated by experts.
sent18: KERP [86] incorporates knowledge at the architectural level using graph neural networks.
sent19: The authors manually designed an abnormality graph and a disease graph, where each node represents an abnormality or disease, and the edges are built based on their co-occurrences in the training set.
sent20: Some example abnormalities are ""low lung volumes"" and ""enlarged heart size"", whereas diseases represent a higher level of abstraction, for example ""emphysema"" or ""consolidation"".
sent21: The information flows from image features (encoded by a CNN) to the abnormality graph, and then to the disease graph, via inter-node message passing.
sent22: This biases the network to encode the visual information in terms of abnormalities, diseases and their relations.
sent23: Similarly, Zhang et al. [162] created an observations graph, containing 20 nodes of chest abnormalities or body parts, where conditions related to the same organ or tissue are connected by edges.
sent24: Their ablation analysis showed some performance gains, thanks to the graph neural network.
sent25: In seven works [16,47,49,76,85,86,98] the authors provided their models with a curated set of template sentences that are further processed in the language component to output a full report.
sent26: Three works [47,49,76] used manually curated templates and if-then based programs to select and fill them.
sent27: CLARA [16] uses a database indexing all sentences from the training set reports for text-based retrieval, which are then paraphrased by a generative module.
sent28: Similarly, KERP [86] has access to a template database mined from the training set, which are also paraphrased later.
sent29: In HRGR [85] the most common sentences in the datasets were mined and then manually grouped by meaning to further reduce repetitions.
sent30: In this work the authors showed that HRGR learned to prefer templates about 80% of the time and only generate sentences from scratch the remaining 20%, suggesting that templates can be quite useful to generate most sentences in reports."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s27,Dataset,"Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9. Additional data contained in each dataset. MeSH [115] and RadLex [81] are sets of medical concepts. MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools. Manual means manually annotated by experts. In all cases, the localization information was manually annotated by experts.    Table 11. Summary of optimization strategies used in the literature.

Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9. Additional data contained in each dataset. MeSH [115] and RadLex [81] are sets of medical concepts. MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools. Manual means manually annotated by experts. In all cases, the localization information was manually annotated by experts.    Table 11. Summary of optimization strategies used in the literature.","[['b27', 'b62', 'b105', 'b124', 'b99', 'b80', 'b114', 'b9', 'b83'], ['b27', 'b62', 'b105', 'b124', 'b99', 'b80', 'b114', 'b9', 'b83']]","[['b27', 'b62', 'b105', 'b124', 'b99', 'b80', 'b114', 'b9', 'b83'], ['b27', 'b62', 'b105', 'b124', 'b99', 'b80', 'b114', 'b9', 'b83']]",18,"sent1: Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9.
sent2: Additional data contained in each dataset.
sent3: MeSH [115] and RadLex [81] are sets of medical concepts.
sent4: MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools.
sent5: Manual means manually annotated by experts.
sent6: In all cases, the localization information was manually annotated by experts.
sent7: Table 11. Summary of optimization strategies used in the literature.
sent8: Text Tags  Tags annotation  method   Localization Public report datasets IU X-ray [28] Indication (1) Table 9.
sent9: Additional data contained in each dataset.
sent10: MeSH [115] and RadLex [81] are sets of medical concepts.
sent11: MTI [100], MetaMap [10], CheXpert labeler [63], NegBio [106], Quick-UMLS [125] and DNorm [84] are automatic labeler tools.
sent12: Manual means manually annotated by experts.
sent13: In all cases, the localization information was manually annotated by experts.
sent14: Table 11. Summary of optimization strategies used in the literature."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s23,Comparison of papers' performance,"To find out which paper holds the state of the art, we need to find a common ground for fair comparison. A natural choice is the IU X-ray dataset [28], since a majority of the surveyed papers report results in this dataset. Table 7 shows these results, separated by which report sections are generated by each paper, findings, impression or both. The findings section consists of multiple sentences, and mainly describes medical conditions observed, while the impression section is a one sentence conclusion or diagnosis. Notice Spinks and Moens [126] filtered the findings section, and kept only sentences referring to one disease (Cardiomegaly). The papers that seem to show the best performance in terms of NLP metrics are KERP [86], CLARA [16] and Xue et al. 2019 [153] for the findings section, MTMA [132] for the impression section, and Yuan et al. [156], MLMA [36] and Xue et al. 2019 [153] for both sections. Of these, only MTMA has a large difference to its competitors, and there is no clear winner in the other sections. Some caveats, however, should be kept in mind when interpreting these results:

(1) The results reported in the literature only allow comparisons in terms of standard natural language metrics (BLEU, ROUGE-L, etc.), but from these results we cannot draw conclusions about medical correctness, since NLP metrics and clinical accuracy are not necessarily correlated.  Table 7. Evaluation results of papers that use the IU X-ray dataset. All values were extracted from their papers, except in some cases where results were not present in the own paper: (1) TieNet [144] results were presented in Liu et al. [92] as a baseline; (2) Xue et al. 2018 [154] results in the findings section were presented in Xue et al. 2019 [153] as a baseline. (3) CLARA [16] results are from the fully automatic version.

(2) MTMA uses additional input, as discussed in section 5.2.1. Specifically, the model receives the indication and findings sections of the report to generate the impression section, at both test and train stages. In a sense, this could be seen as an enhanced summarizing approach, since the impression section contains a conclusion from the findings.

(3) Some NLP metrics, such as CIDEr, ROUGE-L and METEOR, have variants and parameters, as discussed in section 5.4. Unfortunately, most papers do not mention the specific version or implementation used.

(4) The IU X-ray dataset does not have standard training-validation-test splits. This has led researchers to define their own splits, as indicated by column Split of Table 7. These splits are not consistent across papers, making results less comparable. For example, if a model was evaluated in an easier test split, that would give it an unfair advantage over other models evaluated in harder test splits. Additionally, other decisions such the number of images per report (frontal, lateral or both), the tokenization algorithm employed, the removal of noisy sentences, the removal of words with a frequency under a given threshold, the removal of duplicate images, among other preprocessing decisions, are not always explicitly stated in papers, and these may have an impact on the results as well.

(5) These are overall results only, so a more fine-grained performance assessment on specific abnormalities or diseases is missing. This further shows the need for standardizing one or more evaluation metrics to measure the medical correctness of a generated report, considering different aspects of interest.

To find out which paper holds the state of the art, we need to find a common ground for fair comparison. A natural choice is the IU X-ray dataset [28], since a majority of the surveyed papers report results in this dataset. Table 7 shows these results, separated by which report sections are generated by each paper, findings, impression or both. The findings section consists of multiple sentences, and mainly describes medical conditions observed, while the impression section is a one sentence conclusion or diagnosis. Notice Spinks and Moens [126] filtered the findings section, and kept only sentences referring to one disease (Cardiomegaly). The papers that seem to show the best performance in terms of NLP metrics are KERP [86], CLARA [16] and Xue et al. 2019 [153] for the findings section, MTMA [132] for the impression section, and Yuan et al. [156], MLMA [36] and Xue et al. 2019 [153] for both sections. Of these, only MTMA has a large difference to its competitors, and there is no clear winner in the other sections. Some caveats, however, should be kept in mind when interpreting these results:

(1) The results reported in the literature only allow comparisons in terms of standard natural language metrics (BLEU, ROUGE-L, etc.), but from these results we cannot draw conclusions about medical correctness, since NLP metrics and clinical accuracy are not necessarily correlated.  Table 7. Evaluation results of papers that use the IU X-ray dataset. All values were extracted from their papers, except in some cases where results were not present in the own paper: (1) TieNet [144] results were presented in Liu et al. [92] as a baseline; (2) Xue et al. 2018 [154] results in the findings section were presented in Xue et al. 2019 [153] as a baseline. (3) CLARA [16] results are from the fully automatic version.

(2) MTMA uses additional input, as discussed in section 5.2.1. Specifically, the model receives the indication and findings sections of the report to generate the impression section, at both test and train stages. In a sense, this could be seen as an enhanced summarizing approach, since the impression section contains a conclusion from the findings.

(3) Some NLP metrics, such as CIDEr, ROUGE-L and METEOR, have variants and parameters, as discussed in section 5.4. Unfortunately, most papers do not mention the specific version or implementation used.

(4) The IU X-ray dataset does not have standard training-validation-test splits. This has led researchers to define their own splits, as indicated by column Split of Table 7. These splits are not consistent across papers, making results less comparable. For example, if a model was evaluated in an easier test split, that would give it an unfair advantage over other models evaluated in harder test splits. Additionally, other decisions such the number of images per report (frontal, lateral or both), the tokenization algorithm employed, the removal of noisy sentences, the removal of words with a frequency under a given threshold, the removal of duplicate images, among other preprocessing decisions, are not always explicitly stated in papers, and these may have an impact on the results as well.

(5) These are overall results only, so a more fine-grained performance assessment on specific abnormalities or diseases is missing. This further shows the need for standardizing one or more evaluation metrics to measure the medical correctness of a generated report, considering different aspects of interest.","[['b27', 'b85', 'b125', 'b131', 'b155', 'b35', 'b152', 'b15'], ['b153', 'b143', 'b91', 'b1', 'b152', 'b2', 'b15', 'b0'], [], [], [], [], ['b27', 'b85', 'b125', 'b131', 'b155', 'b35', 'b152', 'b15'], ['b153', 'b143', 'b91', 'b1', 'b152', 'b2', 'b15', 'b0'], [], [], [], []]","[['b27', 'b85', 'b125', 'b131', 'b155', 'b35', 'b152', 'b15'], ['b153', 'b143', 'b91', 'b1', 'b152', 'b2', 'b15', 'b0'], [], [], [], [], ['b27', 'b85', 'b125', 'b131', 'b155', 'b35', 'b152', 'b15'], ['b153', 'b143', 'b91', 'b1', 'b152', 'b2', 'b15', 'b0'], [], [], [], []]",32,"sent1: To find out which paper holds the state of the art, we need to find a common ground for fair comparison.
sent2: A natural choice is the IU X-ray dataset [28], since a majority of the surveyed papers report results in this dataset.
sent3: Table 7 shows these results, separated by which report sections are generated by each paper, findings, impression or both.
sent4: The findings section consists of multiple sentences, and mainly describes medical conditions observed, while the impression section is a one sentence conclusion or diagnosis.
sent5: Notice Spinks and Moens [126] filtered the findings section, and kept only sentences referring to one disease (Cardiomegaly).
sent6: The papers that seem to show the best performance in terms of NLP metrics are KERP [86], CLARA [16] and Xue et al. 2019 [153] for the findings section, MTMA [132] for the impression section, and Yuan et al. [156], MLMA [36] and Xue et al. 2019 [153] for both sections.
sent7: Of these, only MTMA has a large difference to its competitors, and there is no clear winner in the other sections.
sent8: Some caveats, however, should be kept in mind when interpreting these results:(1)
sent9: The results reported in the literature only allow comparisons in terms of standard natural language metrics (BLEU, ROUGE-L, etc.), but from these results we cannot draw conclusions about medical correctness, since NLP metrics and clinical accuracy are not necessarily correlated.
sent10: Table 7. Evaluation results of papers that use the IU X-ray dataset.
sent11: All values were extracted from their papers, except in some cases where results were not present in the own paper: (1) TieNet [144] results were presented in Liu et al. [92] as a baseline; (2) Xue et al. 2018 [154] results in the findings section were presented in Xue et al. 2019 [153] as a baseline.
sent12: (3) CLARA [16] results are from the fully automatic version.
sent13: (2) MTMA uses additional input, as discussed in section 5.2.1.
sent14: Specifically, the model receives the indication and findings sections of the report to generate the impression section, at both test and train stages.
sent15: In a sense, this could be seen as an enhanced summarizing approach, since the impression section contains a conclusion from the findings.
sent16: (3) Some NLP metrics, such as CIDEr, ROUGE-L and METEOR, have variants and parameters, as discussed in section 5.4.
sent17: Unfortunately, most papers do not mention the specific version or implementation used.
sent18: (4) The IU X-ray dataset does not have standard training-validation-test splits.
sent19: This has led researchers to define their own splits, as indicated by column Split of Table 7.
sent20: These splits are not consistent across papers, making results less comparable.
sent21: For example, if a model was evaluated in an easier test split, that would give it an unfair advantage over other models evaluated in harder test splits.
sent22: Additionally, other decisions such the number of images per report (frontal, lateral or both), the tokenization algorithm employed, the removal of noisy sentences, the removal of words with a frequency under a given threshold, the removal of duplicate images, among other preprocessing decisions, are not always explicitly stated in papers, and these may have an impact on the results as well.
sent23: (5) These are overall results only, so a more fine-grained performance assessment on specific abnormalities or diseases is missing.
sent24: This further shows the need for standardizing one or more evaluation metrics to measure the medical correctness of a generated report, considering different aspects of interest.
sent25: To find out which paper holds the state of the art, we need to find a common ground for fair comparison.
sent26: A natural choice is the IU X-ray dataset [28], since a majority of the surveyed papers report results in this dataset.
sent27: Table 7 shows these results, separated by which report sections are generated by each paper, findings, impression or both.
sent28: The findings section consists of multiple sentences, and mainly describes medical conditions observed, while the impression section is a one sentence conclusion or diagnosis.
sent29: Notice Spinks and Moens [126] filtered the findings section, and kept only sentences referring to one disease (Cardiomegaly).
sent30: The papers that seem to show the best performance in terms of NLP metrics are KERP [86], CLARA [16] and Xue et al. 2019 [153] for the findings section, MTMA [132] for the impression section, and Yuan et al. [156], MLMA [36] and Xue et al. 2019 [153] for both sections.
sent31: Of these, only MTMA has a large difference to its competitors, and there is no clear winner in the other sections.
sent32: Some caveats, however, should be kept in mind when interpreting these results:(1)
sent33: The results reported in the literature only allow comparisons in terms of standard natural language metrics (BLEU, ROUGE-L, etc.), but from these results we cannot draw conclusions about medical correctness, since NLP metrics and clinical accuracy are not necessarily correlated.
sent34: Table 7. Evaluation results of papers that use the IU X-ray dataset.
sent35: All values were extracted from their papers, except in some cases where results were not present in the own paper: (1) TieNet [144] results were presented in Liu et al. [92] as a baseline; (2) Xue et al. 2018 [154] results in the findings section were presented in Xue et al. 2019 [153] as a baseline.
sent36: (3) CLARA [16] results are from the fully automatic version.
sent37: (2) MTMA uses additional input, as discussed in section 5.2.1.
sent38: Specifically, the model receives the indication and findings sections of the report to generate the impression section, at both test and train stages.
sent39: In a sense, this could be seen as an enhanced summarizing approach, since the impression section contains a conclusion from the findings.
sent40: (3) Some NLP metrics, such as CIDEr, ROUGE-L and METEOR, have variants and parameters, as discussed in section 5.4.
sent41: Unfortunately, most papers do not mention the specific version or implementation used.
sent42: (4) The IU X-ray dataset does not have standard training-validation-test splits.
sent43: This has led researchers to define their own splits, as indicated by column Split of Table 7.
sent44: These splits are not consistent across papers, making results less comparable.
sent45: For example, if a model was evaluated in an easier test split, that would give it an unfair advantage over other models evaluated in harder test splits.
sent46: Additionally, other decisions such the number of images per report (frontal, lateral or both), the tokenization algorithm employed, the removal of noisy sentences, the removal of words with a frequency under a given threshold, the removal of duplicate images, among other preprocessing decisions, are not always explicitly stated in papers, and these may have an impact on the results as well.
sent47: (5) These are overall results only, so a more fine-grained performance assessment on specific abnormalities or diseases is missing.
sent48: This further shows the need for standardizing one or more evaluation metrics to measure the medical correctness of a generated report, considering different aspects of interest."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s55,Explainability,"There have been multiple attempts on providing a definition for explainability in the Explainable Artificial Intelligence (XAI) area [33,91,113]. For the task of report generation from medical images, we use a similar definition by Doshi-Velez and Kim [33]: the ability to justify an outcome in understandable terms for a human, and we use it interchangeably with the term interpretability. In this medical context, an automated system requires high explainability levels as two main facts hold: the decisions derived from the system will probably have direct consequences for patients, and the diagnosis task is not trivial and susceptible to human judgement [33,113]. Furthermore, the explanation methods employed in this medical task should attempt to solve several related aspects: align with clinicians' expectations and acquire their trust, increase system transparency, assess results quality, and allow addressing accountability, fairness and ethical concerns [4,113,134].

There are many ways to address the explainability aspect of AI systems in the medical domain, as listed in the recent survey on interpretable AI for radiology by Reyes et al. [113]. Multiple categories can be identified, starting with global vs local, the former refers to explanations regarding the whole system's operation, and the latter to explanations for one sample. For local explanations, there are different kinds of approaches, such as feature importance, concept-based, example-based, and uncertainty, to mention a few. Feature importance methods attempt to compute a level of importance for each input value, to understand which characteristics were most relevant to make a decision; for example, gradient-based methods for CNNs such as Grad-CAM [118], Guided Backpropagation [127] or DeepLIFT [121]; and other techniques such as LIME [114]. In concept-based methods, like TCAV [75] or RCV [43], the contributions to the prediction from multiple concepts are quantified, so the user can check if the concepts used by the model are correct. Example-based approaches present additional examples with the output, either with a similar outcome, so the user can look for a common pattern, or with an opposite outcome (counter-factual). Uncertainty methods provide the level of confidence of the model for a given prediction. For global explanations, there are sample-based approaches, such as SP-LIME [114], or methods to directly increase the transparency of the system.

Despite the importance of explainability in this area, only two reviewed works focused explicitly on this topic. Gale et al. [38] proposed the automatic generation of a natural language report as an explanation for a classification task; however, their approach does not include an explanation for the report. Spinks and Moens [126] present a counter-factual local explanation, as will be detailed in subsection 5.3.1. Additionally, in 29 works the model architecture generates a secondary output that can also be presented as a local explanation. We distinguish three types of outputs: classification (section 5.3.2), heatmap over the input image (section 5.3.3), and heatmap over the input text (section 5.3.4). These were already summarized in Table 3 in the Input and Output section (5.2.1). Next, the explanaibility aspects of the outputs are discussed. [126] proposed an architecture to both classify a disease and generate a caption from a chest X-ray, based on GANs and autoencoders, as detailed in the Model Design section (5.2). Thus, to provide a local explanation, at inference time the input image is encoded into a latent vector, which is used to generate a new chest X-ray and a new report, both of them subject to result in the nearest alternative classification, i.e., the nearest diagnosis. With this information, a user could compare the original X-ray with the generated image, and attempt to understand why the model has reached its decision.

There have been multiple attempts on providing a definition for explainability in the Explainable Artificial Intelligence (XAI) area [33,91,113]. For the task of report generation from medical images, we use a similar definition by Doshi-Velez and Kim [33]: the ability to justify an outcome in understandable terms for a human, and we use it interchangeably with the term interpretability. In this medical context, an automated system requires high explainability levels as two main facts hold: the decisions derived from the system will probably have direct consequences for patients, and the diagnosis task is not trivial and susceptible to human judgement [33,113]. Furthermore, the explanation methods employed in this medical task should attempt to solve several related aspects: align with clinicians' expectations and acquire their trust, increase system transparency, assess results quality, and allow addressing accountability, fairness and ethical concerns [4,113,134].

There are many ways to address the explainability aspect of AI systems in the medical domain, as listed in the recent survey on interpretable AI for radiology by Reyes et al. [113]. Multiple categories can be identified, starting with global vs local, the former refers to explanations regarding the whole system's operation, and the latter to explanations for one sample. For local explanations, there are different kinds of approaches, such as feature importance, concept-based, example-based, and uncertainty, to mention a few. Feature importance methods attempt to compute a level of importance for each input value, to understand which characteristics were most relevant to make a decision; for example, gradient-based methods for CNNs such as Grad-CAM [118], Guided Backpropagation [127] or DeepLIFT [121]; and other techniques such as LIME [114]. In concept-based methods, like TCAV [75] or RCV [43], the contributions to the prediction from multiple concepts are quantified, so the user can check if the concepts used by the model are correct. Example-based approaches present additional examples with the output, either with a similar outcome, so the user can look for a common pattern, or with an opposite outcome (counter-factual). Uncertainty methods provide the level of confidence of the model for a given prediction. For global explanations, there are sample-based approaches, such as SP-LIME [114], or methods to directly increase the transparency of the system.

Despite the importance of explainability in this area, only two reviewed works focused explicitly on this topic. Gale et al. [38] proposed the automatic generation of a natural language report as an explanation for a classification task; however, their approach does not include an explanation for the report. Spinks and Moens [126] present a counter-factual local explanation, as will be detailed in subsection 5.3.1. Additionally, in 29 works the model architecture generates a secondary output that can also be presented as a local explanation. We distinguish three types of outputs: classification (section 5.3.2), heatmap over the input image (section 5.3.3), and heatmap over the input text (section 5.3.4). These were already summarized in Table 3 in the Input and Output section (5.2.1). Next, the explanaibility aspects of the outputs are discussed. [126] proposed an architecture to both classify a disease and generate a caption from a chest X-ray, based on GANs and autoencoders, as detailed in the Model Design section (5.2). Thus, to provide a local explanation, at inference time the input image is encoded into a latent vector, which is used to generate a new chest X-ray and a new report, both of them subject to result in the nearest alternative classification, i.e., the nearest diagnosis. With this information, a user could compare the original X-ray with the generated image, and attempt to understand why the model has reached its decision.","[['b3', 'b133', 'b112', 'b90', 'b32'], ['b113', 'b126', 'b74', 'b112', 'b42', 'b120', 'b117'], ['b125', 'b37'], ['b3', 'b133', 'b112', 'b90', 'b32'], ['b113', 'b126', 'b74', 'b112', 'b42', 'b120', 'b117'], ['b125', 'b37']]","[['b3', 'b133', 'b112', 'b90', 'b32'], ['b113', 'b126', 'b74', 'b112', 'b42', 'b120', 'b117'], ['b125', 'b37'], ['b3', 'b133', 'b112', 'b90', 'b32'], ['b113', 'b126', 'b74', 'b112', 'b42', 'b120', 'b117'], ['b125', 'b37']]",28,"sent1: There have been multiple attempts on providing a definition for explainability in the Explainable Artificial Intelligence (XAI) area [33,91,113].
sent2: For the task of report generation from medical images, we use a similar definition by Doshi-Velez and Kim [33]: the ability to justify an outcome in understandable terms for a human, and we use it interchangeably with the term interpretability.
sent3: In this medical context, an automated system requires high explainability levels as two main facts hold: the decisions derived from the system will probably have direct consequences for patients, and the diagnosis task is not trivial and susceptible to human judgement [33,113].
sent4: Furthermore, the explanation methods employed in this medical task should attempt to solve several related aspects: align with clinicians' expectations and acquire their trust, increase system transparency, assess results quality, and allow addressing accountability, fairness and ethical concerns [4,113,134].
sent5: There are many ways to address the explainability aspect of AI systems in the medical domain, as listed in the recent survey on interpretable AI for radiology by Reyes et al. [113].
sent6: Multiple categories can be identified, starting with global vs local, the former refers to explanations regarding the whole system's operation, and the latter to explanations for one sample.
sent7: For local explanations, there are different kinds of approaches, such as feature importance, concept-based, example-based, and uncertainty, to mention a few.
sent8: Feature importance methods attempt to compute a level of importance for each input value, to understand which characteristics were most relevant to make a decision; for example, gradient-based methods for CNNs such as Grad-CAM [118], Guided Backpropagation [127] or DeepLIFT [121]; and other techniques such as LIME [114].
sent9: In concept-based methods, like TCAV [75] or RCV [43], the contributions to the prediction from multiple concepts are quantified, so the user can check if the concepts used by the model are correct.
sent10: Example-based approaches present additional examples with the output, either with a similar outcome, so the user can look for a common pattern, or with an opposite outcome (counter-factual).
sent11: Uncertainty methods provide the level of confidence of the model for a given prediction.
sent12: For global explanations, there are sample-based approaches, such as SP-LIME [114], or methods to directly increase the transparency of the system.
sent13: Despite the importance of explainability in this area, only two reviewed works focused explicitly on this topic.
sent14: Gale et al. [38] proposed the automatic generation of a natural language report as an explanation for a classification task; however, their approach does not include an explanation for the report.
sent15: Spinks and Moens [126] present a counter-factual local explanation, as will be detailed in subsection 5.3.1.
sent16: Additionally, in 29 works the model architecture generates a secondary output that can also be presented as a local explanation.
sent17: We distinguish three types of outputs: classification (section 5.3.2), heatmap over the input image (section 5.3.3), and heatmap over the input text (section 5.3.4).
sent18: These were already summarized in Table 3 in the Input and Output section (5.2.1).
sent19: Next, the explanaibility aspects of the outputs are discussed.
sent20: [126] proposed an architecture to both classify a disease and generate a caption from a chest X-ray, based on GANs and autoencoders, as detailed in the Model Design section (5.2).
sent21: Thus, to provide a local explanation, at inference time the input image is encoded into a latent vector, which is used to generate a new chest X-ray and a new report, both of them subject to result in the nearest alternative classification, i.e., the nearest diagnosis.
sent22: With this information, a user could compare the original X-ray with the generated image, and attempt to understand why the model has reached its decision.
sent23: There have been multiple attempts on providing a definition for explainability in the Explainable Artificial Intelligence (XAI) area [33,91,113].
sent24: For the task of report generation from medical images, we use a similar definition by Doshi-Velez and Kim [33]: the ability to justify an outcome in understandable terms for a human, and we use it interchangeably with the term interpretability.
sent25: In this medical context, an automated system requires high explainability levels as two main facts hold: the decisions derived from the system will probably have direct consequences for patients, and the diagnosis task is not trivial and susceptible to human judgement [33,113].
sent26: Furthermore, the explanation methods employed in this medical task should attempt to solve several related aspects: align with clinicians' expectations and acquire their trust, increase system transparency, assess results quality, and allow addressing accountability, fairness and ethical concerns [4,113,134].
sent27: There are many ways to address the explainability aspect of AI systems in the medical domain, as listed in the recent survey on interpretable AI for radiology by Reyes et al. [113].
sent28: Multiple categories can be identified, starting with global vs local, the former refers to explanations regarding the whole system's operation, and the latter to explanations for one sample.
sent29: For local explanations, there are different kinds of approaches, such as feature importance, concept-based, example-based, and uncertainty, to mention a few.
sent30: Feature importance methods attempt to compute a level of importance for each input value, to understand which characteristics were most relevant to make a decision; for example, gradient-based methods for CNNs such as Grad-CAM [118], Guided Backpropagation [127] or DeepLIFT [121]; and other techniques such as LIME [114].
sent31: In concept-based methods, like TCAV [75] or RCV [43], the contributions to the prediction from multiple concepts are quantified, so the user can check if the concepts used by the model are correct.
sent32: Example-based approaches present additional examples with the output, either with a similar outcome, so the user can look for a common pattern, or with an opposite outcome (counter-factual).
sent33: Uncertainty methods provide the level of confidence of the model for a given prediction.
sent34: For global explanations, there are sample-based approaches, such as SP-LIME [114], or methods to directly increase the transparency of the system.
sent35: Despite the importance of explainability in this area, only two reviewed works focused explicitly on this topic.
sent36: Gale et al. [38] proposed the automatic generation of a natural language report as an explanation for a classification task; however, their approach does not include an explanation for the report.
sent37: Spinks and Moens [126] present a counter-factual local explanation, as will be detailed in subsection 5.3.1.
sent38: Additionally, in 29 works the model architecture generates a secondary output that can also be presented as a local explanation.
sent39: We distinguish three types of outputs: classification (section 5.3.2), heatmap over the input image (section 5.3.3), and heatmap over the input text (section 5.3.4).
sent40: These were already summarized in Table 3 in the Input and Output section (5.2.1).
sent41: Next, the explanaibility aspects of the outputs are discussed.
sent42: [126] proposed an architecture to both classify a disease and generate a caption from a chest X-ray, based on GANs and autoencoders, as detailed in the Model Design section (5.2).
sent43: Thus, to provide a local explanation, at inference time the input image is encoded into a latent vector, which is used to generate a new chest X-ray and a new report, both of them subject to result in the nearest alternative classification, i.e., the nearest diagnosis.
sent44: With this information, a user could compare the original X-ray with the generated image, and attempt to understand why the model has reached its decision."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s47,Dataset,"Year Image Type # images # reports # patients Used by papers Report datasets IU X-ray [28] 2015  [19] 2019 Chest X-Ray 160,868 109,931 67,625 None (5) ImageCLEF Caption 2017 [35] 2017 Biomedical (2) 184,614 184,614 -[51] ImageCLEF Caption 2018 [39] 2018 Biomedical (2) 232,305 232,305 -None (5) ROCO [105] 2018 Multiple radiology (3) 81,825 81,825 -None (5) PEIR Gross [68] 2017 Gross lesions 7,442 7,442 - [68] INBreast (pt) [99] 2012 Mammography X-ray 410 115 115 [87,128] STARE [58] 1975 Retinal fundus 400 400 -None (5) RDIF (1) [95] 2019   (1) : the RDIF dataset is pending release. (2) : for the ImageCLEF datasets, images were extracted from PubMed Central papers and filtered automatically in order to keep only clinical images, but some unintended samples from other domains are also included. (3) : contains multiple modalities, namely CT, Ultrasound, X-Ray, Fluoroscopy, PET, Mammography, MRI, Angiography and PET-CT. (4) : the images are frames extracted from videos. (5) : none of the papers reviewed used this dataset.

The third column in Table 2 lists the image modalities for each dataset, showing chest X-rays concentrates most of the efforts in report datasets [19,28,44,69,85], though there are also datasets with biomedical images from varied types [35,39,68,105], mammography [99] and hip X-rays [37], ultrasound images [7,158], retinal images [58], doppler echocardiographies [98], cervical images [94], and kidney [95] and bladder biopsies [163]. This adds an extra challenge, since different kinds of exams may need different solutions, as the clinical conditions will be diverse. For example, a fundus retinal image may differ significantly from a chest X-ray; or a radiologist analyzing an X-ray may follow a different procedure than a pathologist reading a biopsy.

From the public report datasets, IU X-ray [28] is the most commonly used, consisting of 7,470 frontal and lateral chest X-rays and 3,955 reports. Additionally, each report was manually annotated with Medical Subject Heading (MeSH) 2 [115] and RadLex [81] terms, and automatically annotated with MeSH terms using the MTI [100] system plus the negation tool from MetaMap [10]. Figure 1 shows a sample image and report from this dataset. Note that for deep learning methods, the amount of data may seem insufficient, compared to general domain datasets with millions of samples, such as ImageNet [29]. This issue could be addressed with pre-training or data augmentation techniques. Also, this may be partially solved with the more recent datasets MIMIC-CXR [69] or PadChest [19], which contain 377,110 and 160,868 images respectively, but have not been widely used yet.

All report datasets include images and reports, and most of them also include labels for each report. Furthermore, INbreast [99] includes contours locating the labels in the images, the Ultrasound collection [157,158] includes bounding boxes locating organs, and IU X-ray [28] and RDIF [95] include additional text written by the physician who requested the exam. The complete detail of additional information is shown in Table 9 in appendix 9.1. This information can be leveraged as a supplementary context to further improve the system performance. On the one hand, the labels and image localization can be used to design auxiliary tasks (see section 5.2.5), and to further evaluate the text generation process (see section 5.4). On the other hand, the indication may contain additional information not present in the image, such as a patient's previous condition, which in some cases may be essential to address the task [95].

Lastly, many works use classification datasets, which do not provide a report for each image, but a set of clinical conditions or abnormalities present or absent in the image. In most cases, this kind of information is used to perform image classification as pre-training, an intermediate, or an auxiliary task to generate the report. One remarkable case is the CheXpert dataset [63], which contains 224,316 images, and was also presented with the CheXpert labeler, an automatic rule-based tool that annotates 14 labels (abnormalities) as present, absent or uncertain from the natural language reports. This tool was used to label the images from the dataset, is also used in MIMIC-CXR [69] to tag the reports, and in some works to evaluate the generated reports, as discussed in the Metrics section (5.4). Notice the classification dataset list is not comprehensive, as it only includes datasets that were used in at least one of the reviewed works.

Synthesis. The datasets cover multiple image modalities and body parts, though most efforts focus on chest X-rays. This opens a potential research avenue to explore other image types and diseases, using existing solutions or raising new methods. Additionally, most collections provide valuable supplementary information, such as abnormality tags and/or localization, which can be used to design auxiliary tasks and to evaluate the performance.

Year Image Type # images # reports # patients Used by papers Report datasets IU X-ray [28] 2015  [19] 2019 Chest X-Ray 160,868 109,931 67,625 None (5) ImageCLEF Caption 2017 [35] 2017 Biomedical (2) 184,614 184,614 -[51] ImageCLEF Caption 2018 [39] 2018 Biomedical (2) 232,305 232,305 -None (5) ROCO [105] 2018 Multiple radiology (3) 81,825 81,825 -None (5) PEIR Gross [68] 2017 Gross lesions 7,442 7,442 - [68] INBreast (pt) [99] 2012 Mammography X-ray 410 115 115 [87,128] STARE [58] 1975 Retinal fundus 400 400 -None (5) RDIF (1) [95] 2019   (1) : the RDIF dataset is pending release. (2) : for the ImageCLEF datasets, images were extracted from PubMed Central papers and filtered automatically in order to keep only clinical images, but some unintended samples from other domains are also included. (3) : contains multiple modalities, namely CT, Ultrasound, X-Ray, Fluoroscopy, PET, Mammography, MRI, Angiography and PET-CT. (4) : the images are frames extracted from videos. (5) : none of the papers reviewed used this dataset.

The third column in Table 2 lists the image modalities for each dataset, showing chest X-rays concentrates most of the efforts in report datasets [19,28,44,69,85], though there are also datasets with biomedical images from varied types [35,39,68,105], mammography [99] and hip X-rays [37], ultrasound images [7,158], retinal images [58], doppler echocardiographies [98], cervical images [94], and kidney [95] and bladder biopsies [163]. This adds an extra challenge, since different kinds of exams may need different solutions, as the clinical conditions will be diverse. For example, a fundus retinal image may differ significantly from a chest X-ray; or a radiologist analyzing an X-ray may follow a different procedure than a pathologist reading a biopsy.

From the public report datasets, IU X-ray [28] is the most commonly used, consisting of 7,470 frontal and lateral chest X-rays and 3,955 reports. Additionally, each report was manually annotated with Medical Subject Heading (MeSH) 2 [115] and RadLex [81] terms, and automatically annotated with MeSH terms using the MTI [100] system plus the negation tool from MetaMap [10]. Figure 1 shows a sample image and report from this dataset. Note that for deep learning methods, the amount of data may seem insufficient, compared to general domain datasets with millions of samples, such as ImageNet [29]. This issue could be addressed with pre-training or data augmentation techniques. Also, this may be partially solved with the more recent datasets MIMIC-CXR [69] or PadChest [19], which contain 377,110 and 160,868 images respectively, but have not been widely used yet.

All report datasets include images and reports, and most of them also include labels for each report. Furthermore, INbreast [99] includes contours locating the labels in the images, the Ultrasound collection [157,158] includes bounding boxes locating organs, and IU X-ray [28] and RDIF [95] include additional text written by the physician who requested the exam. The complete detail of additional information is shown in Table 9 in appendix 9.1. This information can be leveraged as a supplementary context to further improve the system performance. On the one hand, the labels and image localization can be used to design auxiliary tasks (see section 5.2.5), and to further evaluate the text generation process (see section 5.4). On the other hand, the indication may contain additional information not present in the image, such as a patient's previous condition, which in some cases may be essential to address the task [95].

Lastly, many works use classification datasets, which do not provide a report for each image, but a set of clinical conditions or abnormalities present or absent in the image. In most cases, this kind of information is used to perform image classification as pre-training, an intermediate, or an auxiliary task to generate the report. One remarkable case is the CheXpert dataset [63], which contains 224,316 images, and was also presented with the CheXpert labeler, an automatic rule-based tool that annotates 14 labels (abnormalities) as present, absent or uncertain from the natural language reports. This tool was used to label the images from the dataset, is also used in MIMIC-CXR [69] to tag the reports, and in some works to evaluate the generated reports, as discussed in the Metrics section (5.4). Notice the classification dataset list is not comprehensive, as it only includes datasets that were used in at least one of the reviewed works.

Synthesis. The datasets cover multiple image modalities and body parts, though most efforts focus on chest X-rays. This opens a potential research avenue to explore other image types and diseases, using existing solutions or raising new methods. Additionally, most collections provide valuable supplementary information, such as abnormality tags and/or localization, which can be used to design auxiliary tasks and to evaluate the performance.","[['b4', 'b27', 'b67', 'b104', 'b3', 'b57', 'b127', 'b0', 'b1', 'b38', 'b18', 'b34', 'b2', 'b98', 'b94', 'b86'], ['b27', 'b43', 'b68', 'b67', 'b104', 'b157', 'b57', 'b84', 'b97', 'b94', 'b38', 'b162', 'b18', 'b34', 'b36', 'b93', 'b98', 'b6'], ['b27', 'b68', 'b28', 'b99', 'b80', 'b114', 'b9', 'b18'], ['b27', 'b157', 'b156', 'b98', 'b94'], ['b68', 'b62', None], [], ['b4', 'b27', 'b67', 'b104', 'b3', 'b57', 'b127', 'b0', 'b1', 'b38', 'b18', 'b34', 'b2', 'b98', 'b94', 'b86'], ['b27', 'b43', 'b68', 'b67', 'b104', 'b157', 'b57', 'b84', 'b97', 'b94', 'b38', 'b162', 'b18', 'b34', 'b36', 'b93', 'b98', 'b6'], ['b27', 'b68', 'b28', 'b99', 'b80', 'b114', 'b9', 'b18'], ['b27', 'b157', 'b156', 'b98', 'b94'], ['b68', 'b62', None], []]","[['b4', 'b27', 'b67', 'b104', 'b3', 'b57', 'b127', 'b0', 'b1', 'b38', 'b18', 'b34', 'b2', 'b98', 'b94', 'b86'], ['b27', 'b43', 'b68', 'b67', 'b104', 'b157', 'b57', 'b84', 'b97', 'b94', 'b38', 'b162', 'b18', 'b34', 'b36', 'b93', 'b98', 'b6'], ['b27', 'b68', 'b28', 'b99', 'b80', 'b114', 'b9', 'b18'], ['b27', 'b157', 'b156', 'b98', 'b94'], ['b68', 'b62', None], [], ['b4', 'b27', 'b67', 'b104', 'b3', 'b57', 'b127', 'b0', 'b1', 'b38', 'b18', 'b34', 'b2', 'b98', 'b94', 'b86'], ['b27', 'b43', 'b68', 'b67', 'b104', 'b157', 'b57', 'b84', 'b97', 'b94', 'b38', 'b162', 'b18', 'b34', 'b36', 'b93', 'b98', 'b6'], ['b27', 'b68', 'b28', 'b99', 'b80', 'b114', 'b9', 'b18'], ['b27', 'b157', 'b156', 'b98', 'b94'], ['b68', 'b62', None], []]",100,"sent1: Year Image Type # images # reports # patients Used by papers Report datasets IU X-ray [28] 2015  [19] 2019 Chest X-Ray 160,868 109,931 67,625 None (5) ImageCLEF Caption 2017 [35] 2017 Biomedical (2) 184,614 184,614 -[51] ImageCLEF Caption 2018 [39] 2018 Biomedical (2) 232,305 232,305 -None (5) ROCO [105]
sent2: 2018 Multiple radiology (3) 81,825 81,825 -None (5) PEIR Gross [68] 2017 Gross lesions 7,442 7,442 - [68] INBreast (pt) [99] 2012 Mammography X-ray 410 115 115 [87,128] STARE [58] 1975
sent3: Retinal fundus 400 400 -None (5) RDIF (1) [95] 2019   (1) : the RDIF dataset is pending release.
sent4: (2) : for the ImageCLEF datasets, images were extracted from PubMed Central papers and filtered automatically in order to keep only clinical images, but some unintended samples from other domains are also included.
sent5: (3) : contains multiple modalities, namely CT, Ultrasound, X-Ray, Fluoroscopy, PET, Mammography, MRI, Angiography and PET-CT.
sent6: (4) : the images are frames extracted from videos.
sent7: (5) : none of the papers reviewed used this dataset.
sent8: The third column in Table 2 lists the image modalities for each dataset, showing chest X-rays concentrates most of the efforts in report datasets [19,28,44,69,85], though there are also datasets with biomedical images from varied types [35,39,68,105], mammography [99] and hip X-rays [37], ultrasound images [7,158], retinal images [58], doppler echocardiographies [98], cervical images [94], and kidney [95] and bladder biopsies [163].
sent9: This adds an extra challenge, since different kinds of exams may need different solutions, as the clinical conditions will be diverse.
sent10: For example, a fundus retinal image may differ significantly from a chest X-ray; or a radiologist analyzing an X-ray may follow a different procedure than a pathologist reading a biopsy.
sent11: From the public report datasets, IU X-ray [28] is the most commonly used, consisting of 7,470 frontal and lateral chest X-rays and 3,955 reports.
sent12: Additionally, each report was manually annotated with Medical Subject Heading (MeSH) 2 [115] and RadLex [81] terms, and automatically annotated with MeSH terms using the MTI [100] system plus the negation tool from MetaMap [10].
sent13: Figure 1 shows a sample image and report from this dataset.
sent14: Note that for deep learning methods, the amount of data may seem insufficient, compared to general domain datasets with millions of samples, such as ImageNet [29].
sent15: This issue could be addressed with pre-training or data augmentation techniques.
sent16: Also, this may be partially solved with the more recent datasets MIMIC-CXR [69] or PadChest [19], which contain 377,110 and 160,868 images respectively, but have not been widely used yet.
sent17: All report datasets include images and reports, and most of them also include labels for each report.
sent18: Furthermore, INbreast [99] includes contours locating the labels in the images, the Ultrasound collection [157,158] includes bounding boxes locating organs, and IU X-ray [28] and RDIF [95] include additional text written by the physician who requested the exam.
sent19: The complete detail of additional information is shown in Table 9 in appendix 9.1.
sent20: This information can be leveraged as a supplementary context to further improve the system performance.
sent21: On the one hand, the labels and image localization can be used to design auxiliary tasks (see section 5.2.5), and to further evaluate the text generation process (see section 5.4).
sent22: On the other hand, the indication may contain additional information not present in the image, such as a patient's previous condition, which in some cases may be essential to address the task [95].
sent23: Lastly, many works use classification datasets, which do not provide a report for each image, but a set of clinical conditions or abnormalities present or absent in the image.
sent24: In most cases, this kind of information is used to perform image classification as pre-training, an intermediate, or an auxiliary task to generate the report.
sent25: One remarkable case is the CheXpert dataset [63], which contains 224,316 images, and was also presented with the CheXpert labeler, an automatic rule-based tool that annotates 14 labels (abnormalities) as present, absent or uncertain from the natural language reports.
sent26: This tool was used to label the images from the dataset, is also used in MIMIC-CXR [69] to tag the reports, and in some works to evaluate the generated reports, as discussed in the Metrics section (5.4).
sent27: Notice the classification dataset list is not comprehensive, as it only includes datasets that were used in at least one of the reviewed works.
sent28: Synthesis. The datasets cover multiple image modalities and body parts, though most efforts focus on chest X-rays.
sent29: This opens a potential research avenue to explore other image types and diseases, using existing solutions or raising new methods.
sent30: Additionally, most collections provide valuable supplementary information, such as abnormality tags and/or localization, which can be used to design auxiliary tasks and to evaluate the performance.
sent31: Year Image Type # images # reports # patients Used by papers Report datasets IU X-ray [28] 2015  [19] 2019 Chest X-Ray 160,868 109,931 67,625 None (5) ImageCLEF Caption 2017 [35] 2017 Biomedical (2) 184,614 184,614 -[51] ImageCLEF Caption 2018 [39] 2018 Biomedical (2) 232,305 232,305 -None (5) ROCO [105]
sent32: 2018 Multiple radiology (3) 81,825 81,825 -None (5) PEIR Gross [68] 2017 Gross lesions 7,442 7,442 - [68] INBreast (pt) [99] 2012 Mammography X-ray 410 115 115 [87,128] STARE [58] 1975
sent33: Retinal fundus 400 400 -None (5) RDIF (1) [95] 2019   (1) : the RDIF dataset is pending release.
sent34: (2) : for the ImageCLEF datasets, images were extracted from PubMed Central papers and filtered automatically in order to keep only clinical images, but some unintended samples from other domains are also included.
sent35: (3) : contains multiple modalities, namely CT, Ultrasound, X-Ray, Fluoroscopy, PET, Mammography, MRI, Angiography and PET-CT.
sent36: (4) : the images are frames extracted from videos.
sent37: (5) : none of the papers reviewed used this dataset.
sent38: The third column in Table 2 lists the image modalities for each dataset, showing chest X-rays concentrates most of the efforts in report datasets [19,28,44,69,85], though there are also datasets with biomedical images from varied types [35,39,68,105], mammography [99] and hip X-rays [37], ultrasound images [7,158], retinal images [58], doppler echocardiographies [98], cervical images [94], and kidney [95] and bladder biopsies [163].
sent39: This adds an extra challenge, since different kinds of exams may need different solutions, as the clinical conditions will be diverse.
sent40: For example, a fundus retinal image may differ significantly from a chest X-ray; or a radiologist analyzing an X-ray may follow a different procedure than a pathologist reading a biopsy.
sent41: From the public report datasets, IU X-ray [28] is the most commonly used, consisting of 7,470 frontal and lateral chest X-rays and 3,955 reports.
sent42: Additionally, each report was manually annotated with Medical Subject Heading (MeSH) 2 [115] and RadLex [81] terms, and automatically annotated with MeSH terms using the MTI [100] system plus the negation tool from MetaMap [10].
sent43: Figure 1 shows a sample image and report from this dataset.
sent44: Note that for deep learning methods, the amount of data may seem insufficient, compared to general domain datasets with millions of samples, such as ImageNet [29].
sent45: This issue could be addressed with pre-training or data augmentation techniques.
sent46: Also, this may be partially solved with the more recent datasets MIMIC-CXR [69] or PadChest [19], which contain 377,110 and 160,868 images respectively, but have not been widely used yet.
sent47: All report datasets include images and reports, and most of them also include labels for each report.
sent48: Furthermore, INbreast [99] includes contours locating the labels in the images, the Ultrasound collection [157,158] includes bounding boxes locating organs, and IU X-ray [28] and RDIF [95] include additional text written by the physician who requested the exam.
sent49: The complete detail of additional information is shown in Table 9 in appendix 9.1.
sent50: This information can be leveraged as a supplementary context to further improve the system performance.
sent51: On the one hand, the labels and image localization can be used to design auxiliary tasks (see section 5.2.5), and to further evaluate the text generation process (see section 5.4).
sent52: On the other hand, the indication may contain additional information not present in the image, such as a patient's previous condition, which in some cases may be essential to address the task [95].
sent53: Lastly, many works use classification datasets, which do not provide a report for each image, but a set of clinical conditions or abnormalities present or absent in the image.
sent54: In most cases, this kind of information is used to perform image classification as pre-training, an intermediate, or an auxiliary task to generate the report.
sent55: One remarkable case is the CheXpert dataset [63], which contains 224,316 images, and was also presented with the CheXpert labeler, an automatic rule-based tool that annotates 14 labels (abnormalities) as present, absent or uncertain from the natural language reports.
sent56: This tool was used to label the images from the dataset, is also used in MIMIC-CXR [69] to tag the reports, and in some works to evaluate the generated reports, as discussed in the Metrics section (5.4).
sent57: Notice the classification dataset list is not comprehensive, as it only includes datasets that were used in at least one of the reviewed works.
sent58: Synthesis. The datasets cover multiple image modalities and body parts, though most efforts focus on chest X-rays.
sent59: This opens a potential research avenue to explore other image types and diseases, using existing solutions or raising new methods.
sent60: Additionally, most collections provide valuable supplementary information, such as abnormality tags and/or localization, which can be used to design auxiliary tasks and to evaluate the performance."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s48,Model Design,"In this section, we present an analysis of existent DL model designs, starting with a general overview of common design practices. Most models in the literature follow a standard design pattern. There is a visual component consisting at its core of a Convolutional Neural Network (CNN) [79] that processes one or more input images in order to extract visual features. Then, a language component follows, typically based on well-known NLP neural architectures (e.g., LSTM [57], BiLSTM [42], GRU [26], Transformer [139]) responsible for text processing and report generation. Also, a widespread practice for the language component is to retrieve the visual information in an adaptive manner  via an attention mechanism, as the report is written. Many papers follow variations of this pattern inspired by influential works from the image captioning domain [141,152], which are frequently cited and used as baselines. Optionally, some models receive or generate additional input or output, and a few models incorporate some form of domain knowledge explicitly in the generation process.  Table 3 presents a summary of this analysis. Input. With respect to image type, most papers (24) used chest X-rays, whereas the other papers are more or less equally distributed over other image types. A total of 32 models receive a single image (e.g. a single chest X-ray view), 6 models receive 2 images (both frontal and lateral chest X-ray views), and 2 models receive an arbitrary number of images (e.g. multi-slice abdominal CT scans). Most models in the literature only handle visual input. However, 6 works [7,16,36,61,95,132] explored the use of complementary input text, reporting performance gains in most cases. For example, two works [61,95] encode an indication paragraph with a BiLSTM. Similarly, MTMA [132] encodes the report's indication and findings sections with a BiLSTM per sentence first, and then a LSTM produces a final vector representation. Similarly, two works [7,36] use LSTM/BiLSTM to encode a partial report or caption as input, in order to predict the next word. Unlike other works, CLARA [16] uses a software package, Lucene [18], to perform text-based retrieval of report templates. The input text is processed by Lucene as a search query, and the retrieved templates are paraphrased by an encoder-decoder network to generate the final report.

In this section, we present an analysis of existent DL model designs, starting with a general overview of common design practices. Most models in the literature follow a standard design pattern. There is a visual component consisting at its core of a Convolutional Neural Network (CNN) [79] that processes one or more input images in order to extract visual features. Then, a language component follows, typically based on well-known NLP neural architectures (e.g., LSTM [57], BiLSTM [42], GRU [26], Transformer [139]) responsible for text processing and report generation. Also, a widespread practice for the language component is to retrieve the visual information in an adaptive manner  via an attention mechanism, as the report is written. Many papers follow variations of this pattern inspired by influential works from the image captioning domain [141,152], which are frequently cited and used as baselines. Optionally, some models receive or generate additional input or output, and a few models incorporate some form of domain knowledge explicitly in the generation process.  Table 3 presents a summary of this analysis. Input. With respect to image type, most papers (24) used chest X-rays, whereas the other papers are more or less equally distributed over other image types. A total of 32 models receive a single image (e.g. a single chest X-ray view), 6 models receive 2 images (both frontal and lateral chest X-ray views), and 2 models receive an arbitrary number of images (e.g. multi-slice abdominal CT scans). Most models in the literature only handle visual input. However, 6 works [7,16,36,61,95,132] explored the use of complementary input text, reporting performance gains in most cases. For example, two works [61,95] encode an indication paragraph with a BiLSTM. Similarly, MTMA [132] encodes the report's indication and findings sections with a BiLSTM per sentence first, and then a LSTM produces a final vector representation. Similarly, two works [7,36] use LSTM/BiLSTM to encode a partial report or caption as input, in order to predict the next word. Unlike other works, CLARA [16] uses a software package, Lucene [18], to perform text-based retrieval of report templates. The input text is processed by Lucene as a search query, and the retrieved templates are paraphrased by an encoder-decoder network to generate the final report.","[['b41', 'b17', 'b56', 'b151', 'b25', 'b60', 'b131', 'b35', 'b94', 'b78', 'b140', 'b138', 'b15', 'b6'], ['b41', 'b17', 'b56', 'b151', 'b25', 'b60', 'b131', 'b35', 'b94', 'b78', 'b140', 'b138', 'b15', 'b6']]","[['b41', 'b17', 'b56', 'b151', 'b25', 'b60', 'b131', 'b35', 'b94', 'b78', 'b140', 'b138', 'b15', 'b6'], ['b41', 'b17', 'b56', 'b151', 'b25', 'b60', 'b131', 'b35', 'b94', 'b78', 'b140', 'b138', 'b15', 'b6']]",28,"sent1: In this section, we present an analysis of existent DL model designs, starting with a general overview of common design practices.
sent2: Most models in the literature follow a standard design pattern.
sent3: There is a visual component consisting at its core of a Convolutional Neural Network (CNN) [79] that processes one or more input images in order to extract visual features.
sent4: Then, a language component follows, typically based on well-known NLP neural architectures (e.g., LSTM [57], BiLSTM [42], GRU [26], Transformer [139]) responsible for text processing and report generation.
sent5: Also, a widespread practice for the language component is to retrieve the visual information in an adaptive manner  via an attention mechanism, as the report is written.
sent6: Many papers follow variations of this pattern inspired by influential works from the image captioning domain [141,152], which are frequently cited and used as baselines.
sent7: Optionally, some models receive or generate additional input or output, and a few models incorporate some form of domain knowledge explicitly in the generation process.
sent8: Table 3 presents a summary of this analysis.
sent9: Input. With respect to image type, most papers (24) used chest X-rays, whereas the other papers are more or less equally distributed over other image types.
sent10: A total of 32 models receive a single image (e.g. a single chest X-ray view), 6 models receive 2 images (both frontal and lateral chest X-ray views), and 2 models receive an arbitrary number of images (e.g. multi-slice abdominal CT scans).
sent11: Most models in the literature only handle visual input.
sent12: However, 6 works [7,16,36,61,95,132] explored the use of complementary input text, reporting performance gains in most cases.
sent13: For example, two works [61,95] encode an indication paragraph with a BiLSTM.
sent14: Similarly, MTMA [132] encodes the report's indication and findings sections with a BiLSTM per sentence first, and then a LSTM produces a final vector representation.
sent15: Similarly, two works [7,36] use LSTM/BiLSTM to encode a partial report or caption as input, in order to predict the next word.
sent16: Unlike other works, CLARA [16] uses a software package, Lucene [18], to perform text-based retrieval of report templates.
sent17: The input text is processed by Lucene as a search query, and the retrieved templates are paraphrased by an encoder-decoder network to generate the final report.
sent18: In this section, we present an analysis of existent DL model designs, starting with a general overview of common design practices.
sent19: Most models in the literature follow a standard design pattern.
sent20: There is a visual component consisting at its core of a Convolutional Neural Network (CNN) [79] that processes one or more input images in order to extract visual features.
sent21: Then, a language component follows, typically based on well-known NLP neural architectures (e.g., LSTM [57], BiLSTM [42], GRU [26], Transformer [139]) responsible for text processing and report generation.
sent22: Also, a widespread practice for the language component is to retrieve the visual information in an adaptive manner  via an attention mechanism, as the report is written.
sent23: Many papers follow variations of this pattern inspired by influential works from the image captioning domain [141,152], which are frequently cited and used as baselines.
sent24: Optionally, some models receive or generate additional input or output, and a few models incorporate some form of domain knowledge explicitly in the generation process.
sent25: Table 3 presents a summary of this analysis.
sent26: Input. With respect to image type, most papers (24) used chest X-rays, whereas the other papers are more or less equally distributed over other image types.
sent27: A total of 32 models receive a single image (e.g. a single chest X-ray view), 6 models receive 2 images (both frontal and lateral chest X-ray views), and 2 models receive an arbitrary number of images (e.g. multi-slice abdominal CT scans).
sent28: Most models in the literature only handle visual input.
sent29: However, 6 works [7,16,36,61,95,132] explored the use of complementary input text, reporting performance gains in most cases.
sent30: For example, two works [61,95] encode an indication paragraph with a BiLSTM.
sent31: Similarly, MTMA [132] encodes the report's indication and findings sections with a BiLSTM per sentence first, and then a LSTM produces a final vector representation.
sent32: Similarly, two works [7,36] use LSTM/BiLSTM to encode a partial report or caption as input, in order to predict the next word.
sent33: Unlike other works, CLARA [16] uses a software package, Lucene [18], to perform text-based retrieval of report templates.
sent34: The input text is processed by Lucene as a search query, and the retrieved templates are paraphrased by an encoder-decoder network to generate the final report."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s49,Input and Output.,"Output. All models output a natural language report. According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence. (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic. These models are designed for datasets where reports follow a rigid structure. (3) Generative single-sentence: generate a report word by word, but only output a single sentence. These models are designed for datasets with simple one-sentence  reports. (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling. This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules. And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word. This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86]. In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others. These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3. Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation). Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision. We will discuss all these outputs more in detail and their use in the explainability section (5.3).

Output. All models output a natural language report. According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence. (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic. These models are designed for datasets where reports follow a rigid structure. (3) Generative single-sentence: generate a report word by word, but only output a single sentence. These models are designed for datasets with simple one-sentence  reports. (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling. This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules. And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word. This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86]. In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others. These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3. Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation). Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision. We will discuss all these outputs more in detail and their use in the explainability section (5.3).","[['b85', 'b125', 'b60', 'b84', 'b15'], ['b85', 'b125', 'b60', 'b84', 'b15']]","[['b85', 'b125', 'b60', 'b84', 'b15'], ['b85', 'b125', 'b60', 'b84', 'b15']]",10,"sent1: Output. All models output a natural language report.
sent2: According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence.
sent3: (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic.
sent4: These models are designed for datasets where reports follow a rigid structure.
sent5: (3) Generative single-sentence: generate a report word by word, but only output a single sentence.
sent6: These models are designed for datasets with simple one-sentence  reports.
sent7: (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling.
sent8: This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules.
sent9: And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word.
sent10: This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86].
sent11: In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others.
sent12: These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3.
sent13: Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation).
sent14: Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision.
sent15: We will discuss all these outputs more in detail and their use in the explainability section (5.3).
sent16: Output. All models output a natural language report.
sent17: According to the extension of the report and the general strategy used to produce it, we group papers into five categories: (1) Generative multi-sentence (unstructured): these models generate a multi-sentence report, word by word, with freedom to decide the number of sentences and the words in each sentence.
sent18: (2) Generative multisentence structured: similar to the previous category, but always output a fixed number of sentences, and each sentence always has a pre-defined topic.
sent19: These models are designed for datasets where reports follow a rigid structure.
sent20: (3) Generative single-sentence: generate a report word by word, but only output a single sentence.
sent21: These models are designed for datasets with simple one-sentence  reports.
sent22: (4) Template-based: use human-designed templates to produce the report, for example performing a classification task followed by if-then rules, template selection and template filling.
sent23: This simplifies the report generation task for the model, at the expense of making it less flexible and requiring the human designing of templates and rules.
sent24: And lastly (5) Hybrid template -generation/edition: use templates and also have the freedom to generate sentences word by word.
sent25: This can be accomplished by choosing between a template or generating a sentence from scratch [85], or by editing/paraphrasing a previously selected template [16,86].
sent26: In addition to the report itself, many models also output complementary classification predictions, such as presence or absence of abnormalities or diseases, MeSH concepts, body parts or organs, among others.
sent27: These are often referred to as labels or tags, and are commonly used in the language component, as will be discussed in section 5.2.3.
sent28: Many models can also output heatmaps over an image highlighting relevant regions using different techniques, such as explicit visual attention weights computed during report generation, saliency maps methods (e.g., CAM, Grad-CAM, SmoothGrad or activation-based attention), bounding box regression, and pixel-level classification (image segmentation).
sent29: Also, one model [61] can output a heatmap over its input text and one model [126] can generate a counter-factual example to justify its decision.
sent30: We will discuss all these outputs more in detail and their use in the explainability section (5.3)."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s50,Visual Component.,"The most important observation is that all surveyed works use CNNs to process the input images. This is not surprising since CNNs have dominated the state of the art in computer vision for several years [74]. The typical visual processing pipeline consists of a CNN that receives an input image and outputs a volume of feature maps of dimensions × × , where and denote spatial dimensions (width and height) and denotes the channel dimensions (depth or number of feature maps). These visual features are then leveraged by the language component to make decisions for report generation (e.g., which sentence to write, which template to retrieve, next word to output, etc.), typically by way of an attention mechanism.

However, some works did not strictly follow this pattern. For example, in two works [44,128] a CNN is used for multi-label classification of tags, which are then mapped to embedded vectors via embedding matrix lookup. Thus, the report generation module only has access to these tag vectors but no access to the visual features themselves. Similarly, two works [68,155] classify and look up tag embedding vectors, but unlike the previous works, the language component uses co-attention to access both tags vectors and visual features simultaneously. Their ablation analysis showed that the semantic information provided by these tags complements the visual information and improves the model's performance in report generation. Other works [86,162] used graph neural networks immediately after the CNN to encode the visual information in terms of medical concepts and their relations. Thus, the language component receives the intermediate graph representation instead of the raw visual features. The ablation analysis by Zhang et al. [162] showed some performance gains thanks to the graph neural network. Vispi [89] implements a two-stage procedure, where two distinct CNNs are used. In the first stage a DenseNet 121 [60] classifies abnormalities in the image, and then Grad-CAM [118] is used to localize and crop a region of the image for each detected class. Then, in the second stage the multiple image crops are treated as independent images and processed by a typical CNN+LSTM architecture, with ResNet 101 [52] as the CNN. A similar idea was followed in RTMIC [151], where a DenseNet 121 is pretrained for classification in ChestX-ray14 [143] and CAM is used to get image crops for each class.

We observe a wide variety of CNN architectures used in the literature, though most works employ standard designs. Table 4 presents a summary. The most common ones are ResNet (11 works), VGG (11 works), and DenseNet (9 works). Other standard architectures used are Faster R-CNN, Inception V3, GoogLeNet, MobileNet V2, Spatial Regularization Network (SRN) and U-Net. Five works used ad hoc architectures not previously published (marked with (*) in Table 4). For example, EcNet is an ad hoc architecture used in MDNet [163] and was proposed as an improvement over ResNet. However, the authors acknowledged that its design resembles DenseNet, which was published the same year (2017). RGAN, proposed by Han et al. [47], is a novel architecture that follows the generative adversarial network (GAN) [41] approach, with a generative module comprising the encoder and decoder parts of an atrous convolution autoencoder (ACAE) with a spatial LSTM between them. Similarly, Spinks and Moens [126] used a slightly modified version of a StackGAN [159] to learn the mapping from report encoding to chest X-ray images, and a custom CNN to learn the inverse mapping. Both are trained together, but only the latter is part of the report generation pipeline during inference.  [112] [76, 157] Inception V3 [130] [123] GoogLeNet [129] [120] MobileNet V2 [59] [49] SRN [166] [44] U-Net [116] [128] EcNet (*) [163] 

[47] StackGAN [159] (slightly modified version) (*) [126] CNN (*) [126,132] CNN (unspecified architecture) [148,150] 

The most important observation is that all surveyed works use CNNs to process the input images. This is not surprising since CNNs have dominated the state of the art in computer vision for several years [74]. The typical visual processing pipeline consists of a CNN that receives an input image and outputs a volume of feature maps of dimensions × × , where and denote spatial dimensions (width and height) and denotes the channel dimensions (depth or number of feature maps). These visual features are then leveraged by the language component to make decisions for report generation (e.g., which sentence to write, which template to retrieve, next word to output, etc.), typically by way of an attention mechanism.

However, some works did not strictly follow this pattern. For example, in two works [44,128] a CNN is used for multi-label classification of tags, which are then mapped to embedded vectors via embedding matrix lookup. Thus, the report generation module only has access to these tag vectors but no access to the visual features themselves. Similarly, two works [68,155] classify and look up tag embedding vectors, but unlike the previous works, the language component uses co-attention to access both tags vectors and visual features simultaneously. Their ablation analysis showed that the semantic information provided by these tags complements the visual information and improves the model's performance in report generation. Other works [86,162] used graph neural networks immediately after the CNN to encode the visual information in terms of medical concepts and their relations. Thus, the language component receives the intermediate graph representation instead of the raw visual features. The ablation analysis by Zhang et al. [162] showed some performance gains thanks to the graph neural network. Vispi [89] implements a two-stage procedure, where two distinct CNNs are used. In the first stage a DenseNet 121 [60] classifies abnormalities in the image, and then Grad-CAM [118] is used to localize and crop a region of the image for each detected class. Then, in the second stage the multiple image crops are treated as independent images and processed by a typical CNN+LSTM architecture, with ResNet 101 [52] as the CNN. A similar idea was followed in RTMIC [151], where a DenseNet 121 is pretrained for classification in ChestX-ray14 [143] and CAM is used to get image crops for each class.

We observe a wide variety of CNN architectures used in the literature, though most works employ standard designs. Table 4 presents a summary. The most common ones are ResNet (11 works), VGG (11 works), and DenseNet (9 works). Other standard architectures used are Faster R-CNN, Inception V3, GoogLeNet, MobileNet V2, Spatial Regularization Network (SRN) and U-Net. Five works used ad hoc architectures not previously published (marked with (*) in Table 4). For example, EcNet is an ad hoc architecture used in MDNet [163] and was proposed as an improvement over ResNet. However, the authors acknowledged that its design resembles DenseNet, which was published the same year (2017). RGAN, proposed by Han et al. [47], is a novel architecture that follows the generative adversarial network (GAN) [41] approach, with a generative module comprising the encoder and decoder parts of an atrous convolution autoencoder (ACAE) with a spatial LSTM between them. Similarly, Spinks and Moens [126] used a slightly modified version of a StackGAN [159] to learn the mapping from report encoding to chest X-ray images, and a custom CNN to learn the inverse mapping. Both are trained together, but only the latter is part of the report generation pipeline during inference.  [112] [76, 157] Inception V3 [130] [123] GoogLeNet [129] [120] MobileNet V2 [59] [49] SRN [166] [44] U-Net [116] [128] EcNet (*) [163] 

[47] StackGAN [159] (slightly modified version) (*) [126] CNN (*) [126,132] CNN (unspecified architecture) [148,150] ","[['b73'], ['b88', 'b67', 'b43', 'b85', 'b161', 'b59', 'b142', 'b51', 'b150', 'b117', 'b154', 'b127'], ['b158', 'b111', 'b115', 'b125', 'b129', 'b165', 'b128', 'b46', 'b58', 'b162', 'b40'], ['b158', 'b125', 'b131', 'b147', 'b149'], ['b73'], ['b88', 'b67', 'b43', 'b85', 'b161', 'b59', 'b142', 'b51', 'b150', 'b117', 'b154', 'b127'], ['b158', 'b111', 'b115', 'b125', 'b129', 'b165', 'b128', 'b46', 'b58', 'b162', 'b40'], ['b158', 'b125', 'b131', 'b147', 'b149']]","[['b73'], ['b88', 'b67', 'b43', 'b85', 'b161', 'b59', 'b142', 'b51', 'b150', 'b117', 'b154', 'b127'], ['b158', 'b111', 'b115', 'b125', 'b129', 'b165', 'b128', 'b46', 'b58', 'b162', 'b40'], ['b158', 'b125', 'b131', 'b147', 'b149'], ['b73'], ['b88', 'b67', 'b43', 'b85', 'b161', 'b59', 'b142', 'b51', 'b150', 'b117', 'b154', 'b127'], ['b158', 'b111', 'b115', 'b125', 'b129', 'b165', 'b128', 'b46', 'b58', 'b162', 'b40'], ['b158', 'b125', 'b131', 'b147', 'b149']]",58,"sent1: The most important observation is that all surveyed works use CNNs to process the input images.
sent2: This is not surprising since CNNs have dominated the state of the art in computer vision for several years [74].
sent3: The typical visual processing pipeline consists of a CNN that receives an input image and outputs a volume of feature maps of dimensions ×
sent4: × , where and denote spatial dimensions (width and height) and denotes the channel dimensions (depth or number of feature maps).
sent5: These visual features are then leveraged by the language component to make decisions for report generation (e.g., which sentence to write, which template to retrieve, next word to output, etc.), typically by way of an attention mechanism.
sent6: However, some works did not strictly follow this pattern.
sent7: For example, in two works [44,128] a CNN is used for multi-label classification of tags, which are then mapped to embedded vectors via embedding matrix lookup.
sent8: Thus, the report generation module only has access to these tag vectors but no access to the visual features themselves.
sent9: Similarly, two works [68,155] classify and look up tag embedding vectors, but unlike the previous works, the language component uses co-attention to access both tags vectors and visual features simultaneously.
sent10: Their ablation analysis showed that the semantic information provided by these tags complements the visual information and improves the model's performance in report generation.
sent11: Other works [86,162] used graph neural networks immediately after the CNN to encode the visual information in terms of medical concepts and their relations.
sent12: Thus, the language component receives the intermediate graph representation instead of the raw visual features.
sent13: The ablation analysis by Zhang et al. [162] showed some performance gains thanks to the graph neural network.
sent14: Vispi [89] implements a two-stage procedure, where two distinct CNNs are used.
sent15: In the first stage a DenseNet 121 [60] classifies abnormalities in the image, and then Grad-CAM [118] is used to localize and crop a region of the image for each detected class.
sent16: Then, in the second stage the multiple image crops are treated as independent images and processed by a typical CNN+LSTM architecture, with ResNet 101 [52] as the CNN.
sent17: A similar idea was followed in RTMIC [151], where a DenseNet 121 is pretrained for classification in ChestX-ray14 [143] and CAM is used to get image crops for each class.
sent18: We observe a wide variety of CNN architectures used in the literature, though most works employ standard designs.
sent19: Table 4 presents a summary. The most common ones are ResNet (11 works), VGG (11 works), and DenseNet (9 works).
sent20: Other standard architectures used are Faster R-CNN, Inception V3, GoogLeNet, MobileNet V2, Spatial Regularization Network (SRN) and U-Net.
sent21: Five works used ad hoc architectures not previously published (marked with (*) in Table 4).
sent22: For example, EcNet is an ad hoc architecture used in MDNet [163] and was proposed as an improvement over ResNet.
sent23: However, the authors acknowledged that its design resembles DenseNet, which was published the same year (2017).
sent24: RGAN, proposed by Han et al. [47], is a novel architecture that follows the generative adversarial network (GAN) [41] approach, with a generative module comprising the encoder and decoder parts of an atrous convolution autoencoder (ACAE) with a spatial LSTM between them.
sent25: Similarly, Spinks and Moens [126] used a slightly modified version of a StackGAN [159] to learn the mapping from report encoding to chest X-ray images, and a custom CNN to learn the inverse mapping.
sent26: Both are trained together, but only the latter is part of the report generation pipeline during inference.
sent27: [112] [76, 157] Inception V3 [130] [123] GoogLeNet [129] [120] MobileNet V2 [59] [49] SRN [166] [44]
sent28: U-Net [116] [128] EcNet (*) [163] [47] StackGAN [159] (slightly modified version) (*) [126] CNN (*) [126,132] CNN (unspecified architecture) [148,150] The most important observation is that all surveyed works use CNNs to process the input images.
sent29: This is not surprising since CNNs have dominated the state of the art in computer vision for several years [74].
sent30: The typical visual processing pipeline consists of a CNN that receives an input image and outputs a volume of feature maps of dimensions ×
sent31: × , where and denote spatial dimensions (width and height) and denotes the channel dimensions (depth or number of feature maps).
sent32: These visual features are then leveraged by the language component to make decisions for report generation (e.g., which sentence to write, which template to retrieve, next word to output, etc.), typically by way of an attention mechanism.
sent33: However, some works did not strictly follow this pattern.
sent34: For example, in two works [44,128] a CNN is used for multi-label classification of tags, which are then mapped to embedded vectors via embedding matrix lookup.
sent35: Thus, the report generation module only has access to these tag vectors but no access to the visual features themselves.
sent36: Similarly, two works [68,155] classify and look up tag embedding vectors, but unlike the previous works, the language component uses co-attention to access both tags vectors and visual features simultaneously.
sent37: Their ablation analysis showed that the semantic information provided by these tags complements the visual information and improves the model's performance in report generation.
sent38: Other works [86,162] used graph neural networks immediately after the CNN to encode the visual information in terms of medical concepts and their relations.
sent39: Thus, the language component receives the intermediate graph representation instead of the raw visual features.
sent40: The ablation analysis by Zhang et al. [162] showed some performance gains thanks to the graph neural network.
sent41: Vispi [89] implements a two-stage procedure, where two distinct CNNs are used.
sent42: In the first stage a DenseNet 121 [60] classifies abnormalities in the image, and then Grad-CAM [118] is used to localize and crop a region of the image for each detected class.
sent43: Then, in the second stage the multiple image crops are treated as independent images and processed by a typical CNN+LSTM architecture, with ResNet 101 [52] as the CNN.
sent44: A similar idea was followed in RTMIC [151], where a DenseNet 121 is pretrained for classification in ChestX-ray14 [143] and CAM is used to get image crops for each class.
sent45: We observe a wide variety of CNN architectures used in the literature, though most works employ standard designs.
sent46: Table 4 presents a summary. The most common ones are ResNet (11 works), VGG (11 works), and DenseNet (9 works).
sent47: Other standard architectures used are Faster R-CNN, Inception V3, GoogLeNet, MobileNet V2, Spatial Regularization Network (SRN) and U-Net.
sent48: Five works used ad hoc architectures not previously published (marked with (*) in Table 4).
sent49: For example, EcNet is an ad hoc architecture used in MDNet [163] and was proposed as an improvement over ResNet.
sent50: However, the authors acknowledged that its design resembles DenseNet, which was published the same year (2017).
sent51: RGAN, proposed by Han et al. [47], is a novel architecture that follows the generative adversarial network (GAN) [41] approach, with a generative module comprising the encoder and decoder parts of an atrous convolution autoencoder (ACAE) with a spatial LSTM between them.
sent52: Similarly, Spinks and Moens [126] used a slightly modified version of a StackGAN [159] to learn the mapping from report encoding to chest X-ray images, and a custom CNN to learn the inverse mapping.
sent53: Both are trained together, but only the latter is part of the report generation pipeline during inference.
sent54: [112] [76, 157] Inception V3 [130] [123] GoogLeNet [129] [120] MobileNet V2 [59] [49] SRN [166] [44]
sent55: U-Net [116] [128] EcNet (*) [163] [47] StackGAN [159] (slightly modified version) (*) [126] CNN (*) [126,132] CNN (unspecified architecture) [148,150]"
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s51,Language Component.,"The job of the language component is to generate the report. In contrast to the visual component, in the literature we find a greater variety of approaches and creative ideas applied to this component. Table 5 presents a high-level summary of this analysis. The simplest approach is the use of a recurrent neural network, such as LSTM or GRU, to generate the full report word by word. Nine works [40,44,51,87,123,128,148,157,158] used LSTM and one work [120] tried both GRU and LSTM. All these works have in common that the GRU/LSTM receives an encoding vector from the visual component at the beginning and the full report is decoded from it. This encoding vector is typically a vector of global features output by the CNN. However, two of these works [44,128] compute a weighted sum of tag embedding vectors and provide that as input to the LSTM. Five works [38,89,131,144,163] used LSTM enhanced with an attention mechanism. In addition to the initial input, the LSTM equipped with attention can selectively attend to visual features from the visual component at each recurrent step. This typically leads to improved performance in all papers.

A known problem for recurrent networks such as LSTM is that they are not very good at generating very long texts [103]. This is not a worrying issue when reports are short, however, it can become one for long multi-sentence reports. Two papers [131,163] worked around this by generating each sentence independently with a single LSTM and then concatenating these sentences together. They accomplished this by providing the LSTM with a vector that indicates the sentence type as first input. This worked well in their case because the models were designed for structured reports, i.e., a fixed number of sentences per report and a fixed topic per sentence. Vispi [89] adopts a similar strategy: for each disease a dedicated LSTM generates the corresponding sentence, and the final report is the concatenation of them.

To tackle the generation of unstructured multi-sentence reports, a group of papers followed what we call the Hierarchical LSTM with attention approach: a Sentence LSTM generates a sequence of topic vectors, and a Word LSTM receives a topic vector and generates a sentence word by word. In this setting, the attention mechanism can be present at the sentence level, the word level or both. Figure 3 shows an illustrative example. Seven works [61,68,92,132,155,156,162] followed this approach. A common result in these papers is that a Hierarchical LSTM yields better performance in multi-sentence report generation than a single, flat LSTM. A few papers [48,67,150] went one step further and replaced the normal Word LSTM with a Dual Word LSTM: the model has a gating mechanism at the sentence level that decides if the sentence will describe an abnormality (e.g., a detected cardiomegaly) or a healthy case. Thus, there are two Word LSTMs, one for normal and one for abnormal sentences. The goal is to improve the generation of abnormal sentences by having a Word LSTM that specializes in generating them. In contrast, a single Word LSTM for everything can lead to overlearning of normal sentences and underlearning of abnormal ones, as the latter are typically less frequent due to class imbalances in datasets. The ablation analyses of these works show performance gains, thanks to this approach.

Another approach for multi-sentence report generation is the Recurrent BiLSTM-attention-LSTM approach. The basic idea is to have a LSTM generate one sentence at a time, each time conditioned on a BiLSTM based encoding of the previous sentence and the output of an attention mechanism. The process is repeated recurrently sentence by sentence until the full report is generated. Three papers used this approach [95,153,154].

Two works [7,36] approached report generation as simply learning to predict the next word given a partial report and an image. The models have dedicated components, such as LSTM and BiLSTM, for encoding the partial report and the image, and the next word is predicted by an FC layer. This approach simplifies the task (i.e., predict the next word given everything that comes before), but in practice requires that the model be applied recurrently one word at a time to produce a full report, which has quadratic instead of linear complexity.

Only one work, RTMIC [151], has explored the use of the Transformer [139] architecture for report generation. In RTMIC multiple image crops are obtained using Grad-CAM, then from each crop a feature vector is obtained, and finally a Transformer converts these vectors into a report. The paper's results show some performance gains in CIDEr and BLEU with respect to some baselines that do not use the Transformer. Likewise, Spinks and Moens [126] were the only ones to use an adversarially regularized autoencoder (ARAE) [164] to generate reports. Their model combines an ARAE with a StackGAN and a normal CNN, achieving better performance than a convolutional caption generation baseline in several NLP metrics.

We also identify a group of papers [47,49,76,94,98] following a Template based approach. The language component in these works operates programmatically by following if-then rules or other heuristics in order to retrieve, fill and/or combine templates from a database in order to generate a report. The visual component typically outputs discrete classification labels that the language component processes programmatically. In the case of Harzig et al. 2019b [49], image localizations per class are also recovered using CAM [165], and in the case of Han et al. [47] the visual component outputs an image segmentation. In both cases the language component includes special localization-based rules or templates, thus incorporating location information in the generated report. Kisilev et al. [76] followed a different approach: a multi-layer perceptron learns to map image encodings to doc2vec [83] representations of corresponding reports. During inference, the ground-truth report with the closest doc2vec representation is retrieved.

Lastly, we identify three papers [16,85,86] following the Hybrid template retrieval + generation/edition approach. These works seek to combine the benefits of templates with the flexibility of a generative module to either generate sentences from scratch or paraphrase templates as needed on a case-by-case basis. KERP [86] uses Graph Transformers (GTR) to map the visual input into a sequence of templates from a curated database. A Paraphrase GTR then maps each template to its paraphrased version. HRGR [85] follows the hierarchical LSTM approach with a twist-it replaces the Word LSTM with a gate module that chooses between two options: retrieving a template or generating a sentence from scratch (via a Word LSTM). Lastly, CLARA [16] is somewhat different, as it was designed as an interactive tool to assist a human to write reports. A human introduces anchor words and the prefix of a sentence, and Lucene [18] processes them as a query to retrieve sentence templates from a database. A sequence-to-sequence network then reads and paraphrases each sentence template to get the final report. CLARA can also operate fully automatically by receiving an empty prefix and predicting the anchor words itself. According to reported results, the model consistently achieved better performance than many baselines.

The job of the language component is to generate the report. In contrast to the visual component, in the literature we find a greater variety of approaches and creative ideas applied to this component. Table 5 presents a high-level summary of this analysis. The simplest approach is the use of a recurrent neural network, such as LSTM or GRU, to generate the full report word by word. Nine works [40,44,51,87,123,128,148,157,158] used LSTM and one work [120] tried both GRU and LSTM. All these works have in common that the GRU/LSTM receives an encoding vector from the visual component at the beginning and the full report is decoded from it. This encoding vector is typically a vector of global features output by the CNN. However, two of these works [44,128] compute a weighted sum of tag embedding vectors and provide that as input to the LSTM. Five works [38,89,131,144,163] used LSTM enhanced with an attention mechanism. In addition to the initial input, the LSTM equipped with attention can selectively attend to visual features from the visual component at each recurrent step. This typically leads to improved performance in all papers.

A known problem for recurrent networks such as LSTM is that they are not very good at generating very long texts [103]. This is not a worrying issue when reports are short, however, it can become one for long multi-sentence reports. Two papers [131,163] worked around this by generating each sentence independently with a single LSTM and then concatenating these sentences together. They accomplished this by providing the LSTM with a vector that indicates the sentence type as first input. This worked well in their case because the models were designed for structured reports, i.e., a fixed number of sentences per report and a fixed topic per sentence. Vispi [89] adopts a similar strategy: for each disease a dedicated LSTM generates the corresponding sentence, and the final report is the concatenation of them.

To tackle the generation of unstructured multi-sentence reports, a group of papers followed what we call the Hierarchical LSTM with attention approach: a Sentence LSTM generates a sequence of topic vectors, and a Word LSTM receives a topic vector and generates a sentence word by word. In this setting, the attention mechanism can be present at the sentence level, the word level or both. Figure 3 shows an illustrative example. Seven works [61,68,92,132,155,156,162] followed this approach. A common result in these papers is that a Hierarchical LSTM yields better performance in multi-sentence report generation than a single, flat LSTM. A few papers [48,67,150] went one step further and replaced the normal Word LSTM with a Dual Word LSTM: the model has a gating mechanism at the sentence level that decides if the sentence will describe an abnormality (e.g., a detected cardiomegaly) or a healthy case. Thus, there are two Word LSTMs, one for normal and one for abnormal sentences. The goal is to improve the generation of abnormal sentences by having a Word LSTM that specializes in generating them. In contrast, a single Word LSTM for everything can lead to overlearning of normal sentences and underlearning of abnormal ones, as the latter are typically less frequent due to class imbalances in datasets. The ablation analyses of these works show performance gains, thanks to this approach.

Another approach for multi-sentence report generation is the Recurrent BiLSTM-attention-LSTM approach. The basic idea is to have a LSTM generate one sentence at a time, each time conditioned on a BiLSTM based encoding of the previous sentence and the output of an attention mechanism. The process is repeated recurrently sentence by sentence until the full report is generated. Three papers used this approach [95,153,154].

Two works [7,36] approached report generation as simply learning to predict the next word given a partial report and an image. The models have dedicated components, such as LSTM and BiLSTM, for encoding the partial report and the image, and the next word is predicted by an FC layer. This approach simplifies the task (i.e., predict the next word given everything that comes before), but in practice requires that the model be applied recurrently one word at a time to produce a full report, which has quadratic instead of linear complexity.

Only one work, RTMIC [151], has explored the use of the Transformer [139] architecture for report generation. In RTMIC multiple image crops are obtained using Grad-CAM, then from each crop a feature vector is obtained, and finally a Transformer converts these vectors into a report. The paper's results show some performance gains in CIDEr and BLEU with respect to some baselines that do not use the Transformer. Likewise, Spinks and Moens [126] were the only ones to use an adversarially regularized autoencoder (ARAE) [164] to generate reports. Their model combines an ARAE with a StackGAN and a normal CNN, achieving better performance than a convolutional caption generation baseline in several NLP metrics.

We also identify a group of papers [47,49,76,94,98] following a Template based approach. The language component in these works operates programmatically by following if-then rules or other heuristics in order to retrieve, fill and/or combine templates from a database in order to generate a report. The visual component typically outputs discrete classification labels that the language component processes programmatically. In the case of Harzig et al. 2019b [49], image localizations per class are also recovered using CAM [165], and in the case of Han et al. [47] the visual component outputs an image segmentation. In both cases the language component includes special localization-based rules or templates, thus incorporating location information in the generated report. Kisilev et al. [76] followed a different approach: a multi-layer perceptron learns to map image encodings to doc2vec [83] representations of corresponding reports. During inference, the ground-truth report with the closest doc2vec representation is retrieved.

Lastly, we identify three papers [16,85,86] following the Hybrid template retrieval + generation/edition approach. These works seek to combine the benefits of templates with the flexibility of a generative module to either generate sentences from scratch or paraphrase templates as needed on a case-by-case basis. KERP [86] uses Graph Transformers (GTR) to map the visual input into a sequence of templates from a curated database. A Paraphrase GTR then maps each template to its paraphrased version. HRGR [85] follows the hierarchical LSTM approach with a twist-it replaces the Word LSTM with a gate module that chooses between two options: retrieving a template or generating a sentence from scratch (via a Word LSTM). Lastly, CLARA [16] is somewhat different, as it was designed as an interactive tool to assist a human to write reports. A human introduces anchor words and the prefix of a sentence, and Lucene [18] processes them as a query to retrieve sentence templates from a database. A sequence-to-sequence network then reads and paraphrases each sentence template to get the final report. CLARA can also operate fully automatically by receiving an empty prefix and predicting the anchor words itself. According to reported results, the model consistently achieved better performance than many baselines.","[['b88', 'b43', 'b157', 'b119', 'b50', 'b156', 'b37', 'b143', 'b147', 'b162', 'b39', 'b130', 'b86', 'b122', 'b127'], ['b102', 'b162', 'b130', 'b88'], ['b67', 'b161', 'b131', 'b60', 'b155', 'b47', 'b66', 'b91', 'b149', 'b154'], ['b153', 'b94', 'b152'], ['b35', 'b6'], ['b163', 'b150', 'b138', 'b125'], ['b82', 'b46', 'b97', 'b48', 'b75', 'b93', 'b164'], ['b84', 'b17', 'b85', 'b15'], ['b88', 'b43', 'b157', 'b119', 'b50', 'b156', 'b37', 'b143', 'b147', 'b162', 'b39', 'b130', 'b86', 'b122', 'b127'], ['b102', 'b162', 'b130', 'b88'], ['b67', 'b161', 'b131', 'b60', 'b155', 'b47', 'b66', 'b91', 'b149', 'b154'], ['b153', 'b94', 'b152'], ['b35', 'b6'], ['b163', 'b150', 'b138', 'b125'], ['b82', 'b46', 'b97', 'b48', 'b75', 'b93', 'b164'], ['b84', 'b17', 'b85', 'b15']]","[['b88', 'b43', 'b157', 'b119', 'b50', 'b156', 'b37', 'b143', 'b147', 'b162', 'b39', 'b130', 'b86', 'b122', 'b127'], ['b102', 'b162', 'b130', 'b88'], ['b67', 'b161', 'b131', 'b60', 'b155', 'b47', 'b66', 'b91', 'b149', 'b154'], ['b153', 'b94', 'b152'], ['b35', 'b6'], ['b163', 'b150', 'b138', 'b125'], ['b82', 'b46', 'b97', 'b48', 'b75', 'b93', 'b164'], ['b84', 'b17', 'b85', 'b15'], ['b88', 'b43', 'b157', 'b119', 'b50', 'b156', 'b37', 'b143', 'b147', 'b162', 'b39', 'b130', 'b86', 'b122', 'b127'], ['b102', 'b162', 'b130', 'b88'], ['b67', 'b161', 'b131', 'b60', 'b155', 'b47', 'b66', 'b91', 'b149', 'b154'], ['b153', 'b94', 'b152'], ['b35', 'b6'], ['b163', 'b150', 'b138', 'b125'], ['b82', 'b46', 'b97', 'b48', 'b75', 'b93', 'b164'], ['b84', 'b17', 'b85', 'b15']]",98,"sent1: The job of the language component is to generate the report.
sent2: In contrast to the visual component, in the literature we find a greater variety of approaches and creative ideas applied to this component.
sent3: Table 5 presents a high-level summary of this analysis.
sent4: The simplest approach is the use of a recurrent neural network, such as LSTM or GRU, to generate the full report word by word.
sent5: Nine works [40,44,51,87,123,128,148,157,158] used LSTM and one work [120] tried both GRU and LSTM.
sent6: All these works have in common that the GRU/LSTM receives an encoding vector from the visual component at the beginning and the full report is decoded from it.
sent7: This encoding vector is typically a vector of global features output by the CNN.
sent8: However, two of these works [44,128] compute a weighted sum of tag embedding vectors and provide that as input to the LSTM.
sent9: Five works [38,89,131,144,163] used LSTM enhanced with an attention mechanism.
sent10: In addition to the initial input, the LSTM equipped with attention can selectively attend to visual features from the visual component at each recurrent step.
sent11: This typically leads to improved performance in all papers.
sent12: A known problem for recurrent networks such as LSTM is that they are not very good at generating very long texts [103].
sent13: This is not a worrying issue when reports are short, however, it can become one for long multi-sentence reports.
sent14: Two papers [131,163] worked around this by generating each sentence independently with a single LSTM and then concatenating these sentences together.
sent15: They accomplished this by providing the LSTM with a vector that indicates the sentence type as first input.
sent16: This worked well in their case because the models were designed for structured reports, i.e., a fixed number of sentences per report and a fixed topic per sentence.
sent17: Vispi [89] adopts a similar strategy: for each disease a dedicated LSTM generates the corresponding sentence, and the final report is the concatenation of them.
sent18: To tackle the generation of unstructured multi-sentence reports, a group of papers followed what we call the Hierarchical LSTM with attention approach: a Sentence LSTM generates a sequence of topic vectors, and a Word LSTM receives a topic vector and generates a sentence word by word.
sent19: In this setting, the attention mechanism can be present at the sentence level, the word level or both.
sent20: Figure 3 shows an illustrative example.
sent21: Seven works [61,68,92,132,155,156,162] followed this approach.
sent22: A common result in these papers is that a Hierarchical LSTM yields better performance in multi-sentence report generation than a single, flat LSTM.
sent23: A few papers [48,67,150] went one step further and replaced the normal Word LSTM with a Dual Word LSTM: the model has a gating mechanism at the sentence level that decides if the sentence will describe an abnormality (e.g., a detected cardiomegaly) or a healthy case.
sent24: Thus, there are two Word LSTMs, one for normal and one for abnormal sentences.
sent25: The goal is to improve the generation of abnormal sentences by having a Word LSTM that specializes in generating them.
sent26: In contrast, a single Word LSTM for everything can lead to overlearning of normal sentences and underlearning of abnormal ones, as the latter are typically less frequent due to class imbalances in datasets.
sent27: The ablation analyses of these works show performance gains, thanks to this approach.
sent28: Another approach for multi-sentence report generation is the Recurrent BiLSTM-attention-LSTM approach.
sent29: The basic idea is to have a LSTM generate one sentence at a time, each time conditioned on a BiLSTM based encoding of the previous sentence and the output of an attention mechanism.
sent30: The process is repeated recurrently sentence by sentence until the full report is generated.
sent31: Three papers used this approach [95,153,154].
sent32: Two works [7,36] approached report generation as simply learning to predict the next word given a partial report and an image.
sent33: The models have dedicated components, such as LSTM and BiLSTM, for encoding the partial report and the image, and the next word is predicted by an FC layer.
sent34: This approach simplifies the task (i.e., predict the next word given everything that comes before), but in practice requires that the model be applied recurrently one word at a time to produce a full report, which has quadratic instead of linear complexity.
sent35: Only one work, RTMIC [151], has explored the use of the Transformer [139] architecture for report generation.
sent36: In RTMIC multiple image crops are obtained using Grad-CAM, then from each crop a feature vector is obtained, and finally a Transformer converts these vectors into a report.
sent37: The paper's results show some performance gains in CIDEr and BLEU with respect to some baselines that do not use the Transformer.
sent38: Likewise, Spinks and Moens [126] were the only ones to use an adversarially regularized autoencoder (ARAE) [164] to generate reports.
sent39: Their model combines an ARAE with a StackGAN and a normal CNN, achieving better performance than a convolutional caption generation baseline in several NLP metrics.
sent40: We also identify a group of papers [47,49,76,94,98] following a Template based approach.
sent41: The language component in these works operates programmatically by following if-then rules or other heuristics in order to retrieve, fill and/or combine templates from a database in order to generate a report.
sent42: The visual component typically outputs discrete classification labels that the language component processes programmatically.
sent43: In the case of Harzig et al. 2019b [49], image localizations per class are also recovered using CAM [165], and in the case of Han et al. [47] the visual component outputs an image segmentation.
sent44: In both cases the language component includes special localization-based rules or templates, thus incorporating location information in the generated report.
sent45: Kisilev et al. [76] followed a different approach: a multi-layer perceptron learns to map image encodings to doc2vec [83] representations of corresponding reports.
sent46: During inference, the ground-truth report with the closest doc2vec representation is retrieved.
sent47: Lastly, we identify three papers [16,85,86] following the Hybrid template retrieval + generation/edition approach.
sent48: These works seek to combine the benefits of templates with the flexibility of a generative module to either generate sentences from scratch or paraphrase templates as needed on a case-by-case basis.
sent49: KERP [86] uses Graph Transformers (GTR) to map the visual input into a sequence of templates from a curated database.
sent50: A Paraphrase GTR then maps each template to its paraphrased version.
sent51: HRGR [85] follows the hierarchical LSTM approach with a twist-it replaces the Word LSTM with a gate module that chooses between two options: retrieving a template or generating a sentence from scratch (via a Word LSTM).
sent52: Lastly, CLARA [16] is somewhat different, as it was designed as an interactive tool to assist a human to write reports.
sent53: A human introduces anchor words and the prefix of a sentence, and Lucene [18] processes them as a query to retrieve sentence templates from a database.
sent54: A sequence-to-sequence network then reads and paraphrases each sentence template to get the final report.
sent55: CLARA can also operate fully automatically by receiving an empty prefix and predicting the anchor words itself.
sent56: According to reported results, the model consistently achieved better performance than many baselines.
sent57: The job of the language component is to generate the report.
sent58: In contrast to the visual component, in the literature we find a greater variety of approaches and creative ideas applied to this component.
sent59: Table 5 presents a high-level summary of this analysis.
sent60: The simplest approach is the use of a recurrent neural network, such as LSTM or GRU, to generate the full report word by word.
sent61: Nine works [40,44,51,87,123,128,148,157,158] used LSTM and one work [120] tried both GRU and LSTM.
sent62: All these works have in common that the GRU/LSTM receives an encoding vector from the visual component at the beginning and the full report is decoded from it.
sent63: This encoding vector is typically a vector of global features output by the CNN.
sent64: However, two of these works [44,128] compute a weighted sum of tag embedding vectors and provide that as input to the LSTM.
sent65: Five works [38,89,131,144,163] used LSTM enhanced with an attention mechanism.
sent66: In addition to the initial input, the LSTM equipped with attention can selectively attend to visual features from the visual component at each recurrent step.
sent67: This typically leads to improved performance in all papers.
sent68: A known problem for recurrent networks such as LSTM is that they are not very good at generating very long texts [103].
sent69: This is not a worrying issue when reports are short, however, it can become one for long multi-sentence reports.
sent70: Two papers [131,163] worked around this by generating each sentence independently with a single LSTM and then concatenating these sentences together.
sent71: They accomplished this by providing the LSTM with a vector that indicates the sentence type as first input.
sent72: This worked well in their case because the models were designed for structured reports, i.e., a fixed number of sentences per report and a fixed topic per sentence.
sent73: Vispi [89] adopts a similar strategy: for each disease a dedicated LSTM generates the corresponding sentence, and the final report is the concatenation of them.
sent74: To tackle the generation of unstructured multi-sentence reports, a group of papers followed what we call the Hierarchical LSTM with attention approach: a Sentence LSTM generates a sequence of topic vectors, and a Word LSTM receives a topic vector and generates a sentence word by word.
sent75: In this setting, the attention mechanism can be present at the sentence level, the word level or both.
sent76: Figure 3 shows an illustrative example.
sent77: Seven works [61,68,92,132,155,156,162] followed this approach.
sent78: A common result in these papers is that a Hierarchical LSTM yields better performance in multi-sentence report generation than a single, flat LSTM.
sent79: A few papers [48,67,150] went one step further and replaced the normal Word LSTM with a Dual Word LSTM: the model has a gating mechanism at the sentence level that decides if the sentence will describe an abnormality (e.g., a detected cardiomegaly) or a healthy case.
sent80: Thus, there are two Word LSTMs, one for normal and one for abnormal sentences.
sent81: The goal is to improve the generation of abnormal sentences by having a Word LSTM that specializes in generating them.
sent82: In contrast, a single Word LSTM for everything can lead to overlearning of normal sentences and underlearning of abnormal ones, as the latter are typically less frequent due to class imbalances in datasets.
sent83: The ablation analyses of these works show performance gains, thanks to this approach.
sent84: Another approach for multi-sentence report generation is the Recurrent BiLSTM-attention-LSTM approach.
sent85: The basic idea is to have a LSTM generate one sentence at a time, each time conditioned on a BiLSTM based encoding of the previous sentence and the output of an attention mechanism.
sent86: The process is repeated recurrently sentence by sentence until the full report is generated.
sent87: Three papers used this approach [95,153,154].
sent88: Two works [7,36] approached report generation as simply learning to predict the next word given a partial report and an image.
sent89: The models have dedicated components, such as LSTM and BiLSTM, for encoding the partial report and the image, and the next word is predicted by an FC layer.
sent90: This approach simplifies the task (i.e., predict the next word given everything that comes before), but in practice requires that the model be applied recurrently one word at a time to produce a full report, which has quadratic instead of linear complexity.
sent91: Only one work, RTMIC [151], has explored the use of the Transformer [139] architecture for report generation.
sent92: In RTMIC multiple image crops are obtained using Grad-CAM, then from each crop a feature vector is obtained, and finally a Transformer converts these vectors into a report.
sent93: The paper's results show some performance gains in CIDEr and BLEU with respect to some baselines that do not use the Transformer.
sent94: Likewise, Spinks and Moens [126] were the only ones to use an adversarially regularized autoencoder (ARAE) [164] to generate reports.
sent95: Their model combines an ARAE with a StackGAN and a normal CNN, achieving better performance than a convolutional caption generation baseline in several NLP metrics.
sent96: We also identify a group of papers [47,49,76,94,98] following a Template based approach.
sent97: The language component in these works operates programmatically by following if-then rules or other heuristics in order to retrieve, fill and/or combine templates from a database in order to generate a report.
sent98: The visual component typically outputs discrete classification labels that the language component processes programmatically.
sent99: In the case of Harzig et al. 2019b [49], image localizations per class are also recovered using CAM [165], and in the case of Han et al. [47] the visual component outputs an image segmentation.
sent100: In both cases the language component includes special localization-based rules or templates, thus incorporating location information in the generated report.
sent101: Kisilev et al. [76] followed a different approach: a multi-layer perceptron learns to map image encodings to doc2vec [83] representations of corresponding reports.
sent102: During inference, the ground-truth report with the closest doc2vec representation is retrieved.
sent103: Lastly, we identify three papers [16,85,86] following the Hybrid template retrieval + generation/edition approach.
sent104: These works seek to combine the benefits of templates with the flexibility of a generative module to either generate sentences from scratch or paraphrase templates as needed on a case-by-case basis.
sent105: KERP [86] uses Graph Transformers (GTR) to map the visual input into a sequence of templates from a curated database.
sent106: A Paraphrase GTR then maps each template to its paraphrased version.
sent107: HRGR [85] follows the hierarchical LSTM approach with a twist-it replaces the Word LSTM with a gate module that chooses between two options: retrieving a template or generating a sentence from scratch (via a Word LSTM).
sent108: Lastly, CLARA [16] is somewhat different, as it was designed as an interactive tool to assist a human to write reports.
sent109: A human introduces anchor words and the prefix of a sentence, and Lucene [18] processes them as a query to retrieve sentence templates from a database.
sent110: A sequence-to-sequence network then reads and paraphrases each sentence template to get the final report.
sent111: CLARA can also operate fully automatically by receiving an empty prefix and predicting the anchor words itself.
sent112: According to reported results, the model consistently achieved better performance than many baselines."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s52,Domain knowledge.,"Although all works used datasets from the medical domain to train their models, which can be considered a form of domain knowledge transfer, some works took special steps to explicitly incorporate additional knowledge from experts into their design. Concretely, we identify two incipient trends in the application of domain knowledge: 1) the use of graph neural networks right after the CNN, providing an architectural bias to guide the model to identify medical concepts and their relations from the images; and 2) enhancing the model's report generation with access to an external template database curated by experts.

KERP [86] incorporates knowledge at the architectural level using graph neural networks. The authors manually designed an abnormality graph and a disease graph, where each node represents an abnormality or disease, and the edges are built based on their co-occurrences in the training set. Some example abnormalities are ""low lung volumes"" and ""enlarged heart size"", whereas diseases represent a higher level of abstraction, for example ""emphysema"" or ""consolidation"". The information flows from image features (encoded by a CNN) to the abnormality graph, and then to the disease graph, via inter-node message passing. This biases the network to encode the visual information in terms of abnormalities, diseases and their relations. Similarly, Zhang et al. [162] created an observations graph, containing 20 nodes of chest abnormalities or body parts, where conditions related to the same organ or tissue are connected by edges. Their ablation analysis showed some performance gains, thanks to the graph neural network.

In seven works [16,47,49,76,85,86,98] the authors provided their models with a curated set of template sentences that are further processed in the language component to output a full report. Three works [47,49,76] used manually curated templates and if-then based programs to select and fill them. CLARA [16] uses a database indexing all sentences from the training set reports for text-based retrieval, which are then paraphrased by a generative module. Similarly, KERP [86] has access to a template database mined from the training set, which are also paraphrased later. In HRGR [85] the most common sentences in the datasets were mined and then manually grouped by meaning to further reduce repetitions. In this work the authors showed that HRGR learned to prefer templates about 80% of the time and only generate sentences from scratch the remaining 20%, suggesting that templates can be quite useful to generate most sentences in reports.

Although all works used datasets from the medical domain to train their models, which can be considered a form of domain knowledge transfer, some works took special steps to explicitly incorporate additional knowledge from experts into their design. Concretely, we identify two incipient trends in the application of domain knowledge: 1) the use of graph neural networks right after the CNN, providing an architectural bias to guide the model to identify medical concepts and their relations from the images; and 2) enhancing the model's report generation with access to an external template database curated by experts.

KERP [86] incorporates knowledge at the architectural level using graph neural networks. The authors manually designed an abnormality graph and a disease graph, where each node represents an abnormality or disease, and the edges are built based on their co-occurrences in the training set. Some example abnormalities are ""low lung volumes"" and ""enlarged heart size"", whereas diseases represent a higher level of abstraction, for example ""emphysema"" or ""consolidation"". The information flows from image features (encoded by a CNN) to the abnormality graph, and then to the disease graph, via inter-node message passing. This biases the network to encode the visual information in terms of abnormalities, diseases and their relations. Similarly, Zhang et al. [162] created an observations graph, containing 20 nodes of chest abnormalities or body parts, where conditions related to the same organ or tissue are connected by edges. Their ablation analysis showed some performance gains, thanks to the graph neural network.

In seven works [16,47,49,76,85,86,98] the authors provided their models with a curated set of template sentences that are further processed in the language component to output a full report. Three works [47,49,76] used manually curated templates and if-then based programs to select and fill them. CLARA [16] uses a database indexing all sentences from the training set reports for text-based retrieval, which are then paraphrased by a generative module. Similarly, KERP [86] has access to a template database mined from the training set, which are also paraphrased later. In HRGR [85] the most common sentences in the datasets were mined and then manually grouped by meaning to further reduce repetitions. In this work the authors showed that HRGR learned to prefer templates about 80% of the time and only generate sentences from scratch the remaining 20%, suggesting that templates can be quite useful to generate most sentences in reports.","[[], ['b85', 'b161'], ['b85', 'b84', 'b46', 'b48', 'b97', 'b75', 'b15'], [], ['b85', 'b161'], ['b85', 'b84', 'b46', 'b48', 'b97', 'b75', 'b15']]","[[], ['b85', 'b161'], ['b85', 'b84', 'b46', 'b48', 'b97', 'b75', 'b15'], [], ['b85', 'b161'], ['b85', 'b84', 'b46', 'b48', 'b97', 'b75', 'b15']]",18,"sent1: Although all works used datasets from the medical domain to train their models, which can be considered a form of domain knowledge transfer, some works took special steps to explicitly incorporate additional knowledge from experts into their design.
sent2: Concretely, we identify two incipient trends in the application of domain knowledge: 1) the use of graph neural networks right after the CNN, providing an architectural bias to guide the model to identify medical concepts and their relations from the images; and 2) enhancing the model's report generation with access to an external template database curated by experts.
sent3: KERP [86] incorporates knowledge at the architectural level using graph neural networks.
sent4: The authors manually designed an abnormality graph and a disease graph, where each node represents an abnormality or disease, and the edges are built based on their co-occurrences in the training set.
sent5: Some example abnormalities are ""low lung volumes"" and ""enlarged heart size"", whereas diseases represent a higher level of abstraction, for example ""emphysema"" or ""consolidation"".
sent6: The information flows from image features (encoded by a CNN) to the abnormality graph, and then to the disease graph, via inter-node message passing.
sent7: This biases the network to encode the visual information in terms of abnormalities, diseases and their relations.
sent8: Similarly, Zhang et al. [162] created an observations graph, containing 20 nodes of chest abnormalities or body parts, where conditions related to the same organ or tissue are connected by edges.
sent9: Their ablation analysis showed some performance gains, thanks to the graph neural network.
sent10: In seven works [16,47,49,76,85,86,98] the authors provided their models with a curated set of template sentences that are further processed in the language component to output a full report.
sent11: Three works [47,49,76] used manually curated templates and if-then based programs to select and fill them.
sent12: CLARA [16] uses a database indexing all sentences from the training set reports for text-based retrieval, which are then paraphrased by a generative module.
sent13: Similarly, KERP [86] has access to a template database mined from the training set, which are also paraphrased later.
sent14: In HRGR [85] the most common sentences in the datasets were mined and then manually grouped by meaning to further reduce repetitions.
sent15: In this work the authors showed that HRGR learned to prefer templates about 80% of the time and only generate sentences from scratch the remaining 20%, suggesting that templates can be quite useful to generate most sentences in reports.
sent16: Although all works used datasets from the medical domain to train their models, which can be considered a form of domain knowledge transfer, some works took special steps to explicitly incorporate additional knowledge from experts into their design.
sent17: Concretely, we identify two incipient trends in the application of domain knowledge: 1) the use of graph neural networks right after the CNN, providing an architectural bias to guide the model to identify medical concepts and their relations from the images; and 2) enhancing the model's report generation with access to an external template database curated by experts.
sent18: KERP [86] incorporates knowledge at the architectural level using graph neural networks.
sent19: The authors manually designed an abnormality graph and a disease graph, where each node represents an abnormality or disease, and the edges are built based on their co-occurrences in the training set.
sent20: Some example abnormalities are ""low lung volumes"" and ""enlarged heart size"", whereas diseases represent a higher level of abstraction, for example ""emphysema"" or ""consolidation"".
sent21: The information flows from image features (encoded by a CNN) to the abnormality graph, and then to the disease graph, via inter-node message passing.
sent22: This biases the network to encode the visual information in terms of abnormalities, diseases and their relations.
sent23: Similarly, Zhang et al. [162] created an observations graph, containing 20 nodes of chest abnormalities or body parts, where conditions related to the same organ or tissue are connected by edges.
sent24: Their ablation analysis showed some performance gains, thanks to the graph neural network.
sent25: In seven works [16,47,49,76,85,86,98] the authors provided their models with a curated set of template sentences that are further processed in the language component to output a full report.
sent26: Three works [47,49,76] used manually curated templates and if-then based programs to select and fill them.
sent27: CLARA [16] uses a database indexing all sentences from the training set reports for text-based retrieval, which are then paraphrased by a generative module.
sent28: Similarly, KERP [86] has access to a template database mined from the training set, which are also paraphrased later.
sent29: In HRGR [85] the most common sentences in the datasets were mined and then manually grouped by meaning to further reduce repetitions.
sent30: In this work the authors showed that HRGR learned to prefer templates about 80% of the time and only generate sentences from scratch the remaining 20%, suggesting that templates can be quite useful to generate most sentences in reports."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s53,Auxiliary Tasks.,"Although the main objective in most papers is to learn a model for report generation from medical images, many works also include and optimize auxiliary tasks to boost their performance. A summary of these tasks is presented in Table 10 in appendix 9.2. The most common auxiliary tasks are multi-label (16 papers) and single-label (11 papers) classification. These tasks are generally intended to provide additional supervision to the model's visual component, in order to improve the CNN's capabilities to extract quality visual features. Some common tasks are identifying the presence or absence of different abnormalities, diseases, organs, body parts, medical concepts, detecting image modality, etc. Datasets often used for this purpose are ChestX-ray14 [143] and CheXpert [63], where the common practice is to pretrain the CNN in those datasets before moving on to report generation. Many papers report better performance in report generation thanks to these auxiliary classification tasks. The three works [48,67,150] following the hierarchical approach with Dual Word LSTM used a classification task to supervise the gating mechanism that chooses between generating a normal sentence, an abnormal sentence or stopping. Two models [47,131] perform a segmentation task. Tian et al. [131] trained a fully convolutional network (FCN) with segmentation masks of a liver and tumor, and Han et al. [47] trained an RGAN for pixel level classification. Similarly, two models [76,157] use a Faster-RCNN [112] trained for detection and classification of bounding boxes enclosing lesions or other regions of interest in the images.

Two works [95,155] used regularization supervision on attention weights. CORAL8 [95] receives regularization supervision on its visual attention weights to prevent them from degrading into uniform distribution, which would offer no advantage over average pooling. Similarly, Yin et al. [155] added two regularizations to their model's attention weights: one on the weights over spatial visual features and another on the weights over tag embedding vectors. In both works the attention supervision provided a significant contribution to the performance.

Two works [98,155] included a task to enforce a matching between embeddings from two different sources. Yin et al. [155] projected the topic vectors from the Sentence LSTM and the word embeddings from the respective ground-truth sentence into a common semantic space, and enforced a matching via contrastive loss [24]. This task significantly improved the Sentence LSTM's training and the model's overall performance. Moradi et al. [98] trained a MLP for mapping image visual encodings (obtained by a VGG network) to the vector representation of its corresponding groundtruth report (obtained via doc2vec [83], which in itself was another auxiliary task), by minimizing the Euclidean distance. The trained MLP was then used to predict doc2vec representations for unseen images and retrieve the report with the closest representation. Two works [126,132] used text autoencoders, which allow learning compact representations of unlabeled data in a self-supervised manner: an encoder network maps the input into a latent representation, and a decoder network has to recover the original input back. MTMA [132] uses a BiLSTM to encode the sentences of the indication and findings sections of a report (input text), in order to generate the impression section (output). To improve the encoding quality of the BiLSTM, the authors trained the decoder branch of a hierarchical autoencoder [88] to recover the original sentence from the BiLSTM encoding. The experimental results showed that the autoencoder supervision provided a significant boost to the model's performance. Spinks and Moens [126] trained an ARAE [164] (1) to learn compact representations of reports (serving as input to a GAN that generates chest X-ray images) and (2) to recover a report given an arbitrary compact representation (used in inference mode for report generation).

Lastly, Spinks and Moens [126] were the only ones to also implement cycle-consistency tasks [167] to train a GAN and an inverse mapping CNN together, to make both chest X-ray image generation and encoding more robust. These tasks will be further detailed in the next section.

Although the main objective in most papers is to learn a model for report generation from medical images, many works also include and optimize auxiliary tasks to boost their performance. A summary of these tasks is presented in Table 10 in appendix 9.2. The most common auxiliary tasks are multi-label (16 papers) and single-label (11 papers) classification. These tasks are generally intended to provide additional supervision to the model's visual component, in order to improve the CNN's capabilities to extract quality visual features. Some common tasks are identifying the presence or absence of different abnormalities, diseases, organs, body parts, medical concepts, detecting image modality, etc. Datasets often used for this purpose are ChestX-ray14 [143] and CheXpert [63], where the common practice is to pretrain the CNN in those datasets before moving on to report generation. Many papers report better performance in report generation thanks to these auxiliary classification tasks. The three works [48,67,150] following the hierarchical approach with Dual Word LSTM used a classification task to supervise the gating mechanism that chooses between generating a normal sentence, an abnormal sentence or stopping. Two models [47,131] perform a segmentation task. Tian et al. [131] trained a fully convolutional network (FCN) with segmentation masks of a liver and tumor, and Han et al. [47] trained an RGAN for pixel level classification. Similarly, two models [76,157] use a Faster-RCNN [112] trained for detection and classification of bounding boxes enclosing lesions or other regions of interest in the images.

Two works [95,155] used regularization supervision on attention weights. CORAL8 [95] receives regularization supervision on its visual attention weights to prevent them from degrading into uniform distribution, which would offer no advantage over average pooling. Similarly, Yin et al. [155] added two regularizations to their model's attention weights: one on the weights over spatial visual features and another on the weights over tag embedding vectors. In both works the attention supervision provided a significant contribution to the performance.

Two works [98,155] included a task to enforce a matching between embeddings from two different sources. Yin et al. [155] projected the topic vectors from the Sentence LSTM and the word embeddings from the respective ground-truth sentence into a common semantic space, and enforced a matching via contrastive loss [24]. This task significantly improved the Sentence LSTM's training and the model's overall performance. Moradi et al. [98] trained a MLP for mapping image visual encodings (obtained by a VGG network) to the vector representation of its corresponding groundtruth report (obtained via doc2vec [83], which in itself was another auxiliary task), by minimizing the Euclidean distance. The trained MLP was then used to predict doc2vec representations for unseen images and retrieve the report with the closest representation. Two works [126,132] used text autoencoders, which allow learning compact representations of unlabeled data in a self-supervised manner: an encoder network maps the input into a latent representation, and a decoder network has to recover the original input back. MTMA [132] uses a BiLSTM to encode the sentences of the indication and findings sections of a report (input text), in order to generate the impression section (output). To improve the encoding quality of the BiLSTM, the authors trained the decoder branch of a hierarchical autoencoder [88] to recover the original sentence from the BiLSTM encoding. The experimental results showed that the autoencoder supervision provided a significant boost to the model's performance. Spinks and Moens [126] trained an ARAE [164] (1) to learn compact representations of reports (serving as input to a GAN that generates chest X-ray images) and (2) to recover a report given an arbitrary compact representation (used in inference mode for report generation).

Lastly, Spinks and Moens [126] were the only ones to also implement cycle-consistency tasks [167] to train a GAN and an inverse mapping CNN together, to make both chest X-ray image generation and encoding more robust. These tasks will be further detailed in the next section.","[['b111', 'b62', 'b156', 'b142', 'b47', 'b66', 'b46', 'b75', 'b149', 'b130'], ['b154', 'b94'], ['b125', 'b82', 'b131', 'b163', 'b97', 'b23', 'b87', 'b154'], ['b166', 'b125'], ['b111', 'b62', 'b156', 'b142', 'b47', 'b66', 'b46', 'b75', 'b149', 'b130'], ['b154', 'b94'], ['b125', 'b82', 'b131', 'b163', 'b97', 'b23', 'b87', 'b154'], ['b166', 'b125']]","[['b111', 'b62', 'b156', 'b142', 'b47', 'b66', 'b46', 'b75', 'b149', 'b130'], ['b154', 'b94'], ['b125', 'b82', 'b131', 'b163', 'b97', 'b23', 'b87', 'b154'], ['b166', 'b125'], ['b111', 'b62', 'b156', 'b142', 'b47', 'b66', 'b46', 'b75', 'b149', 'b130'], ['b154', 'b94'], ['b125', 'b82', 'b131', 'b163', 'b97', 'b23', 'b87', 'b154'], ['b166', 'b125']]",44,"sent1: Although the main objective in most papers is to learn a model for report generation from medical images, many works also include and optimize auxiliary tasks to boost their performance.
sent2: A summary of these tasks is presented in Table 10 in appendix 9.2.
sent3: The most common auxiliary tasks are multi-label (16 papers) and single-label (11 papers) classification.
sent4: These tasks are generally intended to provide additional supervision to the model's visual component, in order to improve the CNN's capabilities to extract quality visual features.
sent5: Some common tasks are identifying the presence or absence of different abnormalities, diseases, organs, body parts, medical concepts, detecting image modality, etc.
sent6: Datasets often used for this purpose are ChestX-ray14 [143] and CheXpert [63], where the common practice is to pretrain the CNN in those datasets before moving on to report generation.
sent7: Many papers report better performance in report generation thanks to these auxiliary classification tasks.
sent8: The three works [48,67,150] following the hierarchical approach with Dual Word LSTM used a classification task to supervise the gating mechanism that chooses between generating a normal sentence, an abnormal sentence or stopping.
sent9: Two models [47,131] perform a segmentation task.
sent10: Tian et al. [131] trained a fully convolutional network (FCN) with segmentation masks of a liver and tumor, and Han et al. [47] trained an RGAN for pixel level classification.
sent11: Similarly, two models [76,157] use a Faster-RCNN [112] trained for detection and classification of bounding boxes enclosing lesions or other regions of interest in the images.
sent12: Two works [95,155] used regularization supervision on attention weights.
sent13: CORAL8 [95] receives regularization supervision on its visual attention weights to prevent them from degrading into uniform distribution, which would offer no advantage over average pooling.
sent14: Similarly, Yin et al. [155] added two regularizations to their model's attention weights: one on the weights over spatial visual features and another on the weights over tag embedding vectors.
sent15: In both works the attention supervision provided a significant contribution to the performance.
sent16: Two works [98,155] included a task to enforce a matching between embeddings from two different sources.
sent17: Yin et al. [155] projected the topic vectors from the Sentence LSTM and the word embeddings from the respective ground-truth sentence into a common semantic space, and enforced a matching via contrastive loss [24].
sent18: This task significantly improved the Sentence LSTM's training and the model's overall performance.
sent19: Moradi et al. [98] trained a MLP for mapping image visual encodings (obtained by a VGG network) to the vector representation of its corresponding groundtruth report (obtained via doc2vec [83], which in itself was another auxiliary task), by minimizing the Euclidean distance.
sent20: The trained MLP was then used to predict doc2vec representations for unseen images and retrieve the report with the closest representation.
sent21: Two works [126,132] used text autoencoders, which allow learning compact representations of unlabeled data in a self-supervised manner: an encoder network maps the input into a latent representation, and a decoder network has to recover the original input back.
sent22: MTMA [132] uses a BiLSTM to encode the sentences of the indication and findings sections of a report (input text), in order to generate the impression section (output).
sent23: To improve the encoding quality of the BiLSTM, the authors trained the decoder branch of a hierarchical autoencoder [88] to recover the original sentence from the BiLSTM encoding.
sent24: The experimental results showed that the autoencoder supervision provided a significant boost to the model's performance.
sent25: Spinks and Moens [126] trained an ARAE [164] (1) to learn compact representations of reports (serving as input to a GAN that generates chest X-ray images) and (2) to recover a report given an arbitrary compact representation (used in inference mode for report generation).
sent26: Lastly, Spinks and Moens [126] were the only ones to also implement cycle-consistency tasks [167] to train a GAN and an inverse mapping CNN together, to make both chest X-ray image generation and encoding more robust.
sent27: These tasks will be further detailed in the next section.
sent28: Although the main objective in most papers is to learn a model for report generation from medical images, many works also include and optimize auxiliary tasks to boost their performance.
sent29: A summary of these tasks is presented in Table 10 in appendix 9.2.
sent30: The most common auxiliary tasks are multi-label (16 papers) and single-label (11 papers) classification.
sent31: These tasks are generally intended to provide additional supervision to the model's visual component, in order to improve the CNN's capabilities to extract quality visual features.
sent32: Some common tasks are identifying the presence or absence of different abnormalities, diseases, organs, body parts, medical concepts, detecting image modality, etc.
sent33: Datasets often used for this purpose are ChestX-ray14 [143] and CheXpert [63], where the common practice is to pretrain the CNN in those datasets before moving on to report generation.
sent34: Many papers report better performance in report generation thanks to these auxiliary classification tasks.
sent35: The three works [48,67,150] following the hierarchical approach with Dual Word LSTM used a classification task to supervise the gating mechanism that chooses between generating a normal sentence, an abnormal sentence or stopping.
sent36: Two models [47,131] perform a segmentation task.
sent37: Tian et al. [131] trained a fully convolutional network (FCN) with segmentation masks of a liver and tumor, and Han et al. [47] trained an RGAN for pixel level classification.
sent38: Similarly, two models [76,157] use a Faster-RCNN [112] trained for detection and classification of bounding boxes enclosing lesions or other regions of interest in the images.
sent39: Two works [95,155] used regularization supervision on attention weights.
sent40: CORAL8 [95] receives regularization supervision on its visual attention weights to prevent them from degrading into uniform distribution, which would offer no advantage over average pooling.
sent41: Similarly, Yin et al. [155] added two regularizations to their model's attention weights: one on the weights over spatial visual features and another on the weights over tag embedding vectors.
sent42: In both works the attention supervision provided a significant contribution to the performance.
sent43: Two works [98,155] included a task to enforce a matching between embeddings from two different sources.
sent44: Yin et al. [155] projected the topic vectors from the Sentence LSTM and the word embeddings from the respective ground-truth sentence into a common semantic space, and enforced a matching via contrastive loss [24].
sent45: This task significantly improved the Sentence LSTM's training and the model's overall performance.
sent46: Moradi et al. [98] trained a MLP for mapping image visual encodings (obtained by a VGG network) to the vector representation of its corresponding groundtruth report (obtained via doc2vec [83], which in itself was another auxiliary task), by minimizing the Euclidean distance.
sent47: The trained MLP was then used to predict doc2vec representations for unseen images and retrieve the report with the closest representation.
sent48: Two works [126,132] used text autoencoders, which allow learning compact representations of unlabeled data in a self-supervised manner: an encoder network maps the input into a latent representation, and a decoder network has to recover the original input back.
sent49: MTMA [132] uses a BiLSTM to encode the sentences of the indication and findings sections of a report (input text), in order to generate the impression section (output).
sent50: To improve the encoding quality of the BiLSTM, the authors trained the decoder branch of a hierarchical autoencoder [88] to recover the original sentence from the BiLSTM encoding.
sent51: The experimental results showed that the autoencoder supervision provided a significant boost to the model's performance.
sent52: Spinks and Moens [126] trained an ARAE [164] (1) to learn compact representations of reports (serving as input to a GAN that generates chest X-ray images) and (2) to recover a report given an arbitrary compact representation (used in inference mode for report generation).
sent53: Lastly, Spinks and Moens [126] were the only ones to also implement cycle-consistency tasks [167] to train a GAN and an inverse mapping CNN together, to make both chest X-ray image generation and encoding more robust.
sent54: These tasks will be further detailed in the next section."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s54,Optimization,"Strategies. In addition to the architecture and the tasks a model can perform, a very important aspect is the optimization strategy used to learn the model's parameters. In this section we present an analysis of the optimization strategies used in the literature. A summary of this section is presented in Table 11 in appendix 9.3.

Visual Component. We first analyze the visual component optimization, identifying three general optimization decisions. The first one is whether to use a CNN from the literature with its weights pretrained in ImageNet [29]. This is a very common transfer learning practice from the computer vision literature in general [78], so it is natural to see it used in the medical domain too. However, it has been shown that ImageNet pretraining may not transfer as well to medical image tasks as they normally do to other domains, due to very dissimilar image distributions [109]. Therefore, a very common second decision is whether or not to train/fine-tune the visual component with auxiliary medical image tasks, such as most of the classification and segmentation tasks discussed in the previous section (5.2.5). The third decision is whether to freeze the visual component weights during report generation training or continue updating them in an end-to-end manner.

Report Generation. We identify two general optimization strategies in the literature: Teacherforcing (TF) and Reinforcement Learning (RL). Teacher-forcing [146] is by far the most common, as it is adopted by 32 papers [7, 16, 36, 38, 40, 44, 48, 51, 61, 67, 68, 86, 87, 89, 95, 120, 123, 126, 128, 131, 132, 144, 148, 150, 153-158, 162, 163]. The basic idea in teacher-forcing is to train a model to predict each word of the report conditioned on the previous words, therefore learning to imitate the ground truth word by word. The model typically has a softmax layer that predicts the next word, and cross entropy is the loss function of choice to measure the error and compute gradients for backpropagation. We think teacher-forcing is so widespread in the literature because of its simplicity and general applicability, as it is agnostic to the application domain (whether it be report generation in medicine or captioning of everyday images).

In contrast, 5 works [67,85,87,92,151] explored the use of reinforcement learning (RL) [71]. The main reason to use RL is the flexibility it offers to optimize non-differentiable reward functions, allowing researchers to be more creative and explore new rewards that may guide the model's learning toward domain-specific goals of interest. For example, Liu et al. [92] used RL to train their model to optimize the weighted sum of two rewards: (1) a natural language reward (CIDEr [140]) and (2) a Clinically Coherent Reward (CCR), where the latter was proposed to measure the clinical accuracy of a generated report compared to a ground-truth reference using the CheXpert labeler tool [63]. Their goal was to equip their model with two skills: natural language fluency (encouraged by CIDEr) and clinical accuracy (encouraged by CCR). Other examples of the use of RL are: the direct optimization of CIDEr [85,151], particularly in the training of a complicated hybrid template-retrieval and text generation model [85]; directly optimizing BLEU-4 after a previous teacher-forcing warmup phase [67]; and the training of the generator network of a GAN used for report generation, where the reward is provided by the discriminator network [87].

As a side note, we would like to highlight the work by Zhang et al. [161] on medical report summarization (a related task where the report is the input and with no images), illustrating how RL can be used in this setting to optimize both fluency and factual correctness. As rewards they used ROUGE [90] and a Factual Correctness reward based on the CheXpert labeler tool [63] (very similar to the CCR proposed by Liu et al. [92]). This work is a good example of the benefits of RL over teacher-forcing for text generation in a medical domain. The paper presents the results of a human evaluation with two board-certified radiologists and the model trained with RL achieved better results than the same model trained with teacher-forcing, and even slightly better results than the human baseline.

Other Losses or Training Strategies. This category encompasses the remaining optimization strategies found in the literature. The most important one is multitask learning [21], adopted by 14 papers [48,67,68,76,86,94,95,126,131,132,144,155,157,163]. The main idea is to jointly train a model in multiple complementary tasks, so that the model can learn robust parameters that perform well in all of them. Some works [48,68,131,132,144,155,163] trained the visual and language components simultaneously in multiple tasks in an end-to-end manner, i.e. report generation plus other auxiliary tasks. Other examples are the simultaneous training of object detection and attribute classification [76], diagnostic classification and cycle-consistency tasks [126], among others. Most of these papers report benefits from training in this way.

As already discussed in section 5.2.5, two works [95,155] used auxiliary supervision on the attention weights of their models. These auxiliary losses were jointly optimized with the rest of the model in report generation, effectively having a regularizer effect. Yin et al. [155] are also the only ones that included an auxiliary contrastive loss [24] to provide a direct supervision to the Sentence LSTM, thus improving their model's performance. Notice that all these works are examples of multitask learning too. Three papers [76,98,157] used regression losses. Two of them [76,157] included a bounding box regression loss as part of Faster R-CNN [112] training, and Moradi et al. [98] included a regression loss to minimize the Euclidean distance between VGG and doc2vec embeddings. As previously discussed in section 5.2.5, another optimization strategy is the use of autoencoders for the self-supervised learning of text representations. In MTMA [132] an autoencoder was used to provide an auxiliary supervision over the BiLSTM and was jointly trained with the rest of the model in a multitask learning fashion. Spinks and Moens [126] instead trained an ARAE in a first stage, then froze its weights and used the learned text embedding to support the subsequent training of a GAN.

Lastly, three works used GANs [47,87,126]. As mentioned when discussing RL, Li et al. [47] used a GAN strategy to train their model for report generation, where the generative module generates a report and the discriminator determines whether it is real or fake. Similarly, Han et al. [87] proposed RGAN, where the generator outputs segmentation maps from spine radiographs and the discriminator determines if a given segmentation map is real or fake. Spinks and Moens [126] implemented a modified version of a StackGAN [159] to generate chest X-ray images from input text representations. In their case, they trained the GAN using two cycle-consistency [167] losses: (1) image − → embedding − → image and (2) embedding − → image − → embedding. In both cases, an auxiliary inverse mapping CNN was used to close the cycle.

Synthesis. Overall, we can observe that designing a model for report generation from medical images is a complex task that involves engineering decisions at multiple levels: inputs and outputs, visual component, language component, domain knowledge, auxiliary tasks and optimization strategies. In each of these dimensions there are different approaches adopted in the reviewed literature, and the current state of research does not allow us to recommend an ""optimal model design"", mainly for reasons we will discuss in the Metrics and Performance Comparison sections (5.4 and 5.5). Nevertheless, there are valuable insights in the literature that may lead to better results, and thus are worth having in mind. For example, the use of CNNs (such as DenseNet or ResNet) as visual component and training in auxiliary medical image tasks; the use of input text alongside the images; providing the language component with tag information in addition to the visual features (e.g. medical concepts identified in the image); leveraging template databases curated with domain knowledge; or the use of multitask learning combining multiple sources of supervision. Lastly, to improve report quality from a medical perspective, the use of reinforcement learning with adequate reward functions appears as the most promising approach.

Strategies. In addition to the architecture and the tasks a model can perform, a very important aspect is the optimization strategy used to learn the model's parameters. In this section we present an analysis of the optimization strategies used in the literature. A summary of this section is presented in Table 11 in appendix 9.3.

Visual Component. We first analyze the visual component optimization, identifying three general optimization decisions. The first one is whether to use a CNN from the literature with its weights pretrained in ImageNet [29]. This is a very common transfer learning practice from the computer vision literature in general [78], so it is natural to see it used in the medical domain too. However, it has been shown that ImageNet pretraining may not transfer as well to medical image tasks as they normally do to other domains, due to very dissimilar image distributions [109]. Therefore, a very common second decision is whether or not to train/fine-tune the visual component with auxiliary medical image tasks, such as most of the classification and segmentation tasks discussed in the previous section (5.2.5). The third decision is whether to freeze the visual component weights during report generation training or continue updating them in an end-to-end manner.

Report Generation. We identify two general optimization strategies in the literature: Teacherforcing (TF) and Reinforcement Learning (RL). Teacher-forcing [146] is by far the most common, as it is adopted by 32 papers [7, 16, 36, 38, 40, 44, 48, 51, 61, 67, 68, 86, 87, 89, 95, 120, 123, 126, 128, 131, 132, 144, 148, 150, 153-158, 162, 163]. The basic idea in teacher-forcing is to train a model to predict each word of the report conditioned on the previous words, therefore learning to imitate the ground truth word by word. The model typically has a softmax layer that predicts the next word, and cross entropy is the loss function of choice to measure the error and compute gradients for backpropagation. We think teacher-forcing is so widespread in the literature because of its simplicity and general applicability, as it is agnostic to the application domain (whether it be report generation in medicine or captioning of everyday images).

In contrast, 5 works [67,85,87,92,151] explored the use of reinforcement learning (RL) [71]. The main reason to use RL is the flexibility it offers to optimize non-differentiable reward functions, allowing researchers to be more creative and explore new rewards that may guide the model's learning toward domain-specific goals of interest. For example, Liu et al. [92] used RL to train their model to optimize the weighted sum of two rewards: (1) a natural language reward (CIDEr [140]) and (2) a Clinically Coherent Reward (CCR), where the latter was proposed to measure the clinical accuracy of a generated report compared to a ground-truth reference using the CheXpert labeler tool [63]. Their goal was to equip their model with two skills: natural language fluency (encouraged by CIDEr) and clinical accuracy (encouraged by CCR). Other examples of the use of RL are: the direct optimization of CIDEr [85,151], particularly in the training of a complicated hybrid template-retrieval and text generation model [85]; directly optimizing BLEU-4 after a previous teacher-forcing warmup phase [67]; and the training of the generator network of a GAN used for report generation, where the reward is provided by the discriminator network [87].

As a side note, we would like to highlight the work by Zhang et al. [161] on medical report summarization (a related task where the report is the input and with no images), illustrating how RL can be used in this setting to optimize both fluency and factual correctness. As rewards they used ROUGE [90] and a Factual Correctness reward based on the CheXpert labeler tool [63] (very similar to the CCR proposed by Liu et al. [92]). This work is a good example of the benefits of RL over teacher-forcing for text generation in a medical domain. The paper presents the results of a human evaluation with two board-certified radiologists and the model trained with RL achieved better results than the same model trained with teacher-forcing, and even slightly better results than the human baseline.

Other Losses or Training Strategies. This category encompasses the remaining optimization strategies found in the literature. The most important one is multitask learning [21], adopted by 14 papers [48,67,68,76,86,94,95,126,131,132,144,155,157,163]. The main idea is to jointly train a model in multiple complementary tasks, so that the model can learn robust parameters that perform well in all of them. Some works [48,68,131,132,144,155,163] trained the visual and language components simultaneously in multiple tasks in an end-to-end manner, i.e. report generation plus other auxiliary tasks. Other examples are the simultaneous training of object detection and attribute classification [76], diagnostic classification and cycle-consistency tasks [126], among others. Most of these papers report benefits from training in this way.

As already discussed in section 5.2.5, two works [95,155] used auxiliary supervision on the attention weights of their models. These auxiliary losses were jointly optimized with the rest of the model in report generation, effectively having a regularizer effect. Yin et al. [155] are also the only ones that included an auxiliary contrastive loss [24] to provide a direct supervision to the Sentence LSTM, thus improving their model's performance. Notice that all these works are examples of multitask learning too. Three papers [76,98,157] used regression losses. Two of them [76,157] included a bounding box regression loss as part of Faster R-CNN [112] training, and Moradi et al. [98] included a regression loss to minimize the Euclidean distance between VGG and doc2vec embeddings. As previously discussed in section 5.2.5, another optimization strategy is the use of autoencoders for the self-supervised learning of text representations. In MTMA [132] an autoencoder was used to provide an auxiliary supervision over the BiLSTM and was jointly trained with the rest of the model in a multitask learning fashion. Spinks and Moens [126] instead trained an ARAE in a first stage, then froze its weights and used the learned text embedding to support the subsequent training of a GAN.

Lastly, three works used GANs [47,87,126]. As mentioned when discussing RL, Li et al. [47] used a GAN strategy to train their model for report generation, where the generative module generates a report and the discriminator determines whether it is real or fake. Similarly, Han et al. [87] proposed RGAN, where the generator outputs segmentation maps from spine radiographs and the discriminator determines if a given segmentation map is real or fake. Spinks and Moens [126] implemented a modified version of a StackGAN [159] to generate chest X-ray images from input text representations. In their case, they trained the GAN using two cycle-consistency [167] losses: (1) image − → embedding − → image and (2) embedding − → image − → embedding. In both cases, an auxiliary inverse mapping CNN was used to close the cycle.

Synthesis. Overall, we can observe that designing a model for report generation from medical images is a complex task that involves engineering decisions at multiple levels: inputs and outputs, visual component, language component, domain knowledge, auxiliary tasks and optimization strategies. In each of these dimensions there are different approaches adopted in the reviewed literature, and the current state of research does not allow us to recommend an ""optimal model design"", mainly for reasons we will discuss in the Metrics and Performance Comparison sections (5.4 and 5.5). Nevertheless, there are valuable insights in the literature that may lead to better results, and thus are worth having in mind. For example, the use of CNNs (such as DenseNet or ResNet) as visual component and training in auxiliary medical image tasks; the use of input text alongside the images; providing the language component with tag information in addition to the visual features (e.g. medical concepts identified in the image); leveraging template databases curated with domain knowledge; or the use of multitask learning combining multiple sources of supervision. Lastly, to improve report quality from a medical perspective, the use of reinforcement learning with adequate reward functions appears as the most promising approach.","[[], ['b77', 'b108', 'b28'], [None, 'b145'], ['b70', 'b62', 'b84', 'b66', 'b91', 'b150', 'b86', 'b139'], ['b89', 'b62', 'b160', 'b91'], ['b67', 'b85', 'b125', 'b131', 'b156', 'b20', 'b47', 'b143', 'b66', 'b162', 'b75', 'b93', 'b130', 'b154', 'b94'], ['b111', 'b125', 'b131', 'b156', 'b97', 'b23', 'b75', 'b154', 'b94'], ['b158', 'b125', 'b46', 'b166', 'b86'], [], [], ['b77', 'b108', 'b28'], [None, 'b145'], ['b70', 'b62', 'b84', 'b66', 'b91', 'b150', 'b86', 'b139'], ['b89', 'b62', 'b160', 'b91'], ['b67', 'b85', 'b125', 'b131', 'b156', 'b20', 'b47', 'b143', 'b66', 'b162', 'b75', 'b93', 'b130', 'b154', 'b94'], ['b111', 'b125', 'b131', 'b156', 'b97', 'b23', 'b75', 'b154', 'b94'], ['b158', 'b125', 'b46', 'b166', 'b86'], []]","[[], ['b77', 'b108', 'b28'], [None, 'b145'], ['b70', 'b62', 'b84', 'b66', 'b91', 'b150', 'b86', 'b139'], ['b89', 'b62', 'b160', 'b91'], ['b67', 'b85', 'b125', 'b131', 'b156', 'b20', 'b47', 'b143', 'b66', 'b162', 'b75', 'b93', 'b130', 'b154', 'b94'], ['b111', 'b125', 'b131', 'b156', 'b97', 'b23', 'b75', 'b154', 'b94'], ['b158', 'b125', 'b46', 'b166', 'b86'], [], [], ['b77', 'b108', 'b28'], [None, 'b145'], ['b70', 'b62', 'b84', 'b66', 'b91', 'b150', 'b86', 'b139'], ['b89', 'b62', 'b160', 'b91'], ['b67', 'b85', 'b125', 'b131', 'b156', 'b20', 'b47', 'b143', 'b66', 'b162', 'b75', 'b93', 'b130', 'b154', 'b94'], ['b111', 'b125', 'b131', 'b156', 'b97', 'b23', 'b75', 'b154', 'b94'], ['b158', 'b125', 'b46', 'b166', 'b86'], []]",92,"sent1: Strategies. In addition to the architecture and the tasks a model can perform, a very important aspect is the optimization strategy used to learn the model's parameters.
sent2: In this section we present an analysis of the optimization strategies used in the literature.
sent3: A summary of this section is presented in Table 11 in appendix 9.3.
sent4: Visual Component. We first analyze the visual component optimization, identifying three general optimization decisions.
sent5: The first one is whether to use a CNN from the literature with its weights pretrained in ImageNet [29].
sent6: This is a very common transfer learning practice from the computer vision literature in general [78], so it is natural to see it used in the medical domain too.
sent7: However, it has been shown that ImageNet pretraining may not transfer as well to medical image tasks as they normally do to other domains, due to very dissimilar image distributions [109].
sent8: Therefore, a very common second decision is whether or not to train/fine-tune the visual component with auxiliary medical image tasks, such as most of the classification and segmentation tasks discussed in the previous section (5.2.5).
sent9: The third decision is whether to freeze the visual component weights during report generation training or continue updating them in an end-to-end manner.
sent10: Report Generation. We identify two general optimization strategies in the literature: Teacherforcing (TF) and Reinforcement Learning (RL).
sent11: Teacher-forcing [146] is by far the most common, as it is adopted by 32 papers [7, 16, 36, 38, 40, 44, 48, 51, 61, 67, 68, 86, 87, 89, 95, 120, 123, 126, 128, 131, 132, 144, 148, 150, 153-158, 162, 163].
sent12: The basic idea in teacher-forcing is to train a model to predict each word of the report conditioned on the previous words, therefore learning to imitate the ground truth word by word.
sent13: The model typically has a softmax layer that predicts the next word, and cross entropy is the loss function of choice to measure the error and compute gradients for backpropagation.
sent14: We think teacher-forcing is so widespread in the literature because of its simplicity and general applicability, as it is agnostic to the application domain (whether it be report generation in medicine or captioning of everyday images).
sent15: In contrast, 5 works [67,85,87,92,151] explored the use of reinforcement learning (RL) [71].
sent16: The main reason to use RL is the flexibility it offers to optimize non-differentiable reward functions, allowing researchers to be more creative and explore new rewards that may guide the model's learning toward domain-specific goals of interest.
sent17: For example, Liu et al. [92] used RL to train their model to optimize the weighted sum of two rewards: (1) a natural language reward (CIDEr [140]) and (2) a Clinically Coherent Reward (CCR), where the latter was proposed to measure the clinical accuracy of a generated report compared to a ground-truth reference using the CheXpert labeler tool [63].
sent18: Their goal was to equip their model with two skills: natural language fluency (encouraged by CIDEr) and clinical accuracy (encouraged by CCR).
sent19: Other examples of the use of RL are: the direct optimization of CIDEr [85,151], particularly in the training of a complicated hybrid template-retrieval and text generation model [85]; directly optimizing BLEU-4 after a previous teacher-forcing warmup phase [67]; and the training of the generator network of a GAN used for report generation, where the reward is provided by the discriminator network [87].
sent20: As a side note, we would like to highlight the work by Zhang et al. [161] on medical report summarization (a related task where the report is the input and with no images), illustrating how RL can be used in this setting to optimize both fluency and factual correctness.
sent21: As rewards they used ROUGE [90] and a Factual Correctness reward based on the CheXpert labeler tool [63] (very similar to the CCR proposed by Liu et al. [92]).
sent22: This work is a good example of the benefits of RL over teacher-forcing for text generation in a medical domain.
sent23: The paper presents the results of a human evaluation with two board-certified radiologists and the model trained with RL achieved better results than the same model trained with teacher-forcing, and even slightly better results than the human baseline.
sent24: Other Losses or Training Strategies.
sent25: This category encompasses the remaining optimization strategies found in the literature.
sent26: The most important one is multitask learning [21], adopted by 14 papers [48,67,68,76,86,94,95,126,131,132,144,155,157,163].
sent27: The main idea is to jointly train a model in multiple complementary tasks, so that the model can learn robust parameters that perform well in all of them.
sent28: Some works [48,68,131,132,144,155,163] trained the visual and language components simultaneously in multiple tasks in an end-to-end manner, i.e. report generation plus other auxiliary tasks.
sent29: Other examples are the simultaneous training of object detection and attribute classification [76], diagnostic classification and cycle-consistency tasks [126], among others.
sent30: Most of these papers report benefits from training in this way.
sent31: As already discussed in section 5.2.5, two works [95,155] used auxiliary supervision on the attention weights of their models.
sent32: These auxiliary losses were jointly optimized with the rest of the model in report generation, effectively having a regularizer effect.
sent33: Yin et al. [155] are also the only ones that included an auxiliary contrastive loss [24] to provide a direct supervision to the Sentence LSTM, thus improving their model's performance.
sent34: Notice that all these works are examples of multitask learning too.
sent35: Three papers [76,98,157] used regression losses.
sent36: Two of them [76,157] included a bounding box regression loss as part of Faster R-CNN [112] training, and Moradi et al. [98] included a regression loss to minimize the Euclidean distance between VGG and doc2vec embeddings.
sent37: As previously discussed in section 5.2.5, another optimization strategy is the use of autoencoders for the self-supervised learning of text representations.
sent38: In MTMA [132] an autoencoder was used to provide an auxiliary supervision over the BiLSTM and was jointly trained with the rest of the model in a multitask learning fashion.
sent39: Spinks and Moens [126] instead trained an ARAE in a first stage, then froze its weights and used the learned text embedding to support the subsequent training of a GAN.
sent40: Lastly, three works used GANs [47,87,126].
sent41: As mentioned when discussing RL, Li et al. [47] used a GAN strategy to train their model for report generation, where the generative module generates a report and the discriminator determines whether it is real or fake.
sent42: Similarly, Han et al. [87] proposed RGAN, where the generator outputs segmentation maps from spine radiographs and the discriminator determines if a given segmentation map is real or fake.
sent43: Spinks and Moens [126] implemented a modified version of a StackGAN [159] to generate chest X-ray images from input text representations.
sent44: In their case, they trained the GAN using two cycle-consistency [167] losses: (1) image − → embedding − → image and (2) embedding − → image − → embedding.
sent45: In both cases, an auxiliary inverse mapping CNN was used to close the cycle.
sent46: Synthesis. Overall, we can observe that designing a model for report generation from medical images is a complex task that involves engineering decisions at multiple levels: inputs and outputs, visual component, language component, domain knowledge, auxiliary tasks and optimization strategies.
sent47: In each of these dimensions there are different approaches adopted in the reviewed literature, and the current state of research does not allow us to recommend an ""optimal model design"", mainly for reasons we will discuss in the Metrics and Performance Comparison sections (5.4 and 5.5).
sent48: Nevertheless, there are valuable insights in the literature that may lead to better results, and thus are worth having in mind.
sent49: For example, the use of CNNs (such as DenseNet or ResNet) as visual component and training in auxiliary medical image tasks; the use of input text alongside the images; providing the language component with tag information in addition to the visual features (e.g. medical concepts identified in the image); leveraging template databases curated with domain knowledge; or the use of multitask learning combining multiple sources of supervision.
sent50: Lastly, to improve report quality from a medical perspective, the use of reinforcement learning with adequate reward functions appears as the most promising approach.
sent51: Strategies. In addition to the architecture and the tasks a model can perform, a very important aspect is the optimization strategy used to learn the model's parameters.
sent52: In this section we present an analysis of the optimization strategies used in the literature.
sent53: A summary of this section is presented in Table 11 in appendix 9.3.
sent54: Visual Component. We first analyze the visual component optimization, identifying three general optimization decisions.
sent55: The first one is whether to use a CNN from the literature with its weights pretrained in ImageNet [29].
sent56: This is a very common transfer learning practice from the computer vision literature in general [78], so it is natural to see it used in the medical domain too.
sent57: However, it has been shown that ImageNet pretraining may not transfer as well to medical image tasks as they normally do to other domains, due to very dissimilar image distributions [109].
sent58: Therefore, a very common second decision is whether or not to train/fine-tune the visual component with auxiliary medical image tasks, such as most of the classification and segmentation tasks discussed in the previous section (5.2.5).
sent59: The third decision is whether to freeze the visual component weights during report generation training or continue updating them in an end-to-end manner.
sent60: Report Generation. We identify two general optimization strategies in the literature: Teacherforcing (TF) and Reinforcement Learning (RL).
sent61: Teacher-forcing [146] is by far the most common, as it is adopted by 32 papers [7, 16, 36, 38, 40, 44, 48, 51, 61, 67, 68, 86, 87, 89, 95, 120, 123, 126, 128, 131, 132, 144, 148, 150, 153-158, 162, 163].
sent62: The basic idea in teacher-forcing is to train a model to predict each word of the report conditioned on the previous words, therefore learning to imitate the ground truth word by word.
sent63: The model typically has a softmax layer that predicts the next word, and cross entropy is the loss function of choice to measure the error and compute gradients for backpropagation.
sent64: We think teacher-forcing is so widespread in the literature because of its simplicity and general applicability, as it is agnostic to the application domain (whether it be report generation in medicine or captioning of everyday images).
sent65: In contrast, 5 works [67,85,87,92,151] explored the use of reinforcement learning (RL) [71].
sent66: The main reason to use RL is the flexibility it offers to optimize non-differentiable reward functions, allowing researchers to be more creative and explore new rewards that may guide the model's learning toward domain-specific goals of interest.
sent67: For example, Liu et al. [92] used RL to train their model to optimize the weighted sum of two rewards: (1) a natural language reward (CIDEr [140]) and (2) a Clinically Coherent Reward (CCR), where the latter was proposed to measure the clinical accuracy of a generated report compared to a ground-truth reference using the CheXpert labeler tool [63].
sent68: Their goal was to equip their model with two skills: natural language fluency (encouraged by CIDEr) and clinical accuracy (encouraged by CCR).
sent69: Other examples of the use of RL are: the direct optimization of CIDEr [85,151], particularly in the training of a complicated hybrid template-retrieval and text generation model [85]; directly optimizing BLEU-4 after a previous teacher-forcing warmup phase [67]; and the training of the generator network of a GAN used for report generation, where the reward is provided by the discriminator network [87].
sent70: As a side note, we would like to highlight the work by Zhang et al. [161] on medical report summarization (a related task where the report is the input and with no images), illustrating how RL can be used in this setting to optimize both fluency and factual correctness.
sent71: As rewards they used ROUGE [90] and a Factual Correctness reward based on the CheXpert labeler tool [63] (very similar to the CCR proposed by Liu et al. [92]).
sent72: This work is a good example of the benefits of RL over teacher-forcing for text generation in a medical domain.
sent73: The paper presents the results of a human evaluation with two board-certified radiologists and the model trained with RL achieved better results than the same model trained with teacher-forcing, and even slightly better results than the human baseline.
sent74: Other Losses or Training Strategies.
sent75: This category encompasses the remaining optimization strategies found in the literature.
sent76: The most important one is multitask learning [21], adopted by 14 papers [48,67,68,76,86,94,95,126,131,132,144,155,157,163].
sent77: The main idea is to jointly train a model in multiple complementary tasks, so that the model can learn robust parameters that perform well in all of them.
sent78: Some works [48,68,131,132,144,155,163] trained the visual and language components simultaneously in multiple tasks in an end-to-end manner, i.e. report generation plus other auxiliary tasks.
sent79: Other examples are the simultaneous training of object detection and attribute classification [76], diagnostic classification and cycle-consistency tasks [126], among others.
sent80: Most of these papers report benefits from training in this way.
sent81: As already discussed in section 5.2.5, two works [95,155] used auxiliary supervision on the attention weights of their models.
sent82: These auxiliary losses were jointly optimized with the rest of the model in report generation, effectively having a regularizer effect.
sent83: Yin et al. [155] are also the only ones that included an auxiliary contrastive loss [24] to provide a direct supervision to the Sentence LSTM, thus improving their model's performance.
sent84: Notice that all these works are examples of multitask learning too.
sent85: Three papers [76,98,157] used regression losses.
sent86: Two of them [76,157] included a bounding box regression loss as part of Faster R-CNN [112] training, and Moradi et al. [98] included a regression loss to minimize the Euclidean distance between VGG and doc2vec embeddings.
sent87: As previously discussed in section 5.2.5, another optimization strategy is the use of autoencoders for the self-supervised learning of text representations.
sent88: In MTMA [132] an autoencoder was used to provide an auxiliary supervision over the BiLSTM and was jointly trained with the rest of the model in a multitask learning fashion.
sent89: Spinks and Moens [126] instead trained an ARAE in a first stage, then froze its weights and used the learned text embedding to support the subsequent training of a GAN.
sent90: Lastly, three works used GANs [47,87,126].
sent91: As mentioned when discussing RL, Li et al. [47] used a GAN strategy to train their model for report generation, where the generative module generates a report and the discriminator determines whether it is real or fake.
sent92: Similarly, Han et al. [87] proposed RGAN, where the generator outputs segmentation maps from spine radiographs and the discriminator determines if a given segmentation map is real or fake.
sent93: Spinks and Moens [126] implemented a modified version of a StackGAN [159] to generate chest X-ray images from input text representations.
sent94: In their case, they trained the GAN using two cycle-consistency [167] losses: (1) image − → embedding − → image and (2) embedding − → image − → embedding.
sent95: In both cases, an auxiliary inverse mapping CNN was used to close the cycle.
sent96: Synthesis. Overall, we can observe that designing a model for report generation from medical images is a complex task that involves engineering decisions at multiple levels: inputs and outputs, visual component, language component, domain knowledge, auxiliary tasks and optimization strategies.
sent97: In each of these dimensions there are different approaches adopted in the reviewed literature, and the current state of research does not allow us to recommend an ""optimal model design"", mainly for reasons we will discuss in the Metrics and Performance Comparison sections (5.4 and 5.5).
sent98: Nevertheless, there are valuable insights in the literature that may lead to better results, and thus are worth having in mind.
sent99: For example, the use of CNNs (such as DenseNet or ResNet) as visual component and training in auxiliary medical image tasks; the use of input text alongside the images; providing the language component with tag information in addition to the visual features (e.g. medical concepts identified in the image); leveraging template databases curated with domain knowledge; or the use of multitask learning combining multiple sources of supervision.
sent100: Lastly, to improve report quality from a medical perspective, the use of reinforcement learning with adequate reward functions appears as the most promising approach."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s41,TASK OVERVIEW,"From a purely computational perspective, the following is the main task addressed by most articles analyzed in this survey: given as input one or more medical images of a patient, a text report is output that is as similar as possible to one generated by a radiologist. From a machine learning point of view, creating a system that performs such a task would require learning a generative model from instances of reports written by radiologists. Figure 1 presents one example of such a report, Manual tags: Calcified Granuloma/lung/upper lobe/right Automatic tags: Calcified granuloma Comparison: Chest radiographs XXXX. Indication: XXXX-year-old male, chest pain. Findings: The cardiomediastinal silhouette is within normal limits for size and contour. The lungs are normally inflated without evidence of focal airspace disease, pleural effusion, or pneumothorax. Stable calcified granuloma within the right upper lung. No acute bone abnormality. Impression: No acute cardiopulmonary process. Fig. 1. Example from the IU X-ray dataset, frontal and lateral chest x-rays from a patient, alongside the natural language report and the annotated tags. XXXX is used for anonimization of the report.

taken from the IU X-ray dataset [28]. We see two input X-ray images (frontal and lateral), and below them some annotations (Tags) -some manually annotated by a radiologist and others automatically annotated-, and on the right side the report with four different sections (comparison, indication, findings, and impression). If we consider the clinical workflow of generating a medical imaging report, several aspects should be taken into account before diving into a concrete implementation.

The first aspect is considering additional patient information in the process of report generation. Most of the time, the physician asking for medical imaging is the primary care physician or a medical specialist. This implies that when radiologists write a report, they generally have patientrelevant clinical information, usually provided in the section Indication as shown in Figure 1. Also, the Comparison section can provide information of a serial follow-up procedure, to evaluate the evolution of a patient over time (e.g., aneurysm, congenital heart disease). Then, one decision can be whether or not to use these Indication and Comparison data to generate the sections Findings, Impression, or both of them. Second, the model for report generation should consider the diversity on medical images as well as body regions and conditions. There are several types of medical images, such as X-rays, CT, MRI, PET and SPECT. This implies that a model for text report generation that deals with only one type of input medical image might not solve it for other types. Also, ideally, a model should be able to generate reports from different parts of the human anatomy and diverse medical conditions. To adequately achieve this task, different body regions must have a balanced and sizable training set. Many works surveyed in this article focus on one specific part of the body and particular illnesses which limits the applicability of these methods to generalize to all possible diagnosis tasks.

Lastly, even if an AI system has perfect report generation accuracy, we might wonder if we can trust a machine in such a critical domain. One of the reasons for preferring a radiologist rather than an automated, highly accurate AI system is the chance of understanding the rationale behind the findings and impressions. In this sense, explainable AI [46] is of great importance in securing their adoption in a clinical setting.

From a purely computational perspective, the following is the main task addressed by most articles analyzed in this survey: given as input one or more medical images of a patient, a text report is output that is as similar as possible to one generated by a radiologist. From a machine learning point of view, creating a system that performs such a task would require learning a generative model from instances of reports written by radiologists. Figure 1 presents one example of such a report, Manual tags: Calcified Granuloma/lung/upper lobe/right Automatic tags: Calcified granuloma Comparison: Chest radiographs XXXX. Indication: XXXX-year-old male, chest pain. Findings: The cardiomediastinal silhouette is within normal limits for size and contour. The lungs are normally inflated without evidence of focal airspace disease, pleural effusion, or pneumothorax. Stable calcified granuloma within the right upper lung. No acute bone abnormality. Impression: No acute cardiopulmonary process. Fig. 1. Example from the IU X-ray dataset, frontal and lateral chest x-rays from a patient, alongside the natural language report and the annotated tags. XXXX is used for anonimization of the report.

taken from the IU X-ray dataset [28]. We see two input X-ray images (frontal and lateral), and below them some annotations (Tags) -some manually annotated by a radiologist and others automatically annotated-, and on the right side the report with four different sections (comparison, indication, findings, and impression). If we consider the clinical workflow of generating a medical imaging report, several aspects should be taken into account before diving into a concrete implementation.

The first aspect is considering additional patient information in the process of report generation. Most of the time, the physician asking for medical imaging is the primary care physician or a medical specialist. This implies that when radiologists write a report, they generally have patientrelevant clinical information, usually provided in the section Indication as shown in Figure 1. Also, the Comparison section can provide information of a serial follow-up procedure, to evaluate the evolution of a patient over time (e.g., aneurysm, congenital heart disease). Then, one decision can be whether or not to use these Indication and Comparison data to generate the sections Findings, Impression, or both of them. Second, the model for report generation should consider the diversity on medical images as well as body regions and conditions. There are several types of medical images, such as X-rays, CT, MRI, PET and SPECT. This implies that a model for text report generation that deals with only one type of input medical image might not solve it for other types. Also, ideally, a model should be able to generate reports from different parts of the human anatomy and diverse medical conditions. To adequately achieve this task, different body regions must have a balanced and sizable training set. Many works surveyed in this article focus on one specific part of the body and particular illnesses which limits the applicability of these methods to generalize to all possible diagnosis tasks.

Lastly, even if an AI system has perfect report generation accuracy, we might wonder if we can trust a machine in such a critical domain. One of the reasons for preferring a radiologist rather than an automated, highly accurate AI system is the chance of understanding the rationale behind the findings and impressions. In this sense, explainable AI [46] is of great importance in securing their adoption in a clinical setting.","[[], ['b27'], [], ['b45'], [], ['b27'], [], ['b45']]","[[], ['b27'], [], ['b45'], [], ['b27'], [], ['b45']]",4,"sent1: From a purely computational perspective, the following is the main task addressed by most articles analyzed in this survey: given as input one or more medical images of a patient, a text report is output that is as similar as possible to one generated by a radiologist.
sent2: From a machine learning point of view, creating a system that performs such a task would require learning a generative model from instances of reports written by radiologists.
sent3: Figure 1 presents one example of such a report, Manual tags: Calcified Granuloma/lung/upper lobe/right Automatic tags: Calcified granuloma Comparison: Chest radiographs XXXX.
sent4: Indication: XXXX-year-old male, chest pain.
sent5: Findings: The cardiomediastinal silhouette is within normal limits for size and contour.
sent6: The lungs are normally inflated without evidence of focal airspace disease, pleural effusion, or pneumothorax.
sent7: Stable calcified granuloma within the right upper lung.
sent8: No acute bone abnormality. Impression: No acute cardiopulmonary process.
sent9: Fig. 1. Example from the IU X-ray dataset, frontal and lateral chest x-rays from a patient, alongside the natural language report and the annotated tags.
sent10: XXXX is used for anonimization of the report.
sent11: taken from the IU X-ray dataset [28].
sent12: We see two input X-ray images (frontal and lateral), and below them some annotations (Tags)
sent13: -some manually annotated by a radiologist and others automatically annotated-, and on the right side the report with four different sections (comparison, indication, findings, and impression).
sent14: If we consider the clinical workflow of generating a medical imaging report, several aspects should be taken into account before diving into a concrete implementation.
sent15: The first aspect is considering additional patient information in the process of report generation.
sent16: Most of the time, the physician asking for medical imaging is the primary care physician or a medical specialist.
sent17: This implies that when radiologists write a report, they generally have patientrelevant clinical information, usually provided in the section Indication as shown in Figure 1.
sent18: Also, the Comparison section can provide information of a serial follow-up procedure, to evaluate the evolution of a patient over time (e.g., aneurysm, congenital heart disease).
sent19: Then, one decision can be whether or not to use these Indication and Comparison data to generate the sections Findings, Impression, or both of them.
sent20: Second, the model for report generation should consider the diversity on medical images as well as body regions and conditions.
sent21: There are several types of medical images, such as X-rays, CT, MRI, PET and SPECT.
sent22: This implies that a model for text report generation that deals with only one type of input medical image might not solve it for other types.
sent23: Also, ideally, a model should be able to generate reports from different parts of the human anatomy and diverse medical conditions.
sent24: To adequately achieve this task, different body regions must have a balanced and sizable training set.
sent25: Many works surveyed in this article focus on one specific part of the body and particular illnesses which limits the applicability of these methods to generalize to all possible diagnosis tasks.
sent26: Lastly, even if an AI system has perfect report generation accuracy, we might wonder if we can trust a machine in such a critical domain.
sent27: One of the reasons for preferring a radiologist rather than an automated, highly accurate AI system is the chance of understanding the rationale behind the findings and impressions.
sent28: In this sense, explainable AI [46] is of great importance in securing their adoption in a clinical setting.
sent29: From a purely computational perspective, the following is the main task addressed by most articles analyzed in this survey: given as input one or more medical images of a patient, a text report is output that is as similar as possible to one generated by a radiologist.
sent30: From a machine learning point of view, creating a system that performs such a task would require learning a generative model from instances of reports written by radiologists.
sent31: Figure 1 presents one example of such a report, Manual tags: Calcified Granuloma/lung/upper lobe/right Automatic tags: Calcified granuloma Comparison: Chest radiographs XXXX.
sent32: Indication: XXXX-year-old male, chest pain.
sent33: Findings: The cardiomediastinal silhouette is within normal limits for size and contour.
sent34: The lungs are normally inflated without evidence of focal airspace disease, pleural effusion, or pneumothorax.
sent35: Stable calcified granuloma within the right upper lung.
sent36: No acute bone abnormality. Impression: No acute cardiopulmonary process.
sent37: Fig. 1. Example from the IU X-ray dataset, frontal and lateral chest x-rays from a patient, alongside the natural language report and the annotated tags.
sent38: XXXX is used for anonimization of the report.
sent39: taken from the IU X-ray dataset [28].
sent40: We see two input X-ray images (frontal and lateral), and below them some annotations (Tags)
sent41: -some manually annotated by a radiologist and others automatically annotated-, and on the right side the report with four different sections (comparison, indication, findings, and impression).
sent42: If we consider the clinical workflow of generating a medical imaging report, several aspects should be taken into account before diving into a concrete implementation.
sent43: The first aspect is considering additional patient information in the process of report generation.
sent44: Most of the time, the physician asking for medical imaging is the primary care physician or a medical specialist.
sent45: This implies that when radiologists write a report, they generally have patientrelevant clinical information, usually provided in the section Indication as shown in Figure 1.
sent46: Also, the Comparison section can provide information of a serial follow-up procedure, to evaluate the evolution of a patient over time (e.g., aneurysm, congenital heart disease).
sent47: Then, one decision can be whether or not to use these Indication and Comparison data to generate the sections Findings, Impression, or both of them.
sent48: Second, the model for report generation should consider the diversity on medical images as well as body regions and conditions.
sent49: There are several types of medical images, such as X-rays, CT, MRI, PET and SPECT.
sent50: This implies that a model for text report generation that deals with only one type of input medical image might not solve it for other types.
sent51: Also, ideally, a model should be able to generate reports from different parts of the human anatomy and diverse medical conditions.
sent52: To adequately achieve this task, different body regions must have a balanced and sizable training set.
sent53: Many works surveyed in this article focus on one specific part of the body and particular illnesses which limits the applicability of these methods to generalize to all possible diagnosis tasks.
sent54: Lastly, even if an AI system has perfect report generation accuracy, we might wonder if we can trust a machine in such a critical domain.
sent55: One of the reasons for preferring a radiologist rather than an automated, highly accurate AI system is the chance of understanding the rationale behind the findings and impressions.
sent56: In this sense, explainable AI [46] is of great importance in securing their adoption in a clinical setting."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s57,Classification.,"As explained in the Auxiliary Tasks section (5.2.5), many deep learning architectures include multi-label classification to improve performance, providing a set of classified concepts as secondary output. Even though in most papers this kind of output is not presented as an explanation of the report, we consider that its nature could improve the transparency of the model, which is an important way of improving the interpretability in a medical context [134]. By providing this detection information from an intermediate step of the model's process, an expert could further understand the internal process, validate the decision with their domain knowledge and calibrate their trust in the system.

As shown in Table 3 from section 5.2.1, the terms classified are very diverse. Some works classify very broad concepts, such as body parts or organs [7,98,157,158], or image modality [51]. Other works perform a more specific classification, such as diseases or abnormalities [16,76,86,126,144,157,158,162], or a normal or abnormal status at sentence level [150]. Lastly, several works [44,48,68,120,128,132,155,156] classify over a subset of MeSH terms or similar, which may contain a mix of general broad medical concepts and specific abnormalities or conditions. We believe that this additional output would be useful for an expert, though the specific concepts should provide much richer information. If the classification is more specific, the user will be able to validate on a much narrower scope the system's performance.

As explained in the Auxiliary Tasks section (5.2.5), many deep learning architectures include multi-label classification to improve performance, providing a set of classified concepts as secondary output. Even though in most papers this kind of output is not presented as an explanation of the report, we consider that its nature could improve the transparency of the model, which is an important way of improving the interpretability in a medical context [134]. By providing this detection information from an intermediate step of the model's process, an expert could further understand the internal process, validate the decision with their domain knowledge and calibrate their trust in the system.

As shown in Table 3 from section 5.2.1, the terms classified are very diverse. Some works classify very broad concepts, such as body parts or organs [7,98,157,158], or image modality [51]. Other works perform a more specific classification, such as diseases or abnormalities [16,76,86,126,144,157,158,162], or a normal or abnormal status at sentence level [150]. Lastly, several works [44,48,68,120,128,132,155,156] classify over a subset of MeSH terms or similar, which may contain a mix of general broad medical concepts and specific abnormalities or conditions. We believe that this additional output would be useful for an expert, though the specific concepts should provide much richer information. If the classification is more specific, the user will be able to validate on a much narrower scope the system's performance.","[['b133'], ['b67', 'b85', 'b161', 'b156', 'b143', 'b97', 'b157', 'b155', 'b149', 'b6', 'b43', 'b50', 'b131', 'b75', 'b154', 'b15', 'b119', 'b125', 'b47', 'b127'], ['b133'], ['b67', 'b85', 'b161', 'b156', 'b143', 'b97', 'b157', 'b155', 'b149', 'b6', 'b43', 'b50', 'b131', 'b75', 'b154', 'b15', 'b119', 'b125', 'b47', 'b127']]","[['b133'], ['b67', 'b85', 'b161', 'b156', 'b143', 'b97', 'b157', 'b155', 'b149', 'b6', 'b43', 'b50', 'b131', 'b75', 'b154', 'b15', 'b119', 'b125', 'b47', 'b127'], ['b133'], ['b67', 'b85', 'b161', 'b156', 'b143', 'b97', 'b157', 'b155', 'b149', 'b6', 'b43', 'b50', 'b131', 'b75', 'b154', 'b15', 'b119', 'b125', 'b47', 'b127']]",42,"sent1: As explained in the Auxiliary Tasks section (5.2.5), many deep learning architectures include multi-label classification to improve performance, providing a set of classified concepts as secondary output.
sent2: Even though in most papers this kind of output is not presented as an explanation of the report, we consider that its nature could improve the transparency of the model, which is an important way of improving the interpretability in a medical context [134].
sent3: By providing this detection information from an intermediate step of the model's process, an expert could further understand the internal process, validate the decision with their domain knowledge and calibrate their trust in the system.
sent4: As shown in Table 3 from section 5.2.1, the terms classified are very diverse.
sent5: Some works classify very broad concepts, such as body parts or organs [7,98,157,158], or image modality [51].
sent6: Other works perform a more specific classification, such as diseases or abnormalities [16,76,86,126,144,157,158,162], or a normal or abnormal status at sentence level [150].
sent7: Lastly, several works [44,48,68,120,128,132,155,156] classify over a subset of MeSH terms or similar, which may contain a mix of general broad medical concepts and specific abnormalities or conditions.
sent8: We believe that this additional output would be useful for an expert, though the specific concepts should provide much richer information.
sent9: If the classification is more specific, the user will be able to validate on a much narrower scope the system's performance.
sent10: As explained in the Auxiliary Tasks section (5.2.5), many deep learning architectures include multi-label classification to improve performance, providing a set of classified concepts as secondary output.
sent11: Even though in most papers this kind of output is not presented as an explanation of the report, we consider that its nature could improve the transparency of the model, which is an important way of improving the interpretability in a medical context [134].
sent12: By providing this detection information from an intermediate step of the model's process, an expert could further understand the internal process, validate the decision with their domain knowledge and calibrate their trust in the system.
sent13: As shown in Table 3 from section 5.2.1, the terms classified are very diverse.
sent14: Some works classify very broad concepts, such as body parts or organs [7,98,157,158], or image modality [51].
sent15: Other works perform a more specific classification, such as diseases or abnormalities [16,76,86,126,144,157,158,162], or a normal or abnormal status at sentence level [150].
sent16: Lastly, several works [44,48,68,120,128,132,155,156] classify over a subset of MeSH terms or similar, which may contain a mix of general broad medical concepts and specific abnormalities or conditions.
sent17: We believe that this additional output would be useful for an expert, though the specific concepts should provide much richer information.
sent18: If the classification is more specific, the user will be able to validate on a much narrower scope the system's performance."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s58,Image heatmap.,"In the papers reviewed, there are three different approaches to generating heatmaps over the input image, each of them with a different interpretation. First, many architectures employ an attention mechanism over the image spatial features during the report generation, as it was discussed in the Language Component section (5.2.3). These mechanisms can be leveraged to produce a heatmap indicating the image regions that were most important to generate the report. In particular, some models provide a heatmap for each word [92,144,163], for each sentence [61,68,153,154], or for the whole report [86]. By showing these feature importance maps, an expert should be able to determine if the model is focusing on the correct regions of the image, which could improve their trust on the system. Second, some works use particular deep learning architectures to perform image segmentation, i.e. classification and localization at the same time. The model by Ma et al. [94] uses a CNN to classify the severity of four different key characteristics of cervical cancer, and then uses an attention mechanism over the visual spatial features to generate heatmaps indicating the position of each relevant property. Tian et al. [131] used an FCN to classify each pixel of an image with the presence of a liver or tumor, and the result is averaged with an attention map to further improve localization. Han et al. [47] proposed the ACAE module (see section 5.2.3 for details), which is used to classify at pixel level different parts of the spine (vertebrae, discs or neural foramina), and if they show an abnormality or not. Kisilev et al. [76] and Spinks and Moens [126] used a Faster R-CNN [112] architecture to detect image regions with lesion and body parts of interest.

Lastly, some works use gradient-or activation-based methods for CNNs to generate a saliency map indicating the regions of most importance for a classification, such as CAM [165], Grad-CAM [118], SmoothGrad [124], or the one proposed by Zagoruyko and Komodakis [77]. Refer to Table 3 in the Input and Output section (5.2.1) for a list of the papers using each technique. To determine Cardiomegaly Pneumothorax Fig. 4. Examples from the ChestX-ray14 dataset [143] classified with a CNN based on ResNet-50 [52], and using CAM [165] to provide a heatmap indicating the the spatial regions of most importance as local explanation. The left example presents Cardiomegaly and the right Pneumothorax, and both samples were correctly classified by the CNN. Red boxes represent a localization of the condition annotated by an expert.

which of these methods performs better in a general setting, Adebayo et al. [3] performed multiple evaluations (""sanity checks"") over Grad-CAM, SmoothGrad, and other similar methods, and showed that Grad-CAM should be more reliable in terms of correlation with the input images and the classification made. As an example of these techniques, Figure 4 shows two chest X-rays from the ChestX-ray14 dataset [143] with a heatmap generated with CAM, plus an expert-annotated bounding box locating the abnormality (provided with the dataset).

In both segmentation and saliency map methods, the heatmap information provides much richer information than classification alone, as it also includes the location of an specific concept, such as an abnormality or a body part. Providing this type of explanation should allow an expert to assess the localization capabilities of the model and the system accuracy, thus improving the model's transparency throughout its process.

In the papers reviewed, there are three different approaches to generating heatmaps over the input image, each of them with a different interpretation. First, many architectures employ an attention mechanism over the image spatial features during the report generation, as it was discussed in the Language Component section (5.2.3). These mechanisms can be leveraged to produce a heatmap indicating the image regions that were most important to generate the report. In particular, some models provide a heatmap for each word [92,144,163], for each sentence [61,68,153,154], or for the whole report [86]. By showing these feature importance maps, an expert should be able to determine if the model is focusing on the correct regions of the image, which could improve their trust on the system. Second, some works use particular deep learning architectures to perform image segmentation, i.e. classification and localization at the same time. The model by Ma et al. [94] uses a CNN to classify the severity of four different key characteristics of cervical cancer, and then uses an attention mechanism over the visual spatial features to generate heatmaps indicating the position of each relevant property. Tian et al. [131] used an FCN to classify each pixel of an image with the presence of a liver or tumor, and the result is averaged with an attention map to further improve localization. Han et al. [47] proposed the ACAE module (see section 5.2.3 for details), which is used to classify at pixel level different parts of the spine (vertebrae, discs or neural foramina), and if they show an abnormality or not. Kisilev et al. [76] and Spinks and Moens [126] used a Faster R-CNN [112] architecture to detect image regions with lesion and body parts of interest.

Lastly, some works use gradient-or activation-based methods for CNNs to generate a saliency map indicating the regions of most importance for a classification, such as CAM [165], Grad-CAM [118], SmoothGrad [124], or the one proposed by Zagoruyko and Komodakis [77]. Refer to Table 3 in the Input and Output section (5.2.1) for a list of the papers using each technique. To determine Cardiomegaly Pneumothorax Fig. 4. Examples from the ChestX-ray14 dataset [143] classified with a CNN based on ResNet-50 [52], and using CAM [165] to provide a heatmap indicating the the spatial regions of most importance as local explanation. The left example presents Cardiomegaly and the right Pneumothorax, and both samples were correctly classified by the CNN. Red boxes represent a localization of the condition annotated by an expert.

which of these methods performs better in a general setting, Adebayo et al. [3] performed multiple evaluations (""sanity checks"") over Grad-CAM, SmoothGrad, and other similar methods, and showed that Grad-CAM should be more reliable in terms of correlation with the input images and the classification made. As an example of these techniques, Figure 4 shows two chest X-rays from the ChestX-ray14 dataset [143] with a heatmap generated with CAM, plus an expert-annotated bounding box locating the abnormality (provided with the dataset).

In both segmentation and saliency map methods, the heatmap information provides much richer information than classification alone, as it also includes the location of an specific concept, such as an abnormality or a body part. Providing this type of explanation should allow an expert to assess the localization capabilities of the model and the system accuracy, thus improving the model's transparency throughout its process.","[['b67', 'b153', 'b85', 'b111', 'b125', 'b60', 'b143', 'b46', 'b75', 'b91', 'b162', 'b152', 'b93', 'b130'], ['b142', 'b51', 'b76', 'b123', 'b117', 'b164'], ['b142', 'b2'], [], ['b67', 'b153', 'b85', 'b111', 'b125', 'b60', 'b143', 'b46', 'b75', 'b91', 'b162', 'b152', 'b93', 'b130'], ['b142', 'b51', 'b76', 'b123', 'b117', 'b164'], ['b142', 'b2'], []]","[['b67', 'b153', 'b85', 'b111', 'b125', 'b60', 'b143', 'b46', 'b75', 'b91', 'b162', 'b152', 'b93', 'b130'], ['b142', 'b51', 'b76', 'b123', 'b117', 'b164'], ['b142', 'b2'], [], ['b67', 'b153', 'b85', 'b111', 'b125', 'b60', 'b143', 'b46', 'b75', 'b91', 'b162', 'b152', 'b93', 'b130'], ['b142', 'b51', 'b76', 'b123', 'b117', 'b164'], ['b142', 'b2'], []]",44,"sent1: In the papers reviewed, there are three different approaches to generating heatmaps over the input image, each of them with a different interpretation.
sent2: First, many architectures employ an attention mechanism over the image spatial features during the report generation, as it was discussed in the Language Component section (5.2.3).
sent3: These mechanisms can be leveraged to produce a heatmap indicating the image regions that were most important to generate the report.
sent4: In particular, some models provide a heatmap for each word [92,144,163], for each sentence [61,68,153,154], or for the whole report [86].
sent5: By showing these feature importance maps, an expert should be able to determine if the model is focusing on the correct regions of the image, which could improve their trust on the system.
sent6: Second, some works use particular deep learning architectures to perform image segmentation, i.e. classification and localization at the same time.
sent7: The model by Ma et al. [94] uses a CNN to classify the severity of four different key characteristics of cervical cancer, and then uses an attention mechanism over the visual spatial features to generate heatmaps indicating the position of each relevant property.
sent8: Tian et al. [131] used an FCN to classify each pixel of an image with the presence of a liver or tumor, and the result is averaged with an attention map to further improve localization.
sent9: Han et al. [47] proposed the ACAE module (see section 5.2.3 for details), which is used to classify at pixel level different parts of the spine (vertebrae, discs or neural foramina), and if they show an abnormality or not.
sent10: Kisilev et al. [76] and Spinks and Moens [126] used a Faster R-CNN [112] architecture to detect image regions with lesion and body parts of interest.
sent11: Lastly, some works use gradient-or activation-based methods for CNNs to generate a saliency map indicating the regions of most importance for a classification, such as CAM [165], Grad-CAM [118], SmoothGrad [124], or the one proposed by Zagoruyko and Komodakis [77].
sent12: Refer to Table 3 in the Input and Output section (5.2.1) for a list of the papers using each technique.
sent13: To determine Cardiomegaly Pneumothorax Fig. 4.
sent14: Examples from the ChestX-ray14 dataset [143] classified with a CNN based on ResNet-50 [52], and using CAM [165] to provide a heatmap indicating the the spatial regions of most importance as local explanation.
sent15: The left example presents Cardiomegaly and the right Pneumothorax, and both samples were correctly classified by the CNN.
sent16: Red boxes represent a localization of the condition annotated by an expert.which of these methods performs better in a general setting, Adebayo et al. [3] performed multiple evaluations (""sanity checks"") over Grad-CAM, SmoothGrad, and other similar methods, and showed that Grad-CAM should be more reliable in terms of correlation with the input images and the classification made.
sent17: As an example of these techniques, Figure 4 shows two chest X-rays from the ChestX-ray14 dataset [143] with a heatmap generated with CAM, plus an expert-annotated bounding box locating the abnormality (provided with the dataset).
sent18: In both segmentation and saliency map methods, the heatmap information provides much richer information than classification alone, as it also includes the location of an specific concept, such as an abnormality or a body part.
sent19: Providing this type of explanation should allow an expert to assess the localization capabilities of the model and the system accuracy, thus improving the model's transparency throughout its process.
sent20: In the papers reviewed, there are three different approaches to generating heatmaps over the input image, each of them with a different interpretation.
sent21: First, many architectures employ an attention mechanism over the image spatial features during the report generation, as it was discussed in the Language Component section (5.2.3).
sent22: These mechanisms can be leveraged to produce a heatmap indicating the image regions that were most important to generate the report.
sent23: In particular, some models provide a heatmap for each word [92,144,163], for each sentence [61,68,153,154], or for the whole report [86].
sent24: By showing these feature importance maps, an expert should be able to determine if the model is focusing on the correct regions of the image, which could improve their trust on the system.
sent25: Second, some works use particular deep learning architectures to perform image segmentation, i.e. classification and localization at the same time.
sent26: The model by Ma et al. [94] uses a CNN to classify the severity of four different key characteristics of cervical cancer, and then uses an attention mechanism over the visual spatial features to generate heatmaps indicating the position of each relevant property.
sent27: Tian et al. [131] used an FCN to classify each pixel of an image with the presence of a liver or tumor, and the result is averaged with an attention map to further improve localization.
sent28: Han et al. [47] proposed the ACAE module (see section 5.2.3 for details), which is used to classify at pixel level different parts of the spine (vertebrae, discs or neural foramina), and if they show an abnormality or not.
sent29: Kisilev et al. [76] and Spinks and Moens [126] used a Faster R-CNN [112] architecture to detect image regions with lesion and body parts of interest.
sent30: Lastly, some works use gradient-or activation-based methods for CNNs to generate a saliency map indicating the regions of most importance for a classification, such as CAM [165], Grad-CAM [118], SmoothGrad [124], or the one proposed by Zagoruyko and Komodakis [77].
sent31: Refer to Table 3 in the Input and Output section (5.2.1) for a list of the papers using each technique.
sent32: To determine Cardiomegaly Pneumothorax Fig. 4.
sent33: Examples from the ChestX-ray14 dataset [143] classified with a CNN based on ResNet-50 [52], and using CAM [165] to provide a heatmap indicating the the spatial regions of most importance as local explanation.
sent34: The left example presents Cardiomegaly and the right Pneumothorax, and both samples were correctly classified by the CNN.
sent35: Red boxes represent a localization of the condition annotated by an expert.which of these methods performs better in a general setting, Adebayo et al. [3] performed multiple evaluations (""sanity checks"") over Grad-CAM, SmoothGrad, and other similar methods, and showed that Grad-CAM should be more reliable in terms of correlation with the input images and the classification made.
sent36: As an example of these techniques, Figure 4 shows two chest X-rays from the ChestX-ray14 dataset [143] with a heatmap generated with CAM, plus an expert-annotated bounding box locating the abnormality (provided with the dataset).
sent37: In both segmentation and saliency map methods, the heatmap information provides much richer information than classification alone, as it also includes the location of an specific concept, such as an abnormality or a body part.
sent38: Providing this type of explanation should allow an expert to assess the localization capabilities of the model and the system accuracy, thus improving the model's transparency throughout its process."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s59,Text heatmap.,"The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].

The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient. In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output. With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.

Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example. However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3. Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works. Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].","[['b60'], ['b133'], ['b60'], ['b133']]","[['b60'], ['b133'], ['b60'], ['b133']]",4,"sent1: The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient.
sent2: In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output.
sent3: With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.
sent4: Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example.
sent5: However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3.
sent6: Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works.
sent7: Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134].
sent8: The model proposed by Huang et al. [61] also receives text as input, which indicates the reason for performing the imaging study on the patient.
sent9: In a similar fashion to the input image cases, the architecture includes an attention mechanism over the input text, which provides a heatmap indicating the input phrases or sentences that were most relevant to generate each word in the output.
sent10: With this feature importance map an expert should be able to determine if the model is focusing on the correct words in the input text.
sent11: Synthesis. All the explainability approaches are local explanations given by a secondary output, either indicating feature importance (image and text heatmap), increasing the model's transparency (classification) or providing a counter-factual example.
sent12: However, in most of the works the authors do not explicitly mention it as an interpretability improvement, and in almost all cases there is no formal evaluation, as will be discussed in subsection 5.4.3.
sent13: Hence, we believe this is an understudied aspect of the medical report generation task, given the superficial or nonexistent analysis it receives in most of the reviewed works.
sent14: Additionally, counter-factual techniques could be further studied, and other approaches not found in the literature could be explored, such as prediction uncertainty or global explanations, which may be quite relevant for clinicians [134]."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s61,Text quality metrics.,"The methods in this category measure general quality aspects of the generated text, and are originated from translation, summarizing or captioning tasks. The most widely used metrics in the papers reviewed are BLEU [102], ROUGE-L [90], METEOR [12,82] and CIDEr [140], which measure the similarity of a target text (also referred to as candidate), against one or more reference texts (ground truth). These metrics are mainly based on counting n-gram matchings between the candidate and the ground truth. BLEU is precision-oriented, ROUGE-L and METEOR are F1-scores that can be biased towards precision or recall with a given parameter, and CIDEr attempts to capture both precision and recall through a TF-IDF score. Most of these metrics have variants and parameters for their calculation: ROUGE is a set of multiple metrics, being ROUGE-L the only one used in this task; METEOR has variants presented by the same authors [30][31][32]; and CIDEr was presented with the CIDEr-D variant to prevent gameability effects. SPICE [9] is a metric designed for the image captioning task, and evaluates the underlying meaning of the sentences describing the image scene, partially disregarding fluency or grammatical aspects. Specifically, the text is parsed as a graph, capturing the objects, their described characteristics and relations, which are then measured against the ground truth using an F1-score. Even though SPICE attempts to assess the semantic information in a caption, we believe it is not suitable for medical reports, as the graph parsing is designed for general domain objects. Nonetheless, Zhang et al. [162] presented the medical correctness metric MIRQI, applying a similar idea in a specific medical domain, which we will discuss in the next subsection (5.4.2).

Besides standard captioning metrics, we identified two other approaches to measure text quality. First, Alsharid et al. [7] used Grammar Bot 3 , a rule and statistics based automated system that counts the grammatical errors in sentences. Second, Harzig et al. 2019a [48] measured the sentence variability, by counting the different sentences in a set of reports. They argue that the sentences indicating abnormalities occur very rarely in the dataset, while the ones indicating normality are the most frequent. Hence, a certain level of variability is desired, and a system generating reports with low variability may indicate that not all medical conditions are being captured.

Lastly, both works from Li et al. [85,86] performed human evaluation with non-expert users via Amazon Mechanical Turk (AMT), following the same procedure. The authors presented two reports to the AMT participants, one generated with the proposed model and one generated with the CoAtt model [68] as baseline, and asked them to choose the most similar with the ground truth in terms of fluency, abnormalities correctness and content coverage. The results shown that their report was preferred around 50-60% of the cases, while the baseline around 20-30% (for the rest, none or both were preferred). We categorize this evaluation as a text quality metric, as the participants are not experts, and their answers are not fine-grained (i.e., did not specify what failed: fluency, correctness or coverage; or by how much they failed). 5.4.2 Medical correctness metrics. While the most common purpose of the text quality metrics is to measure the similarity between the generated report and a ground truth, they do not necessarily capture the medical facts in the reports [11,17,92,107,108,161]. For example, the sentences ""effusion is observed"" and ""effusion is not observed"" are very similar, thus may present a very high score for any metric based on n-gram matching, though the medical facts are the exact opposite. Therefore, an evaluation directly measuring the reports correctness is required, not necessarily taking into account fluency, grammatical rules or text quality in general. From the literature reviewed, in ten works [7,16,61,67,85,92,98,148,154,162] the authors presented an automatic metric to address this issue, four works [7,38,126,131] did a formal expert evaluation, and multiple works [49, 76, 86, 89, 94, 120, 126, 131, 144, 155-158, 162, 163] evaluated medical correctness indirectly from auxiliary tasks. The methods are listed in Table 6 and are further discussed next.

In several works the authors presented a method that detects concepts in the generated and ground truth reports, and compare the results using common classification metrics, such as accuracy, F1-score, and more. The main difference between these methods lies in how the concepts are automatically detected in the reports. The simplest approaches are keyword-based, which consists in reporting the ratio of a set of keywords found between the generated report and ground truth, like MeSH Accuracy [61] that uses MeSH terms, and Keyword Accuracy that uses 438 MTI terms (presented by Xue et al. [154] and used in A3FN [150]). Similarly, Medical Abnormality Terminology Detection [85] calculates precision and false positive rate of the 10 most frequent abnormalityrelated terms in the dataset; and Wu et al. [148] calculated accuracy, sensitivity and specificity for a set of keywords.

Other approaches are abnormality-based, which attempt to directly classify abnormalities from the report by different means: Abnormality Detection [67] uses manually designed patterns; Medical Abnormality Detection [92] uses the CheXpert labeler tool [63]; Biswal et al. [16] used a character-level CNN [160] that classifies multiple CheXpert labels [63]; and Moradi et al. [98] used a proprietary software to extract semantic descriptors. Lastly, Anatomical Relevance Score (ARS) [7] is a body-part-based approach, which detects the anatomical elements mentioned in a report considering the vocabulary used. Though these methods may be useful for measuring medical correctness to a certain degree, there is no consensus or standard, and there is no formal evaluation of the correlation with expert judgement. From the discussed techniques, Alsharid et al. [7] are the only authors that also performed an expert evaluation of the generated reports, though they did not conduct a correlation or similar analysis to validate the ARS method.

Zhang et al. [162] went further with the concept extraction and presented Medical Image Report Quality Index (MIRQI), which works in a similar fashion as the SPICE [9] metric presented in the text quality subsection (5.4.1). MIRQI applies ideas from NegBio [106] and the CheXpert labeler [63] to identify diseases or medical conditions in the reports, considering synonyms and negations, and uses the Stanford parser [23] to obtain semantic dependencies and finer-grained attributes from each sentence, such as severity, size, shape, body parts, etc. With this information, an abnormality graph is built for each report, where each node is a disease with its attributes, and the nodes are connected if they belong to the same organ or tissue. Lastly, the graphs from the ground truth and generated reports are matched node-wise, and MIRQI-p (precision), MIRQI-r (recall) and MIRQI-F1 (F1-score) are computed. Compared to the formerly discussed correctness metrics, we believe this approach seems more robust to assess the medical facts in the reports, as it attempts to capture the attributes and relations, opposed to the concepts only. However, the authors did not present an evaluation against expert judgement, so we cannot determine if this metric is sufficient.

Considering human evaluation, only a few works [7,38,126,131] present a formal expert medical correctness assessment. In the work by Alsharid et al. [7] a medical professional assessed the reports on a Likert Scale from 0 to 2 in four different aspects: accurately describes the image, presents no incorrect information, is grammatically correct and is relevant for the image; the results were further separated for samples from different body parts, showing averages between 0.5 and 1. Gale et al. [38] asked a radiologist to evaluate the correctness of the hip fractures description, finding that the fracture's character was properly described 98% of the cases, while the fracture location only for 90%. In the work by Tian et al. [131] a medical expert evaluated 30 randomly selected reports with a rating from 1 (definite accept) to 5 (definite reject), scoring an average of 2.33. Lastly, Spinks and Moens [126] asked four questions to three experts regarding the generated reports, where the third and fourth questions measured correctness: ""Do you agree with the proposed diagnosis?"", answering 0 (no) or 1 (yes) and ""How certain are you about your final diagnosis?"", from 1 (not sure) to 4 (very sure). The average scores were high (0.88 and 3.75), showing agreement with the model's diagnosis, and certainty on the experts' diagnoses. The other questions concerned explainability aspects, and are detailed in the next subsection (5.4.3). So far, there is no standard approach to perform an expert evaluation, though we believe the first two approaches provide finer-grained information than the latter two, and hence should be more useful for determining in which cases the models are failing and for designing improvements. The certainty question should also be very useful, as diagnoses may be susceptible to human judgement.

Lastly, multiple papers [49,86,89,94,120,131,144,155,156,158,162,163] evaluated the performance of the auxiliary tasks with ROC-AUC, accuracy and other typical classification or segmentation metrics, as shown in Table 6. Note that in any of these cases, the task is a previous or intermediary step of the process and is not derived from the report. In consequence, even if the classification has great performance, the language component could be performing poorly, and the generated reports still may be inaccurate. Accordingly, we believe this type of measure should not be used as the primary report correctness evaluation, unless it can be proven that the report reproduces exactly the classification made (e.g. by a template-filling process).

The methods in this category measure general quality aspects of the generated text, and are originated from translation, summarizing or captioning tasks. The most widely used metrics in the papers reviewed are BLEU [102], ROUGE-L [90], METEOR [12,82] and CIDEr [140], which measure the similarity of a target text (also referred to as candidate), against one or more reference texts (ground truth). These metrics are mainly based on counting n-gram matchings between the candidate and the ground truth. BLEU is precision-oriented, ROUGE-L and METEOR are F1-scores that can be biased towards precision or recall with a given parameter, and CIDEr attempts to capture both precision and recall through a TF-IDF score. Most of these metrics have variants and parameters for their calculation: ROUGE is a set of multiple metrics, being ROUGE-L the only one used in this task; METEOR has variants presented by the same authors [30][31][32]; and CIDEr was presented with the CIDEr-D variant to prevent gameability effects. SPICE [9] is a metric designed for the image captioning task, and evaluates the underlying meaning of the sentences describing the image scene, partially disregarding fluency or grammatical aspects. Specifically, the text is parsed as a graph, capturing the objects, their described characteristics and relations, which are then measured against the ground truth using an F1-score. Even though SPICE attempts to assess the semantic information in a caption, we believe it is not suitable for medical reports, as the graph parsing is designed for general domain objects. Nonetheless, Zhang et al. [162] presented the medical correctness metric MIRQI, applying a similar idea in a specific medical domain, which we will discuss in the next subsection (5.4.2).

Besides standard captioning metrics, we identified two other approaches to measure text quality. First, Alsharid et al. [7] used Grammar Bot 3 , a rule and statistics based automated system that counts the grammatical errors in sentences. Second, Harzig et al. 2019a [48] measured the sentence variability, by counting the different sentences in a set of reports. They argue that the sentences indicating abnormalities occur very rarely in the dataset, while the ones indicating normality are the most frequent. Hence, a certain level of variability is desired, and a system generating reports with low variability may indicate that not all medical conditions are being captured.

Lastly, both works from Li et al. [85,86] performed human evaluation with non-expert users via Amazon Mechanical Turk (AMT), following the same procedure. The authors presented two reports to the AMT participants, one generated with the proposed model and one generated with the CoAtt model [68] as baseline, and asked them to choose the most similar with the ground truth in terms of fluency, abnormalities correctness and content coverage. The results shown that their report was preferred around 50-60% of the cases, while the baseline around 20-30% (for the rest, none or both were preferred). We categorize this evaluation as a text quality metric, as the participants are not experts, and their answers are not fine-grained (i.e., did not specify what failed: fluency, correctness or coverage; or by how much they failed). 5.4.2 Medical correctness metrics. While the most common purpose of the text quality metrics is to measure the similarity between the generated report and a ground truth, they do not necessarily capture the medical facts in the reports [11,17,92,107,108,161]. For example, the sentences ""effusion is observed"" and ""effusion is not observed"" are very similar, thus may present a very high score for any metric based on n-gram matching, though the medical facts are the exact opposite. Therefore, an evaluation directly measuring the reports correctness is required, not necessarily taking into account fluency, grammatical rules or text quality in general. From the literature reviewed, in ten works [7,16,61,67,85,92,98,148,154,162] the authors presented an automatic metric to address this issue, four works [7,38,126,131] did a formal expert evaluation, and multiple works [49, 76, 86, 89, 94, 120, 126, 131, 144, 155-158, 162, 163] evaluated medical correctness indirectly from auxiliary tasks. The methods are listed in Table 6 and are further discussed next.

In several works the authors presented a method that detects concepts in the generated and ground truth reports, and compare the results using common classification metrics, such as accuracy, F1-score, and more. The main difference between these methods lies in how the concepts are automatically detected in the reports. The simplest approaches are keyword-based, which consists in reporting the ratio of a set of keywords found between the generated report and ground truth, like MeSH Accuracy [61] that uses MeSH terms, and Keyword Accuracy that uses 438 MTI terms (presented by Xue et al. [154] and used in A3FN [150]). Similarly, Medical Abnormality Terminology Detection [85] calculates precision and false positive rate of the 10 most frequent abnormalityrelated terms in the dataset; and Wu et al. [148] calculated accuracy, sensitivity and specificity for a set of keywords.

Other approaches are abnormality-based, which attempt to directly classify abnormalities from the report by different means: Abnormality Detection [67] uses manually designed patterns; Medical Abnormality Detection [92] uses the CheXpert labeler tool [63]; Biswal et al. [16] used a character-level CNN [160] that classifies multiple CheXpert labels [63]; and Moradi et al. [98] used a proprietary software to extract semantic descriptors. Lastly, Anatomical Relevance Score (ARS) [7] is a body-part-based approach, which detects the anatomical elements mentioned in a report considering the vocabulary used. Though these methods may be useful for measuring medical correctness to a certain degree, there is no consensus or standard, and there is no formal evaluation of the correlation with expert judgement. From the discussed techniques, Alsharid et al. [7] are the only authors that also performed an expert evaluation of the generated reports, though they did not conduct a correlation or similar analysis to validate the ARS method.

Zhang et al. [162] went further with the concept extraction and presented Medical Image Report Quality Index (MIRQI), which works in a similar fashion as the SPICE [9] metric presented in the text quality subsection (5.4.1). MIRQI applies ideas from NegBio [106] and the CheXpert labeler [63] to identify diseases or medical conditions in the reports, considering synonyms and negations, and uses the Stanford parser [23] to obtain semantic dependencies and finer-grained attributes from each sentence, such as severity, size, shape, body parts, etc. With this information, an abnormality graph is built for each report, where each node is a disease with its attributes, and the nodes are connected if they belong to the same organ or tissue. Lastly, the graphs from the ground truth and generated reports are matched node-wise, and MIRQI-p (precision), MIRQI-r (recall) and MIRQI-F1 (F1-score) are computed. Compared to the formerly discussed correctness metrics, we believe this approach seems more robust to assess the medical facts in the reports, as it attempts to capture the attributes and relations, opposed to the concepts only. However, the authors did not present an evaluation against expert judgement, so we cannot determine if this metric is sufficient.

Considering human evaluation, only a few works [7,38,126,131] present a formal expert medical correctness assessment. In the work by Alsharid et al. [7] a medical professional assessed the reports on a Likert Scale from 0 to 2 in four different aspects: accurately describes the image, presents no incorrect information, is grammatically correct and is relevant for the image; the results were further separated for samples from different body parts, showing averages between 0.5 and 1. Gale et al. [38] asked a radiologist to evaluate the correctness of the hip fractures description, finding that the fracture's character was properly described 98% of the cases, while the fracture location only for 90%. In the work by Tian et al. [131] a medical expert evaluated 30 randomly selected reports with a rating from 1 (definite accept) to 5 (definite reject), scoring an average of 2.33. Lastly, Spinks and Moens [126] asked four questions to three experts regarding the generated reports, where the third and fourth questions measured correctness: ""Do you agree with the proposed diagnosis?"", answering 0 (no) or 1 (yes) and ""How certain are you about your final diagnosis?"", from 1 (not sure) to 4 (very sure). The average scores were high (0.88 and 3.75), showing agreement with the model's diagnosis, and certainty on the experts' diagnoses. The other questions concerned explainability aspects, and are detailed in the next subsection (5.4.3). So far, there is no standard approach to perform an expert evaluation, though we believe the first two approaches provide finer-grained information than the latter two, and hence should be more useful for determining in which cases the models are failing and for designing improvements. The certainty question should also be very useful, as diagnoses may be susceptible to human judgement.

Lastly, multiple papers [49,86,89,94,120,131,144,155,156,158,162,163] evaluated the performance of the auxiliary tasks with ROC-AUC, accuracy and other typical classification or segmentation metrics, as shown in Table 6. Note that in any of these cases, the task is a previous or intermediary step of the process and is not derived from the report. In consequence, even if the classification has great performance, the language component could be performing poorly, and the generated reports still may be inaccurate. Accordingly, we believe this type of measure should not be used as the primary report correctness evaluation, unless it can be proven that the report reproduces exactly the classification made (e.g. by a template-filling process).","[['b29', 'b8', 'b161', 'b101', 'b81', 'b89', None, 'b31', 'b30', 'b11', 'b139'], ['b47', 'b6'], ['b67', 'b153', 'b85', 'b161', 'b97', 'b91', 'b147', 'b130', 'b84', 'b66', 'b106', 'b6', 'b16', 'b60', 'b37', 'b107', 'b15', 'b10', 'b125', None, 'b160'], ['b153', 'b60', 'b84', 'b147', 'b149'], ['b62', 'b159', 'b66', 'b97', 'b91', 'b15', 'b6'], ['b8', 'b105', 'b62', 'b161', 'b22'], ['b125', 'b37', None, 'b130', 'b6'], ['b88', 'b85', 'b119', 'b157', 'b161', 'b155', 'b143', 'b162', 'b48', 'b93', 'b130', 'b154'], ['b29', 'b8', 'b161', 'b101', 'b81', 'b89', None, 'b31', 'b30', 'b11', 'b139'], ['b47', 'b6'], ['b67', 'b153', 'b85', 'b161', 'b97', 'b91', 'b147', 'b130', 'b84', 'b66', 'b106', 'b6', 'b16', 'b60', 'b37', 'b107', 'b15', 'b10', 'b125', None, 'b160'], ['b153', 'b60', 'b84', 'b147', 'b149'], ['b62', 'b159', 'b66', 'b97', 'b91', 'b15', 'b6'], ['b8', 'b105', 'b62', 'b161', 'b22'], ['b125', 'b37', None, 'b130', 'b6'], ['b88', 'b85', 'b119', 'b157', 'b161', 'b155', 'b143', 'b162', 'b48', 'b93', 'b130', 'b154']]","[['b29', 'b8', 'b161', 'b101', 'b81', 'b89', None, 'b31', 'b30', 'b11', 'b139'], ['b47', 'b6'], ['b67', 'b153', 'b85', 'b161', 'b97', 'b91', 'b147', 'b130', 'b84', 'b66', 'b106', 'b6', 'b16', 'b60', 'b37', 'b107', 'b15', 'b10', 'b125', None, 'b160'], ['b153', 'b60', 'b84', 'b147', 'b149'], ['b62', 'b159', 'b66', 'b97', 'b91', 'b15', 'b6'], ['b8', 'b105', 'b62', 'b161', 'b22'], ['b125', 'b37', None, 'b130', 'b6'], ['b88', 'b85', 'b119', 'b157', 'b161', 'b155', 'b143', 'b162', 'b48', 'b93', 'b130', 'b154'], ['b29', 'b8', 'b161', 'b101', 'b81', 'b89', None, 'b31', 'b30', 'b11', 'b139'], ['b47', 'b6'], ['b67', 'b153', 'b85', 'b161', 'b97', 'b91', 'b147', 'b130', 'b84', 'b66', 'b106', 'b6', 'b16', 'b60', 'b37', 'b107', 'b15', 'b10', 'b125', None, 'b160'], ['b153', 'b60', 'b84', 'b147', 'b149'], ['b62', 'b159', 'b66', 'b97', 'b91', 'b15', 'b6'], ['b8', 'b105', 'b62', 'b161', 'b22'], ['b125', 'b37', None, 'b130', 'b6'], ['b88', 'b85', 'b119', 'b157', 'b161', 'b155', 'b143', 'b162', 'b48', 'b93', 'b130', 'b154']]",136,"sent1: The methods in this category measure general quality aspects of the generated text, and are originated from translation, summarizing or captioning tasks.
sent2: The most widely used metrics in the papers reviewed are BLEU [102], ROUGE-L [90], METEOR [12,82] and CIDEr [140], which measure the similarity of a target text (also referred to as candidate), against one or more reference texts (ground truth).
sent3: These metrics are mainly based on counting n-gram matchings between the candidate and the ground truth.
sent4: BLEU is precision-oriented, ROUGE-L and METEOR are F1-scores that can be biased towards precision or recall with a given parameter, and CIDEr attempts to capture both precision and recall through a TF-IDF score.
sent5: Most of these metrics have variants and parameters for their calculation: ROUGE is a set of multiple metrics, being ROUGE-L the only one used in this task; METEOR has variants presented by the same authors [30][31][32]; and CIDEr was presented with the CIDEr-D variant to prevent gameability effects.
sent6: SPICE [9] is a metric designed for the image captioning task, and evaluates the underlying meaning of the sentences describing the image scene, partially disregarding fluency or grammatical aspects.
sent7: Specifically, the text is parsed as a graph, capturing the objects, their described characteristics and relations, which are then measured against the ground truth using an F1-score.
sent8: Even though SPICE attempts to assess the semantic information in a caption, we believe it is not suitable for medical reports, as the graph parsing is designed for general domain objects.
sent9: Nonetheless, Zhang et al. [162] presented the medical correctness metric MIRQI, applying a similar idea in a specific medical domain, which we will discuss in the next subsection (5.4.2).
sent10: Besides standard captioning metrics, we identified two other approaches to measure text quality.
sent11: First, Alsharid et al. [7] used Grammar Bot 3 , a rule and statistics based automated system that counts the grammatical errors in sentences.
sent12: Second, Harzig et al. 2019a [48] measured the sentence variability, by counting the different sentences in a set of reports.
sent13: They argue that the sentences indicating abnormalities occur very rarely in the dataset, while the ones indicating normality are the most frequent.
sent14: Hence, a certain level of variability is desired, and a system generating reports with low variability may indicate that not all medical conditions are being captured.
sent15: Lastly, both works from Li et al. [85,86] performed human evaluation with non-expert users via Amazon Mechanical Turk (AMT), following the same procedure.
sent16: The authors presented two reports to the AMT participants, one generated with the proposed model and one generated with the CoAtt model [68] as baseline, and asked them to choose the most similar with the ground truth in terms of fluency, abnormalities correctness and content coverage.
sent17: The results shown that their report was preferred around 50-60% of the cases, while the baseline around 20-30% (for the rest, none or both were preferred).
sent18: We categorize this evaluation as a text quality metric, as the participants are not experts, and their answers are not fine-grained (i.e., did not specify what failed: fluency, correctness or coverage; or by how much they failed).
sent19: 5.4.2 Medical correctness metrics.
sent20: While the most common purpose of the text quality metrics is to measure the similarity between the generated report and a ground truth, they do not necessarily capture the medical facts in the reports [11,17,92,107,108,161].
sent21: For example, the sentences ""effusion is observed"" and ""effusion is not observed"" are very similar, thus may present a very high score for any metric based on n-gram matching, though the medical facts are the exact opposite.
sent22: Therefore, an evaluation directly measuring the reports correctness is required, not necessarily taking into account fluency, grammatical rules or text quality in general.
sent23: From the literature reviewed, in ten works [7,16,61,67,85,92,98,148,154,162] the authors presented an automatic metric to address this issue, four works [7,38,126,131] did a formal expert evaluation, and multiple works [49, 76, 86, 89, 94, 120, 126, 131, 144, 155-158, 162, 163] evaluated medical correctness indirectly from auxiliary tasks.
sent24: The methods are listed in Table 6 and are further discussed next.
sent25: In several works the authors presented a method that detects concepts in the generated and ground truth reports, and compare the results using common classification metrics, such as accuracy, F1-score, and more.
sent26: The main difference between these methods lies in how the concepts are automatically detected in the reports.
sent27: The simplest approaches are keyword-based, which consists in reporting the ratio of a set of keywords found between the generated report and ground truth, like MeSH Accuracy [61] that uses MeSH terms, and Keyword Accuracy that uses 438 MTI terms (presented by Xue et al. [154] and used in A3FN [150]).
sent28: Similarly, Medical Abnormality Terminology Detection [85] calculates precision and false positive rate of the 10 most frequent abnormalityrelated terms in the dataset; and Wu et al. [148] calculated accuracy, sensitivity and specificity for a set of keywords.
sent29: Other approaches are abnormality-based, which attempt to directly classify abnormalities from the report by different means: Abnormality Detection [67] uses manually designed patterns; Medical Abnormality Detection [92] uses the CheXpert labeler tool [63]; Biswal et al. [16] used a character-level CNN [160] that classifies multiple CheXpert labels [63]; and Moradi et al. [98] used a proprietary software to extract semantic descriptors.
sent30: Lastly, Anatomical Relevance Score (ARS) [7] is a body-part-based approach, which detects the anatomical elements mentioned in a report considering the vocabulary used.
sent31: Though these methods may be useful for measuring medical correctness to a certain degree, there is no consensus or standard, and there is no formal evaluation of the correlation with expert judgement.
sent32: From the discussed techniques, Alsharid et al. [7] are the only authors that also performed an expert evaluation of the generated reports, though they did not conduct a correlation or similar analysis to validate the ARS method.
sent33: Zhang et al. [162] went further with the concept extraction and presented Medical Image Report Quality Index (MIRQI), which works in a similar fashion as the SPICE [9] metric presented in the text quality subsection (5.4.1).
sent34: MIRQI applies ideas from NegBio [106] and the CheXpert labeler [63] to identify diseases or medical conditions in the reports, considering synonyms and negations, and uses the Stanford parser [23] to obtain semantic dependencies and finer-grained attributes from each sentence, such as severity, size, shape, body parts, etc.
sent35: With this information, an abnormality graph is built for each report, where each node is a disease with its attributes, and the nodes are connected if they belong to the same organ or tissue.
sent36: Lastly, the graphs from the ground truth and generated reports are matched node-wise, and MIRQI-p (precision), MIRQI-r (recall) and MIRQI-F1 (F1-score) are computed.
sent37: Compared to the formerly discussed correctness metrics, we believe this approach seems more robust to assess the medical facts in the reports, as it attempts to capture the attributes and relations, opposed to the concepts only.
sent38: However, the authors did not present an evaluation against expert judgement, so we cannot determine if this metric is sufficient.
sent39: Considering human evaluation, only a few works [7,38,126,131] present a formal expert medical correctness assessment.
sent40: In the work by Alsharid et al. [7] a medical professional assessed the reports on a Likert Scale from 0 to 2 in four different aspects: accurately describes the image, presents no incorrect information, is grammatically correct and is relevant for the image; the results were further separated for samples from different body parts, showing averages between 0.5 and 1.
sent41: Gale et al. [38] asked a radiologist to evaluate the correctness of the hip fractures description, finding that the fracture's character was properly described 98% of the cases, while the fracture location only for 90%.
sent42: In the work by Tian et al. [131] a medical expert evaluated 30 randomly selected reports with a rating from 1 (definite accept) to 5 (definite reject), scoring an average of 2.33.
sent43: Lastly, Spinks and Moens [126] asked four questions to three experts regarding the generated reports, where the third and fourth questions measured correctness: ""Do you agree with the proposed diagnosis?"", answering 0 (no) or 1 (yes) and ""How certain are you about your final diagnosis?"", from 1 (not sure) to 4 (very sure).
sent44: The average scores were high (0.88 and 3.75), showing agreement with the model's diagnosis, and certainty on the experts' diagnoses.
sent45: The other questions concerned explainability aspects, and are detailed in the next subsection (5.4.3).
sent46: So far, there is no standard approach to perform an expert evaluation, though we believe the first two approaches provide finer-grained information than the latter two, and hence should be more useful for determining in which cases the models are failing and for designing improvements.
sent47: The certainty question should also be very useful, as diagnoses may be susceptible to human judgement.
sent48: Lastly, multiple papers [49,86,89,94,120,131,144,155,156,158,162,163] evaluated the performance of the auxiliary tasks with ROC-AUC, accuracy and other typical classification or segmentation metrics, as shown in Table 6.
sent49: Note that in any of these cases, the task is a previous or intermediary step of the process and is not derived from the report.
sent50: In consequence, even if the classification has great performance, the language component could be performing poorly, and the generated reports still may be inaccurate.
sent51: Accordingly, we believe this type of measure should not be used as the primary report correctness evaluation, unless it can be proven that the report reproduces exactly the classification made (e.g. by a template-filling process).
sent52: The methods in this category measure general quality aspects of the generated text, and are originated from translation, summarizing or captioning tasks.
sent53: The most widely used metrics in the papers reviewed are BLEU [102], ROUGE-L [90], METEOR [12,82] and CIDEr [140], which measure the similarity of a target text (also referred to as candidate), against one or more reference texts (ground truth).
sent54: These metrics are mainly based on counting n-gram matchings between the candidate and the ground truth.
sent55: BLEU is precision-oriented, ROUGE-L and METEOR are F1-scores that can be biased towards precision or recall with a given parameter, and CIDEr attempts to capture both precision and recall through a TF-IDF score.
sent56: Most of these metrics have variants and parameters for their calculation: ROUGE is a set of multiple metrics, being ROUGE-L the only one used in this task; METEOR has variants presented by the same authors [30][31][32]; and CIDEr was presented with the CIDEr-D variant to prevent gameability effects.
sent57: SPICE [9] is a metric designed for the image captioning task, and evaluates the underlying meaning of the sentences describing the image scene, partially disregarding fluency or grammatical aspects.
sent58: Specifically, the text is parsed as a graph, capturing the objects, their described characteristics and relations, which are then measured against the ground truth using an F1-score.
sent59: Even though SPICE attempts to assess the semantic information in a caption, we believe it is not suitable for medical reports, as the graph parsing is designed for general domain objects.
sent60: Nonetheless, Zhang et al. [162] presented the medical correctness metric MIRQI, applying a similar idea in a specific medical domain, which we will discuss in the next subsection (5.4.2).
sent61: Besides standard captioning metrics, we identified two other approaches to measure text quality.
sent62: First, Alsharid et al. [7] used Grammar Bot 3 , a rule and statistics based automated system that counts the grammatical errors in sentences.
sent63: Second, Harzig et al. 2019a [48] measured the sentence variability, by counting the different sentences in a set of reports.
sent64: They argue that the sentences indicating abnormalities occur very rarely in the dataset, while the ones indicating normality are the most frequent.
sent65: Hence, a certain level of variability is desired, and a system generating reports with low variability may indicate that not all medical conditions are being captured.
sent66: Lastly, both works from Li et al. [85,86] performed human evaluation with non-expert users via Amazon Mechanical Turk (AMT), following the same procedure.
sent67: The authors presented two reports to the AMT participants, one generated with the proposed model and one generated with the CoAtt model [68] as baseline, and asked them to choose the most similar with the ground truth in terms of fluency, abnormalities correctness and content coverage.
sent68: The results shown that their report was preferred around 50-60% of the cases, while the baseline around 20-30% (for the rest, none or both were preferred).
sent69: We categorize this evaluation as a text quality metric, as the participants are not experts, and their answers are not fine-grained (i.e., did not specify what failed: fluency, correctness or coverage; or by how much they failed).
sent70: 5.4.2 Medical correctness metrics.
sent71: While the most common purpose of the text quality metrics is to measure the similarity between the generated report and a ground truth, they do not necessarily capture the medical facts in the reports [11,17,92,107,108,161].
sent72: For example, the sentences ""effusion is observed"" and ""effusion is not observed"" are very similar, thus may present a very high score for any metric based on n-gram matching, though the medical facts are the exact opposite.
sent73: Therefore, an evaluation directly measuring the reports correctness is required, not necessarily taking into account fluency, grammatical rules or text quality in general.
sent74: From the literature reviewed, in ten works [7,16,61,67,85,92,98,148,154,162] the authors presented an automatic metric to address this issue, four works [7,38,126,131] did a formal expert evaluation, and multiple works [49, 76, 86, 89, 94, 120, 126, 131, 144, 155-158, 162, 163] evaluated medical correctness indirectly from auxiliary tasks.
sent75: The methods are listed in Table 6 and are further discussed next.
sent76: In several works the authors presented a method that detects concepts in the generated and ground truth reports, and compare the results using common classification metrics, such as accuracy, F1-score, and more.
sent77: The main difference between these methods lies in how the concepts are automatically detected in the reports.
sent78: The simplest approaches are keyword-based, which consists in reporting the ratio of a set of keywords found between the generated report and ground truth, like MeSH Accuracy [61] that uses MeSH terms, and Keyword Accuracy that uses 438 MTI terms (presented by Xue et al. [154] and used in A3FN [150]).
sent79: Similarly, Medical Abnormality Terminology Detection [85] calculates precision and false positive rate of the 10 most frequent abnormalityrelated terms in the dataset; and Wu et al. [148] calculated accuracy, sensitivity and specificity for a set of keywords.
sent80: Other approaches are abnormality-based, which attempt to directly classify abnormalities from the report by different means: Abnormality Detection [67] uses manually designed patterns; Medical Abnormality Detection [92] uses the CheXpert labeler tool [63]; Biswal et al. [16] used a character-level CNN [160] that classifies multiple CheXpert labels [63]; and Moradi et al. [98] used a proprietary software to extract semantic descriptors.
sent81: Lastly, Anatomical Relevance Score (ARS) [7] is a body-part-based approach, which detects the anatomical elements mentioned in a report considering the vocabulary used.
sent82: Though these methods may be useful for measuring medical correctness to a certain degree, there is no consensus or standard, and there is no formal evaluation of the correlation with expert judgement.
sent83: From the discussed techniques, Alsharid et al. [7] are the only authors that also performed an expert evaluation of the generated reports, though they did not conduct a correlation or similar analysis to validate the ARS method.
sent84: Zhang et al. [162] went further with the concept extraction and presented Medical Image Report Quality Index (MIRQI), which works in a similar fashion as the SPICE [9] metric presented in the text quality subsection (5.4.1).
sent85: MIRQI applies ideas from NegBio [106] and the CheXpert labeler [63] to identify diseases or medical conditions in the reports, considering synonyms and negations, and uses the Stanford parser [23] to obtain semantic dependencies and finer-grained attributes from each sentence, such as severity, size, shape, body parts, etc.
sent86: With this information, an abnormality graph is built for each report, where each node is a disease with its attributes, and the nodes are connected if they belong to the same organ or tissue.
sent87: Lastly, the graphs from the ground truth and generated reports are matched node-wise, and MIRQI-p (precision), MIRQI-r (recall) and MIRQI-F1 (F1-score) are computed.
sent88: Compared to the formerly discussed correctness metrics, we believe this approach seems more robust to assess the medical facts in the reports, as it attempts to capture the attributes and relations, opposed to the concepts only.
sent89: However, the authors did not present an evaluation against expert judgement, so we cannot determine if this metric is sufficient.
sent90: Considering human evaluation, only a few works [7,38,126,131] present a formal expert medical correctness assessment.
sent91: In the work by Alsharid et al. [7] a medical professional assessed the reports on a Likert Scale from 0 to 2 in four different aspects: accurately describes the image, presents no incorrect information, is grammatically correct and is relevant for the image; the results were further separated for samples from different body parts, showing averages between 0.5 and 1.
sent92: Gale et al. [38] asked a radiologist to evaluate the correctness of the hip fractures description, finding that the fracture's character was properly described 98% of the cases, while the fracture location only for 90%.
sent93: In the work by Tian et al. [131] a medical expert evaluated 30 randomly selected reports with a rating from 1 (definite accept) to 5 (definite reject), scoring an average of 2.33.
sent94: Lastly, Spinks and Moens [126] asked four questions to three experts regarding the generated reports, where the third and fourth questions measured correctness: ""Do you agree with the proposed diagnosis?"", answering 0 (no) or 1 (yes) and ""How certain are you about your final diagnosis?"", from 1 (not sure) to 4 (very sure).
sent95: The average scores were high (0.88 and 3.75), showing agreement with the model's diagnosis, and certainty on the experts' diagnoses.
sent96: The other questions concerned explainability aspects, and are detailed in the next subsection (5.4.3).
sent97: So far, there is no standard approach to perform an expert evaluation, though we believe the first two approaches provide finer-grained information than the latter two, and hence should be more useful for determining in which cases the models are failing and for designing improvements.
sent98: The certainty question should also be very useful, as diagnoses may be susceptible to human judgement.
sent99: Lastly, multiple papers [49,86,89,94,120,131,144,155,156,158,162,163] evaluated the performance of the auxiliary tasks with ROC-AUC, accuracy and other typical classification or segmentation metrics, as shown in Table 6.
sent100: Note that in any of these cases, the task is a previous or intermediary step of the process and is not derived from the report.
sent101: In consequence, even if the classification has great performance, the language component could be performing poorly, and the generated reports still may be inaccurate.
sent102: Accordingly, we believe this type of measure should not be used as the primary report correctness evaluation, unless it can be proven that the report reproduces exactly the classification made (e.g. by a template-filling process)."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s62,Explainability metrics.,"Providing interpretable justifications for the model's outcome is essential in this medical domain, and furthermore, we should be able to evaluate them to answer questions such as, does the method justify the model's decision?, which method provides a better explanation? However, there is no consensus on evaluation methods for AI explainability, and in many cases the definition of a better explanation remains subjective [22,33,113].

Consequently, none of the papers reviewed used an automatic metric to assess explainability, and only two works [38,126] conduct a formal human expert evaluation. Gale et al. [38] presented the report generation as an explanation of a medical image classification task, and evaluated it by comparing three methods: (a) SmoothGrad [124] to highlight the most important pixels used, (b) a generated report in natural language, and (c) both placed side by side. Five experts assessed 30 images, rating each explanation in a scale from 1 (unsatisfactory) to 10 (perfect); achieving average scores of (a) 4.4, (b) 7 and (c) 8.8 for each method. Though the authors emphasize the importance of the natural language explanations, their approach does not include an explanation for the report itself, so it cannot be directly used for the report generation task.

The model proposed by Spinks and Moens [126] generates a chest X-ray as a counter-factual example, and they compared this explanation method against a feature importance heatmap generated with the Zagoruyko and Komodakis saliency map technique [77]. Three experts evaluated 150 samples answering four questions, the first two regarding explainability aspects: ""Does the explanation justify the diagnosis?"" ""Does the model appear to understand the important parts of the X-ray?"" The answers were in a scale from 1 (no) to 4 (yes), and their method achieved a higher score than the saliency map (2.39 vs 1.31 for the first question, and 2.45 vs 1.81 for the second), showing their counter-factual approach should be better in this setting. The other two questions relate more to medical correctness, and are discussed in the previous section (5.4.2).

We believe the explanation evaluations should be very important in this area, and as there is no consensus, we outline some possible guidelines. Following ideas from Tonekaboni et al. [134], we believe three aspects from the explanations should be assessed: (1) consistency, (2) alignment with domain knowledge, and (3) user impact. First, the consistency across the data should be assessed, answering questions such as: do explanations change with variations to the input data?; or to the prediction?; or to the model design?; or with different images from the same patient? As pointed out by Tonekaboni et al. [134], inconsistent explanations may negatively affect the clinicians' trust, and an interpretability method laying them out should be reviewed. Examples of consistency or robustness evaluations can be found in the work by Adebayo et al. [3] for image saliency maps, and in the work by Jain and Wallace [66] for attention in recurrent neural networks.

Second, the alignment with domain knowledge should evaluate if the explanation is consistent with an expert's knowledge: would they provide the same explanation for that decision? For instance, given a feature importance method, is the model focusing on the correct features? As an example, consider the second and third questions employed by Spinks and Moens [126] detailed earlier. To mention other examples, Wang et al. [143] evaluated CAM [165] generated heatmaps for disease classification against expert provided bounding-boxes locating the diseases, using intersection-overunion like metrics; Kim et al. [75] proposed a model to classify Diabetic Retinopathy from retina fundus images, and they compared the TCAV [75] extracted concepts against expert knowledge. Notice many works reviewed in this survey used classification or segmentation as an auxiliary task, which can be used as local explanations, and evaluated them with common metrics (such as accuracy, precision, etc.), as discussed in the previous sections (5.3 and 5.4.2). As the authors did not mention the secondary outputs as local explanations, we categorized the said evaluations as medical correctness metrics, but they are also measuring alignment with domain knowledge for the interpretability methods, and as such may be very useful.

Lastly, the user impact should attempt to answer questions like, is it a good explanation? Does it provide useful or novel information? Does it justify the model's decision? Is it provided with an appropriate representation for the experts? As examples, the assessment proposed by Gale et al. [38] and the first question used by Spinks and Moens [126] measure user impact. Notice that most of these concepts are very subjective, and the definitions, the questions and assessments will vary for different sub-domains and target experts. We believe more specific definitions and fine-grained aspects should arise in the future, as research in this topic grows. For reference, this category includes the domain appropriate representation and potential actionability concepts presented by Tonekaboni et al. [134]. Synthesis. Almost all the works include text quality metrics, though these are not able to capture the medical facts in a report [11,17,92,107,108,161]. Several works proposed medical correctness assessments over the reports, but unfortunately none of the proposals was evaluated against expert judgement. The auxiliary tasks can be evaluated to measure correctness indirectly from the process, but often it will not be sufficient for the report's correctness. Only two works evaluate explainability directly with experts, and the auxiliary tasks' assessments could be useful to measure alignment between the explanations and domain knowledge. Overall, we believe that medical correctness should be the primary aspect to evaluate in the generated reports, using one or more automatic metrics. For now, and even though none of the metrics proposed has been evaluated against expert judgement, MIRQI [162] seems like the most promising approach to fulfill this purpose, as it should be able to capture richer information from the reports. Additionally, text quality metrics can be used as a secondary evaluation, since they may be useful for measuring fluency, grammar or variability, and to compare with previous baselines. Lastly, explainability evaluation methods should arise to assess multiple key aspects, such as its consistency, alignment with domain knowledge, and the user impact.

Providing interpretable justifications for the model's outcome is essential in this medical domain, and furthermore, we should be able to evaluate them to answer questions such as, does the method justify the model's decision?, which method provides a better explanation? However, there is no consensus on evaluation methods for AI explainability, and in many cases the definition of a better explanation remains subjective [22,33,113].

Consequently, none of the papers reviewed used an automatic metric to assess explainability, and only two works [38,126] conduct a formal human expert evaluation. Gale et al. [38] presented the report generation as an explanation of a medical image classification task, and evaluated it by comparing three methods: (a) SmoothGrad [124] to highlight the most important pixels used, (b) a generated report in natural language, and (c) both placed side by side. Five experts assessed 30 images, rating each explanation in a scale from 1 (unsatisfactory) to 10 (perfect); achieving average scores of (a) 4.4, (b) 7 and (c) 8.8 for each method. Though the authors emphasize the importance of the natural language explanations, their approach does not include an explanation for the report itself, so it cannot be directly used for the report generation task.

The model proposed by Spinks and Moens [126] generates a chest X-ray as a counter-factual example, and they compared this explanation method against a feature importance heatmap generated with the Zagoruyko and Komodakis saliency map technique [77]. Three experts evaluated 150 samples answering four questions, the first two regarding explainability aspects: ""Does the explanation justify the diagnosis?"" ""Does the model appear to understand the important parts of the X-ray?"" The answers were in a scale from 1 (no) to 4 (yes), and their method achieved a higher score than the saliency map (2.39 vs 1.31 for the first question, and 2.45 vs 1.81 for the second), showing their counter-factual approach should be better in this setting. The other two questions relate more to medical correctness, and are discussed in the previous section (5.4.2).

We believe the explanation evaluations should be very important in this area, and as there is no consensus, we outline some possible guidelines. Following ideas from Tonekaboni et al. [134], we believe three aspects from the explanations should be assessed: (1) consistency, (2) alignment with domain knowledge, and (3) user impact. First, the consistency across the data should be assessed, answering questions such as: do explanations change with variations to the input data?; or to the prediction?; or to the model design?; or with different images from the same patient? As pointed out by Tonekaboni et al. [134], inconsistent explanations may negatively affect the clinicians' trust, and an interpretability method laying them out should be reviewed. Examples of consistency or robustness evaluations can be found in the work by Adebayo et al. [3] for image saliency maps, and in the work by Jain and Wallace [66] for attention in recurrent neural networks.

Second, the alignment with domain knowledge should evaluate if the explanation is consistent with an expert's knowledge: would they provide the same explanation for that decision? For instance, given a feature importance method, is the model focusing on the correct features? As an example, consider the second and third questions employed by Spinks and Moens [126] detailed earlier. To mention other examples, Wang et al. [143] evaluated CAM [165] generated heatmaps for disease classification against expert provided bounding-boxes locating the diseases, using intersection-overunion like metrics; Kim et al. [75] proposed a model to classify Diabetic Retinopathy from retina fundus images, and they compared the TCAV [75] extracted concepts against expert knowledge. Notice many works reviewed in this survey used classification or segmentation as an auxiliary task, which can be used as local explanations, and evaluated them with common metrics (such as accuracy, precision, etc.), as discussed in the previous sections (5.3 and 5.4.2). As the authors did not mention the secondary outputs as local explanations, we categorized the said evaluations as medical correctness metrics, but they are also measuring alignment with domain knowledge for the interpretability methods, and as such may be very useful.

Lastly, the user impact should attempt to answer questions like, is it a good explanation? Does it provide useful or novel information? Does it justify the model's decision? Is it provided with an appropriate representation for the experts? As examples, the assessment proposed by Gale et al. [38] and the first question used by Spinks and Moens [126] measure user impact. Notice that most of these concepts are very subjective, and the definitions, the questions and assessments will vary for different sub-domains and target experts. We believe more specific definitions and fine-grained aspects should arise in the future, as research in this topic grows. For reference, this category includes the domain appropriate representation and potential actionability concepts presented by Tonekaboni et al. [134]. Synthesis. Almost all the works include text quality metrics, though these are not able to capture the medical facts in a report [11,17,92,107,108,161]. Several works proposed medical correctness assessments over the reports, but unfortunately none of the proposals was evaluated against expert judgement. The auxiliary tasks can be evaluated to measure correctness indirectly from the process, but often it will not be sufficient for the report's correctness. Only two works evaluate explainability directly with experts, and the auxiliary tasks' assessments could be useful to measure alignment between the explanations and domain knowledge. Overall, we believe that medical correctness should be the primary aspect to evaluate in the generated reports, using one or more automatic metrics. For now, and even though none of the metrics proposed has been evaluated against expert judgement, MIRQI [162] seems like the most promising approach to fulfill this purpose, as it should be able to capture richer information from the reports. Additionally, text quality metrics can be used as a secondary evaluation, since they may be useful for measuring fluency, grammar or variability, and to compare with previous baselines. Lastly, explainability evaluation methods should arise to assess multiple key aspects, such as its consistency, alignment with domain knowledge, and the user impact.","[['b21', 'b32', 'b112'], ['b123', 'b125', 'b37'], ['b76', 'b125', None], ['b65', 'b2', 'b133'], ['b142', 'b74', 'b125', 'b164'], ['b10', 'b16', 'b125', 'b133', 'b37', 'b161', 'b91', 'b107', 'b106', 'b160'], ['b21', 'b32', 'b112'], ['b123', 'b125', 'b37'], ['b76', 'b125', None], ['b65', 'b2', 'b133'], ['b142', 'b74', 'b125', 'b164'], ['b10', 'b16', 'b125', 'b133', 'b37', 'b161', 'b91', 'b107', 'b106', 'b160']]","[['b21', 'b32', 'b112'], ['b123', 'b125', 'b37'], ['b76', 'b125', None], ['b65', 'b2', 'b133'], ['b142', 'b74', 'b125', 'b164'], ['b10', 'b16', 'b125', 'b133', 'b37', 'b161', 'b91', 'b107', 'b106', 'b160'], ['b21', 'b32', 'b112'], ['b123', 'b125', 'b37'], ['b76', 'b125', None], ['b65', 'b2', 'b133'], ['b142', 'b74', 'b125', 'b164'], ['b10', 'b16', 'b125', 'b133', 'b37', 'b161', 'b91', 'b107', 'b106', 'b160']]",52,"sent1: Providing interpretable justifications for the model's outcome is essential in this medical domain, and furthermore, we should be able to evaluate them to answer questions such as, does the method justify the model's decision?, which method provides a better explanation?
sent2: However, there is no consensus on evaluation methods for AI explainability, and in many cases the definition of a better explanation remains subjective [22,33,113].
sent3: Consequently, none of the papers reviewed used an automatic metric to assess explainability, and only two works [38,126] conduct a formal human expert evaluation.
sent4: Gale et al. [38] presented the report generation as an explanation of a medical image classification task, and evaluated it by comparing three methods: (a) SmoothGrad [124] to highlight the most important pixels used, (b) a generated report in natural language, and (c) both placed side by side.
sent5: Five experts assessed 30 images, rating each explanation in a scale from 1 (unsatisfactory) to 10 (perfect); achieving average scores of (a) 4.4, (b) 7 and (c) 8.8 for each method.
sent6: Though the authors emphasize the importance of the natural language explanations, their approach does not include an explanation for the report itself, so it cannot be directly used for the report generation task.
sent7: The model proposed by Spinks and Moens [126] generates a chest X-ray as a counter-factual example, and they compared this explanation method against a feature importance heatmap generated with the Zagoruyko and Komodakis saliency map technique [77].
sent8: Three experts evaluated 150 samples answering four questions, the first two regarding explainability aspects: ""Does the explanation justify the diagnosis?""
sent9: ""Does the model appear to understand the important parts of the X-ray?""
sent10: The answers were in a scale from 1 (no) to 4 (yes), and their method achieved a higher score than the saliency map (2.39 vs 1.31 for the first question, and 2.45 vs 1.81 for the second), showing their counter-factual approach should be better in this setting.
sent11: The other two questions relate more to medical correctness, and are discussed in the previous section (5.4.2).
sent12: We believe the explanation evaluations should be very important in this area, and as there is no consensus, we outline some possible guidelines.
sent13: Following ideas from Tonekaboni et al. [134], we believe three aspects from the explanations should be assessed: (1) consistency, (2) alignment with domain knowledge, and (3) user impact.
sent14: First, the consistency across the data should be assessed, answering questions such as: do explanations change with variations to the input data?; or to the prediction?; or to the model design?; or with different images from the same patient?
sent15: As pointed out by Tonekaboni et al. [134], inconsistent explanations may negatively affect the clinicians' trust, and an interpretability method laying them out should be reviewed.
sent16: Examples of consistency or robustness evaluations can be found in the work by Adebayo et al. [3] for image saliency maps, and in the work by Jain and Wallace [66] for attention in recurrent neural networks.
sent17: Second, the alignment with domain knowledge should evaluate if the explanation is consistent with an expert's knowledge: would they provide the same explanation for that decision?
sent18: For instance, given a feature importance method, is the model focusing on the correct features?
sent19: As an example, consider the second and third questions employed by Spinks and Moens [126] detailed earlier.
sent20: To mention other examples, Wang et al. [143] evaluated CAM [165] generated heatmaps for disease classification against expert provided bounding-boxes locating the diseases, using intersection-overunion like metrics; Kim et al. [75] proposed a model to classify Diabetic Retinopathy from retina fundus images, and they compared the TCAV [75] extracted concepts against expert knowledge.
sent21: Notice many works reviewed in this survey used classification or segmentation as an auxiliary task, which can be used as local explanations, and evaluated them with common metrics (such as accuracy, precision, etc.), as discussed in the previous sections (5.3 and 5.4.2).
sent22: As the authors did not mention the secondary outputs as local explanations, we categorized the said evaluations as medical correctness metrics, but they are also measuring alignment with domain knowledge for the interpretability methods, and as such may be very useful.
sent23: Lastly, the user impact should attempt to answer questions like, is it a good explanation?
sent24: Does it provide useful or novel information?
sent25: Does it justify the model's decision?
sent26: Is it provided with an appropriate representation for the experts?
sent27: As examples, the assessment proposed by Gale et al. [38] and the first question used by Spinks and Moens [126] measure user impact.
sent28: Notice that most of these concepts are very subjective, and the definitions, the questions and assessments will vary for different sub-domains and target experts.
sent29: We believe more specific definitions and fine-grained aspects should arise in the future, as research in this topic grows.
sent30: For reference, this category includes the domain appropriate representation and potential actionability concepts presented by Tonekaboni et al. [134].
sent31: Synthesis. Almost all the works include text quality metrics, though these are not able to capture the medical facts in a report [11,17,92,107,108,161].
sent32: Several works proposed medical correctness assessments over the reports, but unfortunately none of the proposals was evaluated against expert judgement.
sent33: The auxiliary tasks can be evaluated to measure correctness indirectly from the process, but often it will not be sufficient for the report's correctness.
sent34: Only two works evaluate explainability directly with experts, and the auxiliary tasks' assessments could be useful to measure alignment between the explanations and domain knowledge.
sent35: Overall, we believe that medical correctness should be the primary aspect to evaluate in the generated reports, using one or more automatic metrics.
sent36: For now, and even though none of the metrics proposed has been evaluated against expert judgement, MIRQI [162] seems like the most promising approach to fulfill this purpose, as it should be able to capture richer information from the reports.
sent37: Additionally, text quality metrics can be used as a secondary evaluation, since they may be useful for measuring fluency, grammar or variability, and to compare with previous baselines.
sent38: Lastly, explainability evaluation methods should arise to assess multiple key aspects, such as its consistency, alignment with domain knowledge, and the user impact.
sent39: Providing interpretable justifications for the model's outcome is essential in this medical domain, and furthermore, we should be able to evaluate them to answer questions such as, does the method justify the model's decision?, which method provides a better explanation?
sent40: However, there is no consensus on evaluation methods for AI explainability, and in many cases the definition of a better explanation remains subjective [22,33,113].
sent41: Consequently, none of the papers reviewed used an automatic metric to assess explainability, and only two works [38,126] conduct a formal human expert evaluation.
sent42: Gale et al. [38] presented the report generation as an explanation of a medical image classification task, and evaluated it by comparing three methods: (a) SmoothGrad [124] to highlight the most important pixels used, (b) a generated report in natural language, and (c) both placed side by side.
sent43: Five experts assessed 30 images, rating each explanation in a scale from 1 (unsatisfactory) to 10 (perfect); achieving average scores of (a) 4.4, (b) 7 and (c) 8.8 for each method.
sent44: Though the authors emphasize the importance of the natural language explanations, their approach does not include an explanation for the report itself, so it cannot be directly used for the report generation task.
sent45: The model proposed by Spinks and Moens [126] generates a chest X-ray as a counter-factual example, and they compared this explanation method against a feature importance heatmap generated with the Zagoruyko and Komodakis saliency map technique [77].
sent46: Three experts evaluated 150 samples answering four questions, the first two regarding explainability aspects: ""Does the explanation justify the diagnosis?""
sent47: ""Does the model appear to understand the important parts of the X-ray?""
sent48: The answers were in a scale from 1 (no) to 4 (yes), and their method achieved a higher score than the saliency map (2.39 vs 1.31 for the first question, and 2.45 vs 1.81 for the second), showing their counter-factual approach should be better in this setting.
sent49: The other two questions relate more to medical correctness, and are discussed in the previous section (5.4.2).
sent50: We believe the explanation evaluations should be very important in this area, and as there is no consensus, we outline some possible guidelines.
sent51: Following ideas from Tonekaboni et al. [134], we believe three aspects from the explanations should be assessed: (1) consistency, (2) alignment with domain knowledge, and (3) user impact.
sent52: First, the consistency across the data should be assessed, answering questions such as: do explanations change with variations to the input data?; or to the prediction?; or to the model design?; or with different images from the same patient?
sent53: As pointed out by Tonekaboni et al. [134], inconsistent explanations may negatively affect the clinicians' trust, and an interpretability method laying them out should be reviewed.
sent54: Examples of consistency or robustness evaluations can be found in the work by Adebayo et al. [3] for image saliency maps, and in the work by Jain and Wallace [66] for attention in recurrent neural networks.
sent55: Second, the alignment with domain knowledge should evaluate if the explanation is consistent with an expert's knowledge: would they provide the same explanation for that decision?
sent56: For instance, given a feature importance method, is the model focusing on the correct features?
sent57: As an example, consider the second and third questions employed by Spinks and Moens [126] detailed earlier.
sent58: To mention other examples, Wang et al. [143] evaluated CAM [165] generated heatmaps for disease classification against expert provided bounding-boxes locating the diseases, using intersection-overunion like metrics; Kim et al. [75] proposed a model to classify Diabetic Retinopathy from retina fundus images, and they compared the TCAV [75] extracted concepts against expert knowledge.
sent59: Notice many works reviewed in this survey used classification or segmentation as an auxiliary task, which can be used as local explanations, and evaluated them with common metrics (such as accuracy, precision, etc.), as discussed in the previous sections (5.3 and 5.4.2).
sent60: As the authors did not mention the secondary outputs as local explanations, we categorized the said evaluations as medical correctness metrics, but they are also measuring alignment with domain knowledge for the interpretability methods, and as such may be very useful.
sent61: Lastly, the user impact should attempt to answer questions like, is it a good explanation?
sent62: Does it provide useful or novel information?
sent63: Does it justify the model's decision?
sent64: Is it provided with an appropriate representation for the experts?
sent65: As examples, the assessment proposed by Gale et al. [38] and the first question used by Spinks and Moens [126] measure user impact.
sent66: Notice that most of these concepts are very subjective, and the definitions, the questions and assessments will vary for different sub-domains and target experts.
sent67: We believe more specific definitions and fine-grained aspects should arise in the future, as research in this topic grows.
sent68: For reference, this category includes the domain appropriate representation and potential actionability concepts presented by Tonekaboni et al. [134].
sent69: Synthesis. Almost all the works include text quality metrics, though these are not able to capture the medical facts in a report [11,17,92,107,108,161].
sent70: Several works proposed medical correctness assessments over the reports, but unfortunately none of the proposals was evaluated against expert judgement.
sent71: The auxiliary tasks can be evaluated to measure correctness indirectly from the process, but often it will not be sufficient for the report's correctness.
sent72: Only two works evaluate explainability directly with experts, and the auxiliary tasks' assessments could be useful to measure alignment between the explanations and domain knowledge.
sent73: Overall, we believe that medical correctness should be the primary aspect to evaluate in the generated reports, using one or more automatic metrics.
sent74: For now, and even though none of the metrics proposed has been evaluated against expert judgement, MIRQI [162] seems like the most promising approach to fulfill this purpose, as it should be able to capture richer information from the reports.
sent75: Additionally, text quality metrics can be used as a secondary evaluation, since they may be useful for measuring fluency, grammar or variability, and to compare with previous baselines.
sent76: Lastly, explainability evaluation methods should arise to assess multiple key aspects, such as its consistency, alignment with domain knowledge, and the user impact."
245837979,A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images,"Computer Science, Medicine",https://www.semanticscholar.org/paper/c47611ecff497c01e8bebc1da2ffba4e9b4d9de9,s63,Comparison of papers' performance,"To find out which paper holds the state of the art, we need to find a common ground for fair comparison. A natural choice is the IU X-ray dataset [28], since a majority of the surveyed papers report results in this dataset. Table 7 shows these results, separated by which report sections are generated by each paper, findings, impression or both. The findings section consists of multiple sentences, and mainly describes medical conditions observed, while the impression section is a one sentence conclusion or diagnosis. Notice Spinks and Moens [126] filtered the findings section, and kept only sentences referring to one disease (Cardiomegaly). The papers that seem to show the best performance in terms of NLP metrics are KERP [86], CLARA [16] and Xue et al. 2019 [153] for the findings section, MTMA [132] for the impression section, and Yuan et al. [156], MLMA [36] and Xue et al. 2019 [153] for both sections. Of these, only MTMA has a large difference to its competitors, and there is no clear winner in the other sections. Some caveats, however, should be kept in mind when interpreting these results:

(1) The results reported in the literature only allow comparisons in terms of standard natural language metrics (BLEU, ROUGE-L, etc.), but from these results we cannot draw conclusions about medical correctness, since NLP metrics and clinical accuracy are not necessarily correlated.  Table 7. Evaluation results of papers that use the IU X-ray dataset. All values were extracted from their papers, except in some cases where results were not present in the own paper: (1) TieNet [144] results were presented in Liu et al. [92] as a baseline; (2) Xue et al. 2018 [154] results in the findings section were presented in Xue et al. 2019 [153] as a baseline. (3) CLARA [16] results are from the fully automatic version.

(2) MTMA uses additional input, as discussed in section 5.2.1. Specifically, the model receives the indication and findings sections of the report to generate the impression section, at both test and train stages. In a sense, this could be seen as an enhanced summarizing approach, since the impression section contains a conclusion from the findings.

(3) Some NLP metrics, such as CIDEr, ROUGE-L and METEOR, have variants and parameters, as discussed in section 5.4. Unfortunately, most papers do not mention the specific version or implementation used.

(4) The IU X-ray dataset does not have standard training-validation-test splits. This has led researchers to define their own splits, as indicated by column Split of Table 7. These splits are not consistent across papers, making results less comparable. For example, if a model was evaluated in an easier test split, that would give it an unfair advantage over other models evaluated in harder test splits. Additionally, other decisions such the number of images per report (frontal, lateral or both), the tokenization algorithm employed, the removal of noisy sentences, the removal of words with a frequency under a given threshold, the removal of duplicate images, among other preprocessing decisions, are not always explicitly stated in papers, and these may have an impact on the results as well.

(5) These are overall results only, so a more fine-grained performance assessment on specific abnormalities or diseases is missing. This further shows the need for standardizing one or more evaluation metrics to measure the medical correctness of a generated report, considering different aspects of interest.

To find out which paper holds the state of the art, we need to find a common ground for fair comparison. A natural choice is the IU X-ray dataset [28], since a majority of the surveyed papers report results in this dataset. Table 7 shows these results, separated by which report sections are generated by each paper, findings, impression or both. The findings section consists of multiple sentences, and mainly describes medical conditions observed, while the impression section is a one sentence conclusion or diagnosis. Notice Spinks and Moens [126] filtered the findings section, and kept only sentences referring to one disease (Cardiomegaly). The papers that seem to show the best performance in terms of NLP metrics are KERP [86], CLARA [16] and Xue et al. 2019 [153] for the findings section, MTMA [132] for the impression section, and Yuan et al. [156], MLMA [36] and Xue et al. 2019 [153] for both sections. Of these, only MTMA has a large difference to its competitors, and there is no clear winner in the other sections. Some caveats, however, should be kept in mind when interpreting these results:

(1) The results reported in the literature only allow comparisons in terms of standard natural language metrics (BLEU, ROUGE-L, etc.), but from these results we cannot draw conclusions about medical correctness, since NLP metrics and clinical accuracy are not necessarily correlated.  Table 7. Evaluation results of papers that use the IU X-ray dataset. All values were extracted from their papers, except in some cases where results were not present in the own paper: (1) TieNet [144] results were presented in Liu et al. [92] as a baseline; (2) Xue et al. 2018 [154] results in the findings section were presented in Xue et al. 2019 [153] as a baseline. (3) CLARA [16] results are from the fully automatic version.

(2) MTMA uses additional input, as discussed in section 5.2.1. Specifically, the model receives the indication and findings sections of the report to generate the impression section, at both test and train stages. In a sense, this could be seen as an enhanced summarizing approach, since the impression section contains a conclusion from the findings.

(3) Some NLP metrics, such as CIDEr, ROUGE-L and METEOR, have variants and parameters, as discussed in section 5.4. Unfortunately, most papers do not mention the specific version or implementation used.

(4) The IU X-ray dataset does not have standard training-validation-test splits. This has led researchers to define their own splits, as indicated by column Split of Table 7. These splits are not consistent across papers, making results less comparable. For example, if a model was evaluated in an easier test split, that would give it an unfair advantage over other models evaluated in harder test splits. Additionally, other decisions such the number of images per report (frontal, lateral or both), the tokenization algorithm employed, the removal of noisy sentences, the removal of words with a frequency under a given threshold, the removal of duplicate images, among other preprocessing decisions, are not always explicitly stated in papers, and these may have an impact on the results as well.

(5) These are overall results only, so a more fine-grained performance assessment on specific abnormalities or diseases is missing. This further shows the need for standardizing one or more evaluation metrics to measure the medical correctness of a generated report, considering different aspects of interest.","[['b27', 'b85', 'b125', 'b131', 'b155', 'b35', 'b152', 'b15'], ['b153', 'b143', 'b91', 'b1', 'b152', 'b2', 'b15', 'b0'], [], [], [], [], ['b27', 'b85', 'b125', 'b131', 'b155', 'b35', 'b152', 'b15'], ['b153', 'b143', 'b91', 'b1', 'b152', 'b2', 'b15', 'b0'], [], [], [], []]","[['b27', 'b85', 'b125', 'b131', 'b155', 'b35', 'b152', 'b15'], ['b153', 'b143', 'b91', 'b1', 'b152', 'b2', 'b15', 'b0'], [], [], [], [], ['b27', 'b85', 'b125', 'b131', 'b155', 'b35', 'b152', 'b15'], ['b153', 'b143', 'b91', 'b1', 'b152', 'b2', 'b15', 'b0'], [], [], [], []]",32,"sent1: To find out which paper holds the state of the art, we need to find a common ground for fair comparison.
sent2: A natural choice is the IU X-ray dataset [28], since a majority of the surveyed papers report results in this dataset.
sent3: Table 7 shows these results, separated by which report sections are generated by each paper, findings, impression or both.
sent4: The findings section consists of multiple sentences, and mainly describes medical conditions observed, while the impression section is a one sentence conclusion or diagnosis.
sent5: Notice Spinks and Moens [126] filtered the findings section, and kept only sentences referring to one disease (Cardiomegaly).
sent6: The papers that seem to show the best performance in terms of NLP metrics are KERP [86], CLARA [16] and Xue et al. 2019 [153] for the findings section, MTMA [132] for the impression section, and Yuan et al. [156], MLMA [36] and Xue et al. 2019 [153] for both sections.
sent7: Of these, only MTMA has a large difference to its competitors, and there is no clear winner in the other sections.
sent8: Some caveats, however, should be kept in mind when interpreting these results:(1)
sent9: The results reported in the literature only allow comparisons in terms of standard natural language metrics (BLEU, ROUGE-L, etc.), but from these results we cannot draw conclusions about medical correctness, since NLP metrics and clinical accuracy are not necessarily correlated.
sent10: Table 7. Evaluation results of papers that use the IU X-ray dataset.
sent11: All values were extracted from their papers, except in some cases where results were not present in the own paper: (1) TieNet [144] results were presented in Liu et al. [92] as a baseline; (2) Xue et al. 2018 [154] results in the findings section were presented in Xue et al. 2019 [153] as a baseline.
sent12: (3) CLARA [16] results are from the fully automatic version.
sent13: (2) MTMA uses additional input, as discussed in section 5.2.1.
sent14: Specifically, the model receives the indication and findings sections of the report to generate the impression section, at both test and train stages.
sent15: In a sense, this could be seen as an enhanced summarizing approach, since the impression section contains a conclusion from the findings.
sent16: (3) Some NLP metrics, such as CIDEr, ROUGE-L and METEOR, have variants and parameters, as discussed in section 5.4.
sent17: Unfortunately, most papers do not mention the specific version or implementation used.
sent18: (4) The IU X-ray dataset does not have standard training-validation-test splits.
sent19: This has led researchers to define their own splits, as indicated by column Split of Table 7.
sent20: These splits are not consistent across papers, making results less comparable.
sent21: For example, if a model was evaluated in an easier test split, that would give it an unfair advantage over other models evaluated in harder test splits.
sent22: Additionally, other decisions such the number of images per report (frontal, lateral or both), the tokenization algorithm employed, the removal of noisy sentences, the removal of words with a frequency under a given threshold, the removal of duplicate images, among other preprocessing decisions, are not always explicitly stated in papers, and these may have an impact on the results as well.
sent23: (5) These are overall results only, so a more fine-grained performance assessment on specific abnormalities or diseases is missing.
sent24: This further shows the need for standardizing one or more evaluation metrics to measure the medical correctness of a generated report, considering different aspects of interest.
sent25: To find out which paper holds the state of the art, we need to find a common ground for fair comparison.
sent26: A natural choice is the IU X-ray dataset [28], since a majority of the surveyed papers report results in this dataset.
sent27: Table 7 shows these results, separated by which report sections are generated by each paper, findings, impression or both.
sent28: The findings section consists of multiple sentences, and mainly describes medical conditions observed, while the impression section is a one sentence conclusion or diagnosis.
sent29: Notice Spinks and Moens [126] filtered the findings section, and kept only sentences referring to one disease (Cardiomegaly).
sent30: The papers that seem to show the best performance in terms of NLP metrics are KERP [86], CLARA [16] and Xue et al. 2019 [153] for the findings section, MTMA [132] for the impression section, and Yuan et al. [156], MLMA [36] and Xue et al. 2019 [153] for both sections.
sent31: Of these, only MTMA has a large difference to its competitors, and there is no clear winner in the other sections.
sent32: Some caveats, however, should be kept in mind when interpreting these results:(1)
sent33: The results reported in the literature only allow comparisons in terms of standard natural language metrics (BLEU, ROUGE-L, etc.), but from these results we cannot draw conclusions about medical correctness, since NLP metrics and clinical accuracy are not necessarily correlated.
sent34: Table 7. Evaluation results of papers that use the IU X-ray dataset.
sent35: All values were extracted from their papers, except in some cases where results were not present in the own paper: (1) TieNet [144] results were presented in Liu et al. [92] as a baseline; (2) Xue et al. 2018 [154] results in the findings section were presented in Xue et al. 2019 [153] as a baseline.
sent36: (3) CLARA [16] results are from the fully automatic version.
sent37: (2) MTMA uses additional input, as discussed in section 5.2.1.
sent38: Specifically, the model receives the indication and findings sections of the report to generate the impression section, at both test and train stages.
sent39: In a sense, this could be seen as an enhanced summarizing approach, since the impression section contains a conclusion from the findings.
sent40: (3) Some NLP metrics, such as CIDEr, ROUGE-L and METEOR, have variants and parameters, as discussed in section 5.4.
sent41: Unfortunately, most papers do not mention the specific version or implementation used.
sent42: (4) The IU X-ray dataset does not have standard training-validation-test splits.
sent43: This has led researchers to define their own splits, as indicated by column Split of Table 7.
sent44: These splits are not consistent across papers, making results less comparable.
sent45: For example, if a model was evaluated in an easier test split, that would give it an unfair advantage over other models evaluated in harder test splits.
sent46: Additionally, other decisions such the number of images per report (frontal, lateral or both), the tokenization algorithm employed, the removal of noisy sentences, the removal of words with a frequency under a given threshold, the removal of duplicate images, among other preprocessing decisions, are not always explicitly stated in papers, and these may have an impact on the results as well.
sent47: (5) These are overall results only, so a more fine-grained performance assessment on specific abnormalities or diseases is missing.
sent48: This further shows the need for standardizing one or more evaluation metrics to measure the medical correctness of a generated report, considering different aspects of interest."
248530069,Deep Personality Trait Recognition: A Survey,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc,s15,Static Images,"As far as static image-based personality trait recognition is concerned, researchers have found that a facial image presents most of meaningful descriptive cues for personality trait recognition (Willis and Todorov, 2006). Hence, the extracted visual features involve in the analysis of facial features for personality trait prediction. In (Guntuku et al., 2015), the authors proposed to leverage several low-level features of facial images, such as color histograms, local binary patterns (LBP), global descriptor (GIST), and aesthetic features, to train the SVM classifier for detecting mid-level clues (gender, age). Then, they predicted the Big-five personality traits of users in selfportrait images with the lasso regressor. Yan et al. (2016) investigated the connection between facial appearance and personality impression in the manner of trustworthy. They obtained middle-level cues through clustering methods from different low-level features, such as histogram of oriented gradients (HOG), scale-invariant feature transform (SIFT), LBP, and so on. Then, a SVM classifier was used to exploit the connection between facial appearance and personality impression.

In recent years, CNNs were also widely used for facial feature extraction on static image-based personality trait recognition tasks. Zhang et al. (2017) presented an end-to-end CNN structure via fine-tuning a pretrained VGG-face model for feature learning so as to predict personality traits and intelligence jointly. They aimed to explore whether self-reported personality traits and intelligence can be jointly measured from facial images. Segalin et al. (2017) explored the linking the Big-Five personality traits and preferred images in the Flickr social network through image understanding and a deep CNN framework. In particular, they fine-tuned the pretrained AlexNet and VGG-16 modal to capture the aesthetic attributes of the images characterizing the personality traits associated with those images. They changed the last layer of the AlexNet and VGG-16 model to adapt them to a binary classification problem. Experiments results showed that the characterization of each image can be locked within the CNN layers, thereby discovering entangled attributes, such as the aesthetic and semantic information for generalizing the patterns that identify a personality trait. Rodríguez et al. (2020) presented a personality trait analysis in social networks by using a weakly supervised learning method of shared images. They trained a ResNet-50 network to derive personality representations from the posted images in social networks, so as to infer whether the personality scores from the posted images are correlated to those scores obtained from text. For predicting personality traits, the images without manually labeling were used for training the ResNet-50 model. Experiment results indicate that people's personality is not only related to text, but also with the image content. Fu and Zhang (2021) provided a personality trait recognition method by using active shape model (ASM) localization and DBNs. They employed an improved ASM model to extract facial features, followed by a DBN which was used to train and classify the students' four personality traits.","[['b114', 'b118', 'b37'], ['b89', 'b26', 'b83', 'b122']]","[['b114', 'b118', 'b37'], ['b89', 'b26', 'b83', 'b122']]",7,"sent1: As far as static image-based personality trait recognition is concerned, researchers have found that a facial image presents most of meaningful descriptive cues for personality trait recognition (Willis and Todorov, 2006).
sent2: Hence, the extracted visual features involve in the analysis of facial features for personality trait prediction.
sent3: In (Guntuku et al., 2015), the authors proposed to leverage several low-level features of facial images, such as color histograms, local binary patterns (LBP), global descriptor (GIST), and aesthetic features, to train the SVM classifier for detecting mid-level clues (gender, age).
sent4: Then, they predicted the Big-five personality traits of users in selfportrait images with the lasso regressor.
sent5: Yan et al. (2016) investigated the connection between facial appearance and personality impression in the manner of trustworthy.
sent6: They obtained middle-level cues through clustering methods from different low-level features, such as histogram of oriented gradients (HOG), scale-invariant feature transform (SIFT), LBP, and so on.
sent7: Then, a SVM classifier was used to exploit the connection between facial appearance and personality impression.
sent8: In recent years, CNNs were also widely used for facial feature extraction on static image-based personality trait recognition tasks.
sent9: Zhang et al. (2017) presented an end-to-end CNN structure via fine-tuning a pretrained VGG-face model for feature learning so as to predict personality traits and intelligence jointly.
sent10: They aimed to explore whether self-reported personality traits and intelligence can be jointly measured from facial images.
sent11: Segalin et al. (2017) explored the linking the Big-Five personality traits and preferred images in the Flickr social network through image understanding and a deep CNN framework.
sent12: In particular, they fine-tuned the pretrained AlexNet and VGG-16 modal to capture the aesthetic attributes of the images characterizing the personality traits associated with those images.
sent13: They changed the last layer of the AlexNet and VGG-16 model to adapt them to a binary classification problem.
sent14: Experiments results showed that the characterization of each image can be locked within the CNN layers, thereby discovering entangled attributes, such as the aesthetic and semantic information for generalizing the patterns that identify a personality trait.
sent15: Rodríguez et al. (2020) presented a personality trait analysis in social networks by using a weakly supervised learning method of shared images.
sent16: They trained a ResNet-50 network to derive personality representations from the posted images in social networks, so as to infer whether the personality scores from the posted images are correlated to those scores obtained from text.
sent17: For predicting personality traits, the images without manually labeling were used for training the ResNet-50 model.
sent18: Experiment results indicate that people's personality is not only related to text, but also with the image content.
sent19: Fu and Zhang (2021) provided a personality trait recognition method by using active shape model (ASM) localization and DBNs.
sent20: They employed an improved ASM model to extract facial features, followed by a DBN which was used to train and classify the students' four personality traits."
248530069,Deep Personality Trait Recognition: A Survey,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc,s16,Dynamic Video Sequences,"Dynamic video sequences consist of a series of video image frames, thereby providing temporal information and scene dynamics. This brings about certain useful and complementary cues for personality trait analysis .

In , the authors investigated the connection between facial expressions and personality of vloggers in conversation videos (vlogs) from a subset of existing YouTube vlog data set (Biel and Gatica-Perez, 2010). They employed a computer expression recognition toolbox to identify the categories of facial expressions of vloggers. They finally adopted a SVM classifier to predict personality traits in conjunction with facial activity statistics on the basis of frame-by-frame estimation. The results indicate that extraversion has the highest utilization of activity cues. This is consistent with previous findings (Biel et al., 2011;). Aran and Gatica-Perez (2013) adopted the social media contents from conversational videos for analyzing the specific trait of extraversion. To address this issue, they integrated the ridge regression with a SVM classifier on the basis of statistical information derived from the weighted motion energy images. In (Teijeiro-Mosquera et al., 2014), the relations between facial expressions and personality impressions were investigated as an extended version of the used method . To characterize face statistics, they derived four sets of behavioral cues, such as statistic-based cues, Threshold (THR) cues, Hidden Markov Models (HMM) cues, and Winner Takes All (WTA) cues. Their research indicates that when multiple facial expression clues are significantly correlated with a certain number of the Big-Five traits, they could only obviously predict the particular trait of extraversion.

In consideration of the tremendous progress in the areas of deep learning, CNNs and LSTMs are widely for personality trait analysis from dynamic video sequences. Gürpınar et al. (2016) fine-tuned a pretrained VGG-19 network to extract deep facial and scene feature representations, as shown in Figure 3. Then, they were merged and fed into a kernel extreme learning machine (ELM) regressor for first impression estimation. Ventura et al. (2017) adopted an extension of Descriptor Aggregation Networks (DAN) to investigate why CNN models performed well in automatically predicting first impressions. They used class activation maps (CAM) for visualization and provided a possible interpretation on understanding why CNN models succeeded in learning discriminative facial features related to personality traits of users. Figure 4 shows the used CAM to interpret the CNN models in learning facial features. Experimental results indicate that: (1) face presents most of discriminative information for the inference of personality traits, (2) the internal representations of CNNs primarily focus on crucial facial regions including eyes, nose, and mouth, (3) some action units (AUs) provide a partial impact on the inference of facial traits. Beyan et al. (2019) aimed to perceive personality traits by means of using deep visual activity (VA)-based features derived only from key-dynamic images in videos. In order to determine key-dynamic images in videos, they employed three key steps: construction of multiple dynamic images, long-term VA learning with CNN + LSTM, and spatiotemporal saliency detection.","[[], ['b102', 'b8', 'b7'], ['b39', 'b106', 'b6']]","[[], ['b102', 'b8', 'b7'], ['b39', 'b106', 'b6']]",6,"sent1: Dynamic video sequences consist of a series of video image frames, thereby providing temporal information and scene dynamics.
sent2: This brings about certain useful and complementary cues for personality trait analysis .
sent3: In , the authors investigated the connection between facial expressions and personality of vloggers in conversation videos (vlogs) from a subset of existing YouTube vlog data set (Biel and Gatica-Perez, 2010).
sent4: They employed a computer expression recognition toolbox to identify the categories of facial expressions of vloggers.
sent5: They finally adopted a SVM classifier to predict personality traits in conjunction with facial activity statistics on the basis of frame-by-frame estimation.
sent6: The results indicate that extraversion has the highest utilization of activity cues.
sent7: This is consistent with previous findings (Biel et al., 2011;).
sent8: Aran and Gatica-Perez (2013) adopted the social media contents from conversational videos for analyzing the specific trait of extraversion.
sent9: To address this issue, they integrated the ridge regression with a SVM classifier on the basis of statistical information derived from the weighted motion energy images.
sent10: In (Teijeiro-Mosquera et al., 2014), the relations between facial expressions and personality impressions were investigated as an extended version of the used method .
sent11: To characterize face statistics, they derived four sets of behavioral cues, such as statistic-based cues, Threshold (THR) cues, Hidden Markov Models (HMM) cues, and Winner Takes All (WTA) cues.
sent12: Their research indicates that when multiple facial expression clues are significantly correlated with a certain number of the Big-Five traits, they could only obviously predict the particular trait of extraversion.
sent13: In consideration of the tremendous progress in the areas of deep learning, CNNs and LSTMs are widely for personality trait analysis from dynamic video sequences.
sent14: Gürpınar et al. (2016) fine-tuned a pretrained VGG-19 network to extract deep facial and scene feature representations, as shown in Figure 3.
sent15: Then, they were merged and fed into a kernel extreme learning machine (ELM) regressor for first impression estimation.
sent16: Ventura et al. (2017) adopted an extension of Descriptor Aggregation Networks (DAN) to investigate why CNN models performed well in automatically predicting first impressions.
sent17: They used class activation maps (CAM) for visualization and provided a possible interpretation on understanding why CNN models succeeded in learning discriminative facial features related to personality traits of users.
sent18: Figure 4 shows the used CAM to interpret the CNN models in learning facial features.
sent19: Experimental results indicate that: (1) face presents most of discriminative information for the inference of personality traits, (2) the internal representations of CNNs primarily focus on crucial facial regions including eyes, nose, and mouth, (3) some action units (AUs) provide a partial impact on the inference of facial traits.
sent20: Beyan et al. (2019) aimed to perceive personality traits by means of using deep visual activity (VA)-based features derived only from key-dynamic images in videos.
sent21: In order to determine key-dynamic images in videos, they employed three key steps: construction of multiple dynamic images, long-term VA learning with CNN + LSTM, and spatiotemporal saliency detection."
248530069,Deep Personality Trait Recognition: A Survey,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc,s13,Audio-Based Personality Trait Recognition,"The early-used audio features for automatic personality trait recognition are hand-crafted low-level descriptive (LLD) features, such as prosody (intensity, pitch), voice quality (formants), spectral features (Mel Frequency Cepstrum Coefficients, MFCCs), and so on. Specially, Mohammadi and Vinciarelli (2012) utilized the LLD features, such as pitch, formants, energy, and speaking rate to detect personality traits in audio clips with less than 10 s. They adopted Logistic Regression to identify whether an audio clip exceeded the average score for each of the Big-five personality traits. In , 6,373 acoustic-prosodic features like the Interspeech-2013 ComParE feature set (Schuller et al., 2013) were extracted as an input of the SVM classifier for identifying the Big-Five personality traits. In (Carbonneau et al., 2020), the authors learned a discriminating feature dictionary from the extracted patches in the speech spectrograms, followed by the SVM classifier for the classification of the Big-Five personality traits. The recently used audio features for automatic personality trait recognition are deep audio features extracted by deep learning techniques. Su et al. (2017) proposed to employ wavelet-based multiresolution analysis and CNNs for personality trait perception from speech signals. Figure 2 presents the details of the used CNN scheme. The wavelet transform was adopted to decompose the original speech signals at different levels of resolution. Then, based on the extracted prosodic acoustic features, CNNs were leveraged to produce the profiles of the Big-Five Inventory-10 (BFI-10) for a quantitative measure, followed by artificial neural networks (ANNs) for personality trait recognition. Hayat et al. (2019) fine-tuned a pretrained CNN model called AudioSet to learn an audio feature representation for predicting the Big-five personality trait scores of a speaker. They showed the advantages of CNN-based learned features over hand-crafted features.","[['b88', 'b74', 'b93', 'b40', 'b11']]","[['b88', 'b74', 'b93', 'b40', 'b11']]",5,"sent1: The early-used audio features for automatic personality trait recognition are hand-crafted low-level descriptive (LLD) features, such as prosody (intensity, pitch), voice quality (formants), spectral features (Mel Frequency Cepstrum Coefficients, MFCCs), and so on.
sent2: Specially, Mohammadi and Vinciarelli (2012) utilized the LLD features, such as pitch, formants, energy, and speaking rate to detect personality traits in audio clips with less than 10 s.
sent3: They adopted Logistic Regression to identify whether an audio clip exceeded the average score for each of the Big-five personality traits.
sent4: In , 6,373 acoustic-prosodic features like the Interspeech-2013 ComParE feature set (Schuller et al., 2013) were extracted as an input of the SVM classifier for identifying the Big-Five personality traits.
sent5: In (Carbonneau et al., 2020), the authors learned a discriminating feature dictionary from the extracted patches in the speech spectrograms, followed by the SVM classifier for the classification of the Big-Five personality traits.
sent6: The recently used audio features for automatic personality trait recognition are deep audio features extracted by deep learning techniques.
sent7: Su et al. (2017) proposed to employ wavelet-based multiresolution analysis and CNNs for personality trait perception from speech signals.
sent8: Figure 2 presents the details of the used CNN scheme.
sent9: The wavelet transform was adopted to decompose the original speech signals at different levels of resolution.
sent10: Then, based on the extracted prosodic acoustic features, CNNs were leveraged to produce the profiles of the Big-Five Inventory-10 (BFI-10) for a quantitative measure, followed by artificial neural networks (ANNs) for personality trait recognition.
sent11: Hayat et al. (2019) fine-tuned a pretrained CNN model called AudioSet to learn an audio feature representation for predicting the Big-five personality trait scores of a speaker.
sent12: They showed the advantages of CNN-based learned features over hand-crafted features."
248530069,Deep Personality Trait Recognition: A Survey,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc,s18,Text-Based Personality Trait Recognition,"The text modality can effectively display traces of the user's personality (Golbeck et al., 2011). One of the early-used features from text is the popular linguistic inquiry and word count (LIWC; Pennebaker et al., 2001), which is often used to extract lexical features. LIWC divides the words into a variety of psychologically buckets, such as function words (e.g., conjunctions and pronouns), affective words (e.g., amazing and cried), and so on. Then, the used frequency of different categories of words is counted in each bucket in purpose of predicting the personality traits of the writer. Bazelli et al. (2013) predicted the personality traits of Stack Overflow authors by means of analyzing the community's questions and answers on the basis of LIWC. The recently developed Receptiviti API (Golbeck, 2016) is a popular tool using LIWC for personality trait prediction from text in psychology studies. In recent years, several deep learning techniques have been employed for text-based personality trait recognition. Majumder et al. (2017) proposed a deep CNN method for document-level personality prediction from text, as depicted  in Figure 5. The used CNN model consists of seven layers and aims to extract the monogram, bigram, and trigram features from text. Hernandez and Scott (2017) aimed at learning temporal dependencies among sentences by feeding the input text data into simple RNNs and its variants, such as GRU, LSTM, and Bi-LSTM. It was found that LSTM achieved better performance compared to RNN, GRU, and Bi-LSTM on MBTI personality trait recognition tasks. Xue et al. (2018) adopted a hierarchical deep neural network, including an attentive recurrent CNN structure and a variant of the inception structure, to learn deep semantic features from text posts of online social networks for the Big-five personality trait recognition. Sun et al. (2018) presented a model called 2CLSTM, integrating a Bi-LSTM with a CNN, for predicting user's personality on the basis of structures of texts. Mehta et al. (2020a) proposed a deep learningbased model in which conventional psycholinguistic features were combined with language model embeddings like Bidirectional Encoder Representation From Transformers (BERT; Devlin et al., 2018) for personality trait prediction. Ren et al. (2021) presented a multilabel personality prediction model via deep learning, which integrated semantic and emotional features from social media texts. They conducted sentence-level extraction of both semantic and emotion features by means of a BERT model and a SentiNet5 (Vilares et al., 2018) dictionary model, respectively. Then, they fed these features into GRU, LSTM, and CNN for further feature extraction and classification. It was found that BERT+CNN performed best on MBTI and Big-Five personality trait classification tasks.","[['b4', 'b67', 'b29', 'b72', 'b82', 'b116', 'b76', 'b97', 'b42', 'b107', 'b30', 'b19']]","[['b4', 'b67', 'b29', 'b72', 'b82', 'b116', 'b76', 'b97', 'b42', 'b107', 'b30', 'b19']]",12,"sent1: The text modality can effectively display traces of the user's personality (Golbeck et al., 2011).
sent2: One of the early-used features from text is the popular linguistic inquiry and word count (LIWC; Pennebaker et al., 2001), which is often used to extract lexical features.
sent3: LIWC divides the words into a variety of psychologically buckets, such as function words (e.g., conjunctions and pronouns), affective words (e.g., amazing and cried), and so on.
sent4: Then, the used frequency of different categories of words is counted in each bucket in purpose of predicting the personality traits of the writer.
sent5: Bazelli et al. (2013) predicted the personality traits of Stack Overflow authors by means of analyzing the community's questions and answers on the basis of LIWC.
sent6: The recently developed Receptiviti API (Golbeck, 2016) is a popular tool using LIWC for personality trait prediction from text in psychology studies.
sent7: In recent years, several deep learning techniques have been employed for text-based personality trait recognition.
sent8: Majumder et al. (2017) proposed a deep CNN method for document-level personality prediction from text, as depicted  in Figure 5.
sent9: The used CNN model consists of seven layers and aims to extract the monogram, bigram, and trigram features from text.
sent10: Hernandez and Scott (2017) aimed at learning temporal dependencies among sentences by feeding the input text data into simple RNNs and its variants, such as GRU, LSTM, and Bi-LSTM.
sent11: It was found that LSTM achieved better performance compared to RNN, GRU, and Bi-LSTM on MBTI personality trait recognition tasks.
sent12: Xue et al. (2018) adopted a hierarchical deep neural network, including an attentive recurrent CNN structure and a variant of the inception structure, to learn deep semantic features from text posts of online social networks for the Big-five personality trait recognition.
sent13: Sun et al. (2018) presented a model called 2CLSTM, integrating a Bi-LSTM with a CNN, for predicting user's personality on the basis of structures of texts.
sent14: Mehta et al. (2020a) proposed a deep learningbased model in which conventional psycholinguistic features were combined with language model embeddings like Bidirectional Encoder Representation From Transformers (BERT; Devlin et al., 2018) for personality trait prediction.
sent15: Ren et al. (2021) presented a multilabel personality prediction model via deep learning, which integrated semantic and emotional features from social media texts.
sent16: They conducted sentence-level extraction of both semantic and emotion features by means of a BERT model and a SentiNet5 (Vilares et al., 2018) dictionary model, respectively.
sent17: Then, they fed these features into GRU, LSTM, and CNN for further feature extraction and classification.
sent18: It was found that BERT+CNN performed best on MBTI and Big-Five personality trait classification tasks."
248530069,Deep Personality Trait Recognition: A Survey,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc,s10,Convolutional Neural Networks,"Convolutional neural networks (CNNs) were originally proposed by LeCun et al. (1998) in 1998, and initially developed as an advanced version of deep CNNs, such as AlexNet (Krizhevsky et al., 2012) in 2012. The basic structure of CNNs comprises of convolutional layers, pooling layers, as well as fully connected (FC) layers. CNNs usually have multiple convolutional and pooling layers, in which pooling layers are frequently followed by convolutional layers. The convolutional layer adopts a number of learnable filters to perform convolution operation on the whole input image, thereby yielding the corresponding activation feature maps. The pooling layer is employed to reduce the spatial size of produced feature maps by using non-linear down-sampling methods for translation invariance. Two wellknown used pooling strategies are average pooling and max-pooling. The FC layer, in which all neurons are fully connected, is often placed at the end of the CNN network.

It is used to activate the previous layer for producing the final feature representations and classification.

In recent years, several advanced versions of deep CNNs have been presented in various applications. The representative deep CNN models include AlexNet (Krizhevsky et al., 2012), VGGNet (Simonyan and Zisserman, 2014), GoogleNet (Szegedy et al., 2015), ResNet (He et al., 2016), DenseNet , and so on. In particular, DenseNet , in which each layer is connected to each other layer in a feed-forward manner, has been proved that it beats most deep models on objection recognition tasks with less network parameters. Table 2 presents the comparisons of the configurations and characteristics of these typical deep CNNs, as described below.

Compared with the above-mentioned deep CNNs processing 2D images, the recently developed 3D-CNNs (Tran et al., 2015) aim to learn temporal-spatio feature representations by using 3D convolution operations on large-scale video data sets. Some improved versions of 3D-CNNs are also recently proposed to reduce the computation complexity of 3D convolutions. Yang et al. (2019) provided an asymmetric 3D-CNN on the basis of the proposed MicroNets, in which a set of local 3D convolutional networks were adopted so as to incorporate multiscale 3D convolution branches. Kumawat and Raman (2019) proposed a LP-3DCNN in which a rectified local phase volume (ReLPV) block was used to replace the conventional 3D convolutional block. Chen et al. (2020) developed a frequency domain compact 3D-CNN model, in which they utilized a set of learned optimal transformation with few network parameters to implement 3D convolution operations by converting the time domain into the frequency domain.","[['b58', 'b54'], [], ['b54', 'b99', 'b41', 'b92'], ['b55', 'b104', 'b14', 'b119']]","[['b58', 'b54'], [], ['b54', 'b99', 'b41', 'b92'], ['b55', 'b104', 'b14', 'b119']]",10,"sent1: Convolutional neural networks (CNNs) were originally proposed by LeCun et al. (1998) in 1998, and initially developed as an advanced version of deep CNNs, such as AlexNet (Krizhevsky et al., 2012) in 2012.
sent2: The basic structure of CNNs comprises of convolutional layers, pooling layers, as well as fully connected (FC) layers.
sent3: CNNs usually have multiple convolutional and pooling layers, in which pooling layers are frequently followed by convolutional layers.
sent4: The convolutional layer adopts a number of learnable filters to perform convolution operation on the whole input image, thereby yielding the corresponding activation feature maps.
sent5: The pooling layer is employed to reduce the spatial size of produced feature maps by using non-linear down-sampling methods for translation invariance.
sent6: Two wellknown used pooling strategies are average pooling and max-pooling.
sent7: The FC layer, in which all neurons are fully connected, is often placed at the end of the CNN network.
sent8: It is used to activate the previous layer for producing the final feature representations and classification.
sent9: In recent years, several advanced versions of deep CNNs have been presented in various applications.
sent10: The representative deep CNN models include AlexNet (Krizhevsky et al., 2012), VGGNet (Simonyan and Zisserman, 2014), GoogleNet (Szegedy et al., 2015), ResNet (He et al., 2016), DenseNet , and so on.
sent11: In particular, DenseNet , in which each layer is connected to each other layer in a feed-forward manner, has been proved that it beats most deep models on objection recognition tasks with less network parameters.
sent12: Table 2 presents the comparisons of the configurations and characteristics of these typical deep CNNs, as described below.
sent13: Compared with the above-mentioned deep CNNs processing 2D images, the recently developed 3D-CNNs (Tran et al., 2015)
sent14: aim to learn temporal-spatio feature representations by using 3D convolution operations on large-scale video data sets.
sent15: Some improved versions of 3D-CNNs are also recently proposed to reduce the computation complexity of 3D convolutions.
sent16: Yang et al. (2019) provided an asymmetric 3D-CNN on the basis of the proposed MicroNets, in which a set of local 3D convolutional networks were adopted so as to incorporate multiscale 3D convolution branches.
sent17: Kumawat and Raman (2019) proposed a LP-3DCNN in which a rectified local phase volume (ReLPV) block was used to replace the conventional 3D convolutional block.
sent18: Chen et al. (2020) developed a frequency domain compact 3D-CNN model, in which they utilized a set of learned optimal transformation with few network parameters to implement 3D convolution operations by converting the time domain into the frequency domain."
248530069,Deep Personality Trait Recognition: A Survey,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc,s9,Deep Belief Networks,"Deep belief networks (DBNs; Hinton et al., 2006) developed by Hinton et al. in 2006, are a generative model that aim to capture a high-order hierarchical feature representation of input data. The conventional DBN is a multilayered deep architecture, which is built by a sequence of superimposed restricted Boltzmann machines (RBMs; Freund and Haussler, 1994). A RBM is a two-layer generative stochastic neural network consisting of a visual layer and a hidden layer. These two layers in a RBM constitute a bipartite graph without any lateral connection. Training a DBN needs two-stage steps: pretraining and fine-tuning. Pretraining is realized by means of an efficient layer-by-layer greedy learning strategy (Bengio et al., 2007) in an unsupervised manner. During the pretraining process, a contrastive divergence (Hinton, 2002; CD) algorithm is adopted to train RBMs in a DBN to enable the optimization of the weights and bias of DBN models. Then, fine-tuning is performed to update the network parameters by using the back propagation (BP) algorithm.

Several improved versions of DBNs are developed in recent years. Lee et al. (2009), proposed a convolutional deep belief network (CDBN) for full-sized images, in which multiple max-pooling based convolutional RBMs were stacked on the top of one another. Wang et al. (2018) presented a growing DBN with transfer learning (TL-GDBN). TL-GDBN aimed to grow its network structure by means of transferring the learned feature representations from the original structure to the newly developed structure. Then, a partial least squares regression (PLSR)-based fine-tuning was implemented to update the network parameters instead of the traditional BP algorithm.","[['b45', 'b25', 'b5', 'b44'], ['b111', 'b59']]","[['b45', 'b25', 'b5', 'b44'], ['b111', 'b59']]",6,"sent1: Deep belief networks (DBNs; Hinton et al., 2006) developed by Hinton et al. in 2006, are a generative model that aim to capture a high-order hierarchical feature representation of input data.
sent2: The conventional DBN is a multilayered deep architecture, which is built by a sequence of superimposed restricted Boltzmann machines (RBMs; Freund and Haussler, 1994).
sent3: A RBM is a two-layer generative stochastic neural network consisting of a visual layer and a hidden layer.
sent4: These two layers in a RBM constitute a bipartite graph without any lateral connection.
sent5: Training a DBN needs two-stage steps: pretraining and fine-tuning.
sent6: Pretraining is realized by means of an efficient layer-by-layer greedy learning strategy (Bengio et al., 2007) in an unsupervised manner.
sent7: During the pretraining process, a contrastive divergence (Hinton, 2002; CD) algorithm is adopted to train RBMs in a DBN to enable the optimization of the weights and bias of DBN models.
sent8: Then, fine-tuning is performed to update the network parameters by using the back propagation (BP) algorithm.
sent9: Several improved versions of DBNs are developed in recent years.
sent10: Lee et al. (2009), proposed a convolutional deep belief network (CDBN) for full-sized images, in which multiple max-pooling based convolutional RBMs were stacked on the top of one another.
sent11: Wang et al. (2018) presented a growing DBN with transfer learning (TL-GDBN).
sent12: TL-GDBN aimed to grow its network structure by means of transferring the learned feature representations from the original structure to the newly developed structure.
sent13: Then, a partial least squares regression (PLSR)-based fine-tuning was implemented to update the network parameters instead of the traditional BP algorithm."
248530069,Deep Personality Trait Recognition: A Survey,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc,s21,Bimodal Modalities Based Personality Trait Recognition,"For bimodal modalities based personality trait recognition, the widely used one is audio-visual modality. In order to effectively extract audio-visual feature representations of short video sequences, numerical studies have been conducted for audiovisual personality trait recognition. Güçlütürk et al. (2016) developed an end-to-end audio-visual deep residual network for audio-visual apparent personality trait recognition. In detail, the audio data and visual data were firstly extracted from the video clip. Then, the whole audio data were fed into an audio deep residual network for feature learning. Note that the activities of the penultimate layer in the audio deep residual network were temporally pooled. Similarly, the whole visual data were fed into a visual deep residual network with a frame at a time. The activities of the penultimate layer in the visual deep residual network were spatiotemporally pooled. Finally, the pooled activities of the audio and visual stream were concatenated at feature-level as an input of a fully connected layer for personality trait prediction.

Zhang et al., developed a deep bimodal regression (DBR) method so as to capture rich information from the audio and visual modality in videos Wei et al., 2017). Figure 6 shows the flowchart of the proposed DBR method audio-visual personality trait prediction. In particular, for visual feature extraction, they modified the traditional CNNs by means of discarding the fully connected layers. Additionally, they merged the average and max pooled features of the last convolutional layer into a whole feature vector, followed by FIGURE 5 | The flowchart of CNN-based document-level personality prediction from text (Majumder et al., 2017).

the standard L2 normalization. For audio feature extraction, they extracted the logfbank features from the original audio utterances of videos. Then, they trained the linear regressor to produce the Big-Five trait values. To integrate the complementary cues from the audio-visual modality, they fused these predicted regression scores at decision-level. Gürpinar et al. (2016) proposed a multimodal fusion method of audio and visual (scene and face) features for personality trait analysis. They fine-tuned a pretrained VGG model to derive facial emotion and ambient information from images. They also extracted local Gabor binary patterns from three orthogonal planes (LGBP-TOP) video descriptor as video features. The typical acoustic features, such as the INTERSPEECH-2009INTERSPEECH- , 2010INTERSPEECH- , 2012, and 2013 feature set in computational paralinguistics challenges, were employed. The kernel ELM was adopted for personality trait prediction on audio and visual (scene and face) modalities. Finally, a score-level method was leveraged to fuse the results of different modalities. Subramaniam et al. (2016) employed two end-to-end deep learning models for audio-visual first impression analysis. They used a volumetric (3D) convolution network for visual feature extraction from face aligned images. For audio feature extraction, they obtained the statistics, such as mean and standard deviation of hand-crafted features like zero-crossing rate, energy, MFCCs, etc. Then, they concatenated the extracted audio and visual features at feature-level, followed by a multimodal LSTM network of temporal modeling for final personality trait prediction tasks. Xianyu et al. (2016) proposed an unsupervised cross-modal feature learning method, called heterogeneity entropy (HE) Big-five traits FIGURE 6 | The flowchart of the proposed DBR method for audio-visual personality trait prediction (Wei et al., 2017).

neural network (HENN), for multimodal personality trait prediction. The proposed HENN consists of HE-DBN, HE-AE, and common DBN and is used to learn common feature representations among text, image, and behavior statistical modalities, and then map them into the user's personality. The input of HENN is hand-crafted features. In particular, a bag of textual word (BoTW; Li et al., 2016) model was used to extract the text feature vector. Based on the extracted scaleinvariant feature transform (SIFT; Cruz-Mota et al., 2012) features of each image, a bag of visual word model was used to produce visual image features. The time series information related to sharing numbers and comment numbers in both text and image modalities were employed to compute behavior statistical parameters. These hand-crafted features were individually fed into three HE-DBNs for initial feature learning, and then HE-AE and common DBN were separately adopted to fuse these features produced with HE-DBNs at model-level for final Big-Five personality prediction. Principi et al. (2019) developed a multimodal deep learning model combining the raw visual with audio streams to conduct the Big-Five personality trait prediction. For each video sample, different task-specific deep models, related to individual factor, such as facial expressions, attractiveness, age, gender, and ethnicity, were leveraged to estimate per-frame attribute. Then, these estimated results were concatenated at feature-level to produce a video-level attribute prediction by spatio-temporal aggregation methods. For visual feature extraction, they adopted a ResNet-50 network pretrained on the ImageNet data to produce high-level feature representations on each video frame. For audio feature extraction, a 14-layer 1D CNN like the ResNet-18 was used. They fused these modalities in two steps. First, they employed a FC layer for model-level fusion to learn the joint feature representations of the concatenated video-level attribute predictions. This model-level fusion step was also used to reduce the dimensionality of the concatenated videolevel attribute predictions. Second, they combined such learned joint video-level attribute predictions with the extracted audio and visual features at feature-level, to perform final the Big-Five personality trait prediction. Curto et al. (2021) developed the Dyadformer for modeling individual and interpersonal audio-visual features in dyadic interactions for personality trait prediction. The Dyadformer was a multimodal multisubject Transformer framework consisting of a set of attention encoder modules (self, cross-modal, and cross-subject) with Transformer layers. They employed the pretrained VGGish (Hershey et al., 2017) model to produce a 128-dimensional embedding for each audio chunk. They leveraged the pretrained R(2 + 1)D (Tran et al., 2018) model to generate a 512-dimensional embedding for each video chunk. They used cross-modal and cross-subject attentions for multimodal Transformer fusion in model-level. Li et al. (2020b) presented a deep classification-regression network (CR-Net) to predict the multimodal Big-Five personality traits based on video, audio, and text cues and further applied to the job interview recommendation. For the visual input, they extracted the global scene cues and local face cues by using the ResNet-34 network. Considering audio-text inner correlations, they concatenated the extracted acoustic LLD and text-based skip-thought vectors at feature-level as inputs of the ResNet-34 network for audio-text feature learning. Finally, they merged all extracted features from visual, audio, and text modalities at feature-level and fed them into the CR-Net network to analyze the multimodal Big-Five personality traits. Güçlütürk et al. (2017) presented a method of multimodal first impression analysis integrating audio, visual, and text (language) modalities, based on deep residual networks. They adopted two similar 17-layer deep residual networks for extracting audio-visual features. The used 17-layer deep residual networks consist of one convolutional layer and eight residual blocks of two convolutional layers. The pooled activities of audio-visual networks were concatenated as an input of a fully connected layer so as to learn the joint audio-visual feature representations. For text feature extraction, they utilized two language models, including a bag-of-words model and a skip-thought vector model, to produce the annotations as a function of the language data. Both of the language models contain an embedding layer, followed by a linear layer. Finally, they combined the extracted features from audio, visual, and text at feature-level for the multimodal Big-five personality trait analysis and job interview recommendation. Gorbova et al. (2017Gorbova et al. ( , 2018 provided an automatic personality screening method on the basis of visual, audio, and text (lexical) cues from short video clips for predicting the Big-five personality traits. The extracted hand-crafted features contained acoustic LLD features (MFCCs, ZCR, speaking rate, etc.), facial action unit features, as well as negative and positive word scores. This system adopted the weighted average strategy to fuse the final obtained results from three modalities at decision-level. Figure 7 shows the flowchart of integrating audio, vision, and language for first impression personality analysis (Gorbova et al., 2018). In Figure 7, after extracted audio, visual, and lexical features from input video, three separate LSTM cells were used for modeling long dependency. Then, the hidden features in LSTMs were processed by a linear regressor. Finally, the obtained results were fed to an output layer for the Big-five personality trait analysis. Kampman et al. (2018) presented an end-to-end trimodal deep learning architecture for predicting the Big-Five personality traits by means of integrating audio, visual, and text modalities. For audio channel, the raw audio waveform and its energy components with squared amplitude were fed into a CNN network with four convolutional layers and a global average pooling layer for audio feature extraction. For visual channel, based on a random frame image of a video, they fine-tuned the pretrained VGG-16 model for video feature extraction. For text channel, they adopted ""Word2vec"" word embedding from transcriptions as an input of a CNN network for text feature extraction. In this text CNN network, three different convolutional windows corresponding to three, four, and five words over the sentence were used. Finally, they fused audio, visual, and text modalities at both decision-level and model-level. For decisionlevel fusion, a voting scheme was used. For model-level fusion, by means of concatenating the output of FC layers of each CNN, they added another two FC layers on top to learn shared feature representations of input trimodal data.","[['b36'], ['b67', 'b112'], ['b115', 'b112', 'b38', None, 'b94'], ['b43', 'b17', 'b105', 'b60', 'b51', 'b63', 'b35', 'b78', 'b18', 'b31', 'b32']]","[['b36'], ['b67', 'b112'], ['b115', 'b112', 'b38', None, 'b94'], ['b43', 'b17', 'b105', 'b60', 'b51', 'b63', 'b35', 'b78', 'b18', 'b31', 'b32']]",19,"sent1: For bimodal modalities based personality trait recognition, the widely used one is audio-visual modality.
sent2: In order to effectively extract audio-visual feature representations of short video sequences, numerical studies have been conducted for audiovisual personality trait recognition.
sent3: Güçlütürk et al. (2016) developed an end-to-end audio-visual deep residual network for audio-visual apparent personality trait recognition.
sent4: In detail, the audio data and visual data were firstly extracted from the video clip.
sent5: Then, the whole audio data were fed into an audio deep residual network for feature learning.
sent6: Note that the activities of the penultimate layer in the audio deep residual network were temporally pooled.
sent7: Similarly, the whole visual data were fed into a visual deep residual network with a frame at a time.
sent8: The activities of the penultimate layer in the visual deep residual network were spatiotemporally pooled.
sent9: Finally, the pooled activities of the audio and visual stream were concatenated at feature-level as an input of a fully connected layer for personality trait prediction.
sent10: Zhang et al., developed a deep bimodal regression (DBR) method so as to capture rich information from the audio and visual modality in videos Wei et al., 2017).
sent11: Figure 6 shows the flowchart of the proposed DBR method audio-visual personality trait prediction.
sent12: In particular, for visual feature extraction, they modified the traditional CNNs by means of discarding the fully connected layers.
sent13: Additionally, they merged the average and max pooled features of the last convolutional layer into a whole feature vector, followed by FIGURE 5 | The flowchart of CNN-based document-level personality prediction from text (Majumder et al., 2017).
sent14: the standard L2 normalization. For audio feature extraction, they extracted the logfbank features from the original audio utterances of videos.
sent15: Then, they trained the linear regressor to produce the Big-Five trait values.
sent16: To integrate the complementary cues from the audio-visual modality, they fused these predicted regression scores at decision-level.
sent17: Gürpinar et al. (2016) proposed a multimodal fusion method of audio and visual (scene and face) features for personality trait analysis.
sent18: They fine-tuned a pretrained VGG model to derive facial emotion and ambient information from images.
sent19: They also extracted local Gabor binary patterns from three orthogonal planes (LGBP-TOP) video descriptor as video features.
sent20: The typical acoustic features, such as the INTERSPEECH-2009INTERSPEECH- , 2010INTERSPEECH- , 2012, and 2013 feature set in computational paralinguistics challenges, were employed.
sent21: The kernel ELM was adopted for personality trait prediction on audio and visual (scene and face) modalities.
sent22: Finally, a score-level method was leveraged to fuse the results of different modalities.
sent23: Subramaniam et al. (2016) employed two end-to-end deep learning models for audio-visual first impression analysis.
sent24: They used a volumetric (3D) convolution network for visual feature extraction from face aligned images.
sent25: For audio feature extraction, they obtained the statistics, such as mean and standard deviation of hand-crafted features like zero-crossing rate, energy, MFCCs, etc.
sent26: Then, they concatenated the extracted audio and visual features at feature-level, followed by a multimodal LSTM network of temporal modeling for final personality trait prediction tasks.
sent27: Xianyu et al. (2016) proposed an unsupervised cross-modal feature learning method, called heterogeneity entropy (HE) Big-five traits FIGURE 6 | The flowchart of the proposed DBR method for audio-visual personality trait prediction (Wei et al., 2017).neural network (HENN), for multimodal personality trait prediction.
sent28: The proposed HENN consists of HE-DBN, HE-AE, and common DBN and is used to learn common feature representations among text, image, and behavior statistical modalities, and then map them into the user's personality.
sent29: The input of HENN is hand-crafted features.
sent30: In particular, a bag of textual word (BoTW; Li et al., 2016) model was used to extract the text feature vector.
sent31: Based on the extracted scaleinvariant feature transform (SIFT; Cruz-Mota et al., 2012) features of each image, a bag of visual word model was used to produce visual image features.
sent32: The time series information related to sharing numbers and comment numbers in both text and image modalities were employed to compute behavior statistical parameters.
sent33: These hand-crafted features were individually fed into three HE-DBNs for initial feature learning, and then HE-AE and common DBN were separately adopted to fuse these features produced with HE-DBNs at model-level for final Big-Five personality prediction.
sent34: Principi et al. (2019) developed a multimodal deep learning model combining the raw visual with audio streams to conduct the Big-Five personality trait prediction.
sent35: For each video sample, different task-specific deep models, related to individual factor, such as facial expressions, attractiveness, age, gender, and ethnicity, were leveraged to estimate per-frame attribute.
sent36: Then, these estimated results were concatenated at feature-level to produce a video-level attribute prediction by spatio-temporal aggregation methods.
sent37: For visual feature extraction, they adopted a ResNet-50 network pretrained on the ImageNet data to produce high-level feature representations on each video frame.
sent38: For audio feature extraction, a 14-layer 1D CNN like the ResNet-18 was used.
sent39: They fused these modalities in two steps.
sent40: First, they employed a FC layer for model-level fusion to learn the joint feature representations of the concatenated video-level attribute predictions.
sent41: This model-level fusion step was also used to reduce the dimensionality of the concatenated videolevel attribute predictions.
sent42: Second, they combined such learned joint video-level attribute predictions with the extracted audio and visual features at feature-level, to perform final the Big-Five personality trait prediction.
sent43: Curto et al. (2021) developed the Dyadformer for modeling individual and interpersonal audio-visual features in dyadic interactions for personality trait prediction.
sent44: The Dyadformer was a multimodal multisubject Transformer framework consisting of a set of attention encoder modules (self, cross-modal, and cross-subject) with Transformer layers.
sent45: They employed the pretrained VGGish (Hershey et al., 2017) model to produce a 128-dimensional embedding for each audio chunk.
sent46: They leveraged the pretrained R(2 + 1)D (Tran et al., 2018) model to generate a 512-dimensional embedding for each video chunk.
sent47: They used cross-modal and cross-subject attentions for multimodal Transformer fusion in model-level.
sent48: Li et al. (2020b) presented a deep classification-regression network (CR-Net) to predict the multimodal Big-Five personality traits based on video, audio, and text cues and further applied to the job interview recommendation.
sent49: For the visual input, they extracted the global scene cues and local face cues by using the ResNet-34 network.
sent50: Considering audio-text inner correlations, they concatenated the extracted acoustic LLD and text-based skip-thought vectors at feature-level as inputs of the ResNet-34 network for audio-text feature learning.
sent51: Finally, they merged all extracted features from visual, audio, and text modalities at feature-level and fed them into the CR-Net network to analyze the multimodal Big-Five personality traits.
sent52: Güçlütürk et al. (2017) presented a method of multimodal first impression analysis integrating audio, visual, and text (language) modalities, based on deep residual networks.
sent53: They adopted two similar 17-layer deep residual networks for extracting audio-visual features.
sent54: The used 17-layer deep residual networks consist of one convolutional layer and eight residual blocks of two convolutional layers.
sent55: The pooled activities of audio-visual networks were concatenated as an input of a fully connected layer so as to learn the joint audio-visual feature representations.
sent56: For text feature extraction, they utilized two language models, including a bag-of-words model and a skip-thought vector model, to produce the annotations as a function of the language data.
sent57: Both of the language models contain an embedding layer, followed by a linear layer.
sent58: Finally, they combined the extracted features from audio, visual, and text at feature-level for the multimodal Big-five personality trait analysis and job interview recommendation.
sent59: Gorbova et al. (2017Gorbova et al. ( , 2018 provided an automatic personality screening method on the basis of visual, audio, and text (lexical) cues from short video clips for predicting the Big-five personality traits.
sent60: The extracted hand-crafted features contained acoustic LLD features (MFCCs, ZCR, speaking rate, etc.), facial action unit features, as well as negative and positive word scores.
sent61: This system adopted the weighted average strategy to fuse the final obtained results from three modalities at decision-level.
sent62: Figure 7 shows the flowchart of integrating audio, vision, and language for first impression personality analysis (Gorbova et al., 2018).
sent63: In Figure 7, after extracted audio, visual, and lexical features from input video, three separate LSTM cells were used for modeling long dependency.
sent64: Then, the hidden features in LSTMs were processed by a linear regressor.
sent65: Finally, the obtained results were fed to an output layer for the Big-five personality trait analysis.
sent66: Kampman et al. (2018) presented an end-to-end trimodal deep learning architecture for predicting the Big-Five personality traits by means of integrating audio, visual, and text modalities.
sent67: For audio channel, the raw audio waveform and its energy components with squared amplitude were fed into a CNN network with four convolutional layers and a global average pooling layer for audio feature extraction.
sent68: For visual channel, based on a random frame image of a video, they fine-tuned the pretrained VGG-16 model for video feature extraction.
sent69: For text channel, they adopted ""Word2vec"" word embedding from transcriptions as an input of a CNN network for text feature extraction.
sent70: In this text CNN network, three different convolutional windows corresponding to three, four, and five words over the sentence were used.
sent71: Finally, they fused audio, visual, and text modalities at both decision-level and model-level.
sent72: For decisionlevel fusion, a voting scheme was used.
sent73: For model-level fusion, by means of concatenating the output of FC layers of each CNN, they added another two FC layers on top to learn shared feature representations of input trimodal data."
248530069,Deep Personality Trait Recognition: A Survey,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc,s11,Recurrent Neural Networks,"Recurrent neural networks (RNNs; Elman, 1990) are a single feed-forward neural network for capturing temporal information, and thus suitable to deal with sequence data. RNNs contain recurrent edges connecting adjacent time steps, thereby providing the concept of time in this model. In addition, RNNs share the same network parameters across all time steps. For training RNNs, the traditional back propagation through time (BPTT; Werbos, 1990) was usually adopted.

Long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997), proposed by Hochreiter and Schmidhuber in 1997, is a relatively new recurrent network architecture, which is combined with a suitable gradient-based learning fashion. Specially, LSTMs aim to alleviate the gradient vanishing and exploding problems produced during the procedure of training RNNs. There are three types of gates in a LSTM cell unit: input gate, forget gate, and output gate. Input gate is used to control how much of the current input data is flowing into the memory unit of the network. Forget gate, as a key component of the LSTM cell unit, is used for controlling which information to keep and which to forget, and somehow avoiding the gradient loss and explosion problems. Output gate controls the effect of the memory cell on the current output value. On the basis of these three special gates, LSTMs have an ability of modeling long-term dependencies of sequence data, such as video sequences.

In recent years, a variant of LSTMs called gated recurrent unit (GRU; Chung et al., 2014) ","[['b21', 'b113'], ['b46'], ['b15']]","[['b21', 'b113'], ['b46'], ['b15']]",4,"sent1: Recurrent neural networks (RNNs; Elman, 1990) are a single feed-forward neural network for capturing temporal information, and thus suitable to deal with sequence data.
sent2: RNNs contain recurrent edges connecting adjacent time steps, thereby providing the concept of time in this model.
sent3: In addition, RNNs share the same network parameters across all time steps.
sent4: For training RNNs, the traditional back propagation through time (BPTT; Werbos, 1990) was usually adopted.
sent5: Long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997), proposed by Hochreiter and Schmidhuber in 1997, is a relatively new recurrent network architecture, which is combined with a suitable gradient-based learning fashion.
sent6: Specially, LSTMs aim to alleviate the gradient vanishing and exploding problems produced during the procedure of training RNNs.
sent7: There are three types of gates in a LSTM cell unit: input gate, forget gate, and output gate.
sent8: Input gate is used to control how much of the current input data is flowing into the memory unit of the network.
sent9: Forget gate, as a key component of the LSTM cell unit, is used for controlling which information to keep and which to forget, and somehow avoiding the gradient loss and explosion problems.
sent10: Output gate controls the effect of the memory cell on the current output value.
sent11: On the basis of these three special gates, LSTMs have an ability of modeling long-term dependencies of sequence data, such as video sequences.
sent12: In recent years, a variant of LSTMs called gated recurrent unit (GRU; Chung et al., 2014)"
248530069,Deep Personality Trait Recognition: A Survey,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc,s22,Trimodal Modalities Based Personality Trait Recognition,"Escalante et al. explored the explainability in first impressions analysis from video sequences at the first time. They provided a baseline method of integrating audio, visual, and text (audio transcripts) information (Escalante et al., 2020). They used a variant of original 18-layer deep residual networks (ResNet-18) for audio and visual feature extraction, respectively. The featurelevel fusion was adopted after the global average pooling layers of the audio-visual ResNet-18 models via concatenation of their obtained latent features. For text modality, two language models, such as a skip-thought vector model and a bag-of-words model, were employed for text feature extraction. Finally, a concatenation of audio, visual, text-based latent features was implemented at feature-level for multimodal first-impression analysis. Suman et al. (2022) developed a deep learning-based multimodal personality prediction system integrating audio, visual, and text modalities. They extracted facial and ambient features from the visual modality by using Multi-task Cascaded Convolutional Neural Networks (MTCNN; Jiang et al., 2018) and ResNet, individually. They extracted the audio features by using a VGGish (Hershey et al., 2017) model, and the text features by using an n-gram CNN model, respectively. These extracted audio, visual, and text features were fed into a fully connected layer followed by a sigmoid function for the final personality trait prediction. It was concluded that the concatenation of audio, visual, and text features in feature-level fusion showed comparable performance with the averaging method in decision-level fusion.","[['b43', 'b23', 'b96', 'b48']]","[['b43', 'b23', 'b96', 'b48']]",4,"sent1: Escalante et al. explored the explainability in first impressions analysis from video sequences at the first time.
sent2: They provided a baseline method of integrating audio, visual, and text (audio transcripts) information (Escalante et al., 2020).
sent3: They used a variant of original 18-layer deep residual networks (ResNet-18) for audio and visual feature extraction, respectively.
sent4: The featurelevel fusion was adopted after the global average pooling layers of the audio-visual ResNet-18 models via concatenation of their obtained latent features.
sent5: For text modality, two language models, such as a skip-thought vector model and a bag-of-words model, were employed for text feature extraction.
sent6: Finally, a concatenation of audio, visual, text-based latent features was implemented at feature-level for multimodal first-impression analysis.
sent7: Suman et al. (2022) developed a deep learning-based multimodal personality prediction system integrating audio, visual, and text modalities.
sent8: They extracted facial and ambient features from the visual modality by using Multi-task Cascaded Convolutional Neural Networks (MTCNN; Jiang et al., 2018) and ResNet, individually.
sent9: They extracted the audio features by using a VGGish (Hershey et al., 2017) model, and the text features by using an n-gram CNN model, respectively.
sent10: These extracted audio, visual, and text features were fed into a fully connected layer followed by a sigmoid function for the final personality trait prediction.
sent11: It was concluded that the concatenation of audio, visual, and text features in feature-level fusion showed comparable performance with the averaging method in decision-level fusion."
248530069,Deep Personality Trait Recognition: A Survey,"Computer Science, Psychology, Medicine",https://www.semanticscholar.org/paper/9f704ce3bcf044b06050ea966af67974cd6a41dc,s8,REVIEW OF DEEP LEARNING TECHNIQUES,"In recent years, deep learning techniques have been an active research subject and obtained promising performance in various applications, such as object detection and classification, speech processing, natural language processing, and so on (Yu and Deng, 2010;LeCun et al., 2015;Schmidhuber, 2015;Zhao et al., 2015). In essence, deep learning methods aim to achieve high-level abstract representations by means of hierarchical architectures of multiple non-linear transformations. After implementing feature extraction with deep learning techniques, the Softmax (Sigmoid) function is usually for classification or prediction. In this section, we briefly review several representative deep learning methods and its recent variants, which can be potentially used for personality trait analysis.","[['b120', 'b86', 'b57', 'b125']]","[['b120', 'b86', 'b57', 'b125']]",4,"sent1: In recent years, deep learning techniques have been an active research subject and obtained promising performance in various applications, such as object detection and classification, speech processing, natural language processing, and so on (Yu and Deng, 2010;LeCun et al., 2015;Schmidhuber, 2015;Zhao et al., 2015).
sent2: In essence, deep learning methods aim to achieve high-level abstract representations by means of hierarchical architectures of multiple non-linear transformations.
sent3: After implementing feature extraction with deep learning techniques, the Softmax (Sigmoid) function is usually for classification or prediction.
sent4: In this section, we briefly review several representative deep learning methods and its recent variants, which can be potentially used for personality trait analysis."
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s23,Supervised learning for VSR,"There are two mainstream VSR tasks: word-level and sentencelevel. With a limited number of word categories, the former is to recognize isolated words from the input videos (i.e., talking face video classification), usually trained with multi-classification cross-entropy loss. The latter is to make unconstrained sentencelevel sequence prediction. However, due to the unconstrained word categories and video frame length, it is much more complicated than the word-level VSR task.

Supervised learning of end-to-end sentence-level VSR tasks (sentence prediction) can be divided into two types. Given the input sequence, the first type uses a neural network as an emission model, which outputs the likelihood of each output symbol (e.g., phonemes, characters, words). These methods generally employ a second phase of decoding using HMM. A popular version of this variant is the Contortionist Temporal Classification (CTC) [141], where the model predicts framewise labels and then looks for the optimal alignment between the frame-wise predictions and the output sequence. The main weakness of CTC is that the output labels are not conditioned on each other (it assumes each unit is conditional independent), and hence a language model is needed as a post-processing step. Different from the basic CTC, Xu et al. [62] proposed LCANet that feeds the encoded spatio-temporal features into a cascaded attention CTC decoder. The introduction of an attention mechanism improves the defect of the conditional independence assumption CTC in hidden neural layers. Another assumption of this approach is that it assumes a monotonic ordering between input and output sequences, which is suitable for VSR but not for machine translation.

The second type is sequence-to-sequence (seq2seq) models that first read the whole input sequence before predicting the output sentence. A number of works adopted this approach for speech recognition [142]. Chan et al. [143] proposed an elegant seq2seq method to transcribe audio signal to characters. Seq2seq models decode an output symbol at time t (e.g., phonemes, characters, words) conditioned on previous outputs 1, ..., t − 1. Thus, unlike CTC-based models, the model implicitly learns a language model over output symbols, and no further processing is required. However, it has been shown [144] that it is beneficial to incorporate an external language model in the decoding of seq2seq models as well. Chung et al. [28] proposed the WAS (Watch, Attend and Spell) model, which is a classical seq2seq VSR model. With the help of attention mechanism, WAS model is more capable of capturing long-term dependency.

Based on the transformer backbone architecture, Afouras et al. [6] have deeply analyzed the pros and cons of the CTC model and the seq2seq model for VSR. Generally, the seq2seq model performs well than the CTC model in the sentence-level VSR task. But, the seq2seq model needs more training time and inference time. Besides, the CTC model generalizes better and adapts faster as the sequence lengths are increased. Besides the above label-level supervised learning paradigms, feature-level supervised learning is also widely explored in VSR. Knowledge distillation [145] ","[[], ['b123', 'b61'], ['b27', 'b126', 'b125', 'b124'], ['b5', 'b127']]","[[], ['b123', 'b61'], ['b27', 'b126', 'b125', 'b124'], ['b5', 'b127']]",8,"sent1: There are two mainstream VSR tasks: word-level and sentencelevel.
sent2: With a limited number of word categories, the former is to recognize isolated words from the input videos (i.e., talking face video classification), usually trained with multi-classification cross-entropy loss.
sent3: The latter is to make unconstrained sentencelevel sequence prediction.
sent4: However, due to the unconstrained word categories and video frame length, it is much more complicated than the word-level VSR task.
sent5: Supervised learning of end-to-end sentence-level VSR tasks (sentence prediction) can be divided into two types.
sent6: Given the input sequence, the first type uses a neural network as an emission model, which outputs the likelihood of each output symbol (e.g., phonemes, characters, words).
sent7: These methods generally employ a second phase of decoding using HMM.
sent8: A popular version of this variant is the Contortionist Temporal Classification (CTC) [141], where the model predicts framewise labels and then looks for the optimal alignment between the frame-wise predictions and the output sequence.
sent9: The main weakness of CTC is that the output labels are not conditioned on each other (it assumes each unit is conditional independent), and hence a language model is needed as a post-processing step.
sent10: Different from the basic CTC, Xu et al. [62] proposed LCANet that feeds the encoded spatio-temporal features into a cascaded attention CTC decoder.
sent11: The introduction of an attention mechanism improves the defect of the conditional independence assumption CTC in hidden neural layers.
sent12: Another assumption of this approach is that it assumes a monotonic ordering between input and output sequences, which is suitable for VSR but not for machine translation.
sent13: The second type is sequence-to-sequence (seq2seq) models that first read the whole input sequence before predicting the output sentence.
sent14: A number of works adopted this approach for speech recognition [142].
sent15: Chan et al. [143] proposed an elegant seq2seq method to transcribe audio signal to characters.
sent16: Seq2seq models decode an output symbol at time t (e.g., phonemes, characters, words) conditioned on previous outputs 1, ..., t − 1.
sent17: Thus, unlike CTC-based models, the model implicitly learns a language model over output symbols, and no further processing is required.
sent18: However, it has been shown [144] that it is beneficial to incorporate an external language model in the decoding of seq2seq models as well.
sent19: Chung et al. [28] proposed the WAS (Watch, Attend and Spell) model, which is a classical seq2seq VSR model.
sent20: With the help of attention mechanism, WAS model is more capable of capturing long-term dependency.
sent21: Based on the transformer backbone architecture, Afouras et al. [6] have deeply analyzed the pros and cons of the CTC model and the seq2seq model for VSR.
sent22: Generally, the seq2seq model performs well than the CTC model in the sentence-level VSR task.
sent23: But, the seq2seq model needs more training time and inference time.
sent24: Besides, the CTC model generalizes better and adapts faster as the sequence lengths are increased.
sent25: Besides the above label-level supervised learning paradigms, feature-level supervised learning is also widely explored in VSR.
sent26: Knowledge distillation [145]"
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s21,Temporal backend network,"The temporal backend network built upon visual features aims to further aggregate context information. In traditional VSR, classical statistical models (e.g., Hidden Markov Model, HMM) are commonly used for temporal information aggregation.

RNN-based Architectures. In the field of deep learning, Recurrent Neural Networks (RNNs) are representative network structures used to learn sequence data. The typical RNN structures (e.g., LSTM and GRU) are shown in Fig. 6, and their basic structures are similar to that of HMM, in which the dependencies between the observed state sequences are described by the transformation of the hidden state sequence. Compared to HMM, RNNs have a more powerful representation ability due to the nonlinear transformation during hidden state transitions. Bidirectional RNNs (BiRNNs) are variations of basic RNNs, which attempt to aggregate context information from previous timesteps as well as future timesteps. Many works [28,50,55,56,127,132,133,134,135] have adopted RNN-based (BiLSTM or BiGRU) network architectures as the temporal backend network in VSR. Beyond above fundamental RNN structures, various modifications [58,136] have been made to improve feature learning for VSR. For example, Wang et al. [58] utilized BiConvLSTM [137] as temporal backend network. ConvLSTM is a convolutional counterpart of conventional fully connected LSTM, which models temporal dependency while preserving spatial information. Wang et al.integrated the attention mechanism into the model to further improve the BiConvLSTM architecture.

Transformer-based Architectures. Compared to RNN-based architectures, Transformers [138] have significant advantages in long-term dependency and parallel computation. However, transformers usually suffer from some drawbacks. First, transformers are more prone to overfitting than RNNs and TCNs in small-scale datasets. Second, transformers are limited in some specific tasks (e.g., word-level VSR tasks) with shortterm context. Therefore, transformers are more suitable for sentence-level VSR tasks, rather than word-level VSR tasks. [6] is the first work introducing transformers to VSR. Based on the basic transformer architecture, the authors proposed two types of backend models: TM-seq2seq and TM-CTC. The difference between the two models lies in the training target. The experiments clearly showed that the transformer-based backend network performs much better than the RNN-based backend network in the sentence-level VSR task. Since the basic transformer pays no extra attention to short-term dependency, Zhang et al. [61] proposed multiple Temporal Focal blocks (TF-blocks), helping features to look around their neighbors and capturing more short-term temporal dependencies. The results demonstrated that the short-term dependency is as crucial as the long-term dependency in sentence-level VSR.

TCN-based Architectures. In the context of deep sequence models, RNNs and Transformers have a high demand for memory and computation ability. Temporal Convolutional Networks (TCNs) are another type of deep sequence model, and various improvements have been applied to the basic TCN to make them more appropriate for VSR. For example, Afouras et al. [60] used depth-wise separable convolution (DS-TCN) for sentence-level VSR. However, the performance of DS-TCN does not work as well as transformers, as TCN-based models have poor ability on capturing long-term dependency. To enable temporal backend network capturing multi-scale temporal patterns, Martinez [59] proposed to utilize multi-scale TCN (MS-TCN) structure, which achieved SOTA results (87.9% Acc) on the word-level LRW dataset. Table. 2 summarizes the general pros and cons of various visual frontend networks and temporal backend networks, and the available inputs of the corresponding visual frontend network. As we know, most of the existing VSR models are derived from general backbone models used in other fields (e.g., action recognition [139,140], audio speech recognition [137], etc.), and few are designed explicitly for VSR. Therefore, more attention should be paid to the particular structure adaptive to the properties of VSR in the future.","[[], ['b27', 'b115', 'b119', 'b49', 'b57', 'b55', 'b116', 'b54', 'b114', 'b109', 'b118', 'b117'], ['b120', 'b5', 'b60'], ['b119', 'b59', 'b58', 'b121', 'b122']]","[[], ['b27', 'b115', 'b119', 'b49', 'b57', 'b55', 'b116', 'b54', 'b114', 'b109', 'b118', 'b117'], ['b120', 'b5', 'b60'], ['b119', 'b59', 'b58', 'b121', 'b122']]",20,"sent1: The temporal backend network built upon visual features aims to further aggregate context information.
sent2: In traditional VSR, classical statistical models (e.g., Hidden Markov Model, HMM) are commonly used for temporal information aggregation.
sent3: RNN-based Architectures. In the field of deep learning, Recurrent Neural Networks (RNNs) are representative network structures used to learn sequence data.
sent4: The typical RNN structures (e.g., LSTM and GRU) are shown in Fig. 6, and their basic structures are similar to that of HMM, in which the dependencies between the observed state sequences are described by the transformation of the hidden state sequence.
sent5: Compared to HMM, RNNs have a more powerful representation ability due to the nonlinear transformation during hidden state transitions.
sent6: Bidirectional RNNs (BiRNNs) are variations of basic RNNs, which attempt to aggregate context information from previous timesteps as well as future timesteps.
sent7: Many works [28,50,55,56,127,132,133,134,135] have adopted RNN-based (BiLSTM or BiGRU) network architectures as the temporal backend network in VSR.
sent8: Beyond above fundamental RNN structures, various modifications [58,136] have been made to improve feature learning for VSR.
sent9: For example, Wang et al. [58] utilized BiConvLSTM [137] as temporal backend network.
sent10: ConvLSTM is a convolutional counterpart of conventional fully connected LSTM, which models temporal dependency while preserving spatial information.
sent11: Wang et al.integrated the attention mechanism into the model to further improve the BiConvLSTM architecture.
sent12: Transformer-based Architectures.
sent13: Compared to RNN-based architectures, Transformers [138] have significant advantages in long-term dependency and parallel computation.
sent14: However, transformers usually suffer from some drawbacks.
sent15: First, transformers are more prone to overfitting than RNNs and TCNs in small-scale datasets.
sent16: Second, transformers are limited in some specific tasks (e.g., word-level VSR tasks) with shortterm context.
sent17: Therefore, transformers are more suitable for sentence-level VSR tasks, rather than word-level VSR tasks.
sent18: [6] is the first work introducing transformers to VSR.
sent19: Based on the basic transformer architecture, the authors proposed two types of backend models: TM-seq2seq and TM-CTC.
sent20: The difference between the two models lies in the training target.
sent21: The experiments clearly showed that the transformer-based backend network performs much better than the RNN-based backend network in the sentence-level VSR task.
sent22: Since the basic transformer pays no extra attention to short-term dependency, Zhang et al. [61] proposed multiple Temporal Focal blocks (TF-blocks), helping features to look around their neighbors and capturing more short-term temporal dependencies.
sent23: The results demonstrated that the short-term dependency is as crucial as the long-term dependency in sentence-level VSR.
sent24: TCN-based Architectures. In the context of deep sequence models, RNNs and Transformers have a high demand for memory and computation ability.
sent25: Temporal Convolutional Networks (TCNs) are another type of deep sequence model, and various improvements have been applied to the basic TCN to make them more appropriate for VSR.
sent26: For example, Afouras et al. [60] used depth-wise separable convolution (DS-TCN) for sentence-level VSR.
sent27: However, the performance of DS-TCN does not work as well as transformers, as TCN-based models have poor ability on capturing long-term dependency.
sent28: To enable temporal backend network capturing multi-scale temporal patterns, Martinez [59] proposed to utilize multi-scale TCN (MS-TCN) structure, which achieved SOTA results (87.9% Acc) on the word-level LRW dataset.
sent29: Table. 2 summarizes the general pros and cons of various visual frontend networks and temporal backend networks, and the available inputs of the corresponding visual frontend network.
sent30: As we know, most of the existing VSR models are derived from general backbone models used in other fields (e.g., action recognition
sent31: [139,140], audio speech recognition [137], etc.), and few are designed explicitly for VSR.
sent32: Therefore, more attention should be paid to the particular structure adaptive to the properties of VSR in the future."
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s15,Evaluation Metrics on VSG,"Appropriate evaluation for VSG continues to be an open problem, and many recent works have explored various evaluation metrics on VSG. We categorize those metrics based on three learning targets, i.e., identity preservation, visual quality, audio-visual semantic consistency.

Identity Preservation. One of the most important goals of VSG is to preserve the target identity as much as possible during video generation, as humans are quite sensitive to subtle appearance changes in synthesized videos. Since identity is a semantic concept, direct evaluation is not feasible. To evaluate how well the generated video preserves the target identity, existing works usually use the embedding distance of the generated video frames and the ground truth image to measure the identity-preserving performance. For example, Vougioukas et al. [40] adopted the average content distance (ACD) [111] to measure the average Euclidean distance of target image representation, obtained using OpenFace [112], and the representations of generated frames. Besides, Zakharov et al. [113] used the cosine similarity between embedding vectors of the ArcFace network [114] for measuring identity mismatch.","[[], ['b95', 'b96', 'b93', 'b39', 'b94']]","[[], ['b95', 'b96', 'b93', 'b39', 'b94']]",5,"sent1: Appropriate evaluation for VSG continues to be an open problem, and many recent works have explored various evaluation metrics on VSG.
sent2: We categorize those metrics based on three learning targets, i.e., identity preservation, visual quality, audio-visual semantic consistency.
sent3: Identity Preservation. One of the most important goals of VSG is to preserve the target identity as much as possible during video generation, as humans are quite sensitive to subtle appearance changes in synthesized videos.
sent4: Since identity is a semantic concept, direct evaluation is not feasible.
sent5: To evaluate how well the generated video preserves the target identity, existing works usually use the embedding distance of the generated video frames and the ground truth image to measure the identity-preserving performance.
sent6: For example, Vougioukas et al. [40] adopted the average content distance (ACD) [111] to measure the average Euclidean distance of target image representation, obtained using OpenFace [112], and the representations of generated frames.
sent7: Besides, Zakharov et al. [113] used the cosine similarity between embedding vectors of the ArcFace network [114] for measuring identity mismatch."
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s16,Visual Quality.,"To evaluate the quality of the synthesized video frames, reconstruction error measurement (e.g., Mean Squared Error) is a natural evaluation way. However, reconstruction error only focuses on pixel-wise alignments and ignores global visual quality. Therefore, existing works usually adopt Peak Signal-to-Noise Ratio (PSNR) and Structure Similarity Index Measure (SSIM) to evaluate the global visual quality of generated frames. More recently, Prajwal et al. [38] introduced Fréchet Inception Distance (FID) to measure the distance between synthetic and real data distributions, as FID is more consistent with human perception evaluation. Besides, Cumulative Probability Blur Detection (CPBD) [115], a nonreference measure, is also widely used to evaluate the loss of sharpness during video generation.

Audio-visual Semantic Consistency. Semantic consistency of the generated video and the driving source mainly contains audio-visual synchronization and speech consistency. For, audio-visual synchronization, Landmark Distance (LMD) [116] computes the Euclidean distance of the lip region landmarks between the synthesized video frames and ground truth frames. The other synchronization evaluation metric is to use a pre-trained audio-to-video synchronisation network [48] to predict the offset of generated frames and the ground truth. For the speech consistency, Chen et al. [42] proposed a lipsynchronization evaluation metric, i.e., Lip-Reading Similarity Distance (LRSD), which measures the Euclidean distance of semantic-level speech embeddings obtained by lip reading networks. For better evaluation of speech consistency, lip reading results (accuracy, CER, or WER) comparisons of the generated frames and ground truth are also used as consistency evaluation metrics.

In addition to the above objective metrics, subjective metrics like user study are also widely used in VSG.","[['b97', 'b37'], ['b47', 'b41', 'b98'], []]","[['b97', 'b37'], ['b47', 'b41', 'b98'], []]",5,"sent1: To evaluate the quality of the synthesized video frames, reconstruction error measurement (e.g., Mean Squared Error) is a natural evaluation way.
sent2: However, reconstruction error only focuses on pixel-wise alignments and ignores global visual quality.
sent3: Therefore, existing works usually adopt Peak Signal-to-Noise Ratio (PSNR) and Structure Similarity Index Measure (SSIM) to evaluate the global visual quality of generated frames.
sent4: More recently, Prajwal et al. [38] introduced Fréchet Inception Distance (FID) to measure the distance between synthetic and real data distributions, as FID is more consistent with human perception evaluation.
sent5: Besides, Cumulative Probability Blur Detection (CPBD) [115], a nonreference measure, is also widely used to evaluate the loss of sharpness during video generation.
sent6: Audio-visual Semantic Consistency.
sent7: Semantic consistency of the generated video and the driving source mainly contains audio-visual synchronization and speech consistency.
sent8: For, audio-visual synchronization, Landmark Distance (LMD) [116] computes the Euclidean distance of the lip region landmarks between the synthesized video frames and ground truth frames.
sent9: The other synchronization evaluation metric is to use a pre-trained audio-to-video synchronisation network [48] to predict the offset of generated frames and the ground truth.
sent10: For the speech consistency, Chen et al. [42] proposed a lipsynchronization evaluation metric, i.e., Lip-Reading Similarity Distance (LRSD), which measures the Euclidean distance of semantic-level speech embeddings obtained by lip reading networks.
sent11: For better evaluation of speech consistency, lip reading results (accuracy, CER, or WER) comparisons of the generated frames and ground truth are also used as consistency evaluation metrics.
sent12: In addition to the above objective metrics, subjective metrics like user study are also widely used in VSG."
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s20,Visual frontend network,"As shown in Fig. 6, there are mainly three types of input data: mouth-centered videos, dense optical flow, and landmark points. Among them, mouth-centered videos and dense optical flow are regular grid data, so CNNs are the most suitable and commonly used backbone architectures for them. On the other hand, as landmark points are irregular data, some existing works [53,54,118] adopted Graph Convolution Networks (GCNs) to extract visual features from landmark points. Next, we review these backbone architectures.

CNN-based Architectures. CNNs have been becoming one of the most common architectures in the field of deep learning. Since AlexNet [119] was proposed in 2012, researchers have invented a variety of deeper, wider, and lighter CNN models [120]. Representative CNN architectures, such as VGG [121], ResNet [122], MobileNet [123], DenseNet [124], ShuffleNet [125] etc, have been widely used in learning visual representation for VSR.

The first end-to-end deep visual representation learning for word-level VSR was proposed by Chung et al. [27]. Based on the VGG-M backbone network, they compared different image sequence input (Multiple Towers vs. Early Fusion) and temporal fusion (2D CNNs vs. 3D CNNs) architectures and discussed their pros and cons. The experimental results showed that the 2D CNNs are superior to their 3D counterparts by a large margin. However, the above conclusion was not rigorous enough, as the ablation study is insufficient, and word-level VSR datasets have a very short-term dependency. In 2017, Assael et al. [56] proposed LipNet, the first end-to-end sentence-level VSR model. LipNet extracts visual features using a 3-layer Spatio-temporal Convolution Neural Network (STCNN, also known as 3D CNN). The experimental results confirm the intuition that extracting spatio-temporal features using STCNN is better than aggregating spatial-only features. Considering 3D CNNs are more capable of capturing the dynamics of the mouth region while 2D CNNs are more efficient in time and memory, Stafylakis et al. [55] proposed to combine 3D CNNs and 2D CNNs for visual feature extraction. In specific, the proposed visual backbone network consists of a shadow 3D CNN and 2D ResNet. The 3D CNN has just one layer to aggregate short-term temporal information on lip movements. Due to the considerable performance of the model, plenty of VSR models [50,52,59,61,126] adopted it as the backbone network for visual features extraction. Recently, Feng et al. [127] improved this architecture by integrating the Squeeze-and-Extract [128] module. Besides VGG and ResNet, researchers have also adopted other representative 2D CNN architectures, including DenseNet [58], ShuffleNet [52], MobileNet [129], etc.

GCN-based Architectures. Considering CNNs are not suitable for irregular grid data, researchers proposed to utilize Graph Convolution Networks (GCNs) to extract visual features from the facial landmark points [130]. Liu et al. [53] proposed the first end-to-end GCN model (ST-GCN) that extracts shapebased visual features by learning the lip landmark points and their relationships. They firstly proposed lip graph connection relations and defined the graph adjacency matrices based on the manifold distance of nodes. Then, they combined the image features and shape features to extract more discriminative visual features. However, the lip graph connection relations do not naturally exist, and the intuition-guided predefined lip graph restricts the representation ability of shape-based features. Motivated by this, Sheng et al. [54] proposed an Adaptive Semantic-Spatial-Temporal Graph Convolution Network (ASST-GCN). Unlike [53], the ASST-GCN parameterizes graph connections and automatically learns adaptive graph connections. Besides, they introduced two graph structures, i.e., semantic graph and spatial-temporal graph, making graph parameters can be adaptively learned with other parameters in the network training. Existing works show that imagebased features are more discriminative than landmark-based features. Sheng et al. [54] concluded the reason for this. The accuracy and coordinates resolution of landmark point detection significantly influence its feature discrimination. However, facial landmark detection is challenging, especially in uncontrolled environments. Since the complementarity between image and landmark features, the combination of CNNs and GCNs is often widely adopted [53,54,118]. Visual Transformer-based Architectures. Inspired by the significant success of transformer architectures in the field of NLP, researchers have recently applied transformers to computer vision (CV) tasks [131]. Recently, transformers have been showing they are potential alternatives to CNNs. Afouras et al. [57] designed an end-to-end visual transformer-based pooling mechanism that learns to track and aggregate the lip movement representations. The proposed visual backbone network can reduce the need for complicated preprocessing, improving the robustness of visual representation. The ablation study clearly shows that the visual transformer-based pooling mechanism significantly boosts the performance of VSR.

Based on the above backbone architectures, some works further improved visual representation by utilizing two-stream networks. For example, Weng et al. [132] successfully migrated the two-stream (the raw grayscale video stream and the dense optical flow stream) I3D model to VSR, and achieve comparable performance on word-level VSR. However, dense optical flow and 3D convolution calculation is very time consuming, resulting in low feature extraction efficiency. Wang et al. [58] utilized 2D CNNs and 3D CNNs to extract both framewise spatial features and short-term spatio-temporal features, and then fused the features with an adaptive mask to obtain strong, multi-grained visual features.","[['b53', 'b52', 'b100'], ['b104', 'b105', 'b101', 'b102', 'b107', 'b106', 'b103'], ['b111', 'b49', 'b57', 'b55', 'b60', 'b51', 'b26', 'b54', 'b58', 'b110', 'b109', 'b108'], ['b52', 'b113', 'b56', 'b112', 'b53', 'b100'], ['b114', 'b57']]","[['b53', 'b52', 'b100'], ['b104', 'b105', 'b101', 'b102', 'b107', 'b106', 'b103'], ['b111', 'b49', 'b57', 'b55', 'b60', 'b51', 'b26', 'b54', 'b58', 'b110', 'b109', 'b108'], ['b52', 'b113', 'b56', 'b112', 'b53', 'b100'], ['b114', 'b57']]",30,"sent1: As shown in Fig. 6, there are mainly three types of input data: mouth-centered videos, dense optical flow, and landmark points.
sent2: Among them, mouth-centered videos and dense optical flow are regular grid data, so CNNs are the most suitable and commonly used backbone architectures for them.
sent3: On the other hand, as landmark points are irregular data, some existing works [53,54,118] adopted Graph Convolution Networks (GCNs) to extract visual features from landmark points.
sent4: Next, we review these backbone architectures.
sent5: CNN-based Architectures. CNNs have been becoming one of the most common architectures in the field of deep learning.
sent6: Since AlexNet [119] was proposed in 2012, researchers have invented a variety of deeper, wider, and lighter CNN models [120].
sent7: Representative CNN architectures, such as VGG [121], ResNet [122], MobileNet [123], DenseNet [124], ShuffleNet [125] etc, have been widely used in learning visual representation for VSR.
sent8: The first end-to-end deep visual representation learning for word-level VSR was proposed by Chung et al. [27].
sent9: Based on the VGG-M backbone network, they compared different image sequence input (Multiple Towers vs. Early Fusion) and temporal fusion (2D CNNs vs. 3D CNNs) architectures and discussed their pros and cons.
sent10: The experimental results showed that the 2D CNNs are superior to their 3D counterparts by a large margin.
sent11: However, the above conclusion was not rigorous enough, as the ablation study is insufficient, and word-level VSR datasets have a very short-term dependency.
sent12: In 2017, Assael et al. [56] proposed LipNet, the first end-to-end sentence-level VSR model.
sent13: LipNet extracts visual features using a 3-layer Spatio-temporal Convolution Neural Network (STCNN, also known as 3D CNN).
sent14: The experimental results confirm the intuition that extracting spatio-temporal features using STCNN is better than aggregating spatial-only features.
sent15: Considering 3D CNNs are more capable of capturing the dynamics of the mouth region while 2D CNNs are more efficient in time and memory, Stafylakis et al. [55] proposed to combine 3D CNNs and 2D CNNs for visual feature extraction.
sent16: In specific, the proposed visual backbone network consists of a shadow 3D CNN and 2D ResNet.
sent17: The 3D CNN has just one layer to aggregate short-term temporal information on lip movements.
sent18: Due to the considerable performance of the model, plenty of VSR models [50,52,59,61,126] adopted it as the backbone network for visual features extraction.
sent19: Recently, Feng et al. [127] improved this architecture by integrating the Squeeze-and-Extract [128] module.
sent20: Besides VGG and ResNet, researchers have also adopted other representative 2D CNN architectures, including DenseNet [58], ShuffleNet [52], MobileNet [129], etc.GCN-based Architectures.
sent21: Considering CNNs are not suitable for irregular grid data, researchers proposed to utilize Graph Convolution Networks (GCNs) to extract visual features from the facial landmark points [130].
sent22: Liu et al. [53] proposed the first end-to-end GCN model (ST-GCN) that extracts shapebased visual features by learning the lip landmark points and their relationships.
sent23: They firstly proposed lip graph connection relations and defined the graph adjacency matrices based on the manifold distance of nodes.
sent24: Then, they combined the image features and shape features to extract more discriminative visual features.
sent25: However, the lip graph connection relations do not naturally exist, and the intuition-guided predefined lip graph restricts the representation ability of shape-based features.
sent26: Motivated by this, Sheng et al. [54] proposed an Adaptive Semantic-Spatial-Temporal Graph Convolution Network (ASST-GCN).
sent27: Unlike [53], the ASST-GCN parameterizes graph connections and automatically learns adaptive graph connections.
sent28: Besides, they introduced two graph structures, i.e., semantic graph and spatial-temporal graph, making graph parameters can be adaptively learned with other parameters in the network training.
sent29: Existing works show that imagebased features are more discriminative than landmark-based features.
sent30: Sheng et al. [54] concluded the reason for this.
sent31: The accuracy and coordinates resolution of landmark point detection significantly influence its feature discrimination.
sent32: However, facial landmark detection is challenging, especially in uncontrolled environments.
sent33: Since the complementarity between image and landmark features, the combination of CNNs and GCNs is often widely adopted [53,54,118].
sent34: Visual Transformer-based Architectures.
sent35: Inspired by the significant success of transformer architectures in the field of NLP, researchers have recently applied transformers to computer vision (CV) tasks [131].
sent36: Recently, transformers have been showing they are potential alternatives to CNNs.
sent37: Afouras et al. [57] designed an end-to-end visual transformer-based pooling mechanism that learns to track and aggregate the lip movement representations.
sent38: The proposed visual backbone network can reduce the need for complicated preprocessing, improving the robustness of visual representation.
sent39: The ablation study clearly shows that the visual transformer-based pooling mechanism significantly boosts the performance of VSR.
sent40: Based on the above backbone architectures, some works further improved visual representation by utilizing two-stream networks.
sent41: For example, Weng et al. [132] successfully migrated the two-stream (the raw grayscale video stream and the dense optical flow stream) I3D model to VSR, and achieve comparable performance on word-level VSR.
sent42: However, dense optical flow and 3D convolution calculation is very time consuming, resulting in low feature extraction efficiency.
sent43: Wang et al. [58] utilized 2D CNNs and 3D CNNs to extract both framewise spatial features and short-term spatio-temporal features, and then fused the features with an adaptive mask to obtain strong, multi-grained visual features."
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s36,GAN based Methods,"To overcome the above limitations of Speech2Vid, many researchers try to improve VSG performance by utilizing generative adversarial training [37] strategies. As shown in Fig. 8(j), GAN based VSG methods usually consist of three subarchitectures, i.e., encoders, generators, and discriminators.

Taking audio-driven VSG as an example, a piece of audio naturally entangles various information, such as speech, emotion, speaking style, etc. As we have emphasized in Section. 2.2.2, information coupling brings enormous challenges to VSR. To ameliorate this issue, Zhou et al. [71] proposed a novel VSG framework called Disentangled Audio-Visual System (DAVS). Compared with previous VSG approaches, they focus more on the disentangled speech and identity feature extraction, which is realized based on supervised adversarial training. However, DAVS relies on extra Word-ID labels and Person-ID labels in the training stage. Sun et al. [72] improved the model by learning speech and identity features within a selfsupervised contrastive learning framework, with no need for extra annotations. Si et al. [190] utilized knowledge distillation to disentangle emotion features, identity features, and speech features from the audio input with the help of a pretrained emotion recognition teacher network and a pretrained face recognition teacher network. Recently, some works have tried to encode additional facial controllable dynamics like emotion and head pose into the generation pipeline to generate a more natural-spontaneous talking face. For example, [191,192] introduce additional emotion encoders, and [193] devise implicit pose encodings into the generation pipeline.

Considering the drawbacks of only using image reconstruction loss, GAN based methods focus on customizing more effective learning goals for VSG. For example, Prajwal et al. [38,194] introduced a simple audio-visual synchronization discriminator for lip-syncing VSG. In addition, Chen et al. [116] proposed an audio-visual derivative correlation loss to optimize the consistency of the two modalities in feature spaces and a three-stream GAN discriminator to force talking mouth videos generation depending on the input audio signal.

For temporal-dependent video generation, [40,195,196] utilized autoregression style VSG generator networks for talking face generation. Two discriminators, i.e., a frame and sequence discriminator, are used to optimize the generated facial dynamics. Based on [40], Song et al. [39] introduced a VSR discriminator further to improve the lip movement accuracy of generated talking videos. The ablation study demonstrated that the additional VSR discriminator helps achieve more obvious lip movement, proving our motivation that VSR and VSG are dual and mutually promoted. Furthermore, Chen et al. [32] developed the DualLip system to jointly improve VSR and VSG by leveraging the task duality and demonstrating that both VSR and VSG models can be enhanced with the help of extra unlabeled data. Besides the above learning goals, the optical flow discriminator [197], speech-related facial action units [198], and cross-modal mutual information estimator [199] are also utilized to optimize lip motion and cross-modal consistency of generated talking videos with the driving source.","[['b36'], ['b70', 'b172', 'b173', 'b71', 'b174', 'b175'], ['b176', 'b98', 'b37'], ['b178', 'b179', 'b181', 'b38', 'b31', 'b177', 'b180', 'b39']]","[['b36'], ['b70', 'b172', 'b173', 'b71', 'b174', 'b175'], ['b176', 'b98', 'b37'], ['b178', 'b179', 'b181', 'b38', 'b31', 'b177', 'b180', 'b39']]",18,"sent1: To overcome the above limitations of Speech2Vid, many researchers try to improve VSG performance by utilizing generative adversarial training [37] strategies.
sent2: As shown in Fig. 8(j), GAN based VSG methods usually consist of three subarchitectures, i.e., encoders, generators, and discriminators.
sent3: Taking audio-driven VSG as an example, a piece of audio naturally entangles various information, such as speech, emotion, speaking style, etc.
sent4: As we have emphasized in Section.
sent5: 2.2.2, information coupling brings enormous challenges to VSR.
sent6: To ameliorate this issue, Zhou et al. [71] proposed a novel VSG framework called Disentangled Audio-Visual System (DAVS).
sent7: Compared with previous VSG approaches, they focus more on the disentangled speech and identity feature extraction, which is realized based on supervised adversarial training.
sent8: However, DAVS relies on extra Word-ID labels and Person-ID labels in the training stage.
sent9: Sun et al. [72] improved the model by learning speech and identity features within a selfsupervised contrastive learning framework, with no need for extra annotations.
sent10: Si et al. [190] utilized knowledge distillation to disentangle emotion features, identity features, and speech features from the audio input with the help of a pretrained emotion recognition teacher network and a pretrained face recognition teacher network.
sent11: Recently, some works have tried to encode additional facial controllable dynamics like emotion and head pose into the generation pipeline to generate a more natural-spontaneous talking face.
sent12: For example, [191,192] introduce additional emotion encoders, and [193] devise implicit pose encodings into the generation pipeline.
sent13: Considering the drawbacks of only using image reconstruction loss, GAN based methods focus on customizing more effective learning goals for VSG.
sent14: For example, Prajwal et al. [38,194] introduced a simple audio-visual synchronization discriminator for lip-syncing VSG.
sent15: In addition, Chen et al. [116] proposed an audio-visual derivative correlation loss to optimize the consistency of the two modalities in feature spaces and a three-stream GAN discriminator to force talking mouth videos generation depending on the input audio signal.
sent16: For temporal-dependent video generation, [40,195,196] utilized autoregression style VSG generator networks for talking face generation.
sent17: Two discriminators, i.e., a frame and sequence discriminator, are used to optimize the generated facial dynamics.
sent18: Based on [40], Song et al. [39] introduced a VSR discriminator further to improve the lip movement accuracy of generated talking videos.
sent19: The ablation study demonstrated that the additional VSR discriminator helps achieve more obvious lip movement, proving our motivation that VSR and VSG are dual and mutually promoted.
sent20: Furthermore, Chen et al. [32] developed the DualLip system to jointly improve VSR and VSG by leveraging the task duality and demonstrating that both VSR and VSG models can be enhanced with the help of extra unlabeled data.
sent21: Besides the above learning goals, the optical flow discriminator [197], speech-related facial action units [198], and cross-modal mutual information estimator [199] are also utilized to optimize lip motion and cross-modal consistency of generated talking videos with the driving source."
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s12,Datasets under uncontrolled environments,"Recently, researchers are gradually shifting their focus to inthe-wild visual speech learning. As a result, many large-scale in-the-wild audio-visual datasets are constructed to promote the research. We introduce some of the audio-visual datasets collected under in-the-wild environments in the following.

LRW [27] is a word-level audio-visual dataset constructed by a multi-stage data automatic collection pipeline. It revolutionary enlarged the dataset scale and speaker number based on the rich data volume of BBC television programs. It contains over 1,000k word instances spoken by over a thousand people. The main objective of LRW is to test speaker-independent word-level lip reading methods.

LRS2-BBC [6] is a sentence-level audio-visual dataset with a similar data collection pipeline and data source as that LRW dataset. It is built for sentence-level lip reading, a more challenging VSR problem than word-level lip reading. All videos in LRS2-BBC are collected from the BBC program, and it contains over 144.5k utterances with a vocabulary size of about 62.8k.

VoxCeleb1 [30] is a large-scale text-independent audio-visual dataset collected from open-source YouTube media. It contains over 100k utterances from 1,251 celebrities. Although it is mainly built for speaker identification, it also can be used for VSG.

ObamaSet [87] is a specific audio-visual dataset focused on the visual speech analysis of former US President Barack Obama.

All video samples are collected from his weekly address footage. Unlike previous datasets, it focuses on Barack Obama only and does not provide any human annotations. Therefore, it is only used for Obama-oriented VSG.

LRS3-TED [89] is a large-scale sentence-level audio-visual dataset. Compared to LRS2-TED, it has a larger scale in terms of duration, vocabulary, and number of speakers. It consists of talking face videos from over 400 hours of TED and TEDx videos, the corresponding subtitles, and word alignment boundaries. Besides, it is the largest among existing public available annotated English audio-visual datasets.

VoxCeleb2 [29] is a high-level version of VoxCeleb1 extended on ethnic diversity. In VoxCeleb2, the VoxCeleb1 dataset is re-purposed to serve as a test set for speaker verification. Furthermore, it is currently the largest public available audiovisual dataset.

LSVSR [92] is the largest existing visual speech recognition dataset, consisting of pairs of text and video clips of faces speaking (3,886 hours of video). It is collected from public YouTube videos. But unfortunately, this dataset is not publicly available due to the restricted license.

LRW-1000 [31] is the largest word-level Chinese Mandarin audio-visual dataset. It contains 1k word classes with 718,018 samples from more than 2k individual speakers. Each class corresponds to the syllables of a Mandarin word composed of one or several Chinese characters. All videos are collected from television programs on Chinese TV stations.

Faceforensics++ [94] is an automated benchmark for facial manipulation detection. Different from existing audiovisual datasets, all videos have been manipulated based on DeepFakes [102], Face2Face [103], FaceSwap [104], NeuralTextures [105] as main methods for facial manipulations. It is commonly used to test forgery video detection methods.

HDTF [68] is a large-scale in-the-wild audio-visual dataset built for talking face generation. It consists of about 362 different high-resolution videos collected online. Due to the high quality of origin videos, the cropped face-centered videos also have higher visual quality than that of previous datasets like LRW and LRS2-BBC.

In addition to the datasets listed in  [108] etc. also promoted the research of VSA on various languages.

Considering that datasets play a crucial role in VSA, we would like to give a summary and discussion of datasets to help readers to know the development of VSA. Compared with early audio-visual datasets, recent ones have been improved in the number of subjects, dataset scale, recording conditions and script diversity, data quality, etc. Due to the privacy protection laws (e.g., General Data Protection Regulation (GDPR) in Europe Union), some of existing large-scale datasets [83,92] are not public available. An intuitive solution is to automatically collect available data from online media (e.g., Youtube, BBC, or other online television programs). However, existing audio-visual data auto-collection algorithms may cause a large amount of lowquality data. Therefore, an optimized auto-collection algorithm is crucial for VSA datasets in the future.","[[], ['b26'], ['b5'], ['b29'], ['b79'], [], ['b80'], ['b28'], ['b81'], ['b30'], ['b88', 'b87', 'b86', 'b82'], ['b67'], ['b90'], ['b78', 'b81']]","[[], ['b26'], ['b5'], ['b29'], ['b79'], [], ['b80'], ['b28'], ['b81'], ['b30'], ['b88', 'b87', 'b86', 'b82'], ['b67'], ['b90'], ['b78', 'b81']]",16,"sent1: Recently, researchers are gradually shifting their focus to inthe-wild visual speech learning.
sent2: As a result, many large-scale in-the-wild audio-visual datasets are constructed to promote the research.
sent3: We introduce some of the audio-visual datasets collected under in-the-wild environments in the following.
sent4: LRW [27] is a word-level audio-visual dataset constructed by a multi-stage data automatic collection pipeline.
sent5: It revolutionary enlarged the dataset scale and speaker number based on the rich data volume of BBC television programs.
sent6: It contains over 1,000k word instances spoken by over a thousand people.
sent7: The main objective of LRW is to test speaker-independent word-level lip reading methods.
sent8: LRS2-BBC [6] is a sentence-level audio-visual dataset with a similar data collection pipeline and data source as that LRW dataset.
sent9: It is built for sentence-level lip reading, a more challenging VSR problem than word-level lip reading.
sent10: All videos in LRS2-BBC are collected from the BBC program, and it contains over 144.5k utterances with a vocabulary size of about 62.8k.
sent11: VoxCeleb1 [30] is a large-scale text-independent audio-visual dataset collected from open-source YouTube media.
sent12: It contains over 100k utterances from 1,251 celebrities.
sent13: Although it is mainly built for speaker identification, it also can be used for VSG.
sent14: ObamaSet [87] is a specific audio-visual dataset focused on the visual speech analysis of former US President Barack Obama.
sent15: All video samples are collected from his weekly address footage.
sent16: Unlike previous datasets, it focuses on Barack Obama only and does not provide any human annotations.
sent17: Therefore, it is only used for Obama-oriented VSG.LRS3-TED [89] is a large-scale sentence-level audio-visual dataset.
sent18: Compared to LRS2-TED, it has a larger scale in terms of duration, vocabulary, and number of speakers.
sent19: It consists of talking face videos from over 400 hours of TED and TEDx videos, the corresponding subtitles, and word alignment boundaries.
sent20: Besides, it is the largest among existing public available annotated English audio-visual datasets.
sent21: VoxCeleb2 [29] is a high-level version of VoxCeleb1 extended on ethnic diversity.
sent22: In VoxCeleb2, the VoxCeleb1 dataset is re-purposed to serve as a test set for speaker verification.
sent23: Furthermore, it is currently the largest public available audiovisual dataset.
sent24: LSVSR [92] is the largest existing visual speech recognition dataset, consisting of pairs of text and video clips of faces speaking (3,886 hours of video).
sent25: It is collected from public YouTube videos.
sent26: But unfortunately, this dataset is not publicly available due to the restricted license.
sent27: LRW-1000 [31] is the largest word-level Chinese Mandarin audio-visual dataset.
sent28: It contains 1k word classes with 718,018 samples from more than 2k individual speakers.
sent29: Each class corresponds to the syllables of a Mandarin word composed of one or several Chinese characters.
sent30: All videos are collected from television programs on Chinese TV stations.
sent31: Faceforensics++ [94] is an automated benchmark for facial manipulation detection.
sent32: Different from existing audiovisual datasets, all videos have been manipulated based on DeepFakes [102], Face2Face [103], FaceSwap [104], NeuralTextures [105] as main methods for facial manipulations.
sent33: It is commonly used to test forgery video detection methods.
sent34: HDTF [68] is a large-scale in-the-wild audio-visual dataset built for talking face generation.
sent35: It consists of about 362 different high-resolution videos collected online.
sent36: Due to the high quality of origin videos, the cropped face-centered videos also have higher visual quality than that of previous datasets like LRW and LRS2-BBC.
sent37: In addition to the datasets listed in  [108] etc. also promoted the research of VSA on various languages.
sent38: Considering that datasets play a crucial role in VSA, we would like to give a summary and discussion of datasets to help readers to know the development of VSA.
sent39: Compared with early audio-visual datasets, recent ones have been improved in the number of subjects, dataset scale, recording conditions and script diversity, data quality, etc.
sent40: Due to the privacy protection laws (e.g., General Data Protection Regulation (GDPR) in Europe Union), some of existing large-scale datasets [83,92] are not public available.
sent41: An intuitive solution is to automatically collect available data from online media (e.g., Youtube, BBC, or other online television programs).
sent42: However, existing audio-visual data auto-collection algorithms may cause a large amount of lowquality data.
sent43: Therefore, an optimized auto-collection algorithm is crucial for VSA datasets in the future."
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s33,2D Coefficient based.,"Active Appearance Model (AAM) is one of the most commonly used facial coefficient models, representing both the shape and texture variations and their correlation. Fan et al. [26] utilized a two-layer BiLSTM network to estimate AAM coefficients of the mouth area based on the overlapped triphone input, which then is transferred to a face image to produce a photo-realistic talking head. The experiments show that the BiLSTM network has superior performance to previous HMM-based approaches. Similarly, as shown in Fig. 8(e), Taylor et al. [66] introduced a simple and effective DNN as a sliding window predictor to automatically learn AAM coefficients based on the fixed-length phoneme sequence. Furthermore, the model can be retargeted to drive other face models with the help of an effective retargeting approach. The main practical limitation of AAM coefficients is that the reference face AAM parameterization may cause potential errors when retargeting to a new subject.

3D Coefficient based. Besides 2D facial coefficient models, 3D facial coefficients via principal component analysis (PCA) are more commonly used in VSG [67,70,171,172,173,174,175]. Pham et al. [171,172,176] proposed utilizing CNN + RNN based backbone architectures to map audio signals to blendshape coefficients [177] of a 3D face. However, these methods rely heavily on the prior 3D facial models of target speakers. Hussen et al. [173] finetuned a pretrained DNN-based acoustic model to map driving audios to 3D blendshape coefficients, as they hold the idea that a pretrained acoustic model has better generalization on speaker-independent VSG tasks than a randomly initialized model. As shown in Fig. 8(f), Thies et al. [67] proposed a generalized Audio2Expression network and a specialized UNet-based neural face rendering network for audiodriven VSG. The proposed Audio2Expression network aims to estimate temporally stable 3D blenshape coefficients based on the DeepSpeech audio features, using a CNN based backbone architecture and a content-aware filtering network. In this way, the model is able to synthesize talking face videos from an audio sequence from another person.

Besides the 3D blendshape model, Kim et al. [178,179] introduced 3D Morphable Model (3DMM) [180], a more dense 3d face parametric representation, for video-oriented face2face translation. The 3DMM coefficients contain the rigid head pose parameters, facial identity coefficients, expression coefficients, gaze direction parameters for both eyes, and spherical harmonics illumination coefficients. Referring to the 3DMM based face2face translation pipeline mentioned above, [4,68,174,175,181,182,183] converted the driving source from video to audio clips (text scripts) and migrated this pipeline to VSG tasks. These methods have an approximate framework, as shown in Fig. 8(g). The flowchart of this framework generally follows four steps: 1) Train a network to map the driving source to the facial expression coefficients, as visual speech information is implicit in facial expression coefficients. 2) Use a pretrained deep face reconstruction model to get the 3DMM coefficients of the reference identity image. 3) Combine the 3DMM coefficients from the reference identity image and the predicted facial expression coefficients to get hybrid 3DMM coefficients. 4) Synthesize talking videos using GPU rendering or a generation network.

Following the above flowchart, Song et al. [181] designed a novel Audio2Expression network. They empirically find that source identity information embedded in speech features will degrade the performance of mapping speech to mouth movement. Therefore, they explicitly add an ID-Removing sub-network to remove the identity information from the driving audio. Meanwhile, a UNet-style generation network is introduced to complete the mouth region guided by mouth landmarks. Yi et al. [174] proposed an LSTM-based network to map audio MFCC features to facial expression and head pose, as they argue that audio and head pose are correlated in a short period. Besides, they propose a memory-augmented GAN to refine these synthesized video frames into real ones. Wu et al. [182] proposed an arbitrary talking style imitation VSG method. During the mapping stage, they introduced an extra style reference video as input and used a deep 3D reconstruction model to get the style code of the reference video. Next, they concatenate audio features with the reconstructed style code to predict the stylized 3DMM coefficients. However, the above 3DMM based models are not able to disentangle visual speech information from other facial expressions like eyebrow and head pose. Therefore, Zhang et al. [68] proposed a novel flowguided VSG framework, including one style-specific animation generator and one flow-guided video generator, to synthesize high visual quality videos. Moreover, the style-specific animation generator successfully disentangles lip dynamics with eyebrow and head pose. Li et al. [184] employed a similar framework for text-driven VSG. Ji et al. [4] proposed an emotional video portrait (EVP) to achieve audio-driven emotional control for talking face synthesis. Unlike previous methods, they adopt the cross-reconstruction [185] technique in the audio2expression stage to decompose the input audio into disentangled content and emotion embeddings.","[['b25', 'b65'], ['b158', 'b153', 'b157', 'b155', 'b156', 'b159', 'b66', None, 'b154', 'b69'], ['b3', 'b67', 'b157', 'b161', 'b165', 'b156', 'b163', 'b162', 'b160', 'b164'], ['b67', 'b3', 'b156', 'b163', 'b166', 'b167', 'b164']]","[['b25', 'b65'], ['b158', 'b153', 'b157', 'b155', 'b156', 'b159', 'b66', None, 'b154', 'b69'], ['b3', 'b67', 'b157', 'b161', 'b165', 'b156', 'b163', 'b162', 'b160', 'b164'], ['b67', 'b3', 'b156', 'b163', 'b166', 'b167', 'b164']]",29,"sent1: Active Appearance Model (AAM) is one of the most commonly used facial coefficient models, representing both the shape and texture variations and their correlation.
sent2: Fan et al. [26] utilized a two-layer BiLSTM network to estimate AAM coefficients of the mouth area based on the overlapped triphone input, which then is transferred to a face image to produce a photo-realistic talking head.
sent3: The experiments show that the BiLSTM network has superior performance to previous HMM-based approaches.
sent4: Similarly, as shown in Fig. 8(e), Taylor et al. [66] introduced a simple and effective DNN as a sliding window predictor to automatically learn AAM coefficients based on the fixed-length phoneme sequence.
sent5: Furthermore, the model can be retargeted to drive other face models with the help of an effective retargeting approach.
sent6: The main practical limitation of AAM coefficients is that the reference face AAM parameterization may cause potential errors when retargeting to a new subject.
sent7: 3D Coefficient based. Besides 2D facial coefficient models, 3D facial coefficients via principal component analysis (PCA) are more commonly used in VSG [67,70,171,172,173,174,175].
sent8: Pham et al. [171,172,176] proposed utilizing CNN + RNN based backbone architectures to map audio signals to blendshape coefficients [177] of a 3D face.
sent9: However, these methods rely heavily on the prior 3D facial models of target speakers.
sent10: Hussen et al. [173] finetuned a pretrained DNN-based acoustic model to map driving audios to 3D blendshape coefficients, as they hold the idea that a pretrained acoustic model has better generalization on speaker-independent VSG tasks than a randomly initialized model.
sent11: As shown in Fig. 8(f), Thies et al. [67] proposed a generalized Audio2Expression network and a specialized UNet-based neural face rendering network for audiodriven VSG.
sent12: The proposed Audio2Expression network aims to estimate temporally stable 3D blenshape coefficients based on the DeepSpeech audio features, using a CNN based backbone architecture and a content-aware filtering network.
sent13: In this way, the model is able to synthesize talking face videos from an audio sequence from another person.
sent14: Besides the 3D blendshape model, Kim et al. [178,179] introduced 3D Morphable Model (3DMM) [180], a more dense 3d face parametric representation, for video-oriented face2face translation.
sent15: The 3DMM coefficients contain the rigid head pose parameters, facial identity coefficients, expression coefficients, gaze direction parameters for both eyes, and spherical harmonics illumination coefficients.
sent16: Referring to the 3DMM based face2face translation pipeline mentioned above, [4,68,174,175,181,182,183] converted the driving source from video to audio clips (text scripts) and migrated this pipeline to VSG tasks.
sent17: These methods have an approximate framework, as shown in Fig. 8(g).
sent18: The flowchart of this framework generally follows four steps: 1) Train a network to map the driving source to the facial expression coefficients, as visual speech information is implicit in facial expression coefficients.
sent19: 2) Use a pretrained deep face reconstruction model to get the 3DMM coefficients of the reference identity image.
sent20: 3) Combine the 3DMM coefficients from the reference identity image and the predicted facial expression coefficients to get hybrid 3DMM coefficients.
sent21: 4) Synthesize talking videos using GPU rendering or a generation network.
sent22: Following the above flowchart, Song et al. [181] designed a novel Audio2Expression network.
sent23: They empirically find that source identity information embedded in speech features will degrade the performance of mapping speech to mouth movement.
sent24: Therefore, they explicitly add an ID-Removing sub-network to remove the identity information from the driving audio.
sent25: Meanwhile, a UNet-style generation network is introduced to complete the mouth region guided by mouth landmarks.
sent26: Yi et al. [174] proposed an LSTM-based network to map audio MFCC features to facial expression and head pose, as they argue that audio and head pose are correlated in a short period.
sent27: Besides, they propose a memory-augmented GAN to refine these synthesized video frames into real ones.
sent28: Wu et al. [182] proposed an arbitrary talking style imitation VSG method.
sent29: During the mapping stage, they introduced an extra style reference video as input and used a deep 3D reconstruction model to get the style code of the reference video.
sent30: Next, they concatenate audio features with the reconstructed style code to predict the stylized 3DMM coefficients.
sent31: However, the above 3DMM based models are not able to disentangle visual speech information from other facial expressions like eyebrow and head pose.
sent32: Therefore, Zhang et al. [68] proposed a novel flowguided VSG framework, including one style-specific animation generator and one flow-guided video generator, to synthesize high visual quality videos.
sent33: Moreover, the style-specific animation generator successfully disentangles lip dynamics with eyebrow and head pose.
sent34: Li et al. [184] employed a similar framework for text-driven VSG.
sent35: Ji et al. [4] proposed an emotional video portrait (EVP) to achieve audio-driven emotional control for talking face synthesis.
sent36: Unlike previous methods, they adopt the cross-reconstruction [185] technique in the audio2expression stage to decompose the input audio into disentangled content and emotion embeddings."
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s24,Unsupervised learning for VSR,"Unsupervised learning for VSR aims to learn discriminative visual representations without access to manual annotation. Among multiple unsupervised learning frameworks, crossmodal self-supervised learning is dominated in VSR. Despite the remarkable progress witnessed in the past decade, the successes of supervised deep learning rely heavily on vast manually annotated training data, which has severe limitations in many real-world applications, including the VSR task. Firstly, supervised learning is restricted to relatively narrow domains primarily defined by the labeled training data and thus leads to limited generalization ability. Secondly, a large amount of accurately labeled data like a large-scale annotated dataset for VSR is costly to gather. Recently, self-supervised learning has received growing attention due to its high label efficiency and good generalization. Recent advances in cross-modal self-supervised learning have shown that the corresponding audio can serve as a supervisory signal to learn effective visual representations for VSR. As shown in Fig. 7, audio-visual self-supervised learning aims to extract efficient representations from the co-occurring A-V data pairs without any extra annotation. Based on the natural synchronization property of audio and video, existing methods mainly adopt contrastive learning to achieve this goal. Chung et al. [149] are the first to train an A-V synchronization model in an end-to-end manner with margin-based [150] pairwise contrastive loss. Besides VSR, they have proved that the trained network work can effectively be finetuned to other tasks like speaker detection. With the same training strategy, Korbar et al. [151] broadened the scope of the study to encompass arbitrary human activities rather than lip movements. Except for marginbased loss, L1 loss and binary classification loss [47,152,153,154] are also widely used for A-V representations learning. Those works have proved the learned A-V representations can be further transferred to more downstream tasks, such as visualizing the locations of sound sources, action recognition, audio-visual source separation, etcRecently, Chung et al. [101] reformulated the contrastive task as a multi-way matching task and demonstrated that using multiple negative samples can improve the performance. Considering existing methods only exploit the natural synchronization of the video and the corresponding audio, Sheng et al. [50] proposed a novel self-supervised learning framework called Adversarial Dual-Contrast Self-Supervised Learning (ADC-SSL), to go beyond previous methods by explicitly forcing the visual representations disentangled from speech-unrelated information. To achieve this goal, they combine contrastive learning and adversarial training by three pretext tasks: A-V synchronization, identity discrimination, and modality classification.","[['b132', 'b85', 'b49', 'b131', 'b133', 'b46', 'b136', 'b134', 'b135']]","[['b132', 'b85', 'b49', 'b131', 'b133', 'b46', 'b136', 'b134', 'b135']]",9,"sent1: Unsupervised learning for VSR aims to learn discriminative visual representations without access to manual annotation.
sent2: Among multiple unsupervised learning frameworks, crossmodal self-supervised learning is dominated in VSR.
sent3: Despite the remarkable progress witnessed in the past decade, the successes of supervised deep learning rely heavily on vast manually annotated training data, which has severe limitations in many real-world applications, including the VSR task.
sent4: Firstly, supervised learning is restricted to relatively narrow domains primarily defined by the labeled training data and thus leads to limited generalization ability.
sent5: Secondly, a large amount of accurately labeled data like a large-scale annotated dataset for VSR is costly to gather.
sent6: Recently, self-supervised learning has received growing attention due to its high label efficiency and good generalization.
sent7: Recent advances in cross-modal self-supervised learning have shown that the corresponding audio can serve as a supervisory signal to learn effective visual representations for VSR.
sent8: As shown in Fig. 7, audio-visual self-supervised learning aims to extract efficient representations from the co-occurring A-V data pairs without any extra annotation.
sent9: Based on the natural synchronization property of audio and video, existing methods mainly adopt contrastive learning to achieve this goal.
sent10: Chung et al. [149] are the first to train an A-V synchronization model in an end-to-end manner with margin-based [150] pairwise contrastive loss.
sent11: Besides VSR, they have proved that the trained network work can effectively be finetuned to other tasks like speaker detection.
sent12: With the same training strategy, Korbar et al. [151] broadened the scope of the study to encompass arbitrary human activities rather than lip movements.
sent13: Except for marginbased loss, L1 loss and binary classification loss [47,152,153,154] are also widely used for A-V representations learning.
sent14: Those works have proved the learned A-V representations can be further transferred to more downstream tasks, such as visualizing the locations of sound sources, action recognition, audio-visual source separation, etcRecently, Chung et al. [101] reformulated the contrastive task as a multi-way matching task and demonstrated that using multiple negative samples can improve the performance.
sent15: Considering existing methods only exploit the natural synchronization of the video and the corresponding audio, Sheng et al. [50] proposed a novel self-supervised learning framework called Adversarial Dual-Contrast Self-Supervised Learning (ADC-SSL), to go beyond previous methods by explicitly forcing the visual representations disentangled from speech-unrelated information.
sent16: To achieve this goal, they combine contrastive learning and adversarial training by three pretext tasks: A-V synchronization, identity discrimination, and modality classification."
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s34,Vertex based Methods,"3D facial vertices are another popularly used 3D face model in VSG. For example, Karras et al. [5] used a simple CNNbased architecture to learn a nonlinear mapping from input audios to the 3D vertex coordinates (totally 15,066 vertices) of the target face. To make the synthesized video more natural, they introduce an extra emotion code as an intuitive control for the emotional state of the face puppet. However, the proposed model is specialized for a particular speaker. To overcome this issue, as shown in Fig. 8(h), Cudeiro et al. [69] extended the model to multiple subjects. The proposed VOCA model concatenates the Deepspeech audio features and onehot vector of a speaker and outputs 3D vertex (totally 5023 vertices) displacements instead of vertex coordinates. The critical contribution of VOCA is that the additional identity control parameters can vary the identity-dependent visual dynamics. Based on VOCA, Liu et al. [186] proposed a geometry-guided dense perspective network (GDPnet) with two constraints from different perspectives to achieve a more robust generation. Fan et al. [187] proposed a Transformer-based autoregressive VSG model named FaceFormer to encode the long-term audio context information and predict a sequence of 3D face vertices.

Richard et al. [188] proposed a categorical latent space for VSG that disentangles audio-correlated and audio-uncorrelated (facial expressions like eye blinks, eyebrow) information based on a cross-modality loss. Then, a UNet-style architecture with skip connections is used to predict 3D vertex coordinates. Since the modalities disentanglement mechanism, the plausible motion of uncorrelated regions of the face is controllable, making the synthesized video more photo-realistic. Lahiri et al. [70] proposed a speaker-dependent VSR method, which decomposes the audio to talking face mapping problem into the prediction of the 3D face shape and the regressions over the 2D texture atlas. To do so, they first introduced a normalization preprocessing stage to eliminate the effects of head movement and lighting variations. Then, a geometry decoder and an auto-regressive texture synthesis network were trained to learn vertex displacements and the corresponding lip-centered texture, respectively. Finally, a computer graphics based video rendering pipeline is used to generate talking videos for the target speaker.","[['b4', 'b169', 'b68', 'b168'], ['b170', 'b69']]","[['b4', 'b169', 'b68', 'b168'], ['b170', 'b69']]",6,"sent1: 3D facial vertices are another popularly used 3D face model in VSG.
sent2: For example, Karras et al. [5] used a simple CNNbased architecture to learn a nonlinear mapping from input audios to the 3D vertex coordinates (totally 15,066 vertices) of the target face.
sent3: To make the synthesized video more natural, they introduce an extra emotion code as an intuitive control for the emotional state of the face puppet.
sent4: However, the proposed model is specialized for a particular speaker.
sent5: To overcome this issue, as shown in Fig. 8(h), Cudeiro et al. [69] extended the model to multiple subjects.
sent6: The proposed VOCA model concatenates the Deepspeech audio features and onehot vector of a speaker and outputs 3D vertex (totally 5023 vertices) displacements instead of vertex coordinates.
sent7: The critical contribution of VOCA is that the additional identity control parameters can vary the identity-dependent visual dynamics.
sent8: Based on VOCA, Liu et al. [186] proposed a geometry-guided dense perspective network (GDPnet) with two constraints from different perspectives to achieve a more robust generation.
sent9: Fan et al. [187] proposed a Transformer-based autoregressive VSG model named FaceFormer to encode the long-term audio context information and predict a sequence of 3D face vertices.
sent10: Richard et al. [188] proposed a categorical latent space for VSG that disentangles audio-correlated and audio-uncorrelated (facial expressions like eye blinks, eyebrow) information based on a cross-modality loss.
sent11: Then, a UNet-style architecture with skip connections is used to predict 3D vertex coordinates.
sent12: Since the modalities disentanglement mechanism, the plausible motion of uncorrelated regions of the face is controllable, making the synthesized video more photo-realistic.
sent13: Lahiri et al. [70] proposed a speaker-dependent VSR method, which decomposes the audio to talking face mapping problem into the prediction of the 3D face shape and the regressions over the 2D texture atlas.
sent14: To do so, they first introduced a normalization preprocessing stage to eliminate the effects of head movement and lighting variations.
sent15: Then, a geometry decoder and an auto-regressive texture synthesis network were trained to learn vertex displacements and the corresponding lip-centered texture, respectively.
sent16: Finally, a computer graphics based video rendering pipeline is used to generate talking videos for the target speaker."
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s26,Performance Comparison,"In this section, we compare the existing deep learning-based VSR methods. Due to a large number of methods proposed for VSR, it is not possible to list and compare all of them. Thus, we select representative works and several milestone methods. Table. 3 summarizes the performance and some experimental settings of some representative VSR methods on large-scale commonly used benchmark datasets, including LRW [27], LRW-1000 [31], GRID [77], LRS2 [6] and LRS3 [89]. As for the word-level VSR task, various visual frontend networks have been designed to boost the performance, such as VGG-M, C3D-ResNet, ST-GCN, ASST-GCN,etc. Among them, the C3D-ResNet architecture is the most widely used. [55] provided the baseline (C3D-ResNet34 + BiLSTM, 83.5%) on the LRW dataset. Many subsequent works inherited this structure and further improved the performance by introducing some tricks, such as label smoothing, weight decay, dropout, Squeeze-and-Extract module, two-stream, multi-stage KD, etc.

As for temporal backend networks, RNN-based models and TCN-based models have similar performance. Based on C3D-ResNet18 + MSTCN, Ma et al. [52] improved the SOTA to 87.7% on LRW. Recently, more and more works [34,47,49,50,101,149] tried to improve visual representations by utilizing extra audio information in the training stage rather than the design of network architectures, as audio signals can provide more finegrained supervision than text annotations. The SOTA results (88.5% on LRW and 50.5% on LRW-1000) was realized based on cross-modal audio-visual mutual learning [34].

As for the sentence-level VSR task, deep learning-based VSR methods have vastly outperformed human lip-readers [56]. As shown in Table. 3, deep VSR models almost reach performance saturation (SOTA result: 1.3% WER) on the simple (constrained recording environment and limited corpus) GRID dataset. Therefore, researchers pay more attention to VSR in unconstrained environments. Motivated by the practical need, we focus more on the large-scale in-the-wild datasets (e.g., LRS2 and LRS3). The fair performance comparison of sentence-level VSR methods is quite hard, as there are too many extra influencing factors. For example, some methods trained the model with extra datasets (even some of them are not public available). Besides, the outputs of the model are generally optimized by extra language models, while language models are trained with existing large-scale text corpus. The introduction of language models can significantly improve the performance, and it is not fair to compare these methods optimized by different language models. Therefore, to make it clearer for readers, we list some representative sentence-level VSR models as well as their experimental settings in Table. 3.","[['b5', 'b26', 'b54', 'b80', 'b75', 'b30'], ['b85', 'b49', 'b131', 'b51', 'b46', 'b48', 'b33'], ['b55']]","[['b5', 'b26', 'b54', 'b80', 'b75', 'b30'], ['b85', 'b49', 'b131', 'b51', 'b46', 'b48', 'b33'], ['b55']]",14,"sent1: In this section, we compare the existing deep learning-based VSR methods.
sent2: Due to a large number of methods proposed for VSR, it is not possible to list and compare all of them.
sent3: Thus, we select representative works and several milestone methods.
sent4: Table. 3 summarizes the performance and some experimental settings of some representative VSR methods on large-scale commonly used benchmark datasets, including LRW [27], LRW-1000 [31], GRID [77], LRS2 [6] and LRS3 [89].
sent5: As for the word-level VSR task, various visual frontend networks have been designed to boost the performance, such as VGG-M, C3D-ResNet, ST-GCN, ASST-GCN,etc.
sent6: Among them, the C3D-ResNet architecture is the most widely used.
sent7: [55] provided the baseline (C3D-ResNet34 + BiLSTM, 83.5%) on the LRW dataset.
sent8: Many subsequent works inherited this structure and further improved the performance by introducing some tricks, such as label smoothing, weight decay, dropout, Squeeze-and-Extract module, two-stream, multi-stage KD, etc.
sent9: As for temporal backend networks, RNN-based models and TCN-based models have similar performance.
sent10: Based on C3D-ResNet18 + MSTCN, Ma et al. [52] improved the SOTA to 87.7% on LRW.
sent11: Recently, more and more works [34,47,49,50,101,149] tried to improve visual representations by utilizing extra audio information in the training stage rather than the design of network architectures, as audio signals can provide more finegrained supervision than text annotations.
sent12: The SOTA results (88.5% on LRW and 50.5% on LRW-1000) was realized based on cross-modal audio-visual mutual learning [34].
sent13: As for the sentence-level VSR task, deep learning-based VSR methods have vastly outperformed human lip-readers [56].
sent14: As shown in Table. 3, deep VSR models almost reach performance saturation (SOTA result: 1.3% WER) on the simple (constrained recording environment and limited corpus) GRID dataset.
sent15: Therefore, researchers pay more attention to VSR in unconstrained environments.
sent16: Motivated by the practical need, we focus more on the large-scale in-the-wild datasets (e.g., LRS2 and LRS3).
sent17: The fair performance comparison of sentence-level VSR methods is quite hard, as there are too many extra influencing factors.
sent18: For example, some methods trained the model with extra datasets (even some of them are not public available).
sent19: Besides, the outputs of the model are generally optimized by extra language models, while language models are trained with existing large-scale text corpus.
sent20: The introduction of language models can significantly improve the performance, and it is not fair to compare these methods optimized by different language models.
sent21: Therefore, to make it clearer for readers, we list some representative sentence-level VSR models as well as their experimental settings in Table.
sent22: 3."
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s28,VISUAL SPEECH GENERATION,"Visual Speech Generation (VSG), also known as lip sequence generation, aims to synthesize a lip sequence corresponding to the driving source (a clip of audio or a piece of text). Traditional VSG methods suffer from severe practical challenges [45], such as complex generation pipelines, constrained applicable environments, over-reliance on finegrained viseme (phoneme) annotations, etc. To realize mapping driving sources to lip dynamics, representative traditional VSG methods mainly adopted cross-modal retrieval approaches [16,103,155,156] and HMM-based approaches [157,158]. For example, Thies et al. [103] introduced a typical image-based mouth synthesis approach that generates a realistic mouth interior by retrieving and warping best-matching mouth shapes from offline samples. However, retrieval-based methods are static text-phoneme-viseme mappings and do not really consider the contextual information of the speech. Meanwhile, retrieval-based methods are pretty sensitive to head pose changes. HMM-based methods also suffer from some drawbacks, such as the limitation of the prior assumptions (e.g., Gaussian Mixture Model (GMM) and its diagonal covariance). As deep learning technologies have extensively promoted the developments of VSG, we focus on reviewing deep learning based VSG methods in this section.

To make the scope of VSG clear for readers, we first explain the relationship and difference between VSG and another hot topic, i.e., Talking Face Generation (TFG) 1 [71,159].

TFG aims to synthesize a realistic, high-quality talking face video corresponding to the driving source and the target identity. According to the modality of driving sources, TFG can be divided into audio-driven, text-driven, and video-driven. Among them, video-driven TFG mainly focuses on videooriented face-to-face facial expression transferring rather than visual speech generation. Therefore, video-driven TFG methods will not appear in this paper.

Traditionally, VSG can be viewed as a key sub-component of text-driven (audio-driven) TFG. The other component is video editing, following a specific editing pipeline to output the final synthesized talking face video based on the generated lip sequence. Recently, to reduce manual intervention and simplify the complexity of the overall pipeline, more and more researchers have tried to synthesize full talking face in an end-toend manner instead of lip sequence. Consequently, the definition boundary between VSG and text-driven (audio-driven) TFG is getting blurred, which means some text-driven (audio-driven) TFG methods are also in our review scope. Therefore, to give a comprehensive survey on VSG, we also review some TFG methods driven by text and audio, as these works implicitly or explicitly involve VSG modules.","[['b137', 'b140', 'b44', 'b138', 'b87', 'b15', 'b139'], ['b70', 'b141'], [], []]","[['b137', 'b140', 'b44', 'b138', 'b87', 'b15', 'b139'], ['b70', 'b141'], [], []]",9,"sent1: Visual Speech Generation (VSG), also known as lip sequence generation, aims to synthesize a lip sequence corresponding to the driving source (a clip of audio or a piece of text).
sent2: Traditional VSG methods suffer from severe practical challenges [45], such as complex generation pipelines, constrained applicable environments, over-reliance on finegrained viseme (phoneme) annotations, etc.
sent3: To realize mapping driving sources to lip dynamics, representative traditional VSG methods mainly adopted cross-modal retrieval approaches [16,103,155,156] and HMM-based approaches [157,158].
sent4: For example, Thies et al. [103] introduced a typical image-based mouth synthesis approach that generates a realistic mouth interior by retrieving and warping best-matching mouth shapes from offline samples.
sent5: However, retrieval-based methods are static text-phoneme-viseme mappings and do not really consider the contextual information of the speech.
sent6: Meanwhile, retrieval-based methods are pretty sensitive to head pose changes.
sent7: HMM-based methods also suffer from some drawbacks, such as the limitation of the prior assumptions (e.g., Gaussian Mixture Model (GMM) and its diagonal covariance).
sent8: As deep learning technologies have extensively promoted the developments of VSG, we focus on reviewing deep learning based VSG methods in this section.
sent9: To make the scope of VSG clear for readers, we first explain the relationship and difference between VSG and another hot topic, i.e., Talking Face Generation (TFG) 1
sent10: [71,159].TFG aims to synthesize a realistic, high-quality talking face video corresponding to the driving source and the target identity.
sent11: According to the modality of driving sources, TFG can be divided into audio-driven, text-driven, and video-driven.
sent12: Among them, video-driven TFG mainly focuses on videooriented face-to-face facial expression transferring rather than visual speech generation.
sent13: Therefore, video-driven TFG methods will not appear in this paper.
sent14: Traditionally, VSG can be viewed as a key sub-component of text-driven (audio-driven) TFG.
sent15: The other component is video editing, following a specific editing pipeline to output the final synthesized talking face video based on the generated lip sequence.
sent16: Recently, to reduce manual intervention and simplify the complexity of the overall pipeline, more and more researchers have tried to synthesize full talking face in an end-toend manner instead of lip sequence.
sent17: Consequently, the definition boundary between VSG and text-driven (audio-driven) TFG is getting blurred, which means some text-driven (audio-driven) TFG methods are also in our review scope.
sent18: Therefore, to give a comprehensive survey on VSG, we also review some TFG methods driven by text and audio, as these works implicitly or explicitly involve VSG modules."
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s31,Landmark based Methods,"Facial landmark points around facial components capture the rigid and non-rigid facial deformations due to head movements and facial expressions [160]. Facial landmark points are widely used in various facial analysis tasks, including VSG. As a pioneering work, Suwajanakorn et al. [87] adopted a simple single-layer LSTM with the time delay mechanism to learn a nonlinear mapping from audio coefficients to lip landmark points. As shown in Fig. 8(b), the model outputs the synthesized talking face video of former US President Barack Obama, following the pipeline of facial texture synthesis, video re-timing, and target video compositing. Beyond computer graphic video generation methods, as shown in Fig. 8(c), Kumar et al. [64] proposed the LSTM + UNet architecture, improving the model by replacing the complex video generation pipeline with a pix2pix framework [161]. In this way, there is no need to get    involved with details of a face, e.g., synthesizing realistic teeth. However, as the above methods are trained on only Barack Obama with many hours of his weekly address footage, they cannot generalize to new identities or voices. The LSTM + UNet VSG backbone architecture is widely adopted in many subsequent works [15,97,162,163]. Unlike previous methods using audio MFCC features as input, Sinha et al. [162,163] introduced DeepSpeech [164] features instead, as DeepSpeech features are more robust for the variation of speakers. In 2018, Jalalifar et al. [165] proposed LSTM + C-GAN VSG backbone architecture, using the basic conditional generative adversarial network (C-GAN) [166] for generating talking faces given the learned landmarks. As the LSTM network and C-GAN network are mutual independence, this model can reanimate the target face with audio from another person. In 2019, Chen et al. [65] proposed a novel LSTM + Convolutional-RNN structure, further considering the correlation between adjacent video frames during generation. Besides, they also propose a novel dynamic pixel-wise Loss to solve the pixel jittering problem in correlated audio-visual regions. Wang et al. [167] proposed a three-stage VSG framework. Firstly, they use the 3D Hourglass Network as a motion field generator to predict landmark points based on the input audio, head motions, and the reference image. And then convert the predicted landmark points to dense motion fields. Finally, the synthesized talking video is obtained using a first-order motion model [168]. Recently, they further updated the motion field generator by replacing the 3D Hourglass Network with a self-attention architecture [41].

Besides 2D landmark based approaches, mapping driving source to 3D landmarks is also widely explored. Audio signals contain sich semantic-level information, including speech content, speaker's speaking style, emotion, etc. Zhou et al. [15] utilized a voice conversion neural network to learn disentangled speech content and identity features. Then, an LSTM-based network is introduced to predict 3D landmarks based on speech content features. Finally, the final synthesized talking face video is realized using a UNet-style generator network. The key insight is to predict 3D landmarks from disentangled audio content features and speaker-aware features, such that they capture controllable lip synchronization and head motion dynamics. As shown in Fig. 8(d), Lu et al. [169] introduced extracting the high-level speech information using an autoregressive predictive coding (APC) model [170] and manifold projection for better generalization. Then, an audio to lip-related motion module is designed to predict 3D lip landmarks. Finally, an image-to-image translation network (UNet) is introduced to synthesize video frames.","[['b64', 'b14', 'b142', 'b143', 'b79', 'b63', 'b147', 'b148', 'b83', 'b145', 'b149', 'b40', 'b144', 'b150', 'b146'], ['b14', 'b152', 'b151']]","[['b64', 'b14', 'b142', 'b143', 'b79', 'b63', 'b147', 'b148', 'b83', 'b145', 'b149', 'b40', 'b144', 'b150', 'b146'], ['b14', 'b152', 'b151']]",18,"sent1: Facial landmark points around facial components capture the rigid and non-rigid facial deformations due to head movements and facial expressions [160].
sent2: Facial landmark points are widely used in various facial analysis tasks, including VSG.
sent3: As a pioneering work, Suwajanakorn et al. [87] adopted a simple single-layer LSTM with the time delay mechanism to learn a nonlinear mapping from audio coefficients to lip landmark points.
sent4: As shown in Fig. 8(b), the model outputs the synthesized talking face video of former US President Barack Obama, following the pipeline of facial texture synthesis, video re-timing, and target video compositing.
sent5: Beyond computer graphic video generation methods, as shown in Fig. 8(c), Kumar et al. [64] proposed the LSTM + UNet architecture, improving the model by replacing the complex video generation pipeline with a pix2pix framework [161].
sent6: In this way, there is no need to get    involved with details of a face, e.g., synthesizing realistic teeth.
sent7: However, as the above methods are trained on only Barack Obama with many hours of his weekly address footage, they cannot generalize to new identities or voices.
sent8: The LSTM + UNet VSG backbone architecture is widely adopted in many subsequent works [15,97,162,163].
sent9: Unlike previous methods using audio MFCC features as input, Sinha et al. [162,163] introduced DeepSpeech [164] features instead, as DeepSpeech features are more robust for the variation of speakers.
sent10: In 2018, Jalalifar et al. [165] proposed LSTM + C-GAN VSG backbone architecture, using the basic conditional generative adversarial network (C-GAN) [166] for generating talking faces given the learned landmarks.
sent11: As the LSTM network and C-GAN network are mutual independence, this model can reanimate the target face with audio from another person.
sent12: In 2019, Chen et al. [65] proposed a novel LSTM + Convolutional-RNN structure, further considering the correlation between adjacent video frames during generation.
sent13: Besides, they also propose a novel dynamic pixel-wise Loss to solve the pixel jittering problem in correlated audio-visual regions.
sent14: Wang et al. [167] proposed a three-stage VSG framework.
sent15: Firstly, they use the 3D Hourglass Network as a motion field generator to predict landmark points based on the input audio, head motions, and the reference image.
sent16: And then convert the predicted landmark points to dense motion fields.
sent17: Finally, the synthesized talking video is obtained using a first-order motion model [168].
sent18: Recently, they further updated the motion field generator by replacing the 3D Hourglass Network with a self-attention architecture [41].
sent19: Besides 2D landmark based approaches, mapping driving source to 3D landmarks is also widely explored.
sent20: Audio signals contain sich semantic-level information, including speech content, speaker's speaking style, emotion, etc.
sent21: Zhou et al. [15] utilized a voice conversion neural network to learn disentangled speech content and identity features.
sent22: Then, an LSTM-based network is introduced to predict 3D landmarks based on speech content features.
sent23: Finally, the final synthesized talking face video is realized using a UNet-style generator network.
sent24: The key insight is to predict 3D landmarks from disentangled audio content features and speaker-aware features, such that they capture controllable lip synchronization and head motion dynamics.
sent25: As shown in Fig. 8(d), Lu et al. [169] introduced extracting the high-level speech information using an autoregressive predictive coding (APC) model [170] and manifold projection for better generalization.
sent26: Then, an audio to lip-related motion module is designed to predict 3D lip landmarks.
sent27: Finally, an image-to-image translation network (UNet) is introduced to synthesize video frames."
248987614,Deep Learning for Visual Speech Analysis: A Survey,Computer Science,https://www.semanticscholar.org/paper/c804083ec28f78edf62d2bb9ad2ceda16c295785,s11,Datasets under controlled environments,"As we can see from Table 1, before 2015, visual speech research mainly focused on controlled environments. Controllable factors include recording conditions, equipment, data types, scripts, etc. These datasets provide an excellent foundation for visual speech research. Next, we review some representative audio-visual datasets collected under controlled environments.

AVICAR [75] is the most representative public audio-visual dataset recorded in a car-driving environment. As mentioned above, visual speech can contribute to audio-based speech recognition, especially in noisy environments. Motivated by this, AVICAR is collected for modeling bimodal speech in a driving car, as car-driving is a typical acoustic noisy environment.

GRID [77], consisting of high-quality audio and video recordings of 1,000 syntactically identical phrases spoken by 34 talkers, is built for comprehensive audio-visual perceptual analysis and microscopic modeling. Besides speech recognition, it can also support audio-visual speech separation tasks.

MODALITY [79] contains 31 hours of recordings was created to test the robustness of audio-visual speech recognition (AVSR) systems. As for the difference from other datasets, its corpus includes high-resolution, high-framerate stereoscopic video streams from RGB-D cameras.

OuluVS2 [81] is a multi-view audio-visual dataset built for non-rigid mouth motion analysis. It includes 53 speakers uttering three types of utterances. Moreover, it is recorded from five different views spanned between the frontal and profile views. Multiple views of talking mouths simulate a real-world  Table. 1 for a summary of these datasets.

situation, as users may not face the video camera all the time while talking. IBM AV-ASR [83] is a large corpus containing 40 hours of audio-visual recordings from 262 speakers in clean, studio conditions. Compared to previous datasets under controlled environments, it has significant advantages in vocabulary and speaker number. However, this dataset is not publicly available.

VOCASET [69] is a 4D face dataset with about 29 minutes of 4D face scans with synchronized audio from 12 speakers (6 females and 6 males), and the 4D face scans are recorded at 60fps. As a representative high-quality 4D face audio-visual dataset, VOCASET greatly promoted the research on 3D VSG.

MEAD [97], namely Multi-view Emotional Audio-visual Dataset, is a large-scale, high-quality emotional audio-visual dataset. Unlike previous datasets, it focuses on natural emotional talking face generation and takes multiple emotion states (eight different emotions at three intensity levels) into consideration.","[[], ['b74'], ['b75'], ['b76'], ['b77'], ['b78'], ['b68'], ['b83']]","[[], ['b74'], ['b75'], ['b76'], ['b77'], ['b78'], ['b68'], ['b83']]",7,"sent1: As we can see from Table 1, before 2015, visual speech research mainly focused on controlled environments.
sent2: Controllable factors include recording conditions, equipment, data types, scripts, etc.
sent3: These datasets provide an excellent foundation for visual speech research.
sent4: Next, we review some representative audio-visual datasets collected under controlled environments.
sent5: AVICAR [75] is the most representative public audio-visual dataset recorded in a car-driving environment.
sent6: As mentioned above, visual speech can contribute to audio-based speech recognition, especially in noisy environments.
sent7: Motivated by this, AVICAR is collected for modeling bimodal speech in a driving car, as car-driving is a typical acoustic noisy environment.
sent8: GRID [77], consisting of high-quality audio and video recordings of 1,000 syntactically identical phrases spoken by 34 talkers, is built for comprehensive audio-visual perceptual analysis and microscopic modeling.
sent9: Besides speech recognition, it can also support audio-visual speech separation tasks.
sent10: MODALITY [79] contains 31 hours of recordings was created to test the robustness of audio-visual speech recognition (AVSR) systems.
sent11: As for the difference from other datasets, its corpus includes high-resolution, high-framerate stereoscopic video streams from RGB-D cameras.
sent12: OuluVS2 [81] is a multi-view audio-visual dataset built for non-rigid mouth motion analysis.
sent13: It includes 53 speakers uttering three types of utterances.
sent14: Moreover, it is recorded from five different views spanned between the frontal and profile views.
sent15: Multiple views of talking mouths simulate a real-world  Table.
sent16: 1 for a summary of these datasets.
sent17: situation, as users may not face the video camera all the time while talking.
sent18: IBM AV-ASR [83] is a large corpus containing 40 hours of audio-visual recordings from 262 speakers in clean, studio conditions.
sent19: Compared to previous datasets under controlled environments, it has significant advantages in vocabulary and speaker number.
sent20: However, this dataset is not publicly available.
sent21: VOCASET [69] is a 4D face dataset with about 29 minutes of 4D face scans with synchronized audio from 12 speakers (6 females and 6 males), and the 4D face scans are recorded at 60fps.
sent22: As a representative high-quality 4D face audio-visual dataset, VOCASET greatly promoted the research on 3D VSG.
sent23: MEAD [97], namely Multi-view Emotional Audio-visual Dataset, is a large-scale, high-quality emotional audio-visual dataset.
sent24: Unlike previous datasets, it focuses on natural emotional talking face generation and takes multiple emotion states (eight different emotions at three intensity levels) into consideration."
252724722,A Review of the Research on the Evaluation Metrics for Automatic Grammatical Error Correction System,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/4241a0cb51c3780c57cae8b4f4d28341ccba2176,s6,GLEU.,"A variant of the machine translation system measure BLEU (bilateral evaluation understudy) [19], called GLEU (generalized language evaluation understanding), was put forth by IBM researchers in 2001. GLEU is largely used to assess the output of machine translation models, which is typically between 0.0 and 1.0. If the two sentences match perfectly, BLEU � 1.0. Conversely, if the two sentences mismatch perfectly, BLEU � 0.0 [20]. e core of BLEU translation evaluation metric is to detect the number of cooccurrence words between the hypothetical sentences and the reference sentences. e specific implementation method is to calculate the n-grams of the hypothetical sentence and the reference sentence and then count the number of matches to get the score. e more grams the system translation matches with the manual reference translation, the higher the BLEU score. Examples are as follows [21]: Reference: this is a small test. Candidate: this is a test. Table 5 shows that the BLEU score under 1-gram is 0.8 since the hypothetical sentence shares 4 words with the reference sentence. e number of the words in the hypothetical sentence divided by the words in the reference sentence is the final score.

Under 2-gram, as shown in Table 6: Every two words in the sentence are divided into a 2gram group. e calculation logic is the same as that of 1-gram. Under the conditions of the same reference sentence and hypothetical sentence, BLEU score is 0.5.

Napoles et al. [22] contend that because there is still a crucial distinction between translation tasks and error correction tasks, it is inaccurate to consider machine translation as merely a monolingual translation. Direct application of BLEU to GEC tasks could result in less-thanideal output scores. Because of this, researchers have created a simple BLEU metric variant called GLEU that is suited to the requirements of the error correction task. e accuracy of the GEC system is calculated through the comparison of reference sentence and source sentence, giving more weight to the gram with correct correction, rewarding the correct correction result and the correct source text without correction, and punishing the gram with incorrect correction. e calculation formula is as follows (10):

In the formulas, p n is the accuracy after n-gram calculation and BP is the penalty factor. In formula (11), H is the length of the hypothetical sentence and R is the length of the reference sentence. e function of penalty factor is to avoid the bias of system scoring. In the scoring process, the matching degree of n-gram may become better with the ( is ⟶ these), (is ⟶ are),(for ⟶ to) 0.67 0.67 0.67     shortening of sentence length. erefore, in order to control this situation, the length of the sentence will be taken into account in the calculation. When the length of the hypothetical sentence is greater than the source sentence, the penalty factor is 1 and no penalty will be imposed. When the length of the hypothetical sentence is greater than the source sentence, the punishment will be carried out. N is the value of n in n-gram of GLEU formula, and its upper limit is 4. N (H, R) is the overlapping n-grams in the hypothetical sentence and the source sentence, and N(H, S, R) is the overlapping n-grams in the hypothetical sentence, the source sentence, and the reference sentence, respectively. w n is the weighted average adopted by the system, and the value is 1/ N.","[['b20', 'b18', 'b19'], [], ['b21'], ['b10']]","[['b20', 'b18', 'b19'], [], ['b21'], ['b10']]",5,"sent1: A variant of the machine translation system measure BLEU (bilateral evaluation understudy) [19], called GLEU (generalized language evaluation understanding), was put forth by IBM researchers in 2001.
sent2: GLEU is largely used to assess the output of machine translation models, which is typically between 0.0 and 1.0.
sent3: If the two sentences match perfectly, BLEU � 1.0.
sent4: Conversely, if the two sentences mismatch perfectly, BLEU � 0.0 [20].
sent5: e core of BLEU translation evaluation metric is to detect the number of cooccurrence words between the hypothetical sentences and the reference sentences.
sent6: e specific implementation method is to calculate the n-grams of the hypothetical sentence and the reference sentence and then count the number of matches to get the score.
sent7: e more grams the system translation matches with the manual reference translation, the higher the BLEU score.
sent8: Examples are as follows [21]: Reference: this is a small test.
sent9: Candidate: this is a test. Table 5 shows that the BLEU score under 1-gram is 0.8 since the hypothetical sentence shares 4 words with the reference sentence.
sent10: e number of the words in the hypothetical sentence divided by the words in the reference sentence is the final score.
sent11: Under 2-gram, as shown in Table 6: Every two words in the sentence are divided into a 2gram group.
sent12: e calculation logic is the same as that of 1-gram.
sent13: Under the conditions of the same reference sentence and hypothetical sentence, BLEU score is 0.5.Napoles et al. [22] contend that because there is still a crucial distinction between translation tasks and error correction tasks, it is inaccurate to consider machine translation as merely a monolingual translation.
sent14: Direct application of BLEU to GEC tasks could result in less-thanideal output scores.
sent15: Because of this, researchers have created a simple BLEU metric variant called GLEU that is suited to the requirements of the error correction task.
sent16: e accuracy of the GEC system is calculated through the comparison of reference sentence and source sentence, giving more weight to the gram with correct correction, rewarding the correct correction result and the correct source text without correction, and punishing the gram with incorrect correction.
sent17: e calculation formula is as follows (10):In the formulas, p n is the accuracy after n-gram calculation and BP is the penalty factor.
sent18: In formula (11), H is the length of the hypothetical sentence and R is the length of the reference sentence.
sent19: e function of penalty factor is to avoid the bias of system scoring.
sent20: In the scoring process, the matching degree of n-gram may become better with the ( is ⟶ these), (is ⟶ are),(for ⟶ to) 0.67 0.67 0.67     shortening of sentence length.
sent21: erefore, in order to control this situation, the length of the sentence will be taken into account in the calculation.
sent22: When the length of the hypothetical sentence is greater than the source sentence, the penalty factor is 1 and no penalty will be imposed.
sent23: When the length of the hypothetical sentence is greater than the source sentence, the punishment will be carried out.
sent24: N is the value of n in n-gram of GLEU formula, and its upper limit is 4.
sent25: N (H, R) is the overlapping n-grams in the hypothetical sentence and the source sentence, and N(H, S, R) is the overlapping n-grams in the hypothetical sentence, the source sentence, and the reference sentence, respectively.
sent26: w n is the weighted average adopted by the system, and the value is 1/ N."
252724722,A Review of the Research on the Evaluation Metrics for Automatic Grammatical Error Correction System,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/4241a0cb51c3780c57cae8b4f4d28341ccba2176,s4,3),"Taking the sentence Our baseline system feed into PB-SMT pipeline as an example, if the manually modified set g � {feed ⟶ feeds, feed ⟶ feed a word}, the modified set e � {feed ⟶ feeds}, |e i ∩ g i | is 1, then the precision of the system is 100% and the recall rate is 50%.

In an ideal evaluation, both P and R values are expected to be as high as possible. But in practice, the precision rate and the recall rate are inconsistent under certain circumstances. Suppose that the optimal editing set g � 10, the modified set e 1 � 5, |e 1 ∩ g| � 3 in system 1 and the modified set e 2 � 8, |e 2 ∩ g | � 4 of system 2. Under these conditions, the P value of system 1 is 0.6, R value is 0.3, and system 2 has a P value of 0.5 and an R value of 0.4. e results show that the P value of system 1 is higher than that of system 2, but the R value is lower than that of system 2. erefore, for the convenience of comparison, the F (F-measure or F-score) [14] is introduced to evaluate the P value and the R value as a whole. e F value is the weighted harmonic average of the P and R values, as shown in formula (4):

In the M 2 evaluation, the value of α is usually set to 1, and the calculation formula of F 1 is shown in formula (5).

Later, CoNLL-2014 shared mission [15] was introduced and M 2 was used as the official evaluation indicator for the mission evaluation. But because of the nature of the data set, the researchers changed the value of weight α from 1 to 0.5. Because in the shared task evaluation process, the task recognizes that the precision rate weighs more than the recall rate and thus gives it a higher weight, twice as much weight as before, and twice as much precision, f 0.5 as shown in formula (6).

Currently, M 2 is the evaluation index most frequently used in the field of GEC, largely as a result of CoNLL-2014's shared task's promotion.

is tool has evolved into the standard for figuring out the GEC system's precision and recall rate. ere is a strong association between this evaluation metric and manual error correction . [16].","[[], ['b13'], [], ['b14', 'b5'], [], ['b15']]","[[], ['b13'], [], ['b14', 'b5'], [], ['b15']]",4,"sent1: Taking the sentence Our baseline system feed into PB-SMT pipeline as an example, if the manually modified set g � {feed ⟶ feeds, feed ⟶ feed a word}, the modified set e � {feed ⟶ feeds}, |e i ∩ g i | is 1, then the precision of the system is 100% and the recall rate is 50%.
sent2: In an ideal evaluation, both P and R values are expected to be as high as possible.
sent3: But in practice, the precision rate and the recall rate are inconsistent under certain circumstances.
sent4: Suppose that the optimal editing set g � 10, the modified set e 1 � 5, |e 1 ∩ g| � 3 in system 1 and the modified set e 2 � 8, |e 2 ∩ g | � 4 of system 2.
sent5: Under these conditions, the P value of system 1 is 0.6, R value is 0.3, and system 2 has a P value of 0.5 and an R value of 0.4.
sent6: e results show that the P value of system 1 is higher than that of system 2, but the R value is lower than that of system 2.
sent7: erefore, for the convenience of comparison, the F (F-measure or F-score) [14] is introduced to evaluate the P value and the R value as a whole.
sent8: e F value is the weighted harmonic average of the P and R values, as shown in formula (4):
sent9: In the M 2 evaluation, the value of α is usually set to 1, and the calculation formula of F 1 is shown in formula (5).
sent10: Later, CoNLL-2014 shared mission [15] was introduced and M 2 was used as the official evaluation indicator for the mission evaluation.
sent11: But because of the nature of the data set, the researchers changed the value of weight α from 1 to 0.5.
sent12: Because in the shared task evaluation process, the task recognizes that the precision rate weighs more than the recall rate and thus gives it a higher weight, twice as much weight as before, and twice as much precision, f 0.5 as shown in formula (6).
sent13: Currently, M 2 is the evaluation index most frequently used in the field of GEC, largely as a result of CoNLL-2014's shared task's promotion.
sent14: is tool has evolved into the standard for figuring out the GEC system's precision and recall rate.
sent15: ere is a strong association between this evaluation metric and manual error correction .[16]."
255024417,Application of named entity recognition method for Indonesian datasets: a review,"Computer Science, Engineering",https://www.semanticscholar.org/paper/941c79b384bc652cca8e8fe5b0e406ae971af713,s22,METHOD 2.1. Systematic literature review,"First, this article presents a SLR of the field of NER research. A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research. Then follows a discussion of how NER has been applied to Indonesian texts. SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].

A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1). In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method. At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords. This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.  There follows the RQ that guided the following analysis: RQ. ""What are the trends in the application of NER to extract information from Indonesian online news and social media?"" In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa). According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results. In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process. These processes are sequential processes where each process aims to find the right study to be used in this research. The search and selection process are an elimination process based on the criteria specified in each process.

The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data. Some irrelevant papers were omitted in the first stage of collection based on the title and abstract. The second stage of selection articles is a full-text selection. Figure 2 illustrates the procedure of text-selection. The total number of papers obtained from the four databases was initially 241. Upon completion of the selection procedure, however, only 20 papers remained. The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset. The third stage is reporting the results and analyzing the results of this review. We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research. Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021

Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset 

First, this article presents a SLR of the field of NER research. A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research. Then follows a discussion of how NER has been applied to Indonesian texts. SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].

A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1). In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method. At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords. This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.  There follows the RQ that guided the following analysis: RQ. ""What are the trends in the application of NER to extract information from Indonesian online news and social media?"" In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa). According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results. In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process. These processes are sequential processes where each process aims to find the right study to be used in this research. The search and selection process are an elimination process based on the criteria specified in each process.

The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data. Some irrelevant papers were omitted in the first stage of collection based on the title and abstract. The second stage of selection articles is a full-text selection. Figure 2 illustrates the procedure of text-selection. The total number of papers obtained from the four databases was initially 241. Upon completion of the selection procedure, however, only 20 papers remained. The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset. The third stage is reporting the results and analyzing the results of this review. We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research. Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021

Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset ","[['b17', 'b16', 'b12', 'b13', 'b15'], [], [], [], ['b17', 'b16', 'b12', 'b13', 'b15'], [], [], []]","[['b17', 'b16', 'b12', 'b13', 'b15'], [], [], [], ['b17', 'b16', 'b12', 'b13', 'b15'], [], [], []]",10,"sent1: First, this article presents a SLR of the field of NER research.
sent2: A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research.
sent3: Then follows a discussion of how NER has been applied to Indonesian texts.
sent4: SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].
sent5: A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1).
sent6: In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method.
sent7: At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords.
sent8: This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.
sent9: There follows the RQ that guided the following analysis: RQ.
sent10: ""What are the trends in the application of NER to extract information from Indonesian online news and social media?""
sent11: In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa).
sent12: According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results.
sent13: In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process.
sent14: These processes are sequential processes where each process aims to find the right study to be used in this research.
sent15: The search and selection process are an elimination process based on the criteria specified in each process.
sent16: The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data.
sent17: Some irrelevant papers were omitted in the first stage of collection based on the title and abstract.
sent18: The second stage of selection articles is a full-text selection.
sent19: Figure 2 illustrates the procedure of text-selection.
sent20: The total number of papers obtained from the four databases was initially 241.
sent21: Upon completion of the selection procedure, however, only 20 papers remained.
sent22: The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset.
sent23: The third stage is reporting the results and analyzing the results of this review.
sent24: We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research.
sent25: Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria
sent26: The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset First, this article presents a SLR of the field of NER research.
sent27: A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research.
sent28: Then follows a discussion of how NER has been applied to Indonesian texts.
sent29: SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].
sent30: A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1).
sent31: In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method.
sent32: At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords.
sent33: This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.
sent34: There follows the RQ that guided the following analysis: RQ.
sent35: ""What are the trends in the application of NER to extract information from Indonesian online news and social media?""
sent36: In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa).
sent37: According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results.
sent38: In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process.
sent39: These processes are sequential processes where each process aims to find the right study to be used in this research.
sent40: The search and selection process are an elimination process based on the criteria specified in each process.
sent41: The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data.
sent42: Some irrelevant papers were omitted in the first stage of collection based on the title and abstract.
sent43: The second stage of selection articles is a full-text selection.
sent44: Figure 2 illustrates the procedure of text-selection.
sent45: The total number of papers obtained from the four databases was initially 241.
sent46: Upon completion of the selection procedure, however, only 20 papers remained.
sent47: The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset.
sent48: The third stage is reporting the results and analyzing the results of this review.
sent49: We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research.
sent50: Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria
sent51: The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset"
255024417,Application of named entity recognition method for Indonesian datasets: a review,"Computer Science, Engineering",https://www.semanticscholar.org/paper/941c79b384bc652cca8e8fe5b0e406ae971af713,s1,METHOD 2.1. Systematic literature review,"First, this article presents a SLR of the field of NER research. A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research. Then follows a discussion of how NER has been applied to Indonesian texts. SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].

A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1). In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method. At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords. This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.  There follows the RQ that guided the following analysis: RQ. ""What are the trends in the application of NER to extract information from Indonesian online news and social media?"" In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa). According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results. In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process. These processes are sequential processes where each process aims to find the right study to be used in this research. The search and selection process are an elimination process based on the criteria specified in each process.

The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data. Some irrelevant papers were omitted in the first stage of collection based on the title and abstract. The second stage of selection articles is a full-text selection. Figure 2 illustrates the procedure of text-selection. The total number of papers obtained from the four databases was initially 241. Upon completion of the selection procedure, however, only 20 papers remained. The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset. The third stage is reporting the results and analyzing the results of this review. We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research. Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021

Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset 

First, this article presents a SLR of the field of NER research. A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research. Then follows a discussion of how NER has been applied to Indonesian texts. SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].

A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1). In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method. At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords. This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.  There follows the RQ that guided the following analysis: RQ. ""What are the trends in the application of NER to extract information from Indonesian online news and social media?"" In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa). According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results. In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process. These processes are sequential processes where each process aims to find the right study to be used in this research. The search and selection process are an elimination process based on the criteria specified in each process.

The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data. Some irrelevant papers were omitted in the first stage of collection based on the title and abstract. The second stage of selection articles is a full-text selection. Figure 2 illustrates the procedure of text-selection. The total number of papers obtained from the four databases was initially 241. Upon completion of the selection procedure, however, only 20 papers remained. The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset. The third stage is reporting the results and analyzing the results of this review. We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research. Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021

Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset ","[['b17', 'b16', 'b12', 'b13', 'b15'], [], [], [], ['b17', 'b16', 'b12', 'b13', 'b15'], [], [], []]","[['b17', 'b16', 'b12', 'b13', 'b15'], [], [], [], ['b17', 'b16', 'b12', 'b13', 'b15'], [], [], []]",10,"sent1: First, this article presents a SLR of the field of NER research.
sent2: A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research.
sent3: Then follows a discussion of how NER has been applied to Indonesian texts.
sent4: SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].
sent5: A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1).
sent6: In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method.
sent7: At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords.
sent8: This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.
sent9: There follows the RQ that guided the following analysis: RQ.
sent10: ""What are the trends in the application of NER to extract information from Indonesian online news and social media?""
sent11: In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa).
sent12: According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results.
sent13: In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process.
sent14: These processes are sequential processes where each process aims to find the right study to be used in this research.
sent15: The search and selection process are an elimination process based on the criteria specified in each process.
sent16: The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data.
sent17: Some irrelevant papers were omitted in the first stage of collection based on the title and abstract.
sent18: The second stage of selection articles is a full-text selection.
sent19: Figure 2 illustrates the procedure of text-selection.
sent20: The total number of papers obtained from the four databases was initially 241.
sent21: Upon completion of the selection procedure, however, only 20 papers remained.
sent22: The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset.
sent23: The third stage is reporting the results and analyzing the results of this review.
sent24: We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research.
sent25: Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria
sent26: The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset First, this article presents a SLR of the field of NER research.
sent27: A SLR aims to collect all research on a particular topic, evaluates it critically, and reaches conclusions that synthesize that research.
sent28: Then follows a discussion of how NER has been applied to Indonesian texts.
sent29: SLR has been used in various research domains such as P2P lending [13], Fintech [14], Teaching and learning via webinars [16], supply chain management model [17], and software engineering [18].
sent30: A SLR was carried out in three stages: the planning stage, the implementation stage and the reporting stage (see Figure 1).
sent31: In the first stage, the planning stage is carried out to identify the need for a systematic review of the use of the agile project management (APM) method.
sent32: At this stage, a review protocol was also developed by setting research questions (RQ) and formulating a boolean search to determine search keywords.
sent33: This study used the population, intervention, comparison, outcomes, and context (PICOC) strategy to determine the RQ, as shown in Table 1.
sent34: There follows the RQ that guided the following analysis: RQ.
sent35: ""What are the trends in the application of NER to extract information from Indonesian online news and social media?""
sent36: In this study, the search string is (""named-entity recognition"" OR NER OR ""named entity recognition"") AND (""online news"" OR ""social media"") AND (Indonesia* OR Bahasa).
sent37: According to the research question, the criteria for inclusion and exclusion in Table 2 were used to define the results.
sent38: In the second stage, this research defines a search strategy, namely selecting a publication database, selection results for research, data extraction and the synthesis process.
sent39: These processes are sequential processes where each process aims to find the right study to be used in this research.
sent40: The search and selection process are an elimination process based on the criteria specified in each process.
sent41: The authors collected papers from relevant electronic databases such as SCOPUS, ACM, IEEEXplore, and Science Direct, then used Mendeley software to organize the data.
sent42: Some irrelevant papers were omitted in the first stage of collection based on the title and abstract.
sent43: The second stage of selection articles is a full-text selection.
sent44: Figure 2 illustrates the procedure of text-selection.
sent45: The total number of papers obtained from the four databases was initially 241.
sent46: Upon completion of the selection procedure, however, only 20 papers remained.
sent47: The low number of papers is both a challenge to and an opportunity for NER research in an Indonesian context, as few studies have used the ""Bahasa"" dataset.
sent48: The third stage is reporting the results and analyzing the results of this review.
sent49: We mapped research results from previous studies and examined how the experimental process in NER was, what libraries could be used for Indonesian language datasets, how to approach NER, and proposed future research.
sent50: Table 2. Criteria of selection studies process Inclusion criteria Exclusion criteria
sent51: The paper studied about NER The paper is not using English Studies published in the last 5 years, between 2016-2021Not full-text paper The paper being studied is in the form of a journal or proceedings/conference Same papers from different database Papers discussing NER but not the Indonesian text dataset"
256461385,Narrative Why-Question Answering: A Review of Challenges and Datasets,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/98f4b43487b8386728c14aba24f848b2eadad1e2,s2,Causality,"Causality is a semantic relationship between events showing that an event occurs or holds due to another event (Mostafazadeh et al., 2016b). Mostafazadeh et al. (2016b) distinguish four types of lexical causality relations: cause, enable, prevent, and cause-to-end based on the works by Wolff and Song (2003), Wolff (2007), and Khemlani et al. (2014). Moreover, causality has temporal implications such that if an event A causes/enables/prevents an event B, then A should start before B, or if an event A causes an event B to end, then B should start before A. Causality relations can hold one of the three temporal implications: before, overlaps, and during (Mostafazadeh et al., 2016b). Thus, while answering a whyquestion, the temporal relation between the events should also be taken into account in addition to the causality relation.

A causal relation is constructed from two components: cause and effect. Based on how the cause and the effect are conveyed in a text, causation can be distinguished into the following categories: explicit vs implicit, marked vs unmarked, and ambiguous vs unambiguous.

Explicit vs Implicit. Causation is explicit if both the cause and the effect are present in the text. Causation is implicit if either the cause or the effect of both are missing from the text (Blanco et al., 2008). For instance, ""She was accepted to a top university after receiving a high score in the state examination"" is explicit, while ""I did not attend the mandatory final exam."" is implicit because the effect of ""failing the course"" is not explicitly stated.

Marked vs Unmarked. Causation is marked if the text contains the causal signal words that indicate the causal relation (Blanco et al., 2008). For example, ""I was late because of traffic"" is marked, but ""Do not buy any bread. We have already got two at home"" is unmarked.

Ambiguous vs Unambiguous. If the causal relation is presented in the text with causal keywords (e.g., cause, effect, consequence) or with causal signals (e.g., because of, due to, as a result of ), it is considered unambiguous (Girju, 2003). On the other hand, if a causal relation is constructed in the form of an expression containing affect verbs (e.g., affect, change, influence) or link verbs (e.g., link, lead, depend), it is considered ambiguous. Furthermore, if a marked signal always refers to causation (e.g., because), it is unambiguous, while if a marked word occasionally signals causation (e.g., since), it is ambiguous (Blanco et al., 2008).","[['b25', 'b43', 'b16', 'b44'], [], ['b2'], ['b2'], ['b2', 'b12']]","[['b25', 'b43', 'b16', 'b44'], [], ['b2'], ['b2'], ['b2', 'b12']]",8,"sent1: Causality is a semantic relationship between events showing that an event occurs or holds due to another event (Mostafazadeh et al., 2016b).
sent2: Mostafazadeh et al. (2016b) distinguish four types of lexical causality relations: cause, enable, prevent, and cause-to-end based on the works by Wolff and Song (2003), Wolff (2007), and Khemlani et al. (2014).
sent3: Moreover, causality has temporal implications such that if an event A causes/enables/prevents an event B, then A should start before B, or if an event A causes an event B to end, then B should start before A. Causality relations can hold one of the three temporal implications: before, overlaps, and during (Mostafazadeh et al., 2016b).
sent4: Thus, while answering a whyquestion, the temporal relation between the events should also be taken into account in addition to the causality relation.
sent5: A causal relation is constructed from two components: cause and effect.
sent6: Based on how the cause and the effect are conveyed in a text, causation can be distinguished into the following categories: explicit vs implicit, marked vs unmarked, and ambiguous vs unambiguous.
sent7: Explicit vs Implicit. Causation is explicit if both the cause and the effect are present in the text.
sent8: Causation is implicit if either the cause or the effect of both are missing from the text (Blanco et al., 2008).
sent9: For instance, ""She was accepted to a top university after receiving a high score in the state examination"" is explicit, while ""I did not attend the mandatory final exam.""
sent10: is implicit because the effect of ""failing the course"" is not explicitly stated.
sent11: Marked vs Unmarked. Causation is marked if the text contains the causal signal words that indicate the causal relation (Blanco et al., 2008).
sent12: For example, ""I was late because of traffic"" is marked, but ""Do not buy any bread.
sent13: We have already got two at home"" is unmarked.
sent14: Ambiguous vs Unambiguous. If the causal relation is presented in the text with causal keywords (e.g., cause, effect, consequence) or with causal signals (e.g., because of, due to, as a result of ), it is considered unambiguous (Girju, 2003).
sent15: On the other hand, if a causal relation is constructed in the form of an expression containing affect verbs (e.g., affect, change, influence) or link verbs (e.g., link, lead, depend), it is considered ambiguous.
sent16: Furthermore, if a marked signal always refers to causation (e.g., because), it is unambiguous, while if a marked word occasionally signals causation (e.g., since), it is ambiguous (Blanco et al., 2008)."
256461385,Narrative Why-Question Answering: A Review of Challenges and Datasets,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/98f4b43487b8386728c14aba24f848b2eadad1e2,s15,Short vs Long narratives,"All reviewed datasets have short narratives as their context. The NarrativeQA short texts have a more complex narrative structure than other datasets, since the short context versions of the NarrativeQA are summaries of the larger narratives, and not single scenes from the long narratives. In short narratives, if there is a common lexical pattern between the question and a part of the narrative, or a large lexical overlap between the answer and the narrative, sophisticated models can treat free-form QA as an extractive task. For example, models trained on the TellMeWhy dataset generally try to find the answer span in the text and copy a part of the narrative as an answer (Lal et al., 2021).

The NarrativeQA dataset is the only dataset that has long narratives as its context. Linking narrative elements to answer questions in large narratives is harder than in short narratives (see section 3.2). Typically, in order to reason about long narratives, the parts relevant to reasoning are retrieved first (Kočiský et al., 2018;Tay et al., 2019;Frermann, 2019;Mou et al., 2020Mou et al., , 2021. The retrieval is difficult even with the state-of-the-art models due to the characteristics of narratives and the necessity of high-level narrative comprehension (Mou et al., 2021).","[['b19'], ['b27', 'b10', 'b26', 'b18', 'b40']]","[['b19'], ['b27', 'b10', 'b26', 'b18', 'b40']]",6,"sent1: All reviewed datasets have short narratives as their context.
sent2: The NarrativeQA short texts have a more complex narrative structure than other datasets, since the short context versions of the NarrativeQA are summaries of the larger narratives, and not single scenes from the long narratives.
sent3: In short narratives, if there is a common lexical pattern between the question and a part of the narrative, or a large lexical overlap between the answer and the narrative, sophisticated models can treat free-form QA as an extractive task.
sent4: For example, models trained on the TellMeWhy dataset generally try to find the answer span in the text and copy a part of the narrative as an answer (Lal et al., 2021).
sent5: The NarrativeQA dataset is the only dataset that has long narratives as its context.
sent6: Linking narrative elements to answer questions in large narratives is harder than in short narratives (see section 3.2).
sent7: Typically, in order to reason about long narratives, the parts relevant to reasoning are retrieved first (Kočiský et al., 2018;Tay et al., 2019;Frermann, 2019;Mou et al., 2020Mou et al., , 2021. The retrieval is difficult even with the state-of-the-art models due to the characteristics of narratives and the necessity of high-level narrative comprehension (Mou et al., 2021)."
256461385,Narrative Why-Question Answering: A Review of Challenges and Datasets,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/98f4b43487b8386728c14aba24f848b2eadad1e2,s6,Narratives,"Narratives are texts in which events are causally or thematically linked and develop within a temporal framework (Brewer, 2017). Narratives are generally agent-oriented and their main scope is centered on characters, their actions, and motivations (Sang et al., 2022). In narrative QA, stories, fairytales, books, and (movie) scripts are commonly utilized as narrative texts. Characteristics of narrative texts, such as causality of events and motivations of agents, make narratives a suitable context for asking why-questions. Additionally, fictional narratives can ensure the test of comprehension because they are self-contained, meaning that all elements needed to understand the narrative, such as events, characters, and settings, are present in the text and QA models need to comprehend the narrative in order to answer questions (Dunietz et al., 2020;Richardson et al., 2013;Kočiský et al., 2018). Implicitness is a key feature of narratives that makes it different from other types of texts. Length is another characteristic dimension of narratives which is also very important for QA systems. In the following subsections, we will review these characteristics in more detail.","[['b4', 'b8', 'b35', 'b18', 'b36']]","[['b4', 'b8', 'b35', 'b18', 'b36']]",5,"sent1: Narratives are texts in which events are causally or thematically linked and develop within a temporal framework (Brewer, 2017).
sent2: Narratives are generally agent-oriented and their main scope is centered on characters, their actions, and motivations (Sang et al., 2022).
sent3: In narrative QA, stories, fairytales, books, and (movie) scripts are commonly utilized as narrative texts.
sent4: Characteristics of narrative texts, such as causality of events and motivations of agents, make narratives a suitable context for asking why-questions.
sent5: Additionally, fictional narratives can ensure the test of comprehension because they are self-contained, meaning that all elements needed to understand the narrative, such as events, characters, and settings, are present in the text and QA models need to comprehend the narrative in order to answer questions (Dunietz et al., 2020;Richardson et al., 2013;Kočiský et al., 2018).
sent6: Implicitness is a key feature of narratives that makes it different from other types of texts.
sent7: Length is another characteristic dimension of narratives which is also very important for QA systems.
sent8: In the following subsections, we will review these characteristics in more detail."
256461385,Narrative Why-Question Answering: A Review of Challenges and Datasets,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/98f4b43487b8386728c14aba24f848b2eadad1e2,s5,Answer ambiguity,"Answer ambiguity occurs because most questions can have multiple answers belonging to different answer types and because often the desired type is not expressed in the question. Several partially overlapping taxonomies of reasons, which is the cause component of a causal relation, have been proposed (Verberne et al., 2006;Dunietz et al., 2017;Tan et al., 2022). Verberne et al. (2006) distinguish four types of reasons based on Quirk et al. (1985):

• Cause -a temporal and causal relation without the involvement of the human intention: an event mechanistically leads to another event;

• Motivation -a temporal and causal relation with an involvement of the human intention: a goal or a motivation of an agent leads to their action;

• Circumstance -a temporal and causal relation based on conditionality: one event is a condition for another event to occur;

• Generic purpose -a causal relation stemming from physical functions of the objects.

Similarly, Dunietz et al. (2017) defines three types of causalities while annotating causal relations: (1) Consequence: similar to the Cause type above, (2) Motivation and (3) Purpose: similar to the Motivation type above. Tan et al. (2022) defines four senses for causality based on Webber et al. (2019) for annotating causal relations: (1) Cause: similar to the Cause type above (2) Purpose: similar to the Motivation type above, (3) Condition and (4) Negative-Condition, which can fit into the Circumstance type above. Although the types of reasons introduced by Verberne et al. (2006) are broader than the taxonomies of Dunietz et al. (2017) and Tan et al. (2022), this list is not complete, as Verberne et al. (2006) demonstrated that not all why-questions can be classified into these categories.

Context: ""He opened the box to take a slice of pizza."" Question: ""Why did he open the box?"" Answers:

(1) The pizza was in the box.

(2) The box was closed.

(3) He was hungry.

(4) He wanted to eat pizza. (5) He wanted to take a slice of pizza.  (1) and (2) refer to causal reasons, answers (3), (4) and (5) refer to motivational reasons.

Valid answers to a why-question about an event or a state can include at least one of the cause, motivation, circumstance, or generic purpose of an event or state according to the above taxonomy. Since a why-question can often be answered with answers falling into several type categories, the necessity to choose the correct answer type creates ambiguity since the desired type is typically not explicitly stated in the question. Furthermore, a whyquestion can be answered with several causes in the causal chain (Verberne et al., 2006), and in that case all these answers can be considered as correct. For instance, consider the example shown in Table 1. For this example question, several potential causes can be the basis for the answer. Consequently, this why-question can be answered according to both mechanistically causal (answers 1, 2) and motivational (answers 3, 4, 5) reasons.","[[None, 'b9', 'b41', 'b34'], [], [], [], [], [None, 'b9', 'b41', 'b42'], [], [], [], [], [], ['b41']]","[[None, 'b9', 'b41', 'b34'], [], [], [], [], [None, 'b9', 'b41', 'b42'], [], [], [], [], [], ['b41']]",9,"sent1: Answer ambiguity occurs because most questions can have multiple answers belonging to different answer types and because often the desired type is not expressed in the question.
sent2: Several partially overlapping taxonomies of reasons, which is the cause component of a causal relation, have been proposed (Verberne et al., 2006;Dunietz et al., 2017;Tan et al., 2022).
sent3: Verberne et al. (2006) distinguish four types of reasons based on Quirk et al. (1985):• Cause -a temporal and causal relation without the involvement of the human intention: an event mechanistically leads to another event;• Motivation -a temporal and causal relation with an involvement of the human intention: a goal or a motivation of an agent leads to their action;• Circumstance -a temporal and causal relation based on conditionality: one event is a condition for another event to occur;• Generic purpose -a causal relation stemming from physical functions of the objects.
sent4: Similarly, Dunietz et al. (2017) defines three types of causalities while annotating causal relations: (1) Consequence: similar to the Cause type above, (2) Motivation and (3) Purpose: similar to the Motivation type above.
sent5: Tan et al. (2022) defines four senses for causality based on Webber et al. (2019) for annotating causal relations: (1) Cause: similar to the Cause type above (2) Purpose: similar to the Motivation type above, (3) Condition and (4) Negative-Condition, which can fit into the Circumstance type above.
sent6: Although the types of reasons introduced by Verberne et al. (2006) are broader than the taxonomies of Dunietz et al. (2017) and Tan et al. (2022), this list is not complete, as Verberne et al. (2006) demonstrated that not all why-questions can be classified into these categories.
sent7: Context: ""He opened the box to take a slice of pizza.""
sent8: Question: ""Why did he open the box?""
sent9: Answers:(1) The pizza was in the box.
sent10: (2) The box was closed. (3) He was hungry.
sent11: (4) He wanted to eat pizza. (5) He wanted to take a slice of pizza.
sent12: (1) and (2) refer to causal reasons, answers (3), (4) and (5) refer to motivational reasons.
sent13: Valid answers to a why-question about an event or a state can include at least one of the cause, motivation, circumstance, or generic purpose of an event or state according to the above taxonomy.
sent14: Since a why-question can often be answered with answers falling into several type categories, the necessity to choose the correct answer type creates ambiguity since the desired type is typically not explicitly stated in the question.
sent15: Furthermore, a whyquestion can be answered with several causes in the causal chain (Verberne et al., 2006), and in that case all these answers can be considered as correct.
sent16: For instance, consider the example shown in Table 1.
sent17: For this example question, several potential causes can be the basis for the answer.
sent18: Consequently, this why-question can be answered according to both mechanistically causal (answers 1, 2) and motivational (answers 3, 4, 5) reasons."
256461385,Narrative Why-Question Answering: A Review of Challenges and Datasets,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/98f4b43487b8386728c14aba24f848b2eadad1e2,s8,Short vs long narratives,"Narratives can be short or long based on the scope of the text stream and the number of events it contains.

Short narratives cover a small number of events and briefly narrate the actions of fewer agents. The local structure of a longer narrative such as an individual scene can be also considered and used as a short narrative. In short narratives, the reader can make inferences by linking local narrative elements and creating a local narrative representation (Sang et al., 2022;Kintsch, 1988).

Long narratives, on the other hand, have large textual content, cover many events, and focus on the actions and interactions of many agents. Long narratives require the readers to comprehend the underlying deep structure of the narrative and analyze the high-level abstractions. Answering questions in this setting requires understanding the global narrative structure, such as the whole story (Sang et al., 2022;Kintsch, 1988) and the integration of various information stated in different parts of the long narrative by connecting individual scenes (McNamara and Magliano, 2009).","[[], ['b36', 'b17'], ['b36', 'b17', 'b22']]","[[], ['b36', 'b17'], ['b36', 'b17', 'b22']]",5,"sent1: Narratives can be short or long based on the scope of the text stream and the number of events it contains.
sent2: Short narratives cover a small number of events and briefly narrate the actions of fewer agents.
sent3: The local structure of a longer narrative such as an individual scene can be also considered and used as a short narrative.
sent4: In short narratives, the reader can make inferences by linking local narrative elements and creating a local narrative representation (Sang et al., 2022;Kintsch, 1988).
sent5: Long narratives, on the other hand, have large textual content, cover many events, and focus on the actions and interactions of many agents.
sent6: Long narratives require the readers to comprehend the underlying deep structure of the narrative and analyze the high-level abstractions.
sent7: Answering questions in this setting requires understanding the global narrative structure, such as the whole story (Sang et al., 2022;Kintsch, 1988) and the integration of various information stated in different parts of the long narrative by connecting individual scenes (McNamara and Magliano, 2009)."
256461385,Narrative Why-Question Answering: A Review of Challenges and Datasets,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/98f4b43487b8386728c14aba24f848b2eadad1e2,s10,Datasets,"We selected several multiple-choice, extractive, and free-form QA datasets that utilize narrative as their context. In order to identify why-questions in these datasets, we first extracted all questions including the word why. We then manually removed any nonwhy questions (e.g, ""what did the king's son do after he wondered why the girl was crying"") from the questions that do not start with why. The relevant statistics of all datasets are shown in Table 2.

TellMeWhy (Lal et al., 2021) dataset presents free-form why-questions over events in short narratives. It is the only existing dataset created with the Narrative Why-Question Answering task in mind. The questions were created using templatebased transformations and the answers to questions were crowdsourced. Narratives were collected from ROCStories (Mostafazadeh et al., 2016a) and CATERS (Mostafazadeh et al., 2016b). The dataset has a total of 30,519 why-questions with three golden free-form answers for each question. According to data annotators, 28.82% of questions in the dataset cannot be answered explicitly based on the narrative (context).

MCTest (Richardson et al., 2013) is a multiplechoice MRC dataset based on fictional stories. The dataset is created via crowdsourcing and it is designed for the level of understanding of 7-year-old children. The fictional and basic comprehension nature of the dataset decreases the need for additional world knowledge and makes it possible to find the answer only based on the text.

MCScript (Ostermann et al., 2018a) is a multiple-choice MRC dataset based on stories about daily activities. It is created to evaluate machine comprehension using commonsense (script) knowledge (Ostermann et al., 2018b). Stories are collected by crowdsourcing new texts based on selected scenarios. Questions are crowdsourced based on scenarios independent of narratives and then matched with narratives randomly. Similar to MCTest, texts and questions are created according to the understanding level of a child. In general, 27.4% of questions require commonsense (script) knowledge to correctly infer the answer.  Table 2: Statistics of the narrative why-QA datasets. # of Why shows the number of why-questions in the datasets. % of Why refers to the proportion of why-questions in the datasets. The percentage of implicit questions is taken from the respective dataset papers, except for the NarrativeQA for which this number is due to the analysis done by Bauer et al. (2018) MCScript2.0 (Ostermann et al., 2019) is another multiple-choice MRC dataset focused on script knowledge. The stories were collected by reusing narratives from the MCScript, and crowdsourcing texts based on new scenarios. Questions were collected based on target sentences of stories rather than scenarios or complete stories. Similar to MC-Script and MCTest, the texts and questions are created according to the understanding level of a child. Correct and incorrect answers were crowdsourced by showing questions and hiding the target sentences in the story. In total, 50% of the questions require commonsense knowledge to be answered.

Cosmos QA (Huang et al., 2019) is a multiplechoice commonsense-based reading comprehension dataset. 93.8% of the questions in the dataset require contextual commonsense reasoning. Context paragraphs were collected from the spinn3r blog story corpus Burton et al. (2009) and a dataset by Gordon and Swanson (2009). Both questions and answers were crowdsourced. Questions are based on the causes and effects of events, facts about entities, and counterfactuals.

NarrativeQA (Kočiský et al., 2018) is a narrative reading comprehension dataset based on books and movies. Books from the Project Gutenberg and movie scripts from the web are used as stories. Moreover, summaries for long narratives are obtained from Wikipedia. Questions and answers are crowdsourced based on summaries only. Since both original long stories and summaries exist for each question, this dataset can be used for two tasks: narrative QA based on long narratives (books and movie scripts) and short narratives (summaries). Manual analysis on the validation set by Bauer et al. (2018) showed that 42% of the questions need commonsense knowledge for inference.

FairytaleQA (Xu et al., 2022) is a narrative com-prehension dataset designed for both question answering and question generation tasks. The narratives were collected from the Project Gutenberg by considering the reading difficulty up to the 10thgrade level. Small sections were extracted from fairytales as context paragraphs. Following the narrative comprehension frameworks by Paris and Paris (2003) and Alonzo et al. (2009), trained annotators created questions and answers for the contexts. The most common questions are about characters' behavior and causal relationships. 25.5% of the questions are implicit (free-form) and 74.5% of the questions are explicit (span-based). The amount of why-questions in the reviewed datasets is reported in Table 2. Among the multiple-choice QA datasets, CosmosQA has a higher number of why-questions compared to others. Among the free-form QA datasets, TellMe-Why dataset contains approximately 4.5 times more why-questions than the other two free-form QA datasets combined. Considering the proportion of why-questions in these datasets (also shown in Table 2), why-questions are well-represented in the CosmosQA and FairytaleQA datasets where they make up a sizeable part of the whole dataset, while in the MCTest, MCScript, MCScript2.0, and Narra-tiveQA datasets, why-questions cover only a small portion of the whole dataset.","[[], ['b25', 'b24'], ['b35'], ['b31', 'b1', 'b29', 'b30'], ['b5', 'b13', 'b15'], ['b1', 'b18'], [None, 'b33', 'b0']]","[[], ['b25', 'b24'], ['b35'], ['b31', 'b1', 'b29', 'b30'], ['b5', 'b13', 'b15'], ['b1', 'b18'], [None, 'b33', 'b0']]",15,"sent1: We selected several multiple-choice, extractive, and free-form QA datasets that utilize narrative as their context.
sent2: In order to identify why-questions in these datasets, we first extracted all questions including the word why.
sent3: We then manually removed any nonwhy questions (e.g, ""what did the king's son do after he wondered why the girl was crying"") from the questions that do not start with why.
sent4: The relevant statistics of all datasets are shown in Table 2.
sent5: TellMeWhy (Lal et al., 2021) dataset presents free-form why-questions over events in short narratives.
sent6: It is the only existing dataset created with the Narrative Why-Question Answering task in mind.
sent7: The questions were created using templatebased transformations and the answers to questions were crowdsourced.
sent8: Narratives were collected from ROCStories (Mostafazadeh et al., 2016a) and CATERS (Mostafazadeh et al., 2016b).
sent9: The dataset has a total of 30,519 why-questions with three golden free-form answers for each question.
sent10: According to data annotators, 28.82% of questions in the dataset cannot be answered explicitly based on the narrative (context).
sent11: MCTest (Richardson et al., 2013) is a multiplechoice MRC dataset based on fictional stories.
sent12: The dataset is created via crowdsourcing and it is designed for the level of understanding of 7-year-old children.
sent13: The fictional and basic comprehension nature of the dataset decreases the need for additional world knowledge and makes it possible to find the answer only based on the text.
sent14: MCScript (Ostermann et al., 2018a) is a multiple-choice MRC dataset based on stories about daily activities.
sent15: It is created to evaluate machine comprehension using commonsense (script) knowledge (Ostermann et al., 2018b).
sent16: Stories are collected by crowdsourcing new texts based on selected scenarios.
sent17: Questions are crowdsourced based on scenarios independent of narratives and then matched with narratives randomly.
sent18: Similar to MCTest, texts and questions are created according to the understanding level of a child.
sent19: In general, 27.4% of questions require commonsense (script) knowledge to correctly infer the answer.
sent20: Table 2: Statistics of the narrative why-QA datasets.
sent21: # of Why shows the number of why-questions in the datasets.
sent22: % of Why refers to the proportion of why-questions in the datasets.
sent23: The percentage of implicit questions is taken from the respective dataset papers, except for the NarrativeQA for which this number is due to the analysis done by Bauer et al. (2018) MCScript2.0 (Ostermann et al., 2019) is another multiple-choice MRC dataset focused on script knowledge.
sent24: The stories were collected by reusing narratives from the MCScript, and crowdsourcing texts based on new scenarios.
sent25: Questions were collected based on target sentences of stories rather than scenarios or complete stories.
sent26: Similar to MC-Script and MCTest, the texts and questions are created according to the understanding level of a child.
sent27: Correct and incorrect answers were crowdsourced by showing questions and hiding the target sentences in the story.
sent28: In total, 50% of the questions require commonsense knowledge to be answered.
sent29: Cosmos QA (Huang et al., 2019) is a multiplechoice commonsense-based reading comprehension dataset.
sent30: 93.8% of the questions in the dataset require contextual commonsense reasoning.
sent31: Context paragraphs were collected from the spinn3r blog story corpus Burton et al. (2009) and a dataset by Gordon and Swanson (2009).
sent32: Both questions and answers were crowdsourced.
sent33: Questions are based on the causes and effects of events, facts about entities, and counterfactuals.
sent34: NarrativeQA (Kočiský et al., 2018) is a narrative reading comprehension dataset based on books and movies.
sent35: Books from the Project Gutenberg and movie scripts from the web are used as stories.
sent36: Moreover, summaries for long narratives are obtained from Wikipedia.
sent37: Questions and answers are crowdsourced based on summaries only.
sent38: Since both original long stories and summaries exist for each question, this dataset can be used for two tasks: narrative QA based on long narratives (books and movie scripts) and short narratives (summaries).
sent39: Manual analysis on the validation set by Bauer et al. (2018) showed that 42% of the questions need commonsense knowledge for inference.
sent40: FairytaleQA (Xu et al., 2022) is a narrative com-prehension dataset designed for both question answering and question generation tasks.
sent41: The narratives were collected from the Project Gutenberg by considering the reading difficulty up to the 10thgrade level.
sent42: Small sections were extracted from fairytales as context paragraphs.
sent43: Following the narrative comprehension frameworks by Paris and Paris (2003) and Alonzo et al. (2009), trained annotators created questions and answers for the contexts.
sent44: The most common questions are about characters' behavior and causal relationships.
sent45: 25.5% of the questions are implicit (free-form) and 74.5% of the questions are explicit (span-based).
sent46: The amount of why-questions in the reviewed datasets is reported in Table 2.
sent47: Among the multiple-choice QA datasets, CosmosQA has a higher number of why-questions compared to others.
sent48: Among the free-form QA datasets, TellMe-Why dataset contains approximately 4.5 times more why-questions than the other two free-form QA datasets combined.
sent49: Considering the proportion of why-questions in these datasets (also shown in Table 2), why-questions are well-represented in the CosmosQA and FairytaleQA datasets where they make up a sizeable part of the whole dataset, while in the MCTest, MCScript, MCScript2.0, and Narra-tiveQA datasets, why-questions cover only a small portion of the whole dataset."
256461385,Narrative Why-Question Answering: A Review of Challenges and Datasets,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/98f4b43487b8386728c14aba24f848b2eadad1e2,s11,Evaluation measures,"For multiple-choice QA datasets, accuracy is a commonly used metric to measure the performance of a model. For free-form QA datasets, both automatic and human evaluation measures are utilized to evaluate the capabilities of the QA model. Most commonly, ROUGE-L (Lin, 2004), Meteor (Denkowski and Lavie, 2011), BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020) and

BertScore (Zhang et al., 2020) have been used to automatically evaluate the performance of the freeform QA models in narrative setting. Overall, F1 score of the ROUGE-L is the most commonly reported automatic evaluation measure.

In terms of human evaluation, Lal et al. (2021) proposed to assess the grammaticality and validity of the answers based on a 5-point Likert scale. The scale of the grammaticality ranges from strongly ungrammatical (1) to strongly grammatical (5), where a strongly grammatical answer must follow all the rules of the English grammar and a neutral score (3) is indicated when the meaning of the answer can be still inferred despite clear grammatical mistakes. The validity scale assesses whether the answer is valid and makes sense in the given context.","[['b32', 'b38', 'b21', 'b7'], ['b46'], ['b19']]","[['b32', 'b38', 'b21', 'b7'], ['b46'], ['b19']]",6,"sent1: For multiple-choice QA datasets, accuracy is a commonly used metric to measure the performance of a model.
sent2: For free-form QA datasets, both automatic and human evaluation measures are utilized to evaluate the capabilities of the QA model.
sent3: Most commonly, ROUGE-L (Lin, 2004), Meteor (Denkowski and Lavie, 2011), BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020) andBertScore (Zhang et al., 2020) have been used to automatically evaluate the performance of the freeform QA models in narrative setting.
sent4: Overall, F1 score of the ROUGE-L is the most commonly reported automatic evaluation measure.
sent5: In terms of human evaluation, Lal et al. (2021) proposed to assess the grammaticality and validity of the answers based on a 5-point Likert scale.
sent6: The scale of the grammaticality ranges from strongly ungrammatical (1) to strongly grammatical (5), where a strongly grammatical answer must follow all the rules of the English grammar and a neutral score (3) is indicated when the meaning of the answer can be still inferred despite clear grammatical mistakes.
sent7: The validity scale assesses whether the answer is valid and makes sense in the given context."
256461385,Narrative Why-Question Answering: A Review of Challenges and Datasets,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/98f4b43487b8386728c14aba24f848b2eadad1e2,s7,"Implicitness: ""Reading between the lines""","People often think and communicate with each other in the form of a narrative (story) (Dunietz et al., 2020). They assume that other people with whom they interact share a common ground with them, so they do not have to mention or specify commonly known knowledge (Ostermann et al., 2018a). Similar to the implicitness characteristic of the natural narrative-style communication, narrative texts tend to exclude common knowledge, such as commonsense and script (typical sequences of events to accomplish common tasks) knowledge, and assume that the reader has the background knowledge required to infer relevant implicit information (Schank and Abelson, 1975). For instance, not all causes of events and reasons for actions of agents are explicitly stated in narratives. Thus, the ability to ""read between the lines"" is necessary for properly understanding narratives (Norvig, 1987).","[['b29', 'b8', 'b28', 'b37']]","[['b29', 'b8', 'b28', 'b37']]",4,"sent1: People often think and communicate with each other in the form of a narrative (story) (Dunietz et al., 2020).
sent2: They assume that other people with whom they interact share a common ground with them, so they do not have to mention or specify commonly known knowledge (Ostermann et al., 2018a).
sent3: Similar to the implicitness characteristic of the natural narrative-style communication, narrative texts tend to exclude common knowledge, such as commonsense and script (typical sequences of events to accomplish common tasks) knowledge, and assume that the reader has the background knowledge required to infer relevant implicit information (Schank and Abelson, 1975).
sent4: For instance, not all causes of events and reasons for actions of agents are explicitly stated in narratives.
sent5: Thus, the ability to ""read between the lines"" is necessary for properly understanding narratives (Norvig, 1987)."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s18,Others.,"Here we present works that use a sentence-level resolution but differ from the majority of the other works that follow the traditional timeline summarization approach. In particular, we consider works on extracting disaster storylines from news and works that present variations on the traditional TLS task.

Disaster Storylines Zhou et al. 's works [133,134] present a framework to construct spatio-temporal storylines for disaster management from news data based on how the disaster location moves over time (e.g., a typhoon moving through different areas). This approach generates timelines for two levels of representation: a global level that follows the progress of the disaster through each location and a local level that focuses on a specific location. To extract the storyline, a series of snippets (i.e., event sentences) are extracted from the news articles using named entity recognition methods and grouped together based on a similarity graph. Then, a set of representative sentences is selected by finding the minimum dominating set [102] using a greedy algorithm. Next, an integer linear programming approach is used to select the optimal sequence for the main route of the disaster by maximizing the coherence of the story chain, subject to a series of structural, chronological, and length constraints. In this method, coherence is defined based on consecutive content similarity rather than word influence. However, the key difference is that this formulation includes a smoothness constraint, which is specifically designed to track the moving location of disasters through time. Smoothness is based on simulating the natural trajectory of a disaster. In particular, the constraints set a maximum distance for consecutive events (i.e., avoiding jumps to locations too far away) and seek to avoid acute angles that could be formed by two consecutive connections (i.e., avoiding sharp turns in the trajectory of the disaster). Once the main storyline has been constructed, the next step is to analyze the local level storylines. For each main storyline event, a set of similar articles are selected and used to construct a multi-view graph that represents the event relationships based on content similarity.

Then, a Steiner tree algorithm is used on the multi-view graph to generate a local storyline for that location.

Yuan et al. [131] propose dTexSL, a disaster storyline extraction approach that extends Zhou et al. 's works [133,134].

Unlike the previous approach, the news articles are first divided into different subsets based on location and are represented using neural embeddings. Locations are found by measuring the distance of the locations described in each article-using named entity recognition to find location references-and merging locations that are close enough based on a user-defined threshold. Then, an integer linear programming approach is used to select the key locations Manuscript submitted to ACM (i.e., document clusters). Instead of choosing events to maximize coherence like before, the goal is to maximize the number of documents covered on the map. The model has similar constraints as the original approach: chronological order, length, and smoothness. Once the main storyline has been constructed, a word embedding method is used to construct a multi-view graph that represents the event relationships based on content similarity. Using this graph, a set of representative articles are selected based on two criteria: uniqueness-computed using information gain-and relevance-computed using a measure of node importance. Then, a dynamic Steiner tree algorithm is used on the multiview graph to generate a local storyline for that specific location. Finally, a traditional multi-document summarization method [39] is applied to generate a high-level event description for that specific location.

Task Variations Duan et al. [28] introduce another variation on the timeline summarization task called comparative timeline summarization. In this task, the goal is to provide timelines consisting of major contrasting events from two data sets. Their approach is based on three core characteristics: coverage, distinctness, and diversity. Coverage is based on the idea that the timelines should cover most of the important information or topics from each data set. Distinctness is based on the idea that the events in a timeline should be distinct from the events on the other timeline at each time point, to allow for a proper contrast between them. Diversity is based on the idea that each timeline should cover a diverse set of events from its data set. To model these attributes, the authors propose a dynamic Markov model that is built around sentence similarity at a document level for each time step. In particular, sentences are selected from news articles to describe events based on local and global importance measures through the use of an affinity-preserving mutually reinforced Markov random walk model based on the PageRank algorithm. The output is a timeline that contains contrasting events from both data sets.

Yu et al. [130] propose a variation on the basic timeline summarization task, called Multi-TimeLine Summarization (MTLS). In this task, events are represented as sets of sentences and computationally represented by the neural embedding model sentence-BERT [90]. Given a set of time-stamped news articles, MLTS seeks to automatically extract timelines for important and different stories found in the data set. The authors propose a framework to solve this task called 2SAPS (Two-Stage Affinity Propagation Summarization). There are two key components in their framework: an event generation module and a timeline generation module. The event generation module seeks to extract important events from the document collection. To do so, it uses an affinity propagation approach to cluster similar sentences [34] and to identify the event of the article and any other previously referenced event. Furthermore, there is a temporal similarity term that uses an exponential decay function to penalize similarities of events that are temporally far away.

Once the events are identified, a subset of these events is selected based on a weighted average of a salience metric-based on event frequency-and a consistency metric-based on the intra-event similarity. Next, the timeline generation module has three internal steps: event link, time selection, and timeline summarization itself. Event linking is based on the weighted average between a co-reference score (based on entities or terms shared between events) and semantic similarity (e.g., cosine similarity). Based on these average scores, the system builds an event graph and uses affinity propagation on it to determine the initial clusters (i.e., timeline sets). Next, there is a timeline selection based on the weighted average of timeline salience-the average event salience of the timeline-and timeline coherence-the average semantic similarity scores between chronologically adjacent events. The timeline summarizing step selects an exemplar sentence for each event in the timelines, as the most typical and representative member of each event. Finally, there is an add-on timeline tagging step which assigns a label to each timeline, based on the most frequent words of the events.

Summarize Dates First (SDF) [55] is a timeline summarization pipeline that follows a different paradigm for timeline summarization based on generating a summary for each individual date first, and then selecting the most relevant dates using these summaries. This is different from the traditional approach where the relevant dates are selected first. Furthermore, this approach aggregates dates by leveraging higher-level temporal references (i.e., references to previous events in the article). SDF consists of three steps: temporal tagging, per-date summary extraction, and summary-drive date selection. In the temporal tagging stage, the raw text is annotated to identify date-level references (e.g., 31 December 2021) and high-level references (e.g., last December). The per-date summary extraction step uses any traditional sentence-based summarization algorithm from the multi-document summarization literature (e.g., TextRank [73]). Summary-driven date selection is the last step and uses a selection strategy, called Graph-Based Date Selection, which uses graph ranking algorithms (e.g., PageRank, HITS). In particular, a directed date graph model is built using the temporal references of the data set, where the edge weight connecting two dates is influenced by the count of date-level references and the similarity between the date summary and the high-level references to the earlier date.","[[], ['b132', 'b101', 'b133'], [], ['b132', 'b130', 'b133'], ['b38'], ['b27'], ['b89', 'b33', 'b129'], [], ['b54', 'b72', 'b30']]","[[], ['b132', 'b101', 'b133'], [], ['b132', 'b130', 'b133'], ['b38'], ['b27'], ['b89', 'b33', 'b129'], [], ['b54', 'b72', 'b30']]",14,"sent1: Here we present works that use a sentence-level resolution but differ from the majority of the other works that follow the traditional timeline summarization approach.
sent2: In particular, we consider works on extracting disaster storylines from news and works that present variations on the traditional TLS task.
sent3: Disaster Storylines Zhou et al. 's works [133,134] present a framework to construct spatio-temporal storylines for disaster management from news data based on how the disaster location moves over time (e.g., a typhoon moving through different areas).
sent4: This approach generates timelines for two levels of representation: a global level that follows the progress of the disaster through each location and a local level that focuses on a specific location.
sent5: To extract the storyline, a series of snippets (i.e., event sentences) are extracted from the news articles using named entity recognition methods and grouped together based on a similarity graph.
sent6: Then, a set of representative sentences is selected by finding the minimum dominating set [102] using a greedy algorithm.
sent7: Next, an integer linear programming approach is used to select the optimal sequence for the main route of the disaster by maximizing the coherence of the story chain, subject to a series of structural, chronological, and length constraints.
sent8: In this method, coherence is defined based on consecutive content similarity rather than word influence.
sent9: However, the key difference is that this formulation includes a smoothness constraint, which is specifically designed to track the moving location of disasters through time.
sent10: Smoothness is based on simulating the natural trajectory of a disaster.
sent11: In particular, the constraints set a maximum distance for consecutive events (i.e., avoiding jumps to locations too far away) and seek to avoid acute angles that could be formed by two consecutive connections (i.e., avoiding sharp turns in the trajectory of the disaster).
sent12: Once the main storyline has been constructed, the next step is to analyze the local level storylines.
sent13: For each main storyline event, a set of similar articles are selected and used to construct a multi-view graph that represents the event relationships based on content similarity.
sent14: Then, a Steiner tree algorithm is used on the multi-view graph to generate a local storyline for that location.
sent15: Yuan et al. [131] propose dTexSL, a disaster storyline extraction approach that extends Zhou et al. 's works [133,134].
sent16: Unlike the previous approach, the news articles are first divided into different subsets based on location and are represented using neural embeddings.
sent17: Locations are found by measuring the distance of the locations described in each article-using named entity recognition to find location references-and merging locations that are close enough based on a user-defined threshold.
sent18: Then, an integer linear programming approach is used to select the key locations Manuscript submitted to ACM (i.e., document clusters).
sent19: Instead of choosing events to maximize coherence like before, the goal is to maximize the number of documents covered on the map.
sent20: The model has similar constraints as the original approach: chronological order, length, and smoothness.
sent21: Once the main storyline has been constructed, a word embedding method is used to construct a multi-view graph that represents the event relationships based on content similarity.
sent22: Using this graph, a set of representative articles are selected based on two criteria: uniqueness-computed using information gain-and relevance-computed using a measure of node importance.
sent23: Then, a dynamic Steiner tree algorithm is used on the multiview graph to generate a local storyline for that specific location.
sent24: Finally, a traditional multi-document summarization method [39] is applied to generate a high-level event description for that specific location.
sent25: Task Variations Duan et al. [28] introduce another variation on the timeline summarization task called comparative timeline summarization.
sent26: In this task, the goal is to provide timelines consisting of major contrasting events from two data sets.
sent27: Their approach is based on three core characteristics: coverage, distinctness, and diversity.
sent28: Coverage is based on the idea that the timelines should cover most of the important information or topics from each data set.
sent29: Distinctness is based on the idea that the events in a timeline should be distinct from the events on the other timeline at each time point, to allow for a proper contrast between them.
sent30: Diversity is based on the idea that each timeline should cover a diverse set of events from its data set.
sent31: To model these attributes, the authors propose a dynamic Markov model that is built around sentence similarity at a document level for each time step.
sent32: In particular, sentences are selected from news articles to describe events based on local and global importance measures through the use of an affinity-preserving mutually reinforced Markov random walk model based on the PageRank algorithm.
sent33: The output is a timeline that contains contrasting events from both data sets.
sent34: Yu et al. [130] propose a variation on the basic timeline summarization task, called Multi-TimeLine Summarization (MTLS).
sent35: In this task, events are represented as sets of sentences and computationally represented by the neural embedding model sentence-BERT [90].
sent36: Given a set of time-stamped news articles, MLTS seeks to automatically extract timelines for important and different stories found in the data set.
sent37: The authors propose a framework to solve this task called 2SAPS (Two-Stage Affinity Propagation Summarization).
sent38: There are two key components in their framework: an event generation module and a timeline generation module.
sent39: The event generation module seeks to extract important events from the document collection.
sent40: To do so, it uses an affinity propagation approach to cluster similar sentences [34] and to identify the event of the article and any other previously referenced event.
sent41: Furthermore, there is a temporal similarity term that uses an exponential decay function to penalize similarities of events that are temporally far away.
sent42: Once the events are identified, a subset of these events is selected based on a weighted average of a salience metric-based on event frequency-and a consistency metric-based on the intra-event similarity.
sent43: Next, the timeline generation module has three internal steps: event link, time selection, and timeline summarization itself.
sent44: Event linking is based on the weighted average between a co-reference score (based on entities or terms shared between events) and semantic similarity (e.g., cosine similarity).
sent45: Based on these average scores, the system builds an event graph and uses affinity propagation on it to determine the initial clusters (i.e., timeline sets).
sent46: Next, there is a timeline selection based on the weighted average of timeline salience-the average event salience of the timeline-and timeline coherence-the average semantic similarity scores between chronologically adjacent events.
sent47: The timeline summarizing step selects an exemplar sentence for each event in the timelines, as the most typical and representative member of each event.
sent48: Finally, there is an add-on timeline tagging step which assigns a label to each timeline, based on the most frequent words of the events.
sent49: Summarize Dates First (SDF) [55] is a timeline summarization pipeline that follows a different paradigm for timeline summarization based on generating a summary for each individual date first, and then selecting the most relevant dates using these summaries.
sent50: This is different from the traditional approach where the relevant dates are selected first.
sent51: Furthermore, this approach aggregates dates by leveraging higher-level temporal references (i.e., references to previous events in the article).
sent52: SDF consists of three steps: temporal tagging, per-date summary extraction, and summary-drive date selection.
sent53: In the temporal tagging stage, the raw text is annotated to identify date-level references (e.g., 31 December 2021) and high-level references (e.g., last December).
sent54: The per-date summary extraction step uses any traditional sentence-based summarization algorithm from the multi-document summarization literature (e.g., TextRank [73]).
sent55: Summary-driven date selection is the last step and uses a selection strategy, called Graph-Based Date Selection, which uses graph ranking algorithms (e.g., PageRank, HITS).
sent56: In particular, a directed date graph model is built using the temporal references of the data set, where the edge weight connecting two dates is influenced by the count of date-level references and the similarity between the date summary and the high-level references to the earlier date."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s20,Connect the Dots Approaches. Linear Representations Shahaf and Guestrin,"Expanding upon the Connect the Dots method, Zhu and Oates [135] propose an algorithm to extract story chains from newswire articles that connect two user-defined endpoints based on the following characteristics: relevance (the articles on the chain should be relevant to the endpoints), coherence (the transition between events should be smooth), low Furthermore, the model adds a named entity bias that assigns a higher weight to named entities compared to other terms. This is modeled through a co-occurrence frequency matrix for entity pairs, which is then used to compute a relevance score for each document in the data set based on the named entities. In turn, these elements are used to modify the cluster and document weights in the correlation graph.

Camacho Barranco et al. [18] propose a storyline extraction algorithm that takes a set of user-defined articles as a seed and generates a timeline of articles based on a series of evaluation metrics. First, the authors propose a temporal criterion to filter candidate documents based on a range between the latest publication date of the seed articles and a maximum threshold away from the earliest publication date of the seed articles (i.e., in the interval [ min − ℎ ℎ , max ]).

Next, there is a topical criterion that measures how much a candidate article can deviate from the seed articles based on KLD and LDA topics. Having defined their basic framework, the authors then formalize an optimization problem to extract the storylines by selecting article connections based on different criteria: incoherence, similarity, overlap, and uniformity. Incoherence is based on the average pairwise Soergel distance between documents-measured using TF-IDF information for the entities of the document-with a temporal factor to penalize temporally distant articles.

Similarity is used as a penalty factor to enforce diversity in non-adjacent articles of the storyline, implemented as a negative exponential factor based on the Soergel distance. Both of these metrics are weighted by a relevance factor of the documents and are smoothed using modified Gaussian distributions to measure event overlap. Next, an overall overlap factor for the storyline is computed, assigning a penalty based on the difference between publication dates and a user-defined threshold. The overlap factor ensures that the breakpoints occur at sufficiently distinct dates. The uniformity penalty seeks to avoid the case where the optimal solution selects purely irrelevant events as optimal by penalizing uniform weights. The objective function to minimize consists of the sum of the product between incoherence and similarity, multiplied by the overlap and uniformity penalties.

Graph-based Representations Metro Maps [98,99] are an extension of the Connect the Dots approach that represents more than a single storyline using a directed acyclic graph of events. In particular, the metro maps method is a structured summarization approach that captures the evolution of multiple stories and their interactions. The stories are represented using a metro map metaphor, where each metro line represents a story and stations represent key events.

Metro lines intersect in specific stations, representing how storylines connect with each other. This representation is extracted by solving an optimization problem. In particular, the goal is to maximize connectivity, subject to coverage and coherence constraints. Coverage is computed based on how well specific terms or keywords are represented in the selected events and is defined using a submodular function that encourages diversity (e.g., if a term is already covered, adding a document that covers it provides little extra coverage). These keywords depend on the specific corpus or domain of application. Coherence is defined following Shahaf et al.'s previous work [96,97]. Finally, connectivity is defined as the number of stories that intersect which is used to ensure that the final metro map is connected. The optimization problem is solved in phases. First, a series of coherent candidate metro lines are selected based on a divide-and-conquer approach, which constructs long lines from shorter ones and encodes them in a graph. Then, the method extracts a set of coherent lines that maximize coverage using an approximation algorithm based on the submodularity of the coverage function (otherwise finding these lines is an NP-hard problem). Finally, connectivity is increased using a local search approach that substitutes lines without sacrificing coverage.

Similar to the metro maps metaphor, the Narrative Maps model [51] provides a framework to extract and represent narratives based on a route map metaphor. The narrative and its stories are shown as a series of routes through landmarks, which represent the events. In computational terms, the narrative is modeled through a directed acyclic graph of events. The events are represented through neural embeddings of article headlines. The graph is extracted by solving an optimization problem defined following a linear programming formulation similar to the Connect the Dots approach. The optimization problem is based on maximizing coherence subject to coverage constraints. Coherence measures how much sense it makes to connect two events together and is defined as the geometric mean of the content similarity of events-using cosine or angular similarity-and their topical similarity-based on JS similarity of their topic distributions based on clustering. Coverage is measured by the average percentage of topical clusters covered by the selected events based on their topic distributions. Once the optimal map has been found, the main storyline is extracted by normalizing the coherence values of the edges into probabilities and finding the maximum likelihood path.

Then, a set of representative landmarks (i.e., important events) of each story by finding the maximum antichain, which corresponds to the point of the maximum width of the graph.","[['b134'], ['b17'], [], [], ['b97', 'b98'], ['b95', 'b96'], ['b50'], []]","[['b134'], ['b17'], [], [], ['b97', 'b98'], ['b95', 'b96'], ['b50'], []]",7,"sent1: Expanding upon the Connect the Dots method, Zhu and Oates [135] propose an algorithm to extract story chains from newswire articles that connect two user-defined endpoints based on the following characteristics: relevance (the articles on the chain should be relevant to the endpoints), coherence (the transition between events should be smooth), low Furthermore, the model adds a named entity bias that assigns a higher weight to named entities compared to other terms.
sent2: This is modeled through a co-occurrence frequency matrix for entity pairs, which is then used to compute a relevance score for each document in the data set based on the named entities.
sent3: In turn, these elements are used to modify the cluster and document weights in the correlation graph.
sent4: Camacho Barranco et al. [18] propose a storyline extraction algorithm that takes a set of user-defined articles as a seed and generates a timeline of articles based on a series of evaluation metrics.
sent5: First, the authors propose a temporal criterion to filter candidate documents based on a range between the latest publication date of the seed articles and a maximum threshold away from the earliest publication date of the seed articles (i.e., in the interval [ min − ℎ ℎ , max ]).
sent6: Next, there is a topical criterion that measures how much a candidate article can deviate from the seed articles based on KLD and LDA topics.
sent7: Having defined their basic framework, the authors then formalize an optimization problem to extract the storylines by selecting article connections based on different criteria: incoherence, similarity, overlap, and uniformity.
sent8: Incoherence is based on the average pairwise Soergel distance between documents-measured using TF-IDF information for the entities of the document-with a temporal factor to penalize temporally distant articles.
sent9: Similarity is used as a penalty factor to enforce diversity in non-adjacent articles of the storyline, implemented as a negative exponential factor based on the Soergel distance.
sent10: Both of these metrics are weighted by a relevance factor of the documents and are smoothed using modified Gaussian distributions to measure event overlap.
sent11: Next, an overall overlap factor for the storyline is computed, assigning a penalty based on the difference between publication dates and a user-defined threshold.
sent12: The overlap factor ensures that the breakpoints occur at sufficiently distinct dates.
sent13: The uniformity penalty seeks to avoid the case where the optimal solution selects purely irrelevant events as optimal by penalizing uniform weights.
sent14: The objective function to minimize consists of the sum of the product between incoherence and similarity, multiplied by the overlap and uniformity penalties.
sent15: Graph-based Representations Metro Maps [98,99] are an extension of the Connect the Dots approach that represents more than a single storyline using a directed acyclic graph of events.
sent16: In particular, the metro maps method is a structured summarization approach that captures the evolution of multiple stories and their interactions.
sent17: The stories are represented using a metro map metaphor, where each metro line represents a story and stations represent key events.
sent18: Metro lines intersect in specific stations, representing how storylines connect with each other.
sent19: This representation is extracted by solving an optimization problem.
sent20: In particular, the goal is to maximize connectivity, subject to coverage and coherence constraints.
sent21: Coverage is computed based on how well specific terms or keywords are represented in the selected events and is defined using a submodular function that encourages diversity (e.g., if a term is already covered, adding a document that covers it provides little extra coverage).
sent22: These keywords depend on the specific corpus or domain of application.
sent23: Coherence is defined following Shahaf et al.'s previous work [96,97].
sent24: Finally, connectivity is defined as the number of stories that intersect which is used to ensure that the final metro map is connected.
sent25: The optimization problem is solved in phases.
sent26: First, a series of coherent candidate metro lines are selected based on a divide-and-conquer approach, which constructs long lines from shorter ones and encodes them in a graph.
sent27: Then, the method extracts a set of coherent lines that maximize coverage using an approximation algorithm based on the submodularity of the coverage function (otherwise finding these lines is an NP-hard problem).
sent28: Finally, connectivity is increased using a local search approach that substitutes lines without sacrificing coverage.
sent29: Similar to the metro maps metaphor, the Narrative Maps model [51] provides a framework to extract and represent narratives based on a route map metaphor.
sent30: The narrative and its stories are shown as a series of routes through landmarks, which represent the events.
sent31: In computational terms, the narrative is modeled through a directed acyclic graph of events.
sent32: The events are represented through neural embeddings of article headlines.
sent33: The graph is extracted by solving an optimization problem defined following a linear programming formulation similar to the Connect the Dots approach.
sent34: The optimization problem is based on maximizing coherence subject to coverage constraints.
sent35: Coherence measures how much sense it makes to connect two events together and is defined as the geometric mean of the content similarity of events-using cosine or angular similarity-and their topical similarity-based on JS similarity of their topic distributions based on clustering.
sent36: Coverage is measured by the average percentage of topical clusters covered by the selected events based on their topic distributions.
sent37: Once the optimal map has been found, the main storyline is extracted by normalizing the coherence values of the edges into probabilities and finding the maximum likelihood path.
sent38: Then, a set of representative landmarks (i.e., important events) of each story by finding the maximum antichain, which corresponds to the point of the maximum width of the graph."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s22,Manuscript submitted to ACM,"Graph Representations Uramoto and Takeda [112] proposed a graph-based approach to model the relationships between news articles. In particular, they use a directed graph based on temporal ordering and event similarity. This is the earliest article that fits with our definitions of event-based narrative representations for news narratives that we found. In particular, the authors use the concepts of genus and differentia words. For adjacent articles, genus words are computed using the intersection of their word sets and represent already known information in the story. In contrast, differentia words are built from the set difference between the articles (in temporal order) and represent new knowledge in the story. Thus, differentia words are more important when trying to find coherent sequences of articles. The events are represented with a variation of TF-IDF that assigns more weight to differentia words.

Tannier and Moriceau [106] propose an approach for building multi-document event threads from news articles.

In particular, they use a supervised learning approach with a series of classifiers to define the type of relationship between news articles: same-event, continuation, or reaction. The output of this method is a temporal event graph,

where the nodes correspond to events (represented as news articles) and the edges are labeled with the corresponding relationships. In particular, the first step is to determine whether there is a connection at all between the articles. To do so, an initial classifier is implemented using a series of content similarity features (e.g., word overlap, cosine similarity, and similarity of the first sentences) to construct the initial temporal graph. However, this is not enough to find all potential relationships and a second-level classifier is included that takes into account the results from the previous classifier by using degree-based features from the temporal graph. Next, after a connection has been established, another classifier determines whether this connection is based on the articles referring to the same news event-same-event connection-or based on a continuation-when an event is a direct continuation or consequence of a previous one. This classifier relies on date-based features (e.g., differences in publication time, date references, and references between events themselves) and keyword-based features (e.g., usage of temporal words, reaction words, or opinion words). The output is fed into another classifier that leverages degree-based features again to find more relationships. Due to the transitive nature of the same-event and continuation relationships, a post-processing step takes the graph and constructs the transitive closure for these specific relations. Afterward, a final classifier uses the same features to determine whether a continuation is a reaction-a subset of continuations that relate the reactions of people (or organizations) to an event.

Hu et al. [47] propose a system to model storyline interactions from news events. Their approach generates a series of event timelines focusing on specific entities or topics and their interactions with each other. In particular, this results in a directed graph connecting multiple events. In contrast to other approaches, the underlying representation of events is based on the main event descriptors (i.e., the answers to Who, What, When, Where, Why, and How) [50] which are extracted directly from each article and represent the key elements of the event. Based on this information, a coherence graph is constructed and used to identify the storylines through a random walk. Coherence is defined by three factors:

subtopic consistency, entity relatedness, and time continuity. To measure subtopic consistency, the first step is to use a generative probabilistic mixture model to discover latent subtopics. Then, JS divergence is used to measure the distance of topic distributions between articles. Next, entity relatedness is measured by the average affinity of the entities from each pair of articles using normalized point-wise mutual information. The time continuity factor is simply defined as an exponential penalty term dependent on the temporal distance between events. The coherence graph is built by creating edges between documents that have a coherence score above a given threshold. Based on the coherence graph, a series of informative events that connect multiple storylines are identified. Specifically, a topic-sensitive PageRank algorithm [43] is used to discover these events. In turn, these events feed the storyline generation algorithm, an iterative algorithm that selects a single informative event for each story for each day.

Bögel and Gertz [14] present a temporal linking framework based on the concept of article references. In particular, they exploit the structure of news articles to construct an information network. Instead of comparing articles based on overall content similarity, they exploit the use of lead paragraphs, explanatory paragraphs, and additional information paragraphs in typical news articles. Specifically, they construct the network based on temporal expressions, keywords, and entity names. To select valid event connections, the first step is to filter based on temporal information contained in the text based on a temporal tagger. Next, connections are evaluated based on the similarity of the lead paragraph of a news article with all the other paragraphs of another news article (i.e., capturing references to the event). Similarity is computed based on the entities and keywords mentioned in each paragraph based on a weighted average of Jaccard and cosine similarity. Finally, irrelevant edges are pruned based on a user-defined threshold. However, some non-relevant edges are kept if they fulfill the role of a support path-paths that have non-relevant edges but share endpoints with fully relevant paths-that provide more evidence of two events being connected. The output is a directed graph based on references, not necessarily acyclic, as there are future temporal references in some articles.","[['b111'], ['b105'], [], [], ['b46', 'b49'], ['b42'], ['b13']]","[['b111'], ['b105'], [], [], ['b46', 'b49'], ['b42'], ['b13']]",6,"sent1: Graph Representations Uramoto and Takeda [112] proposed a graph-based approach to model the relationships between news articles.
sent2: In particular, they use a directed graph based on temporal ordering and event similarity.
sent3: This is the earliest article that fits with our definitions of event-based narrative representations for news narratives that we found.
sent4: In particular, the authors use the concepts of genus and differentia words.
sent5: For adjacent articles, genus words are computed using the intersection of their word sets and represent already known information in the story.
sent6: In contrast, differentia words are built from the set difference between the articles (in temporal order) and represent new knowledge in the story.
sent7: Thus, differentia words are more important when trying to find coherent sequences of articles.
sent8: The events are represented with a variation of TF-IDF that assigns more weight to differentia words.
sent9: Tannier and Moriceau [106] propose an approach for building multi-document event threads from news articles.
sent10: In particular, they use a supervised learning approach with a series of classifiers to define the type of relationship between news articles: same-event, continuation, or reaction.
sent11: The output of this method is a temporal event graph,where the nodes correspond to events (represented as news articles) and the edges are labeled with the corresponding relationships.
sent12: In particular, the first step is to determine whether there is a connection at all between the articles.
sent13: To do so, an initial classifier is implemented using a series of content similarity features (e.g., word overlap, cosine similarity, and similarity of the first sentences) to construct the initial temporal graph.
sent14: However, this is not enough to find all potential relationships and a second-level classifier is included that takes into account the results from the previous classifier by using degree-based features from the temporal graph.
sent15: Next, after a connection has been established, another classifier determines whether this connection is based on the articles referring to the same news event-same-event connection-or based on a continuation-when an event is a direct continuation or consequence of a previous one.
sent16: This classifier relies on date-based features (e.g., differences in publication time, date references, and references between events themselves) and keyword-based features (e.g., usage of temporal words, reaction words, or opinion words).
sent17: The output is fed into another classifier that leverages degree-based features again to find more relationships.
sent18: Due to the transitive nature of the same-event and continuation relationships, a post-processing step takes the graph and constructs the transitive closure for these specific relations.
sent19: Afterward, a final classifier uses the same features to determine whether a continuation is a reaction-a subset of continuations that relate the reactions of people (or organizations) to an event.
sent20: Hu et al. [47] propose a system to model storyline interactions from news events.
sent21: Their approach generates a series of event timelines focusing on specific entities or topics and their interactions with each other.
sent22: In particular, this results in a directed graph connecting multiple events.
sent23: In contrast to other approaches, the underlying representation of events is based on the main event descriptors (i.e., the answers to Who, What, When, Where, Why, and How) [50] which are extracted directly from each article and represent the key elements of the event.
sent24: Based on this information, a coherence graph is constructed and used to identify the storylines through a random walk.
sent25: Coherence is defined by three factors:subtopic consistency, entity relatedness, and time continuity.
sent26: To measure subtopic consistency, the first step is to use a generative probabilistic mixture model to discover latent subtopics.
sent27: Then, JS divergence is used to measure the distance of topic distributions between articles.
sent28: Next, entity relatedness is measured by the average affinity of the entities from each pair of articles using normalized point-wise mutual information.
sent29: The time continuity factor is simply defined as an exponential penalty term dependent on the temporal distance between events.
sent30: The coherence graph is built by creating edges between documents that have a coherence score above a given threshold.
sent31: Based on the coherence graph, a series of informative events that connect multiple storylines are identified.
sent32: Specifically, a topic-sensitive PageRank algorithm [43] is used to discover these events.
sent33: In turn, these events feed the storyline generation algorithm, an iterative algorithm that selects a single informative event for each story for each day.
sent34: Bögel and Gertz [14] present a temporal linking framework based on the concept of article references.
sent35: In particular, they exploit the structure of news articles to construct an information network.
sent36: Instead of comparing articles based on overall content similarity, they exploit the use of lead paragraphs, explanatory paragraphs, and additional information paragraphs in typical news articles.
sent37: Specifically, they construct the network based on temporal expressions, keywords, and entity names.
sent38: To select valid event connections, the first step is to filter based on temporal information contained in the text based on a temporal tagger.
sent39: Next, connections are evaluated based on the similarity of the lead paragraph of a news article with all the other paragraphs of another news article (i.e., capturing references to the event).
sent40: Similarity is computed based on the entities and keywords mentioned in each paragraph based on a weighted average of Jaccard and cosine similarity.
sent41: Finally, irrelevant edges are pruned based on a user-defined threshold.
sent42: However, some non-relevant edges are kept if they fulfill the role of a support path-paths that have non-relevant edges but share endpoints with fully relevant paths-that provide more evidence of two events being connected.
sent43: The output is a directed graph based on references, not necessarily acyclic, as there are future temporal references in some articles."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s24,Event Threading and Evolution.,"Nallapati et al. [76] use a directed graph model to represent to capture the structure and dependencies of events in a news topic. They call this extraction process event threading. They represent each event as a cluster of news articles. Event threading is a supervised method that consists of two phases: clustering documents and modeling dependencies. The clustering process starts with a cluster for each document in the data set and merges them iteratively based on similarity until the similarities fall below a predefined threshold. The authors evaluate three types of cluster similarity on the average link, complete link, or single link of the clusters based on document similarities. Document similarities are based on content similarity (e.g., cosine similarity), common locations, and common entities. Furthermore, there is an exponential decay term based on the temporal distance to penalize larger temporal distances between documents. Next, dependency modeling uses surface-level features of the document clusters, such as word distributions and time-ordering of the news articles. Based on this information, the authors propose several link extraction criteria (complete-link, simple threshold, nearest parent, best similarity, and maximum spanning tree). These approaches rely on temporal order, similarity information, or structural information.

SToRe (Storyline-based Topic Retrospection) is a topic retrospective system [63][64][65] that extracts the main storyline from a given news topic and provides a summary of the topic based on this storyline. In particular, the extraction process consists of four phases: event identification, topic structure identification, main storyline construction, and storyline-based summarization. In the event identification phase, similar news articles will be clustered together to represent a single event using self-organizing maps. In the topic structure identification step, the events are linked together based on whether their similarity exceeds a specific threshold. To compute similarity, the events are represented with a vector of term weights using the concepts of genus and differentia words [112]. Then, cosine similarity is used to compare the event vectors. Next, in the main storyline construction step, an MST is extracted from the constructed topic structure. The MST is based on the relevance of each event with respect to the topic. The MST is used to generate a timeline of events, and it is further extended with small side branches of other relevant events based on a specific threshold. Finally, in the storyline-based summarization, a summary is generated for each event based on the news articles contained in its cluster using accumulated weight summary [39].

Manuscript submitted to ACM Yang et al. [126,127] use directed acyclic graphs to represent the evolution of events in online news. They call their approach event evolution graphs, which represent temporal and causal relationships between events. Events are defined as sets of news articles and are represented as the average of the TF-IDF vectors of each article they contain. We note that the proposed method assumes that events and their corresponding articles are already computed. In practice, this would require a clustering step before constructing the graph. These events are linked together based on their similarity and a user-specified threshold, which is computed based on content similarity (e.g., cosine similarity), temporal proximity, and document distributional proximity (which penalizes bursty periods with many articles about the same event). The latter two terms are represented through exponential decay factors. Furthermore, users are able to reduce the temporal granularity of the event evolution graph, which merges specific events that occur in short time frames.

Qiu et al. [87] propose another event evolution graph extraction method. Their construction method follows an iterative approach based on content similarity and temporal order. In particular, documents are first grouped into clusters using the OHC method [88] in the first time period, which gives rise to the initial events. Next, the PRAC method [89] is used to build classifiers and determine whether the documents of the next time period are continuations of a cluster identified in the previous period. If so, a new event node is created using the identified cluster as its parent. This process is repeated until the last time period. Next, twigs-paths that die before the end of the timeline-are removed based on a user-set tolerance, and equivalent event nodes are merged to reduce graph complexity.

TSCAN (Topic Summarization and Content ANatomy) [20,21] is a method to analyze news data that produces a global summary and constructs an event evolution graph. We focus on the event graph component of this method.

First, news articles are grouped into themes obtained through a matrix factorization approach with TF-IDF document representations. Next, the news articles of each theme are temporally segmented using an energy value threshold based on eigenvalues from the matrix representation. In practice, this generates clusters of documents based on frequency, which are associated with the nodes of the event evolution graph. The evolution graph is a directed acyclic graph,

where the edges are constructed using temporal similarity-computed using the temporal distance between events, with special cases to consider event overlap-and content similarity-based on cosine similarity.

Khurdiya et al. [52] propose a system that extracts directed graphs to represent stories from news data using multiperspective links. Each node of this graph is associated with multiple news articles. The system uses LDA to extract topics in each time unit (e.g., a day). The extracted topics are associated with sets of articles based on the strength of the topic in each article and form the basis of the story identification model. We note that these topics and their article sets correspond to the notion of event that we use in this survey. Next, article sets are linked chronologically based on topic correlation (e.g., Pearson's correlation coefficient) and a user-defined threshold, generating a directed graph of events.

Wei et al. [117] identify event episodes in news data sets and construct a temporal episode graph (i.e., an event graph under our definitions in the survey). In particular, this article shows a discovery mechanism that organizes news documents into events using novel TF-IDF representations that incorporate a temporal component. Then, the system builds a link structure based on intercluster similarity measures. The first proposed event representation, called TF-IDF Tempo , gives more weight to features with consecutive occurrences in a sequence of documents (i.e., it incorporates the surrounding context of the document) by modifying the IDF component of TF-IDF to consider the order of the documents. However, this approach is too strict and is unable to model overlapping events. Moreover, it also has a high bias towards low-frequency articles that are temporally close. Thus, the authors propose a second representation, called TF-Enhanced-IDF Tempo which modifies the IDF component by adopting the significance factor proposed by Luhn [68] and a temporal gap threshold to allow for short discontinuities in feature appearances. These representations are used with Hierarchical Agglomerative Clustering (HAC) [114] to construct the article clusters that represent the events. For the purposes of clustering, document similarity is defined by content similarity (e.g., cosine similarity) and a negative exponential penalty for temporally distant documents.

Huang et al. [48] propose a different event evolution approach to build and analyze event relationships based on three types of event connections. In particular, they define a co-occurrence dependence relationship, an event reference relationship, and a temporal proximity relationship. The authors define events as a set of news articles and identify them through clustering and topic modeling using a combined similarity measure that leverages LDA and a TF-IDF document model with cosine similarity. Once the events are identified, the method extracts a series of core features (i.e., key entities and terms of the article) by analyzing the lead of the articles and evaluating whether their frequency is above a specified threshold. These core features are used to construct a vectorial representation of the events. For the co-occurrence relationship, the method computes the aggregation of all mutual information between all features of the event, generating a symmetric matrix that represents all event-event relationships. For the event reference analysis, the method identifies shared core features and defines the degree of event reference based on the frequency of references in an event to the core features of a previous event, adjusted by the weight of these terms in the referencing event.

Temporal dependency is evaluated using an exponential decay formula.

Event Phase Oriented News Summarization (EPONS) [116] is a timeline summarization approach that assumes that a story summary contains multiple timelines, each one corresponding to a specific event. To model the semantic relations of news articles, EPONS uses a graph model, called Temporal Content Coherence Graph (TCCG), which is an event graph based on two metrics: content coherence and temporal influence. Content coherence is based on the weighted average of topic level similarity-modeled by JS divergence over an LDA topic distribution-and entity-level similarity-modeled over a ranking of named entities using the Tanimoto coefficient. Temporal influence is modeled through a Hamming (cosine) kernel to properly separate temporally distinct events. The TCCG is built by selecting edges that are above user-specified thresholds in each metric. Based on this graph, EPONS uses a modified structural clustering approach to group the news articles into different events. Furthermore, small clusters of similar articles are filtered out to ensure that the events are modeled properly. This post-processing is done by using four quality metrics on a pre-trained logistic regression classifier: percentage of new articles, time interval length, pairwise topic similarity, and pairwise entity similarity. Having identified the events, it is now necessary to construct the individual summaries and finalize the timeline. To do so, a vertex-reinforced random walk [70,84] is used to rank the relevance of news articles inside each event, in a similar manner to PageRank. Next, a supervised model is used to determine whether the headlines are factual (i.e., they are reporting a specific event) or an opinion, as opinion-based headlines are not considered useful for timelines and must be filtered out. Finally, an optimization method is used to maximize the total relevance, subject to non-redundancy constraints (i.e., disallowing events that are too similar) to select the news articles.

Cai et al. [17] propose a method to extract Temporal Event Maps (TEM) based on the content dependence degree and component event reference degree for each pair of events. TEMs are directed graphs that have events as nodes, relations as edges, edge weights representing the strength of event relationships, and node weights representing the importance of each event. Events are defined as groups of related documents and identified using a LDA model. After obtaining the events, the next step is to compute the two core metrics that define the temporal event maps. The content dependence degree is defined as the aggregation of all mutual information among the features of each event. The content reference degree is defined by the presence of core features of an event-salient terms based on frequency-in other events. Unlike content dependence, this is not a symmetric relationship between events. To construct the temporal event maps, the first step is to order events based on starting time. Then, connections are added for events that surpass a user-specified Manuscript submitted to ACM threshold for the product of content dependence and event reference degrees, which provides the edge weights for the graph. Finally, a ranking procedure based on PageRank is used to generate the event importance values. co-occurrence graphs. To extract the maps, an optimization problem is defined based on finding the best structure for the map, relying on the idea of minimizing the total number of storylines (to reduce unneeded complexity) and maximizing the number of covered clusters (to ensure that the stories are well covered). This approach leads to simple stories being modeled as a single metro line and more complex stories requiring the use of multiple shorter lines. Furthermore, a series of additional constraints for story coherence, cluster quality, and map size is imposed.","[['b75'], ['b64', 'b111', 'b62', 'b63', 'b38'], ['b125', 'b126'], ['b88', 'b87', 'b86'], ['b20', 'b19'], [], [], ['b51'], ['b116', 'b67', 'b113'], ['b47'], [], ['b115', 'b83', 'b69'], ['b16']]","[['b75'], ['b64', 'b111', 'b62', 'b63', 'b38'], ['b125', 'b126'], ['b88', 'b87', 'b86'], ['b20', 'b19'], [], [], ['b51'], ['b116', 'b67', 'b113'], ['b47'], [], ['b115', 'b83', 'b69'], ['b16']]",22,"sent1: Nallapati et al. [76] use a directed graph model to represent to capture the structure and dependencies of events in a news topic.
sent2: They call this extraction process event threading.
sent3: They represent each event as a cluster of news articles.
sent4: Event threading is a supervised method that consists of two phases: clustering documents and modeling dependencies.
sent5: The clustering process starts with a cluster for each document in the data set and merges them iteratively based on similarity until the similarities fall below a predefined threshold.
sent6: The authors evaluate three types of cluster similarity on the average link, complete link, or single link of the clusters based on document similarities.
sent7: Document similarities are based on content similarity (e.g., cosine similarity), common locations, and common entities.
sent8: Furthermore, there is an exponential decay term based on the temporal distance to penalize larger temporal distances between documents.
sent9: Next, dependency modeling uses surface-level features of the document clusters, such as word distributions and time-ordering of the news articles.
sent10: Based on this information, the authors propose several link extraction criteria (complete-link, simple threshold, nearest parent, best similarity, and maximum spanning tree).
sent11: These approaches rely on temporal order, similarity information, or structural information.
sent12: SToRe (Storyline-based Topic Retrospection) is a topic retrospective system [63][64][65] that extracts the main storyline from a given news topic and provides a summary of the topic based on this storyline.
sent13: In particular, the extraction process consists of four phases: event identification, topic structure identification, main storyline construction, and storyline-based summarization.
sent14: In the event identification phase, similar news articles will be clustered together to represent a single event using self-organizing maps.
sent15: In the topic structure identification step, the events are linked together based on whether their similarity exceeds a specific threshold.
sent16: To compute similarity, the events are represented with a vector of term weights using the concepts of genus and differentia words [112].
sent17: Then, cosine similarity is used to compare the event vectors.
sent18: Next, in the main storyline construction step, an MST is extracted from the constructed topic structure.
sent19: The MST is based on the relevance of each event with respect to the topic.
sent20: The MST is used to generate a timeline of events, and it is further extended with small side branches of other relevant events based on a specific threshold.
sent21: Finally, in the storyline-based summarization, a summary is generated for each event based on the news articles contained in its cluster using accumulated weight summary [39].
sent22: Manuscript submitted to ACM Yang et al. [126,127] use directed acyclic graphs to represent the evolution of events in online news.
sent23: They call their approach event evolution graphs, which represent temporal and causal relationships between events.
sent24: Events are defined as sets of news articles and are represented as the average of the TF-IDF vectors of each article they contain.
sent25: We note that the proposed method assumes that events and their corresponding articles are already computed.
sent26: In practice, this would require a clustering step before constructing the graph.
sent27: These events are linked together based on their similarity and a user-specified threshold, which is computed based on content similarity (e.g., cosine similarity), temporal proximity, and document distributional proximity (which penalizes bursty periods with many articles about the same event).
sent28: The latter two terms are represented through exponential decay factors.
sent29: Furthermore, users are able to reduce the temporal granularity of the event evolution graph, which merges specific events that occur in short time frames.
sent30: Qiu et al. [87] propose another event evolution graph extraction method.
sent31: Their construction method follows an iterative approach based on content similarity and temporal order.
sent32: In particular, documents are first grouped into clusters using the OHC method [88] in the first time period, which gives rise to the initial events.
sent33: Next, the PRAC method [89] is used to build classifiers and determine whether the documents of the next time period are continuations of a cluster identified in the previous period.
sent34: If so, a new event node is created using the identified cluster as its parent.
sent35: This process is repeated until the last time period.
sent36: Next, twigs-paths that die before the end of the timeline-are removed based on a user-set tolerance, and equivalent event nodes are merged to reduce graph complexity.
sent37: TSCAN (Topic Summarization and Content ANatomy) [20,21] is a method to analyze news data that produces a global summary and constructs an event evolution graph.
sent38: We focus on the event graph component of this method.
sent39: First, news articles are grouped into themes obtained through a matrix factorization approach with TF-IDF document representations.
sent40: Next, the news articles of each theme are temporally segmented using an energy value threshold based on eigenvalues from the matrix representation.
sent41: In practice, this generates clusters of documents based on frequency, which are associated with the nodes of the event evolution graph.
sent42: The evolution graph is a directed acyclic graph,where the edges are constructed using temporal similarity-computed using the temporal distance between events, with special cases to consider event overlap-and content similarity-based on cosine similarity.
sent43: Khurdiya et al. [52] propose a system that extracts directed graphs to represent stories from news data using multiperspective links.
sent44: Each node of this graph is associated with multiple news articles.
sent45: The system uses LDA to extract topics in each time unit (e.g., a day).
sent46: The extracted topics are associated with sets of articles based on the strength of the topic in each article and form the basis of the story identification model.
sent47: We note that these topics and their article sets correspond to the notion of event that we use in this survey.
sent48: Next, article sets are linked chronologically based on topic correlation (e.g., Pearson's correlation coefficient) and a user-defined threshold, generating a directed graph of events.
sent49: Wei et al. [117] identify event episodes in news data sets and construct a temporal episode graph (i.e., an event graph under our definitions in the survey).
sent50: In particular, this article shows a discovery mechanism that organizes news documents into events using novel TF-IDF representations that incorporate a temporal component.
sent51: Then, the system builds a link structure based on intercluster similarity measures.
sent52: The first proposed event representation, called TF-IDF Tempo , gives more weight to features with consecutive occurrences in a sequence of documents (i.e., it incorporates the surrounding context of the document) by modifying the IDF component of TF-IDF to consider the order of the documents.
sent53: However, this approach is too strict and is unable to model overlapping events.
sent54: Moreover, it also has a high bias towards low-frequency articles that are temporally close.
sent55: Thus, the authors propose a second representation, called TF-Enhanced-IDF Tempo which modifies the IDF component by adopting the significance factor proposed by Luhn [68] and a temporal gap threshold to allow for short discontinuities in feature appearances.
sent56: These representations are used with Hierarchical Agglomerative Clustering (HAC) [114] to construct the article clusters that represent the events.
sent57: For the purposes of clustering, document similarity is defined by content similarity (e.g., cosine similarity) and a negative exponential penalty for temporally distant documents.
sent58: Huang et al. [48] propose a different event evolution approach to build and analyze event relationships based on three types of event connections.
sent59: In particular, they define a co-occurrence dependence relationship, an event reference relationship, and a temporal proximity relationship.
sent60: The authors define events as a set of news articles and identify them through clustering and topic modeling using a combined similarity measure that leverages LDA and a TF-IDF document model with cosine similarity.
sent61: Once the events are identified, the method extracts a series of core features (i.e., key entities and terms of the article) by analyzing the lead of the articles and evaluating whether their frequency is above a specified threshold.
sent62: These core features are used to construct a vectorial representation of the events.
sent63: For the co-occurrence relationship, the method computes the aggregation of all mutual information between all features of the event, generating a symmetric matrix that represents all event-event relationships.
sent64: For the event reference analysis, the method identifies shared core features and defines the degree of event reference based on the frequency of references in an event to the core features of a previous event, adjusted by the weight of these terms in the referencing event.
sent65: Temporal dependency is evaluated using an exponential decay formula.
sent66: Event Phase Oriented News Summarization (EPONS) [116] is a timeline summarization approach that assumes that a story summary contains multiple timelines, each one corresponding to a specific event.
sent67: To model the semantic relations of news articles, EPONS uses a graph model, called Temporal Content Coherence Graph (TCCG), which is an event graph based on two metrics: content coherence and temporal influence.
sent68: Content coherence is based on the weighted average of topic level similarity-modeled by JS divergence over an LDA topic distribution-and entity-level similarity-modeled over a ranking of named entities using the Tanimoto coefficient.
sent69: Temporal influence is modeled through a Hamming (cosine) kernel to properly separate temporally distinct events.
sent70: The TCCG is built by selecting edges that are above user-specified thresholds in each metric.
sent71: Based on this graph, EPONS uses a modified structural clustering approach to group the news articles into different events.
sent72: Furthermore, small clusters of similar articles are filtered out to ensure that the events are modeled properly.
sent73: This post-processing is done by using four quality metrics on a pre-trained logistic regression classifier: percentage of new articles, time interval length, pairwise topic similarity, and pairwise entity similarity.
sent74: Having identified the events, it is now necessary to construct the individual summaries and finalize the timeline.
sent75: To do so, a vertex-reinforced random walk [70,84] is used to rank the relevance of news articles inside each event, in a similar manner to PageRank.
sent76: Next, a supervised model is used to determine whether the headlines are factual (i.e., they are reporting a specific event) or an opinion, as opinion-based headlines are not considered useful for timelines and must be filtered out.
sent77: Finally, an optimization method is used to maximize the total relevance, subject to non-redundancy constraints (i.e., disallowing events that are too similar) to select the news articles.
sent78: Cai et al. [17] propose a method to extract Temporal Event Maps (TEM) based on the content dependence degree and component event reference degree for each pair of events.
sent79: TEMs are directed graphs that have events as nodes, relations as edges, edge weights representing the strength of event relationships, and node weights representing the importance of each event.
sent80: Events are defined as groups of related documents and identified using a LDA model.
sent81: After obtaining the events, the next step is to compute the two core metrics that define the temporal event maps.
sent82: The content dependence degree is defined as the aggregation of all mutual information among the features of each event.
sent83: The content reference degree is defined by the presence of core features of an event-salient terms based on frequency-in other events.
sent84: Unlike content dependence, this is not a symmetric relationship between events.
sent85: To construct the temporal event maps, the first step is to order events based on starting time.
sent86: Then, connections are added for events that surpass a user-specified Manuscript submitted to ACM threshold for the product of content dependence and event reference degrees, which provides the edge weights for the graph.
sent87: Finally, a ranking procedure based on PageRank is used to generate the event importance values.
sent88: co-occurrence graphs. To extract the maps, an optimization problem is defined based on finding the best structure for the map, relying on the idea of minimizing the total number of storylines (to reduce unneeded complexity) and maximizing the number of covered clusters (to ensure that the stories are well covered).
sent89: This approach leads to simple stories being modeled as a single metro line and more complex stories requiring the use of multiple shorter lines.
sent90: Furthermore, a series of additional constraints for story coherence, cluster quality, and map size is imposed."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s26,NARRATIVE EXTRACTION CRITERIA,"In this section, we present a summary of the different construction criteria found in the reviewed articles. These criteria refer to either an evaluation metric or additional information used in the extraction algorithms themselves as part of an objective function (e.g., coherence optimization), selection criteria (e.g., filtering based on content similarity or topic distribution similarity), and other types of extraction heuristics (e.g., leveraging article structure to compute content similarity or evaluating the use of opinionated language). The first part of Table 2 provides an overview of the different construction criteria. We note that these criteria are not mutually exclusive and can be combined as needed.

Relevance Relevance metrics evaluate whether the events in the narrative are relevant or significant to a given query or topic [58,60,77,108,124,125]. In general, relevance is measured by borrowing techniques from traditional search methods in Information Retrieval, such as PageRank and its variations [109,116,135,136]. However, some approaches use supervised methods to learn a ranking function [10,110]. The results from such techniques are used to feed other parts of the algorithm or could be directly used to select relevant events, turning this issue into more of a traditional information retrieval problem rather than a narratological one.

Content Similarity Another approach to extracting narratives is based on modeling content similarity between events. Over two-thirds of the methods use some sort of content similarity measure. There are many ways to do this, in particular, we found the following approaches: surface-level similarity comparisons (e.g., Jaccard similarity or cosine similarity) [46,77,112], topic similarity based on topic distribution information (e.g., comparing topic vectors extracted from LDA models) [23,60], and entity-based comparisons (e.g., entity co-occurrence in events) [76,136].

The exact choice of approach is highly dependent on the event representation. In recent years, researchers have started leveraging advances in text representation with neural embeddings (e.g., BERT) [28,51,61,108,130], which have several advantages over traditional frequency-based models and are better able to capture semantic similarities.

The use of entity-based information in event-based narrative extraction methods to measure event content similarity remains limited in scope, with sparse usage over the years compared to other content similarity measures [14,18,46,47,76,116,136]. Combining entity information with other types of similarities would provide a much more holistic view of content similarity. Furthermore, expanding upon this approach, content similarity metrics could exploit the main event descriptors [50] to compute a more precise similarity measure.

Coherence Coherence metrics evaluate whether the narrative makes sense. Due to their importance as an extraction metric, we show some mathematical formulations of coherence and coherence-like metrics in Table 3.

While coherence has a formal definition in narratological terms [1], it is just as complex and ill-defined as relevance in computational terms. One particular motivation for the definition of coherence that stands out is the idea of smoothness from the Connect the Dots [96,97,100,101] series of works. In particular, they use the concept of word influence and word activations (i.e., the sustained importance of the word in a storyline) to construct stories that have smooth transitions.

Other approaches compute coherence based on content similarity. These works also seek to generate smooth stories by avoiding drastic local changes based on content similarity [18,51,60,66,67,110,123,125,[133][134][135][136], without explicitly defining active words or topics like the original Connect the Dots approach. Finally, one approach also considers coherence around the idea of causality [10,110] in a supervised setting (e.g., causal signals in text).

Coverage-like Metrics Coverage-like metrics evaluate whether the extracted narrative properly covers the relevant events, stories, or topics. These metrics include coverage itself and related metrics, such as redundancy and diversity. The most basic form of coverage is simply the percentage of topics or relevant events covered by the extracted representation Manuscript submitted to ACM  [51] × × × × × Table 2. Summary of the extraction criteria and evaluation metrics used in the reviewed articles.

(or some variation of this metric) [28,40,51,131], or a probability estimation [98][99][100][101]123]. Equation 1 shows an example formulation of coverage for a cluster , where Π represents an extracted narrative with storylines .

Events as Clusters This is a measure of coherence based on average Jaccard Similarity along a story based on cluster words.

[123]

Events as Clusters This is a measure of coherence based on average Cosine Similarity along a story based on cluster centroids. Events as Documents This is the full form of the coherence for a storyline from the original Connect the Dots algorithm. It is based on maximizing the sum of word influences over active words in the storyline. Influence can be changed for any other type of scoring mechanism.

[96]

Events as Documents This is a measure of coherence based on the minimum Cosine Similarity along a story based on document vectors.

Events as Documents This is a measure of coherence for a narrative based on the minimum geometric mean of Surface-level similarity (e.g., cosine similarity) and Topic-level similarity (e.g., Jensen-Shannon divergence). It is based on document vectors and topic distribution vectors.. [51] ,

Events as Documents This is a measure of incoherence rather than coherence. It is based on the average Soergel distance and includes a temporal distance term as well. The events are weighted by their relevance ( and ) and their temporal distance using a custom kernel Φ. ∑︁ ∈ Count match( , ) (gram ) Count Previous( ) (gram )

Events as Sentences This is a measure of coherence based on the n-gram overlap between the current event sentences and the sentences of the previous summary of the timeline.

[110]

Events as Sentences This is a measure of coherence based on the Jensen-Shannon divergence between the current event sentence and the previous event sentence of the timeline.

[60]

Events as Sentences This is a measure of coherence based on Kullback-Leibler divergence between the current event at time and all the other local events in a Δ time window surrounding the event. The events are weighted by their temporal distance based on parameter .

[58] Table 3. A sample of different formulations of coherence from the reviewed articles.

Another approach to compute coverage is to do a content similarity comparison between the output and the full data set (or a relevant subset) [58,60,125]. In contrast, redundancy and diversity [28,116,125,135,136] metrics are based on the idea that events should not be covered more than necessary, thus high redundancy can lead to coverage problems.

Structural Information Some works evaluate the structure of the output narrative representation. In particular, these metrics consider aspects such as size (in general) or connectivity (in graph-based narratives).

Size can be used as a proxy for complexity (e.g., length of the timeline) [123]. In most cases, rather than as an evaluation metric, size is used as a constraint (e.g., setting a maximum story length) [98-101, 131, 133, 134].

Connectivity metrics [98][99][100][101] are used to ensure that narrative graphs avoid isolated stories, as they should be interwoven throughout the narrative. Structure metrics are mostly analyzed at a global level (e.g., the total number of connected stories). However, it is possible to consider local structural features, such as node degrees [106].

Exploiting the internal article structure [14,108,110] is another piece of structural information used by some methods. Most breaking news articles are written following the inverted pyramid structure [50], where the most important information-the main event descriptors-is shown first in the lead. Thus, the first few lines of an article describe its main event [110] and subsequent paragraphs may contain more details and reference previous events.","[[], ['b115', 'b59', 'b57', 'b124', 'b76', 'b123', 'b107', 'b9', 'b109', 'b134', 'b108', 'b135'], ['b111', 'b59', 'b22', 'b45', 'b76', 'b75', 'b135'], ['b27', 'b50', 'b129', 'b60', 'b107'], ['b115', 'b17', 'b49', 'b45', 'b46', 'b75', 'b13', 'b135'], [], ['b95', 'b96', 'b99', 'b0', 'b100'], ['b132', 'b50', 'b59', 'b17', 'b124', 'b133', 'b66', 'b9', 'b109', 'b65', 'b134', 'b122', 'b135'], ['b50'], ['b27', 'b50', 'b99', 'b97', 'b39', 'b130', 'b98', 'b122', 'b100'], [], [], [], [], [], ['b50'], [], [], [], [], [], [], [], ['b27', 'b115', 'b59', 'b124', 'b57', 'b134', 'b135'], [], [None, 'b122'], ['b105', 'b99', 'b97', 'b98', 'b100'], ['b49', 'b107', 'b109', 'b13']]","[[], ['b115', 'b59', 'b57', 'b124', 'b76', 'b123', 'b107', 'b9', 'b109', 'b134', 'b108', 'b135'], ['b111', 'b59', 'b22', 'b45', 'b76', 'b75', 'b135'], ['b27', 'b50', 'b129', 'b60', 'b107'], ['b115', 'b17', 'b49', 'b45', 'b46', 'b75', 'b13', 'b135'], [], ['b95', 'b96', 'b99', 'b0', 'b100'], ['b132', 'b50', 'b59', 'b17', 'b124', 'b133', 'b66', 'b9', 'b109', 'b65', 'b134', 'b122', 'b135'], ['b50'], ['b27', 'b50', 'b99', 'b97', 'b39', 'b130', 'b98', 'b122', 'b100'], [], [], [], [], [], ['b50'], [], [], [], [], [], [], [], ['b27', 'b115', 'b59', 'b124', 'b57', 'b134', 'b135'], [], [None, 'b122'], ['b105', 'b99', 'b97', 'b98', 'b100'], ['b49', 'b107', 'b109', 'b13']]",79,"sent1: In this section, we present a summary of the different construction criteria found in the reviewed articles.
sent2: These criteria refer to either an evaluation metric or additional information used in the extraction algorithms themselves as part of an objective function (e.g., coherence optimization), selection criteria (e.g., filtering based on content similarity or topic distribution similarity), and other types of extraction heuristics (e.g., leveraging article structure to compute content similarity or evaluating the use of opinionated language).
sent3: The first part of Table 2 provides an overview of the different construction criteria.
sent4: We note that these criteria are not mutually exclusive and can be combined as needed.
sent5: Relevance Relevance metrics evaluate whether the events in the narrative are relevant or significant to a given query or topic [58,60,77,108,124,125].
sent6: In general, relevance is measured by borrowing techniques from traditional search methods in Information Retrieval, such as PageRank and its variations [109,116,135,136].
sent7: However, some approaches use supervised methods to learn a ranking function [10,110].
sent8: The results from such techniques are used to feed other parts of the algorithm or could be directly used to select relevant events, turning this issue into more of a traditional information retrieval problem rather than a narratological one.
sent9: Content Similarity Another approach to extracting narratives is based on modeling content similarity between events.
sent10: Over two-thirds of the methods use some sort of content similarity measure.
sent11: There are many ways to do this, in particular, we found the following approaches: surface-level similarity comparisons (e.g., Jaccard similarity or cosine similarity) [46,77,112], topic similarity based on topic distribution information (e.g., comparing topic vectors extracted from LDA models)
sent12: [23,60], and entity-based comparisons (e.g., entity co-occurrence in events) [76,136].
sent13: The exact choice of approach is highly dependent on the event representation.
sent14: In recent years, researchers have started leveraging advances in text representation with neural embeddings (e.g., BERT) [28,51,61,108,130], which have several advantages over traditional frequency-based models and are better able to capture semantic similarities.
sent15: The use of entity-based information in event-based narrative extraction methods to measure event content similarity remains limited in scope, with sparse usage over the years compared to other content similarity measures [14,18,46,47,76,116,136].
sent16: Combining entity information with other types of similarities would provide a much more holistic view of content similarity.
sent17: Furthermore, expanding upon this approach, content similarity metrics could exploit the main event descriptors [50] to compute a more precise similarity measure.
sent18: Coherence Coherence metrics evaluate whether the narrative makes sense.
sent19: Due to their importance as an extraction metric, we show some mathematical formulations of coherence and coherence-like metrics in Table 3.
sent20: While coherence has a formal definition in narratological terms [1], it is just as complex and ill-defined as relevance in computational terms.
sent21: One particular motivation for the definition of coherence that stands out is the idea of smoothness from the Connect the Dots [96,97,100,101] series of works.
sent22: In particular, they use the concept of word influence and word activations (i.e., the sustained importance of the word in a storyline) to construct stories that have smooth transitions.
sent23: Other approaches compute coherence based on content similarity.
sent24: These works also seek to generate smooth stories by avoiding drastic local changes based on content similarity [18,51,60,66,67,110,123,125,[133][134][135][136], without explicitly defining active words or topics like the original Connect the Dots approach.
sent25: Finally, one approach also considers coherence around the idea of causality [10,110] in a supervised setting (e.g., causal signals in text).Coverage-like Metrics Coverage-like metrics evaluate whether the extracted narrative properly covers the relevant events, stories, or topics.
sent26: These metrics include coverage itself and related metrics, such as redundancy and diversity.
sent27: The most basic form of coverage is simply the percentage of topics or relevant events covered by the extracted representation Manuscript submitted to ACM
sent28: [51] × × × × × Table 2. Summary of the extraction criteria and evaluation metrics used in the reviewed articles.
sent29: (or some variation of this metric) [28,40,51,131], or a probability estimation [98][99][100][101]123].
sent30: Equation 1 shows an example formulation of coverage for a cluster , where Π represents an extracted narrative with storylines .
sent31: Events as Clusters This is a measure of coherence based on average Jaccard Similarity along a story based on cluster words.
sent32: [123]Events as Clusters This is a measure of coherence based on average Cosine Similarity along a story based on cluster centroids.
sent33: Events as Documents This is the full form of the coherence for a storyline from the original Connect the Dots algorithm.
sent34: It is based on maximizing the sum of word influences over active words in the storyline.
sent35: Influence can be changed for any other type of scoring mechanism.
sent36: [96]Events as Documents This is a measure of coherence based on the minimum Cosine Similarity along a story based on document vectors.
sent37: Events as Documents This is a measure of coherence for a narrative based on the minimum geometric mean of Surface-level similarity (e.g., cosine similarity) and Topic-level similarity (e.g., Jensen-Shannon divergence).
sent38: It is based on document vectors and topic distribution vectors..
sent39: [51] ,Events as Documents This is a measure of incoherence rather than coherence.
sent40: It is based on the average Soergel distance and includes a temporal distance term as well.
sent41: The events are weighted by their relevance ( and ) and their temporal distance using a custom kernel Φ. ∑︁ ∈ Count match( , ) (gram ) Count Previous( ) (gram )Events as Sentences This is a measure of coherence based on the n-gram overlap between the current event sentences and the sentences of the previous summary of the timeline.
sent42: [110]Events as Sentences This is a measure of coherence based on the Jensen-Shannon divergence between the current event sentence and the previous event sentence of the timeline.
sent43: [60]Events as Sentences This is a measure of coherence based on Kullback-Leibler divergence between the current event at time and all the other local events in a Δ time window surrounding the event.
sent44: The events are weighted by their temporal distance based on parameter .
sent45: [58] Table 3. A sample of different formulations of coherence from the reviewed articles.
sent46: Another approach to compute coverage is to do a content similarity comparison between the output and the full data set (or a relevant subset) [58,60,125].
sent47: In contrast, redundancy and diversity [28,116,125,135,136] metrics are based on the idea that events should not be covered more than necessary, thus high redundancy can lead to coverage problems.
sent48: Structural Information Some works evaluate the structure of the output narrative representation.
sent49: In particular, these metrics consider aspects such as size (in general) or connectivity (in graph-based narratives).
sent50: Size can be used as a proxy for complexity (e.g., length of the timeline) [123].
sent51: In most cases, rather than as an evaluation metric, size is used as a constraint (e.g., setting a maximum story length) [98-101, 131, 133, 134].Connectivity metrics [98][99][100][101] are used to ensure that narrative graphs avoid isolated stories, as they should be interwoven throughout the narrative.
sent52: Structure metrics are mostly analyzed at a global level (e.g., the total number of connected stories).
sent53: However, it is possible to consider local structural features, such as node degrees [106].
sent54: Exploiting the internal article structure [14,108,110] is another piece of structural information used by some methods.
sent55: Most breaking news articles are written following the inverted pyramid structure [50], where the most important information-the main event descriptors-is shown first in the lead.
sent56: Thus, the first few lines of an article describe its main event [110] and subsequent paragraphs may contain more details and reference previous events."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s27,Content References,"Another criterion to consider in news narrative extraction is the use of content references. As mentioned before, some news articles make explicit references to previous works in their body. Note that this differs from explicit date-based references discussed before, which rely on explicit temporal information. This approach also differs from general content similarity because of its goal of identifying specific references rather than global similarity.

Manuscript submitted to ACM One way to identify these references is to compare the lead of a news article with the additional information paragraphs of another article [108]. Other approaches identify references based on sentence co-occurrence without considering article structure [130]. Alternatively, a set of core features [17,48] (e.g., relevant keywords or main event descriptors) could be identified and used to detect references in other articles. Once identified, these references can be used to identify relevant events based on reference-based metrics (e.g., bibliographic coupling).

Temporal Features Temporal information, such as the temporal distance between events or specific date references, has been used. In particular, temporal distance is commonly used to penalize events that would otherwise be similar in content. For example, consider two articles describing separate protests in a city, one during the year 2000 and another

in the year 2010. These two articles would likely be very similar in terms of content, including both surface-level features and topic distributions. However, given the temporal separation between them, they would likely refer to different events. Thus, a common strategy is to define an exponentially decreasing term of the form 0 exp −Δ (or similar), where 0 and are pre-defined constants [47,48,66,67,76,117,124,126,127,130], although there are other approaches, such as kernels to perform temporal proximity projections [116,124] or overlap-based measures [20,21].

However, we note that the use of a temporal penalty is not always desired. Some events are continuations of stories that did not have anything new to report for a long time. For example, the investigation results of a flight accident might come much after the accident itself has been covered, leading to temporal gaps in story coverage [56,117]. Thus, it is necessary to distinguish between continuations and completely new storylines when the time gap is high enough.

Burstiness and frequency measures and metrics based on these (e.g., energy values) are other time-based criteria used to identify relevant events and dates [10,20,21,23,49,56,77,108,109,117,124,127,130]. For example, periods with many publications are likely to contain important events. Alternatively, a specific event might be reported several times by different outlets. Finally, other temporal features include the use of specific temporal expressions or date references in the text [10,14,55,61,106,109,110] to identify temporal cross-references between documents.","[[], ['b47', 'b107', 'b16', 'b129'], [], ['b115', 'b125', 'b126', 'b129', 'b20', 'b47', 'b116', 'b66', 'b46', 'b123', 'b65', 'b75', 'b19'], ['b116', 'b55'], ['b105', 'b126', 'b22', 'b20', 'b55', 'b76', 'b116', 'b129', 'b60', 'b123', 'b54', 'b107', 'b9', 'b109', 'b48', 'b108', 'b13', 'b19']]","[[], ['b47', 'b107', 'b16', 'b129'], [], ['b115', 'b125', 'b126', 'b129', 'b20', 'b47', 'b116', 'b66', 'b46', 'b123', 'b65', 'b75', 'b19'], ['b116', 'b55'], ['b105', 'b126', 'b22', 'b20', 'b55', 'b76', 'b116', 'b129', 'b60', 'b123', 'b54', 'b107', 'b9', 'b109', 'b48', 'b108', 'b13', 'b19']]",37,"sent1: Another criterion to consider in news narrative extraction is the use of content references.
sent2: As mentioned before, some news articles make explicit references to previous works in their body.
sent3: Note that this differs from explicit date-based references discussed before, which rely on explicit temporal information.
sent4: This approach also differs from general content similarity because of its goal of identifying specific references rather than global similarity.
sent5: Manuscript submitted to ACM One way to identify these references is to compare the lead of a news article with the additional information paragraphs of another article [108].
sent6: Other approaches identify references based on sentence co-occurrence without considering article structure [130].
sent7: Alternatively, a set of core features [17,48] (e.g., relevant keywords or main event descriptors) could be identified and used to detect references in other articles.
sent8: Once identified, these references can be used to identify relevant events based on reference-based metrics (e.g., bibliographic coupling).
sent9: Temporal Features Temporal information, such as the temporal distance between events or specific date references, has been used.
sent10: In particular, temporal distance is commonly used to penalize events that would otherwise be similar in content.
sent11: For example, consider two articles describing separate protests in a city, one during the year 2000 and anotherin the year 2010.
sent12: These two articles would likely be very similar in terms of content, including both surface-level features and topic distributions.
sent13: However, given the temporal separation between them, they would likely refer to different events.
sent14: Thus, a common strategy is to define an exponentially decreasing term of the form 0 exp −Δ (or similar), where 0 and are pre-defined constants [47,48,66,67,76,117,124,126,127,130], although there are other approaches, such as kernels to perform temporal proximity projections [116,124] or overlap-based measures [20,21].
sent15: However, we note that the use of a temporal penalty is not always desired.
sent16: Some events are continuations of stories that did not have anything new to report for a long time.
sent17: For example, the investigation results of a flight accident might come much after the accident itself has been covered, leading to temporal gaps in story coverage [56,117].
sent18: Thus, it is necessary to distinguish between continuations and completely new storylines when the time gap is high enough.
sent19: Burstiness and frequency measures and metrics based on these (e.g., energy values) are other time-based criteria used to identify relevant events and dates [10,20,21,23,49,56,77,108,109,117,124,127,130].
sent20: For example, periods with many publications are likely to contain important events.
sent21: Alternatively, a specific event might be reported several times by different outlets.
sent22: Finally, other temporal features include the use of specific temporal expressions or date references in the text [10,14,55,61,106,109,110] to identify temporal cross-references between documents."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s13,Event Resolution,"Narrative Structure ""The first debate between President Obama and Mitt Romney, so long anticipated, quickly sunk into an unenlightening recitation of tired talking points and mendacity."" ""Mr. Romney wants to restore the Bush-era tax cut that expires at the end of this year and largely benefits the wealthy"" ""BP's shares fall 2% amid fears that the cost of cleanup and legal claims will hit the London-based company hard""

""Aides say Clinton is angered as Gore Tries to break away"" most graph-based works are designed to represent multiple storylines, due to their inherent more complex nature compared to timelines. However, there are some works that represent a single story, but provide extra information by exploiting graph structures. For example, appending additional nodes with related events to the central story [65].

Type of approach represents whether the method is supervised-which requires training data-or unsupervisedwhich does not require training data. In general, we considered any method where the authors had to train the model with labeled data before using it as supervised. However, some approaches only did this to find the optimal value of a small set of hyperparameters [60,76,125] and it could be possible to use them in an unsupervised manner, provided that those hyperparameters were fixed in some other way (e.g., heuristics or previous work information).

Finally, event representation provides information about the computational representation of the events. Note that this is separate from the resolution level of the event. In general, we found four types of representations: word frequency models (e.g., TF-IDF and Bag of Words vectors), topic distribution models (e.g., Latent Dirichlet Allocation (LDA) vectors), neural embeddings (e.g., BERT), and entity-based models (e.g., entity frequency vectors). Some works combine these approaches and have a mixed event representation that leverages all these elements in some way to extract the final narrative model. There are some works that did not fit in any of these approaches and were marked as ""Other"".","[[], ['b64'], ['b59', 'b124', 'b75'], []]","[[], ['b64'], ['b59', 'b124', 'b75'], []]",4,"sent1: Narrative Structure ""The first debate between President Obama and Mitt Romney, so long anticipated, quickly sunk into an unenlightening recitation of tired talking points and mendacity.""
sent2: ""Mr. Romney wants to restore the Bush-era tax cut that expires at the end of this year and largely benefits the wealthy"" ""BP's shares fall 2% amid fears that the cost of cleanup and legal claims will hit the London-based company hard""""Aides say Clinton is angered as Gore Tries to break away"" most graph-based works are designed to represent multiple storylines, due to their inherent more complex nature compared to timelines.
sent3: However, there are some works that represent a single story, but provide extra information by exploiting graph structures.
sent4: For example, appending additional nodes with related events to the central story [65].Type of approach represents whether the method is supervised-which requires training data-or unsupervisedwhich does not require training data.
sent5: In general, we considered any method where the authors had to train the model with labeled data before using it as supervised.
sent6: However, some approaches only did this to find the optimal value of a small set of hyperparameters [60,76,125]
sent7: and it could be possible to use them in an unsupervised manner, provided that those hyperparameters were fixed in some other way (e.g., heuristics or previous work information).
sent8: Finally, event representation provides information about the computational representation of the events.
sent9: Note that this is separate from the resolution level of the event.
sent10: In general, we found four types of representations: word frequency models (e.g., TF-IDF and Bag of Words vectors), topic distribution models (e.g., Latent Dirichlet Allocation (LDA) vectors), neural embeddings (e.g., BERT), and entity-based models (e.g., entity frequency vectors).
sent11: Some works combine these approaches and have a mixed event representation that leverages all these elements in some way to extract the final narrative model.
sent12: There are some works that did not fit in any of these approaches and were marked as ""Other""."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s32,Unsupervised Metrics.,"We now discuss unsupervised approaches. In general, there are far fewer works relying on unsupervised metrics to evaluate the final narrative output.

Coherence In general, coherence is not used to evaluate the output narrative despite being a useful metric during the extraction process. One exception is Xu et al. [123], who evaluate their output using a weighted average of story coherence (based on Jaccard similarity) and story size.

Coverage Xu et al. [123] evaluate their output by treating coverage as a structural measure, making the assumption that good coverage of topics means that the structure of their metro map representation is good. However, due to Manuscript submitted to ACM the formulation of coverage based on whether the topical clusters of the data set are covered, it does not explicitly consider the structure of the output, which makes it inappropriate to evaluate the structure. In contrast, Bögel et al. [14] use a notion of coverage based on event connections in a graph (i.e., an article is covered if there is at least one edge connecting it) that could be treated more as a structural measure than the topical concept of coverage.

Dispersion Camacho et al. [18] use the dispersion coefficient-originally proposed as an evaluation metric for storytelling in the intelligence analysis domain [45]-to evaluate their storyline. In particular, the dispersion coefficient is based on the Soergel distance, although other distance metrics could be used [92]. In particular, dispersion is based on Swanson's complementary but disjoint hypothesis [104]-where articles that have no explicit common elements yield important inferences or insights when combined. These insights are not apparent from the separate documents.

Furthermore, the authors propose a new evaluation metric to measure story flow based on Swanson's hypothesis called the dispersion coefficient, shown in Equation 3. We note that this particular version is based on the Soergel distance ( ), but any other distance metric between documents could be used in practice.

Diversity and Redundancy Finally, another alternative is a diversity metric to ensure proper coverage or low redundancy. In particular, Duan et al. [28] used diversity-based on the average pairwise similarity of sentences (see Equation 4-to evaluate the performance of their comparative timeline extraction method.","[[], ['b122'], ['b13', 'b122'], ['b103', 'b91', 'b17', 'b44'], ['b2'], ['b27', None]]","[[], ['b122'], ['b13', 'b122'], ['b103', 'b91', 'b17', 'b44'], ['b2'], ['b27', None]]",10,"sent1: We now discuss unsupervised approaches.
sent2: In general, there are far fewer works relying on unsupervised metrics to evaluate the final narrative output.
sent3: Coherence In general, coherence is not used to evaluate the output narrative despite being a useful metric during the extraction process.
sent4: One exception is Xu et al. [123], who evaluate their output using a weighted average of story coherence (based on Jaccard similarity) and story size.
sent5: Coverage Xu et al. [123] evaluate their output by treating coverage as a structural measure, making the assumption that good coverage of topics means that the structure of their metro map representation is good.
sent6: However, due to Manuscript submitted to ACM the formulation of coverage based on whether the topical clusters of the data set are covered, it does not explicitly consider the structure of the output, which makes it inappropriate to evaluate the structure.
sent7: In contrast, Bögel et al. [14] use a notion of coverage based on event connections in a graph (i.e., an article is covered if there is at least one edge connecting it)
sent8: that could be treated more as a structural measure than the topical concept of coverage.
sent9: Dispersion Camacho et al. [18] use the dispersion coefficient-originally proposed as an evaluation metric for storytelling in the intelligence analysis domain [45]-to evaluate their storyline.
sent10: In particular, the dispersion coefficient is based on the Soergel distance, although other distance metrics could be used [92].
sent11: In particular, dispersion is based on Swanson's complementary but disjoint hypothesis [104]-where articles that have no explicit common elements yield important inferences or insights when combined.
sent12: These insights are not apparent from the separate documents.
sent13: Furthermore, the authors propose a new evaluation metric to measure story flow based on Swanson's hypothesis called the dispersion coefficient, shown in Equation 3.
sent14: We note that this particular version is based on the Soergel distance ( ), but any other distance metric between documents could be used in practice.
sent15: Diversity and Redundancy Finally, another alternative is a diversity metric to ensure proper coverage or low redundancy.
sent16: In particular, Duan et al. [28] used diversity-based on the average pairwise similarity of sentences (see Equation 4-to evaluate the performance of their comparative timeline extraction method."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s15,Query-based Approaches.,"These approaches perform an information retrieval step before or during the narrative extraction process based on a user-defined query. In some cases, the query just acts as a simple filter, in others, they explicitly include the query into the narrative extraction model.

Chieu and Lee [24] present a query-based timeline extraction approach where each event is represented as a sentence.

This is the earliest form of the ""events as sentences"" that we could find in the literature. Sentences are first filtered based on the query and then ranked according to two criteria: interest-based on the frequency of the reported event in the query-and burstiness-based on the idea that important events form clusters around their date of occurrence.

To determine whether two sentences are reporting the same event, the authors use cosine similarity. Furthermore, interest is determined based on a time window to avoid combining events that should be separated due to their temporal distance. To reduce redundancy, duplicated sentences are removed based on a time window around an important event that depends on the interest value.

Yan et al. [125] proposed a timeline summarization method based on balanced optimization and iterative substitution of sentences. Their optimization problem is defined in terms of relevance, coverage, coherence, and diversity. All these terms are based on the Kullback-Leibler divergence (KLD) [53] of the summary items with a target distribution. Relevance is related to a user-defined query and is defined as the KLD between the summary items and the internal representation of the query. Coverage is based on a global term-KLD between the summary items and the whole corpus-and a local term-KLD between the summary items and the set of sentences from the same date. Coherence is defined locally, based on the KLD between each summary item and its neighboring summaries by using an exponential temporal decay term (i.e., consecutive dates should have relatively similar summaries). Diversity is measured across dates and measures the average KLD of each sentence with respect to all other sentences in a leave-one-out manner. The final utility function is a weighted average of these terms with user-defined weights and can be defined at a local level (to evaluate individual time periods) and a global level (to evaluate the full timeline). To find the sentences, this utility function is optimized in an iterative manner by replacing sentences in the date summaries and improving the utility value in each step using a dynamic programming algorithm that considers both local and global constraints.

Li et al. [58] propose a topic modeling approach for timeline extraction from news called Evolutionary Hierarchical Dirichlet Process (EHDP) to capture the evolution pattern of news topics. This model extends Hierarchical Dirichlet

Process models [107]  RaRE (Rank and RErank) [77] is a system for building timelines of events from news articles based on a user query.

In particular, it extracts timelines in three steps: temporal clustering based on salient dates, event relevance, salience scoring, and sentence re-ranking using an iterative algorithm that seeks to reduce redundancy. The method has an underlying assumption that each document represents a single event that can be described by a single sentence. The temporal clustering step identifies salient dates based on the number of occurrences of the date in the documents. The sets of events linked to a specific salient date are called temporal clusters. Furthermore, as a preprocessing step, events are clustered into thematic clusters inside each date using hierarchical clustering based on normalized Manhattan distance and a user-specified threshold. The event relevance and salience scoring steps use these criteria to rank events (i.e., documents) inside each temporal cluster. In particular, it uses four metrics: event relevance, thematic cluster relevance, event salience, and date salience. Event relevance is based on cosine similarity with the initial query. Thematic cluster relevance is based on the similarity of its thematic cluster with the initial query based on the average relevance of each event in the cluster. Event salience is based on the frequency of terms on a specific date. Date salience is based on the (normalized) total relevance of all events happening on that date. Finally, the sentence re-ranking step measures the frequency of unused terms on each date for a specific event to reduce redundancy.

Another topic modeling approach uses a time-dependent Hierarchical Dirichlet Tree Model [60] to capture the evolution of news topics using the Dirichlet Tree distribution-a generalization of the Dirichlet distribution [26]. In particular, the model represents topic distributions in sentences using a tree of fixed depth. Each sentence is associated with a path and with a topic vector and each node has its own topic distribution over words. Using the proposed topic model, sentences are selected by first locating candidate words on the nodes of the tree based on the Jensen-Shannon (JS) divergence of sentences and KLD between word collections. Next, the candidate sentences are scored based on the weighted average of the following criteria: focus (the timeline should be relevant to a given query), coherence (the sentences should be correlated), and coverage (the sentences and documents should be representative).

Wu et al. [122] propose a sentence-based approach to generate timelines. In particular, all the sentences that contain a user-defined query word are split by date and used to generate a date vector representing that specific date. Sentences that do not include parseable dates are grouped based on similarity with the date vector. All sentences are then ranked based on similarity with their corresponding date vector and unrelated sentences are filtered out based on a user-defined threshold. The highest-ranking sentence is used to summarize each date.

Tikhomirov and Dobrov [108] propose a news timeline generation approach from a query based on three steps: query extension, inter-document graph extraction, and intra-document sentence ranking. Query extension is based on pseudo-relevance feedback and consists of three query levels, which are constructed using the most significant terms based on TF-IDF weights. Next, as a preprocessing step, dates that have a frequency below a statistically determined threshold are discarded. The next two steps use an inverted pyramid [50] heuristic, which assumes that the upper part of the article contains the most important information and the lower part of the article may contain references to important events from the past. In particular, the inter-document graph extraction step constructs a similarity matrix between the upper and lower parts of the documents. If the similarity is above a specified threshold, then the articles are considered to be linked, creating a similarity graph. Next, a ranking algorithm-LexRank [30]-is used to determine the importance of each document. Documents that are above a specified importance threshold are used to further expand the original query one more time. Finally, to rank the final selected sentences for the summary, a ranking metric is defined by taking into account content similarity (using cosine similarity) with the extended query (i.e., maximizing relevance) and subtracting similarity with already extracted sentences (i.e., minimizing redundancy).","[[], ['b23'], [], [], ['b52', 'b124'], ['b57'], ['b76', 'b106'], [], ['b25', 'b59'], ['b121'], ['b29', 'b107', 'b49']]","[[], ['b23'], [], [], ['b52', 'b124'], ['b57'], ['b76', 'b106'], [], ['b25', 'b59'], ['b121'], ['b29', 'b107', 'b49']]",12,"sent1: These approaches perform an information retrieval step before or during the narrative extraction process based on a user-defined query.
sent2: In some cases, the query just acts as a simple filter, in others, they explicitly include the query into the narrative extraction model.
sent3: Chieu and Lee [24] present a query-based timeline extraction approach where each event is represented as a sentence.
sent4: This is the earliest form of the ""events as sentences"" that we could find in the literature.
sent5: Sentences are first filtered based on the query and then ranked according to two criteria: interest-based on the frequency of the reported event in the query-and burstiness-based on the idea that important events form clusters around their date of occurrence.
sent6: To determine whether two sentences are reporting the same event, the authors use cosine similarity.
sent7: Furthermore, interest is determined based on a time window to avoid combining events that should be separated due to their temporal distance.
sent8: To reduce redundancy, duplicated sentences are removed based on a time window around an important event that depends on the interest value.
sent9: Yan et al. [125] proposed a timeline summarization method based on balanced optimization and iterative substitution of sentences.
sent10: Their optimization problem is defined in terms of relevance, coverage, coherence, and diversity.
sent11: All these terms are based on the Kullback-Leibler divergence (KLD) [53] of the summary items with a target distribution.
sent12: Relevance is related to a user-defined query and is defined as the KLD between the summary items and the internal representation of the query.
sent13: Coverage is based on a global term-KLD between the summary items and the whole corpus-and a local term-KLD between the summary items and the set of sentences from the same date.
sent14: Coherence is defined locally, based on the KLD between each summary item and its neighboring summaries by using an exponential temporal decay term (i.e., consecutive dates should have relatively similar summaries).
sent15: Diversity is measured across dates and measures the average KLD of each sentence with respect to all other sentences in a leave-one-out manner.
sent16: The final utility function is a weighted average of these terms with user-defined weights and can be defined at a local level (to evaluate individual time periods) and a global level (to evaluate the full timeline).
sent17: To find the sentences, this utility function is optimized in an iterative manner by replacing sentences in the date summaries and improving the utility value in each step using a dynamic programming algorithm that considers both local and global constraints.
sent18: Li et al. [58] propose a topic modeling approach for timeline extraction from news called Evolutionary Hierarchical Dirichlet Process (EHDP) to capture the evolution pattern of news topics.
sent19: This model extends Hierarchical DirichletProcess models [107]  RaRE (Rank and RErank) [77] is a system for building timelines of events from news articles based on a user query.
sent20: In particular, it extracts timelines in three steps: temporal clustering based on salient dates, event relevance, salience scoring, and sentence re-ranking using an iterative algorithm that seeks to reduce redundancy.
sent21: The method has an underlying assumption that each document represents a single event that can be described by a single sentence.
sent22: The temporal clustering step identifies salient dates based on the number of occurrences of the date in the documents.
sent23: The sets of events linked to a specific salient date are called temporal clusters.
sent24: Furthermore, as a preprocessing step, events are clustered into thematic clusters inside each date using hierarchical clustering based on normalized Manhattan distance and a user-specified threshold.
sent25: The event relevance and salience scoring steps use these criteria to rank events (i.e., documents) inside each temporal cluster.
sent26: In particular, it uses four metrics: event relevance, thematic cluster relevance, event salience, and date salience.
sent27: Event relevance is based on cosine similarity with the initial query.
sent28: Thematic cluster relevance is based on the similarity of its thematic cluster with the initial query based on the average relevance of each event in the cluster.
sent29: Event salience is based on the frequency of terms on a specific date.
sent30: Date salience is based on the (normalized) total relevance of all events happening on that date.
sent31: Finally, the sentence re-ranking step measures the frequency of unused terms on each date for a specific event to reduce redundancy.
sent32: Another topic modeling approach uses a time-dependent Hierarchical Dirichlet Tree Model [60] to capture the evolution of news topics using the Dirichlet Tree distribution-a generalization of the Dirichlet distribution [26].
sent33: In particular, the model represents topic distributions in sentences using a tree of fixed depth.
sent34: Each sentence is associated with a path and with a topic vector and each node has its own topic distribution over words.
sent35: Using the proposed topic model, sentences are selected by first locating candidate words on the nodes of the tree based on the Jensen-Shannon (JS) divergence of sentences and KLD between word collections.
sent36: Next, the candidate sentences are scored based on the weighted average of the following criteria: focus (the timeline should be relevant to a given query), coherence (the sentences should be correlated), and coverage (the sentences and documents should be representative).
sent37: Wu et al. [122] propose a sentence-based approach to generate timelines.
sent38: In particular, all the sentences that contain a user-defined query word are split by date and used to generate a date vector representing that specific date.
sent39: Sentences that do not include parseable dates are grouped based on similarity with the date vector.
sent40: All sentences are then ranked based on similarity with their corresponding date vector and unrelated sentences are filtered out based on a user-defined threshold.
sent41: The highest-ranking sentence is used to summarize each date.
sent42: Tikhomirov and Dobrov [108] propose a news timeline generation approach from a query based on three steps: query extension, inter-document graph extraction, and intra-document sentence ranking.
sent43: Query extension is based on pseudo-relevance feedback and consists of three query levels, which are constructed using the most significant terms based on TF-IDF weights.
sent44: Next, as a preprocessing step, dates that have a frequency below a statistically determined threshold are discarded.
sent45: The next two steps use an inverted pyramid [50] heuristic, which assumes that the upper part of the article contains the most important information and the lower part of the article may contain references to important events from the past.
sent46: In particular, the inter-document graph extraction step constructs a similarity matrix between the upper and lower parts of the documents.
sent47: If the similarity is above a specified threshold, then the articles are considered to be linked, creating a similarity graph.
sent48: Next, a ranking algorithm-LexRank [30]-is used to determine the importance of each document.
sent49: Documents that are above a specified importance threshold are used to further expand the original query one more time.
sent50: Finally, to rank the final selected sentences for the summary, a ranking metric is defined by taking into account content similarity (using cosine similarity) with the extended query (i.e., maximizing relevance) and subtracting similarity with already extracted sentences (i.e., minimizing redundancy)."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s33,User Evaluation Metrics,"These metrics seek to evaluate the extracted narrative based on subjective user measures or task performance measures.

Task-oriented Evaluation Task-oriented metrics require designing a series of benchmark tasks to measure the number of correct answers, accuracy, how much time the users take to complete the task or some other measure of correctness or quality. Few works use task-oriented evaluation metrics: Metro Maps [98,99], Information Cartography [100,101], and the SToRe system [64,65]. These works rely on event-based representations and all of them evaluate extraction methods as retrieval tools following a similar approach. In particular, there are micro-knowledge tasks that measure how the extracted narratives help users retrieve information faster and macro-knowledge tasks that measure how the extracted narratives help users understand the big picture.

For micro-knowledge tasks, all works create a series of simple retrieval questions such that the answers can be easily classified as right or wrong. For example, retrieving dates, facts, relevant entities or the main event descriptors. Users are evaluated by measuring how many correct answers (i.e., accuracy) they can get in a fixed amount of time and the rate at which they answer these questions [64,65,98]. Another metric used at the micro-knowledge level is the ease of navigation, estimated by the number of documents that users clicked per correct answer [64,65,98].

For macro-knowledge, some form of summarization is used to evaluate the narratives. Shahaf et al. [98] asked users to create summaries based on different narrative representations and then used crowdsourcing to evaluate user preference over those summaries. However, these benchmark tasks do not go beyond basic retrieval and summarization. Tasks that require higher levels of knowledge and cognitive work (e.g., analysis tasks) are not covered by these evaluations. In general, the inherent difficulty of designing benchmark tasks that can be easily evaluated might be one of the reasons why user-based evaluations of extraction methods usually rely on subjective ratings rather than task-oriented metrics.

Subjective Evaluation Most of the works that rely on user evaluations use subjective measures (i.e., user perception metrics). These subjective metrics include concepts from usability, including criteria such as user preference [47,51,101], visual presentation [51], and ease of use [51,64]. Other metrics include effectiveness as perceived by the users (e.g., perceived helpfulness or usefulness), satisfaction, and comprehensibility [64,65,109]. Alternatively perceived familiarity before and after using the extracted narrative can be a useful measure of usefulness [96].

Lastly, user-perceived quality is another widely used approach to evaluate extracted narratives. The user-perceived quality criteria mostly correspond to the quality criteria metrics defined before [18,51,96,97,109,135,136], including coherence, coverage, redundancy, relevance, dispersion, and similar variations (e.g., broadness). We note that these user perception metrics suffer a similar problem as their computational counterparts-they are fuzzy concepts that could be defined differently. This is further compounded by the subjective nature of these evaluations.

Other works rely on asking users whether they consider specific elements of the narrative as correct. For example, asking whether a specific connection is correct, whether the selected documents are relevant, whether a specific storyline is logically coherent, or about the number of coherent and relevant documents [18,66,67]. This is similar to traditional information retrieval metrics that rely on ground truth information. However, in this case, rather than using a previously defined gold standard, the accuracy measures are defined purely on subjective perceptions. Finally, another approach is to ask users to compare the ground truth with the output narrative-from potentially multiple methods-and rank them according to their preference based on their knowledge of the topic [61].","[[], ['b64', 'b99', 'b97', 'b63', 'b98', 'b100'], ['b64', 'b97', 'b63'], ['b97'], ['b64', 'b50', 'b95', 'b46', 'b63', 'b108', 'b100'], ['b50', 'b17', 'b95', 'b96', 'b134', 'b108', 'b135'], ['b60', 'b66', 'b17', 'b65']]","[[], ['b64', 'b99', 'b97', 'b63', 'b98', 'b100'], ['b64', 'b97', 'b63'], ['b97'], ['b64', 'b50', 'b95', 'b46', 'b63', 'b108', 'b100'], ['b50', 'b17', 'b95', 'b96', 'b134', 'b108', 'b135'], ['b60', 'b66', 'b17', 'b65']]",28,"sent1: These metrics seek to evaluate the extracted narrative based on subjective user measures or task performance measures.
sent2: Task-oriented Evaluation Task-oriented metrics require designing a series of benchmark tasks to measure the number of correct answers, accuracy, how much time the users take to complete the task or some other measure of correctness or quality.
sent3: Few works use task-oriented evaluation metrics: Metro Maps [98,99], Information Cartography [100,101], and the SToRe system [64,65].
sent4: These works rely on event-based representations and all of them evaluate extraction methods as retrieval tools following a similar approach.
sent5: In particular, there are micro-knowledge tasks that measure how the extracted narratives help users retrieve information faster and macro-knowledge tasks that measure how the extracted narratives help users understand the big picture.
sent6: For micro-knowledge tasks, all works create a series of simple retrieval questions such that the answers can be easily classified as right or wrong.
sent7: For example, retrieving dates, facts, relevant entities or the main event descriptors.
sent8: Users are evaluated by measuring how many correct answers (i.e., accuracy) they can get in a fixed amount of time and the rate at which they answer these questions [64,65,98].
sent9: Another metric used at the micro-knowledge level is the ease of navigation, estimated by the number of documents that users clicked per correct answer [64,65,98].
sent10: For macro-knowledge, some form of summarization is used to evaluate the narratives.
sent11: Shahaf et al. [98] asked users to create summaries based on different narrative representations and then used crowdsourcing to evaluate user preference over those summaries.
sent12: However, these benchmark tasks do not go beyond basic retrieval and summarization.
sent13: Tasks that require higher levels of knowledge and cognitive work (e.g., analysis tasks) are not covered by these evaluations.
sent14: In general, the inherent difficulty of designing benchmark tasks that can be easily evaluated might be one of the reasons why user-based evaluations of extraction methods usually rely on subjective ratings rather than task-oriented metrics.
sent15: Subjective Evaluation Most of the works that rely on user evaluations use subjective measures (i.e., user perception metrics).
sent16: These subjective metrics include concepts from usability, including criteria such as user preference [47,51,101], visual presentation [51], and ease of use [51,64].
sent17: Other metrics include effectiveness as perceived by the users (e.g., perceived helpfulness or usefulness), satisfaction, and comprehensibility [64,65,109].
sent18: Alternatively perceived familiarity before and after using the extracted narrative can be a useful measure of usefulness [96].
sent19: Lastly, user-perceived quality is another widely used approach to evaluate extracted narratives.
sent20: The user-perceived quality criteria mostly correspond to the quality criteria metrics defined before [18,51,96,97,109,135,136], including coherence, coverage, redundancy, relevance, dispersion, and similar variations (e.g., broadness).
sent21: We note that these user perception metrics suffer a similar problem as their computational counterparts-they are fuzzy concepts that could be defined differently.
sent22: This is further compounded by the subjective nature of these evaluations.
sent23: Other works rely on asking users whether they consider specific elements of the narrative as correct.
sent24: For example, asking whether a specific connection is correct, whether the selected documents are relevant, whether a specific storyline is logically coherent, or about the number of coherent and relevant documents [18,66,67].
sent25: This is similar to traditional information retrieval metrics that rely on ground truth information.
sent26: However, in this case, rather than using a previously defined gold standard, the accuracy measures are defined purely on subjective perceptions.
sent27: Finally, another approach is to ask users to compare the ground truth with the output narrative-from potentially multiple methods-and rank them according to their preference based on their knowledge of the topic [61]."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s35,Narrative Structure,"The choice of the core structure is an important aspect of narrative representation. Using a linear structure provides a simple approach to represent a narrative with a single storyline, but it does not appropriately model the nuances of narratives with multiple stories. In contrast, graph-based structures allow the modeling of different interactions between storylines (e.g., convergent and divergent stories) [51]. Linear representations are implicitly directed, but graph-based representations may or may not be directed. Directed graphs usually exploit the underlying temporal relationships to determine the direction of the connections between elements. When the connection between basic units is guided by temporal constraints it naturally gives rise to directed acyclic graphs. Directed acyclic graphs provide the most flexibility while also accounting for the temporal nature of a narrative. However, not all directed graph models are acyclic, as some use specific types of relationships that allow the creation of cycles (e.g., same-event relations).

A representation that falls between linear and fully graph-based representations is the tree-based representation [66,67,131,133,134]. Such models allow for more flexible structures than linear representations. In particular, they are able to model story divergence (i.e., multiple storylines splitting off from the root or other nodes). Unlike graph-based models, they are not able to model story convergence (e.g., two stories joining into a final event), as that would break the tree structure. Tree-based structures have not been deeply explored in the literature and could provide an intermediate approach between linear and graph-based representations in terms of complexity, allowing easier understanding by users while retaining some flexibility. However, the inability to model story convergence might limit their applications.

Manuscript submitted to ACM","[['b50'], ['b132', 'b133', 'b66', 'b65', 'b130'], []]","[['b50'], ['b132', 'b133', 'b66', 'b65', 'b130'], []]",6,"sent1: The choice of the core structure is an important aspect of narrative representation.
sent2: Using a linear structure provides a simple approach to represent a narrative with a single storyline, but it does not appropriately model the nuances of narratives with multiple stories.
sent3: In contrast, graph-based structures allow the modeling of different interactions between storylines (e.g., convergent and divergent stories)[51].
sent4: Linear representations are implicitly directed, but graph-based representations may or may not be directed.
sent5: Directed graphs usually exploit the underlying temporal relationships to determine the direction of the connections between elements.
sent6: When the connection between basic units is guided by temporal constraints it naturally gives rise to directed acyclic graphs.
sent7: Directed acyclic graphs provide the most flexibility while also accounting for the temporal nature of a narrative.
sent8: However, not all directed graph models are acyclic, as some use specific types of relationships that allow the creation of cycles (e.g., same-event relations).
sent9: A representation that falls between linear and fully graph-based representations is the tree-based representation [66,67,131,133,134].
sent10: Such models allow for more flexible structures than linear representations.
sent11: In particular, they are able to model story divergence (i.e., multiple storylines splitting off from the root or other nodes).
sent12: Unlike graph-based models, they are not able to model story convergence (e.g., two stories joining into a final event), as that would break the tree structure.
sent13: Tree-based structures have not been deeply explored in the literature and could provide an intermediate approach between linear and graph-based representations in terms of complexity, allowing easier understanding by users while retaining some flexibility.
sent14: However, the inability to model story convergence might limit their applications.
sent15: Manuscript submitted to ACM"
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s38,Data Set Source URL,Timeline 17 [110] https://github.com/complementizer/news-tls Crisis [111] https://github.com/complementizer/news-tls COVID-TLS [55] https://github.com/MorenoLaQuatra/SDF-TLS TLS-COVID19 [83] https://github.com/LIAAD/tls-covid19 Entities [37] https://github.com/complementizer/news-tls MTLS Data [130] https://yiyualt.github.io/mtlsdata/ Table 4. Benchmark data in the timeline summarization works.,"[['b82', 'b129', 'b54', 'b110', 'b109', 'b36']]","[['b82', 'b129', 'b54', 'b110', 'b109', 'b36']]",6,"sent1: Timeline 17 [110] https://github.com/complementizer/news-tls Crisis [111] https://github.com/complementizer/news-tls COVID-TLS [55] https://github.com/MorenoLaQuatra/SDF-TLS TLS-COVID19
sent2: [83] https://github.com/LIAAD/tls-covid19
sent3: Entities [37] https://github.com/complementizer/news-tls MTLS Data
sent4: [130] https://yiyualt.github.io/mtlsdata/
sent5: Table 4. Benchmark data in the timeline summarization works."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s40,Practical Applications,"Event-based news narrative extraction has several practical applications beyond journalistic analysis tasks. Most of these applications seek to help with the issue of information overload in different contexts [101]. We briefly discuss some potential applications explored or mentioned in some of the reviewed works.

Disaster Management Disaster management [123,131,133,134] could benefit from using extraction approaches to keep track of disasters or other similarly negative incidents. In particular, to minimize losses caused by a disaster, one of the critical tasks in disaster management is to efficiently analyze and understand situation updates. Doing this Manuscript submitted to ACM requires effective methods to navigate a multitude of documents such as news or reports related to the disaster. Domain experts need to obtain condensed information about the disaster and its evolution [59]. Thus, news narrative extraction could help experts to understand the evolving situation and devise a proper strategy.

Open Source Intelligence Open source intelligence (OSINT) is intelligence that is synthesized using publicly available data [32]. While OSINT data sources leverage more than just traditional news articles [38], OSINT could still benefit from news narrative extraction techniques. In particular, news narrative extraction methods could help intelligence analysts explore the information landscape and find key events [51]. Furthermore, these techniques could help analysts in prediction tasks by providing support and evidence [18].

Misinformation and Fact-Checking News narrative extraction methods could aid fact-checkers in their tasks by providing them with an overview of the current narrative and highlighting key relevant events [51]. However, current methods do not include explicit ways to model misleading or outright false information.

Financial Markets News narrative extraction could aid financial analysts to understand the information landscape [17]. For example, market news is regarded as an important data source in the context of financial analysis [17]. In particular, being able to understand and exploit the hidden information in the raw news data could help analysts adapt their strategies and reduce their financial risk.","[['b100'], ['b132', 'b133', 'b58', 'b130', 'b122'], ['b17', 'b31', 'b50', 'b37'], ['b50'], ['b16']]","[['b100'], ['b132', 'b133', 'b58', 'b130', 'b122'], ['b17', 'b31', 'b50', 'b37'], ['b50'], ['b16']]",12,"sent1: Event-based news narrative extraction has several practical applications beyond journalistic analysis tasks.
sent2: Most of these applications seek to help with the issue of information overload in different contexts [101].
sent3: We briefly discuss some potential applications explored or mentioned in some of the reviewed works.
sent4: Disaster Management Disaster management [123,131,133,134] could benefit from using extraction approaches to keep track of disasters or other similarly negative incidents.
sent5: In particular, to minimize losses caused by a disaster, one of the critical tasks in disaster management is to efficiently analyze and understand situation updates.
sent6: Doing this Manuscript submitted to ACM requires effective methods to navigate a multitude of documents such as news or reports related to the disaster.
sent7: Domain experts need to obtain condensed information about the disaster and its evolution [59].
sent8: Thus, news narrative extraction could help experts to understand the evolving situation and devise a proper strategy.
sent9: Open Source Intelligence Open source intelligence (OSINT) is intelligence that is synthesized using publicly available data [32].
sent10: While OSINT data sources leverage more than just traditional news articles [38], OSINT could still benefit from news narrative extraction techniques.
sent11: In particular, news narrative extraction methods could help intelligence analysts explore the information landscape and find key events [51].
sent12: Furthermore, these techniques could help analysts in prediction tasks by providing support and evidence [18].Misinformation and Fact-Checking News narrative extraction methods could aid fact-checkers in their tasks by providing them with an overview of the current narrative and highlighting key relevant events [51].
sent13: However, current methods do not include explicit ways to model misleading or outright false information.
sent14: Financial Markets News narrative extraction could aid financial analysts to understand the information landscape [17].
sent15: For example, market news is regarded as an important data source in the context of financial analysis [17].
sent16: In particular, being able to understand and exploit the hidden information in the raw news data could help analysts adapt their strategies and reduce their financial risk."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s41,Recent Trends and Open Challenges,"Timeline Summarization Variations Recent works have proposed some variations on the traditional timeline summarization task. In particular, Duan et al. [28] proposed the comparative TLS task and Yu et al. [130] proposed the Multi-TLS task. These two works highlight the fact that simple linear representations of narratives are naturally limiting unless applied to the most simple of narratives. Thus, the creation of similar tasks to address some of the shortcomings of these representations is a natural progression. However, it raises the question of whether these extensions would benefit from borrowing elements from the methods that use more complex representations discussed in this survey. A natural extension would be to consider a graph-based representation that allows for multiple storylines and comparisons without further modifications. This approach would address both the comparative TLS and MLTS tasks.

In this context, we note that most of the reviewed articles with a sentence-level event resolution used a linear structure (see Table 1). The only exceptions were the disaster storyline extraction systems [131,133,134] with their local tree representation. However, these methods are designed with a specific news topic in mind-disaster news-and are able to leverage specific characteristics of the topic (e.g., the disaster moves over time). Thus, it would not be possible to directly adapt it to other types of news without addressing this issue.

Furthermore, we note that there are no inherent limitations to sentence-level representations that prevent them from being extended beyond linear narratives, which makes the lack of graph-based approaches an opportunity for future research. Finally, while we did not find such a suitable graph-based approach in the traditional news domain, there is one example from the social media domain-which has its own set of challenges in terms of narrative extraction-that can be found in Ansah et al. [5]. This work proposes a tree-based narrative representation with sentence-level event representation using tweets. This approach extends the traditional TLS by allowing divergent storylines to emerge instead of just a single timeline. Such an approach could be adapted to traditional news narrative extraction.","[['b27', 'b129'], ['b132', 'b130', 'b133'], ['b4']]","[['b27', 'b129'], ['b132', 'b130', 'b133'], ['b4']]",6,"sent1: Timeline Summarization Variations Recent works have proposed some variations on the traditional timeline summarization task.
sent2: In particular, Duan et al. [28] proposed the comparative TLS task and Yu et al. [130] proposed the Multi-TLS task.
sent3: These two works highlight the fact that simple linear representations of narratives are naturally limiting unless applied to the most simple of narratives.
sent4: Thus, the creation of similar tasks to address some of the shortcomings of these representations is a natural progression.
sent5: However, it raises the question of whether these extensions would benefit from borrowing elements from the methods that use more complex representations discussed in this survey.
sent6: A natural extension would be to consider a graph-based representation that allows for multiple storylines and comparisons without further modifications.
sent7: This approach would address both the comparative TLS and MLTS tasks.
sent8: In this context, we note that most of the reviewed articles with a sentence-level event resolution used a linear structure (see Table 1).
sent9: The only exceptions were the disaster storyline extraction systems [131,133,134] with their local tree representation.
sent10: However, these methods are designed with a specific news topic in mind-disaster news-and are able to leverage specific characteristics of the topic (e.g., the disaster moves over time).
sent11: Thus, it would not be possible to directly adapt it to other types of news without addressing this issue.
sent12: Furthermore, we note that there are no inherent limitations to sentence-level representations that prevent them from being extended beyond linear narratives, which makes the lack of graph-based approaches an opportunity for future research.
sent13: Finally, while we did not find such a suitable graph-based approach in the traditional news domain, there is one example from the social media domain-which has its own set of challenges in terms of narrative extraction-that can be found in Ansah et al. [5].
sent14: This work proposes a tree-based narrative representation with sentence-level event representation using tweets.
sent15: This approach extends the traditional TLS by allowing divergent storylines to emerge instead of just a single timeline.
sent16: Such an approach could be adapted to traditional news narrative extraction."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s17,Pre-filtered Approaches.,"These approaches assume that the data set has already been filtered as part of a preprocessing step. Thus, they do not explicitly model the query in their extraction model.

Yan et al. [124] propose a system to generate news timelines using a trans-temporal summarization approach, where the summary for each time period depends on its context-that is, nearby time periods. Before generating the timeline, the system chooses the important time periods (e.g., specific days) to be summarized based on burstiness. The timeline extraction approach is based on two components: a global component-which defines the structure of the overall summary and the inter-temporal relationships between each period of the timeline-and a local component-which defines the summary in each time period. The global component is based on a global graph that uses inter-date dependency, which is computed using temporal proximity and a global affinity model for each sentence based on PageRank. Furthermore, to ensure a diverse set of sentences in the global component, the system incorporates DivRank into the affinity model [70] to penalize the lack of diversity in the sentence selection. Next, the local component is based on a local sentence graph for each time period following a similar approach to the global graph. To generate the final sentence selection in each time period, the system optimizes a weighted ranking generated by both components.

Hu et al. [46] propose a timeline overview method for news based on the concept of breakpoints-points in time where a significant development or change occurs (i.e., important events). Their extraction approach consists of three steps. First, they analyze topic activity using a Topic-Activeness Hidden Markov Model (HMM) and discard inactive periods. In practice, this is done by measuring whether there is new information using KLD and document frequency.

Manuscript submitted to ACM Next, the breakpoints are identified by detecting topic variations in each time period using a topic mixture model-in particular, a generative probabilistic mixture model [71]-and a Theme-Transition HMM model to model topic evolution.

Specifically, breakpoints are identified by using JS divergence to measure topic variation between two consecutive time points. Then, a summary for each breakpoint is generated by selected representative sentences-based on Jaccard similarity with topic keywords and relevant entities.

Tran et al. [10] present a supervised learning method to extract timelines from news articles based on Linear Regression. Their model first identifies salient dates based on burstiness (i.e., high-frequency periods), and then selects the most representative sentences from the news articles on each of these dates. In particular, the model uses surface-level features (e.g., length and position of the sentences), coherence features (e.g., causal and temporal signals), topic features (e.g., TF-IDF information and cross-entropy), and time-related features (e.g., popularity over time and use of temporal expressions) to determine the key sentences of each date. Subsequent work by Tran et al. [110] used SVM-Rank instead of linear regression and expanded upon this supervised framework. In particular, they leverage three metrics to evaluate the event sentences: relevance, novelty, and continuity. Relevance is learned using the SVM-Rank mentioned earlier.

Novelty is evaluated by measuring the non-overlapping n-grams over the total n-grams between a candidate sentence and previously selected sentences. Continuity is a measure of local coherence-there should be smooth transitions in the timeline-that is computed as the average n-gram overlap of all sentences in the current day with the previous summary. The final score is based on a weighted average of these metrics. To learn the relevance function, the authors leverage the same set of features from their previous work [10], but they also added an extra set of features about the event itself. For example, they evaluated whether the sentences properly represent the main event of the article, using the fact that the first sentences should contain the most relevant information (following the inverted pyramid structure).

Thus, they evaluate the similarity between the sentence summary and the first four sentences. Once the SVM-Rank method was trained, the ranking is fed to a dynamic programming approach to optimize the final score.

Huang and Huang [49] present an event storyline generation method based on a mixture-event-aspect probabilistic model that can detect and distinguish the different types of sub-events in the article data set. Their model is an extension of Probabilistic Latent Semantic Analysis [44] and LDA [12]. In particular, their model detects global aspects (i.e., terms that are important throughout the whole story) and local aspects (i.e., terms that are important in a specific event inside the story). Based on the extracted aspect model, the bursty periods for each aspect are extracted to measure their popularity on a certain date and detect relevant events. Based on these results, it is possible to extract a timeline and select the most representative sentences associated with both global and local aspects to compose the final storyline with adjustable weights for the aspects. Sentences are selected by minimizing the overall information loss over each aspect. In particular, the LexPageRank algorithm [29] is used to rank sentences and KLD is used for sentence similarity.

Tran et al. [109] propose a timeline summarization approach based on article headlines. Their approach is based on a random walk model using a topic-sensitive version of PageRank [43] that selects relevant headlines from the data set for each time period. There are three key metrics to evaluate the relevance of a headline: informing value, spread, and influence. The informing value depends on whether the headline provides factual information or an opinion, review, or another non-informing category. It is a binary value computed using a supervised learning approach based on an SVM classifier to separate facts from opinion [129]. Influence tries to measure the impact of an event on future events (e.g., ""the president resigns"" would lead to a ""new election"" event) based on references from future events using similarity between future news articles and the headline of the event. Spread is based on the intuitive idea that a relevant event will be reported in multiple news outlets-that is, its reporting will be spread over multiple headlines. Thus, it is a measure of positive redundancy and it is formally defined as the probability of a headline being duplicated. To estimate whether two headlines are duplicates, the system uses a supervised logistic regression model trained on semantic similarity measures based on paraphrase detection literature [72]. Having defined these elements, the goal is to maximize all three aspects to select the best headlines. This is done by using PageRank on a graph of headlines, taking into account both spread (graph edges) and influence (random walk probability), to generate the final rankings. Next, to generate the final timeline for each day, the resulting rankings are selected greedily, subject to redundancy constraints, informativeness constraints, and a maximum number of headlines per day.

Chen et al. [23] present a supervised timeline summarization algorithm based on aging theory for news data sets.

Aging theory [22] is a model that tracks the life cycle of events using an energy function, which increases when an event becomes popular and diminishes with time. The method works by extracting sentences (i.e., specific events) and the publication time from news articles and using a classification model built with SVM to determine whether they belong in the output timeline. This approach is based on surface-level features (e.g., noun frequencies and stop word frequencies), importance features (e.g., latent semantic analysis scores), topic features (e.g., topic word frequencies), an aging score feature (i.e., changing coverage of an event over time), and a novelty feature. The aging score is used to measure the life cycle of each term over time using a recurrence relation with TF-IDF representations. The novelty score is based on the Jaccard similarity of the current summary and the candidate sentence.","[[], ['b123', 'b69'], ['b45'], ['b70'], [], ['b9', 'b109'], ['b9'], [], ['b43', 'b11', 'b28', 'b48'], ['b128', 'b108', 'b71', 'b42'], ['b22'], ['b21']]","[[], ['b123', 'b69'], ['b45'], ['b70'], [], ['b9', 'b109'], ['b9'], [], ['b43', 'b11', 'b28', 'b48'], ['b128', 'b108', 'b71', 'b42'], ['b22'], ['b21']]",17,"sent1: These approaches assume that the data set has already been filtered as part of a preprocessing step.
sent2: Thus, they do not explicitly model the query in their extraction model.
sent3: Yan et al. [124] propose a system to generate news timelines using a trans-temporal summarization approach, where the summary for each time period depends on its context-that is, nearby time periods.
sent4: Before generating the timeline, the system chooses the important time periods (e.g., specific days) to be summarized based on burstiness.
sent5: The timeline extraction approach is based on two components: a global component-which defines the structure of the overall summary and the inter-temporal relationships between each period of the timeline-and a local component-which defines the summary in each time period.
sent6: The global component is based on a global graph that uses inter-date dependency, which is computed using temporal proximity and a global affinity model for each sentence based on PageRank.
sent7: Furthermore, to ensure a diverse set of sentences in the global component, the system incorporates DivRank into the affinity model [70] to penalize the lack of diversity in the sentence selection.
sent8: Next, the local component is based on a local sentence graph for each time period following a similar approach to the global graph.
sent9: To generate the final sentence selection in each time period, the system optimizes a weighted ranking generated by both components.
sent10: Hu et al. [46] propose a timeline overview method for news based on the concept of breakpoints-points in time where a significant development or change occurs (i.e., important events).
sent11: Their extraction approach consists of three steps.
sent12: First, they analyze topic activity using a Topic-Activeness Hidden Markov Model (HMM) and discard inactive periods.
sent13: In practice, this is done by measuring whether there is new information using KLD and document frequency.
sent14: Manuscript submitted to ACM Next, the breakpoints are identified by detecting topic variations in each time period using a topic mixture model-in particular, a generative probabilistic mixture model [71]-and a Theme-Transition HMM model to model topic evolution.
sent15: Specifically, breakpoints are identified by using JS divergence to measure topic variation between two consecutive time points.
sent16: Then, a summary for each breakpoint is generated by selected representative sentences-based on Jaccard similarity with topic keywords and relevant entities.
sent17: Tran et al. [10] present a supervised learning method to extract timelines from news articles based on Linear Regression.
sent18: Their model first identifies salient dates based on burstiness (i.e., high-frequency periods), and then selects the most representative sentences from the news articles on each of these dates.
sent19: In particular, the model uses surface-level features (e.g., length and position of the sentences), coherence features (e.g., causal and temporal signals), topic features (e.g., TF-IDF information and cross-entropy), and time-related features (e.g., popularity over time and use of temporal expressions) to determine the key sentences of each date.
sent20: Subsequent work by Tran et al. [110] used SVM-Rank instead of linear regression and expanded upon this supervised framework.
sent21: In particular, they leverage three metrics to evaluate the event sentences: relevance, novelty, and continuity.
sent22: Relevance is learned using the SVM-Rank mentioned earlier.
sent23: Novelty is evaluated by measuring the non-overlapping n-grams over the total n-grams between a candidate sentence and previously selected sentences.
sent24: Continuity is a measure of local coherence-there should be smooth transitions in the timeline-that is computed as the average n-gram overlap of all sentences in the current day with the previous summary.
sent25: The final score is based on a weighted average of these metrics.
sent26: To learn the relevance function, the authors leverage the same set of features from their previous work [10], but they also added an extra set of features about the event itself.
sent27: For example, they evaluated whether the sentences properly represent the main event of the article, using the fact that the first sentences should contain the most relevant information (following the inverted pyramid structure).
sent28: Thus, they evaluate the similarity between the sentence summary and the first four sentences.
sent29: Once the SVM-Rank method was trained, the ranking is fed to a dynamic programming approach to optimize the final score.
sent30: Huang and Huang [49] present an event storyline generation method based on a mixture-event-aspect probabilistic model that can detect and distinguish the different types of sub-events in the article data set.
sent31: Their model is an extension of Probabilistic Latent Semantic Analysis [44] and LDA [12].
sent32: In particular, their model detects global aspects (i.e., terms that are important throughout the whole story) and local aspects (i.e., terms that are important in a specific event inside the story).
sent33: Based on the extracted aspect model, the bursty periods for each aspect are extracted to measure their popularity on a certain date and detect relevant events.
sent34: Based on these results, it is possible to extract a timeline and select the most representative sentences associated with both global and local aspects to compose the final storyline with adjustable weights for the aspects.
sent35: Sentences are selected by minimizing the overall information loss over each aspect.
sent36: In particular, the LexPageRank algorithm [29] is used to rank sentences and KLD is used for sentence similarity.
sent37: Tran et al. [109] propose a timeline summarization approach based on article headlines.
sent38: Their approach is based on a random walk model using a topic-sensitive version of PageRank [43] that selects relevant headlines from the data set for each time period.
sent39: There are three key metrics to evaluate the relevance of a headline: informing value, spread, and influence.
sent40: The informing value depends on whether the headline provides factual information or an opinion, review, or another non-informing category.
sent41: It is a binary value computed using a supervised learning approach based on an SVM classifier to separate facts from opinion [129].
sent42: Influence tries to measure the impact of an event on future events (e.g., ""the president resigns"" would lead to a ""new election"" event) based on references from future events using similarity between future news articles and the headline of the event.
sent43: Spread is based on the intuitive idea that a relevant event will be reported in multiple news outlets-that is, its reporting will be spread over multiple headlines.
sent44: Thus, it is a measure of positive redundancy and it is formally defined as the probability of a headline being duplicated.
sent45: To estimate whether two headlines are duplicates, the system uses a supervised logistic regression model trained on semantic similarity measures based on paraphrase detection literature [72].
sent46: Having defined these elements, the goal is to maximize all three aspects to select the best headlines.
sent47: This is done by using PageRank on a graph of headlines, taking into account both spread (graph edges) and influence (random walk probability), to generate the final rankings.
sent48: Next, to generate the final timeline for each day, the resulting rankings are selected greedily, subject to redundancy constraints, informativeness constraints, and a maximum number of headlines per day.
sent49: Chen et al. [23] present a supervised timeline summarization algorithm based on aging theory for news data sets.
sent50: Aging theory [22] is a model that tracks the life cycle of events using an energy function, which increases when an event becomes popular and diminishes with time.
sent51: The method works by extracting sentences (i.e., specific events) and the publication time from news articles and using a classification model built with SVM to determine whether they belong in the output timeline.
sent52: This approach is based on surface-level features (e.g., noun frequencies and stop word frequencies), importance features (e.g., latent semantic analysis scores), topic features (e.g., topic word frequencies), an aging score feature (i.e., changing coverage of an event over time), and a novelty feature.
sent53: The aging score is used to measure the life cycle of each term over time using a recurrence relation with TF-IDF representations.
sent54: The novelty score is based on the Jaccard similarity of the current summary and the candidate sentence."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s16,WILSON (neWs tImeLine SummarizatiON) [61] is a query-based timeline summarization method for news based,"on a divide-and-conquer approach consisting of two major components: date selection and text summarization for each selected date. For date selection, the method first tags temporal expressions in sentences and constructs a date reference graph based on these annotations. Next, the method assigns weights to the edges of the date reference graph by taking the product of the number of references and temporal distances with the references. Then, it uses the PageRank algorithm [82] on the extracted graph to find the most salient dates. However, this approach leads to a bias towards older dates, as they have had more time to get references. Thus, the model is augmented with an exponential recency adjustment weight, which is used to initialize the Personalized PageRank algorithm [8], which allows for non-uniform initial distributions. Next, the daily summarization can be done using any multi-document summarization approach.

Specifically, the authors use TextRank [73] based on BERT [27] representations to generate the summaries.","[['b81', 'b7'], ['b72', 'b26']]","[['b81', 'b7'], ['b72', 'b26']]",4,"sent1: on a divide-and-conquer approach consisting of two major components: date selection and text summarization for each selected date.
sent2: For date selection, the method first tags temporal expressions in sentences and constructs a date reference graph based on these annotations.
sent3: Next, the method assigns weights to the edges of the date reference graph by taking the product of the number of references and temporal distances with the references.
sent4: Then, it uses the PageRank algorithm [82] on the extracted graph to find the most salient dates.
sent5: However, this approach leads to a bias towards older dates, as they have had more time to get references.
sent6: Thus, the model is augmented with an exponential recency adjustment weight, which is used to initialize the Personalized PageRank algorithm [8], which allows for non-uniform initial distributions.
sent7: Next, the daily summarization can be done using any multi-document summarization approach.
sent8: Specifically, the authors use TextRank [73] based on BERT [27] representations to generate the summaries."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s42,Multi-resolution Methods,"Currently, all the narrative extraction approaches that we reviewed work on a singular resolution level (sentences, documents, or clusters). Existing attempts at multiple resolution levels only change the scope of the data [100,101] (i.e., applying the method again on a new subset of the data), they do not seek to change the underlying event resolution. Another perspective corresponds to the multi-level presentations of disaster storylines by Zhou et al. and Yuan et al. [131,133,134], which use global and local levels to represent the narrative. However, the underlying event representation remains the same and no efforts have been made to make a model that handles multiple levels of event resolution. Developing models that provide a multi-resolution approach remains an open challenge.

Interactivity Most works on news narrative extraction provide surface-level interactions [100,101,106] such as re-arranging elements and changing the layout, showing details on demand (e.g., all details about a news article), zooming, or performing basic filtering, highlighting, and searching. However, there is still a need for better interaction models that give users more control and feedback when exploring and manipulating the narrative. Some models [96,97] allow more in-depth refinement by letting the user specify elements that need to be changed and then evaluating all possible replacement and insertion actions. Building upon this feature-based feedback, Shahaf et al. [98] designed a method to learn a personalized coverage function that can be optimized to find a personalized narrative.

Another approach by Bögel et al. [14] allows parametric interaction to modify the extracted graph in real time, helping the user understand how the narrative changes based on the parameters. However, this approach requires the users to understand the underlying model parameters. In this context, semantic interactions could be useful to aid users modify the model without deep understanding of the underlying parameters. Semantic interactions [119] are used in sensemaking applications to directly reflect the analytical thought process of analysts about data (e.g., by using information about how analysts organize documents or highlight text), as opposed to parametric interaction that manipulates model parameters (e.g., sliders and keyword weights). Thus, capturing a user model through semantic interaction could lead to a better narrative model.","[['b132', 'b133', 'b99', 'b130', 'b100'], ['b105', 'b99', 'b96', 'b95', 'b97', 'b100'], ['b118', 'b13']]","[['b132', 'b133', 'b99', 'b130', 'b100'], ['b105', 'b99', 'b96', 'b95', 'b97', 'b100'], ['b118', 'b13']]",13,"sent1: Currently, all the narrative extraction approaches that we reviewed work on a singular resolution level (sentences, documents, or clusters).
sent2: Existing attempts at multiple resolution levels only change the scope of the data [100,101]
sent3: (i.e., applying the method again on a new subset of the data), they do not seek to change the underlying event resolution.
sent4: Another perspective corresponds to the multi-level presentations of disaster storylines by Zhou et al. and Yuan et al. [131,133,134], which use global and local levels to represent the narrative.
sent5: However, the underlying event representation remains the same and no efforts have been made to make a model that handles multiple levels of event resolution.
sent6: Developing models that provide a multi-resolution approach remains an open challenge.
sent7: Interactivity Most works on news narrative extraction provide surface-level interactions [100,101,106] such as re-arranging elements and changing the layout, showing details on demand (e.g., all details about a news article), zooming, or performing basic filtering, highlighting, and searching.
sent8: However, there is still a need for better interaction models that give users more control and feedback when exploring and manipulating the narrative.
sent9: Some models [96,97] allow more in-depth refinement by letting the user specify elements that need to be changed and then evaluating all possible replacement and insertion actions.
sent10: Building upon this feature-based feedback, Shahaf et al. [98] designed a method to learn a personalized coverage function that can be optimized to find a personalized narrative.
sent11: Another approach by Bögel et al. [14] allows parametric interaction to modify the extracted graph in real time, helping the user understand how the narrative changes based on the parameters.
sent12: However, this approach requires the users to understand the underlying model parameters.
sent13: In this context, semantic interactions could be useful to aid users modify the model without deep understanding of the underlying parameters.
sent14: Semantic interactions [119] are used in sensemaking applications to directly reflect the analytical thought process of analysts about data (e.g., by using information about how analysts organize documents or highlight text), as opposed to parametric interaction that manipulates model parameters (e.g., sliders and keyword weights).
sent15: Thus, capturing a user model through semantic interaction could lead to a better narrative model."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s43,Misinformation in News,"Recent works have highlighted the need for future work to model source bias, information validity, transparency, and credibility as an effort to model and counter misinformation [51,56]. Existing narrative representations could be enhanced by including additional attributes in their representations and extraction algorithms.

Works on misinformation detection focus on the propagation structure and content to determine whether a certain article or publication contains misinformation [41,121]. Other methods rely on crowdsourcing [41] to detect misinformative content early. However, these methods do not model misinformation as part of an overarching narrative. Instead, they focus on local elements (e.g., a specific event). Thus, a holistic narrative approach could be useful in this context.

The issue of misinformation is also highly relevant for a series of recent works on disaster tracking by using news narrative extraction [123,131,134]. However, none of these methods address this issue and rely on the underlying assumption that the set does not contain false or misleading information. Thus, creating a narrative extraction model that accounts for misinformation would be of vital importance in the context of disaster tracking.","[['b50', 'b55'], ['b40', 'b120'], ['b130', 'b122', 'b133']]","[['b50', 'b55'], ['b40', 'b120'], ['b130', 'b122', 'b133']]",7,"sent1: Recent works have highlighted the need for future work to model source bias, information validity, transparency, and credibility as an effort to model and counter misinformation [51,56].
sent2: Existing narrative representations could be enhanced by including additional attributes in their representations and extraction algorithms.
sent3: Works on misinformation detection focus on the propagation structure and content to determine whether a certain article or publication contains misinformation [41,121].
sent4: Other methods rely on crowdsourcing [41] to detect misinformative content early.
sent5: However, these methods do not model misinformation as part of an overarching narrative.
sent6: Instead, they focus on local elements (e.g., a specific event).
sent7: Thus, a holistic narrative approach could be useful in this context.
sent8: The issue of misinformation is also highly relevant for a series of recent works on disaster tracking by using news narrative extraction [123,131,134].
sent9: However, none of these methods address this issue and rely on the underlying assumption that the set does not contain false or misleading information.
sent10: Thus, creating a narrative extraction model that accounts for misinformation would be of vital importance in the context of disaster tracking."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s5,TDT Query,"Initial Query ((""event* thread*"" OR ""link* news article*"" OR ""article network"" OR ""event* map"" OR ""narrative* map"" OR ""gener* information map"" OR ""information map gener*"" OR ""information map extract*"" OR ""event* timeline* analysis"" OR ""topic anatomy"" OR (""event graph"" AND ""text"") OR ""extract* story chain*"" OR ""find* story chain*"" OR ""story* gener*"" OR ""topic retrospection"" OR ""track new events"" OR (""connect* the dots"" AND ""algorithm*"") OR ""discover* event episodes"" OR ""event* track*"" OR ""topic* chain*"" OR (""building"" AND ""timeline*"") OR ""evolutionary theme* pattern*"" OR ""evolutionary topic* pattern*"" OR ""metro map*"" OR ""generate timeline*"" OR ""extract timeline*"") AND (""news*"" OR ""intel* analys*"" OR ""journalism"" OR ""social media*"" OR ""information overload"" OR ""sensemaking"" OR ""sense making"" OR ""information map*"" OR ""storyline generation"" OR ""event* evolution"" OR ""evolution graph*""))  to obtain a computational representation of the narrative and then use it to analyze the narrative [79,95]. However, they do not provide new insight into the extraction task itself, unless they include a novel extraction method as well.

Moreover, we exclude interactive narratives, as these are a fundamentally different type of narratives where the story can be changed through user feedback and actions [19], which would not make sense in the context of news narratives.

However, while the underlying story cannot be changed, it might still be possible to interact with the narrative model.

In fact, several works rely on interactivity at a presentation level.

Finally, we exclude works that focus on news narratives extracted from social media [9,62], as social media narratives follow a different approach that requires not only analyzing content but also the users spreading it, leading to unique challenges that are left beyond the scope of this survey.","[['b78', 'b94'], ['b18'], [], [], ['b8', 'b61']]","[['b78', 'b94'], ['b18'], [], [], ['b8', 'b61']]",5,"sent1: Initial Query ((""event* thread*"" OR ""link* news article*"" OR ""article network"" OR ""event* map"" OR ""narrative* map"" OR ""gener* information map"" OR ""information map gener*"" OR ""information map extract*"" OR ""event* timeline* analysis"" OR ""topic anatomy"" OR (""event graph"" AND ""text"") OR ""extract* story chain*"" OR ""find* story chain*"" OR ""story* gener*"" OR ""topic retrospection"" OR ""track new events"" OR (""connect* the dots"" AND ""algorithm*"") OR ""discover* event episodes"" OR ""event* track*"" OR ""topic* chain*"" OR (""building"" AND ""timeline*"") OR ""evolutionary theme* pattern*"" OR ""evolutionary topic* pattern*"" OR ""metro map*"" OR ""generate timeline*"" OR ""extract timeline*"") AND (""news*"" OR ""intel* analys*"" OR ""journalism"" OR ""social media*"" OR ""information overload"" OR ""sensemaking"" OR ""sense making"" OR ""information map*"" OR ""storyline generation"" OR ""event* evolution"" OR ""evolution graph*""))  to obtain a computational representation of the narrative and then use it to analyze the narrative [79,95].
sent2: However, they do not provide new insight into the extraction task itself, unless they include a novel extraction method as well.
sent3: Moreover, we exclude interactive narratives, as these are a fundamentally different type of narratives where the story can be changed through user feedback and actions [19], which would not make sense in the context of news narratives.
sent4: However, while the underlying story cannot be changed, it might still be possible to interact with the narrative model.
sent5: In fact, several works rely on interactivity at a presentation level.
sent6: Finally, we exclude works that focus on news narratives extracted from social media [9,62], as social media narratives follow a different approach that requires not only analyzing content but also the users spreading it, leading to unique challenges that are left beyond the scope of this survey."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s10,News Narra�ve Extrac�on,"Events   Table 1 summarizes the reviewed articles. In particular, we include the following columns in this table: event resolution, number of stories, structure, type of approach, and event representation. We now provide a brief description of these elements and their possible values.

Event resolution refers to the abstraction level at which the events are extracted. As mentioned in the scope definition, we consider three levels: sentences, documents, and clusters. Sentence-level works represent events as either a single sentence (e.g., the most important sentence or a headline) or a set of sentences (e.g., a sample of representative sentences).

Document-level works represent events directly as a single document (e.g., a full news article). Cluster-level works represent events as sets of documents (e.g., multiple news articles that talk about the same basic event). Structure represents whether the extraction method generates a linear structure of events (e.g., a timeline [96,124]) or a graph-like structure (e.g., a directed acyclic graph [51] or tree [67]). Figure 3 exemplifies these concepts.

Number of stories refers to whether the method is designed to handle a single storyline or multiple storylines. Recall that our definition of a story as a sequence of events. Most timeline extraction methods extract a single story, but some of them extract parallel timelines, where each timeline represents a different story from the data [56,130]. In contrast,","[[], [], ['b123', 'b95', 'b66', 'b50'], ['b129', 'b55']]","[[], [], ['b123', 'b95', 'b66', 'b50'], ['b129', 'b55']]",6,"sent1: Events   Table 1 summarizes the reviewed articles.
sent2: In particular, we include the following columns in this table: event resolution, number of stories, structure, type of approach, and event representation.
sent3: We now provide a brief description of these elements and their possible values.
sent4: Event resolution refers to the abstraction level at which the events are extracted.
sent5: As mentioned in the scope definition, we consider three levels: sentences, documents, and clusters.
sent6: Sentence-level works represent events as either a single sentence (e.g., the most important sentence or a headline) or a set of sentences (e.g., a sample of representative sentences).Document-level works represent events directly as a single document (e.g., a full news article).
sent7: Cluster-level works represent events as sets of documents (e.g., multiple news articles that talk about the same basic event).
sent8: Structure represents whether the extraction method generates a linear structure of events (e.g., a timeline [96,124]) or a graph-like structure
sent9: (e.g., a directed acyclic graph [51] or tree [67]). Figure 3 exemplifies these concepts.
sent10: Number of stories refers to whether the method is designed to handle a single storyline or multiple storylines.
sent11: Recall that our definition of a story as a sequence of events.
sent12: Most timeline extraction methods extract a single story, but some of them extract parallel timelines, where each timeline represents a different story from the data [56,130].
sent13: In contrast,"
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s31,Traditional IR Metrics,"Several works-about a third of the reviewed articles-rely on classical evaluation metrics such as accuracy, precision, recall, and the 1 score [14, 17, 23, 28, 46-48, 52, 63, 76, 106, 109, 116, 117, 126, 127] taken from traditional information retrieval and machine learning literature. In particular, these approaches evaluate the quality of the output by measuring whether events or their connections were identified correctly. Some methods also use variations of these basic metrics, such as the mean average precision [10,77] over multiple dates.

Summarization Metrics Specialized metrics from the summarization domain have also been used to evaluate narratives in several works-about a third of the reviewed works use them.

In particular, ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics [81] have been used to evaluate the output of narrative extraction methods, mostly in timeline summarization works with a sentence-level event resolution [10,23,49,55,58,60,61,77,108,110,125,130], but also in works with a document-level resolution [47,131,133,134]. .

In Equation 2, represents the length of the -gram, and represents the reference summaries (i.e., the ground truth). Count match (gram ) represents the maximum number of -grams that co-occur in a candidate summary and the reference summaries. Count (gram ) represents the number of -grams in the reference summaries.

An alternative is to measure the average summary-to-document content similarity where the summary is compared against the documents in the data set using a text similarity measure (e.g., cosine similarity) [20,21].

We note that these metrics are mostly used with linear representations rather than graph-based models-only three of the reviewed works that extract graphs use summarization metrics [20,21,47]. This contrasts heavily with the case of traditional information retrieval metrics, where the split was much more balanced between linear (~40%) and graph representations (~60%). This might be due to the inability of these metrics to handle complex structures.

Ranking Metrics Other works rely on ranking-based metrics, like those used in traditional search tasks from information retrieval. For example, Wang et al. [116] use a relevance-based approach to evaluate their event phase summaries. Liao et al. [61] evaluate the ranking performance of WILSON with the mean reciprocal rank and discounted cumulative gain [6]. Cai et al. [17] use the normalized discounted cumulative gain [128] to evaluate all their events.

Clustering Metrics Liu et al. [66,67] used clustering metrics to evaluate the event nodes-which are represented as clusters of articles-in their Story Forest method. In particular, they use the homogeneity, completeness, and V-measure scores [93]. These metrics require labeled data sets to be computed, thus they are supervised despite being designed to evaluate unsupervised clustering methods. In particular, homogeneity is larger when each extracted cluster only contains members of a single class. In contrast, completeness is maximized when all members of a true class are in the same cluster. Finally, the V-measure takes both of these metrics and computes the harmonic mean between them, similar to how the 1 score treats precision and recall in traditional classification metrics. We note that none of the other events as clusters methods used these metrics or other similar clustering metrics to evaluate their models. Instead, they relied on traditional information retrieval metrics like accuracy, precision, recall, and the 1 score.","[['b76', None, 'b9'], [], ['b132', 'b59', 'b57', 'b22', 'b60', 'b124', 'b76', 'b129', 'b46', 'b133', 'b54', 'b80', 'b107', 'b9', 'b109', 'b48', 'b130'], [], ['b20', 'b19'], ['b20', 'b46', 'b19'], ['b115', 'b16', 'b60', 'b5', 'b127'], ['b66', 'b92', 'b65']]","[['b76', None, 'b9'], [], ['b132', 'b59', 'b57', 'b22', 'b60', 'b124', 'b76', 'b129', 'b46', 'b133', 'b54', 'b80', 'b107', 'b9', 'b109', 'b48', 'b130'], [], ['b20', 'b19'], ['b20', 'b46', 'b19'], ['b115', 'b16', 'b60', 'b5', 'b127'], ['b66', 'b92', 'b65']]",33,"sent1: Several works-about a third of the reviewed articles-rely on classical evaluation metrics such as accuracy, precision, recall, and the 1 score [14, 17, 23, 28, 46-48, 52, 63, 76, 106, 109, 116, 117, 126, 127] taken from traditional information retrieval and machine learning literature.
sent2: In particular, these approaches evaluate the quality of the output by measuring whether events or their connections were identified correctly.
sent3: Some methods also use variations of these basic metrics, such as the mean average precision [10,77] over multiple dates.
sent4: Summarization Metrics Specialized metrics from the summarization domain have also been used to evaluate narratives in several works-about a third of the reviewed works use them.
sent5: In particular, ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics [81] have been used to evaluate the output of narrative extraction methods, mostly in timeline summarization works with a sentence-level event resolution [10,23,49,55,58,60,61,77,108,110,125,130], but also in works with a document-level resolution [47,131,133,134]. .
sent6: In Equation 2, represents the length of the -gram, and represents the reference summaries (i.e., the ground truth).
sent7: Count match (gram ) represents the maximum number of -grams that co-occur in a candidate summary and the reference summaries.
sent8: Count (gram ) represents the number of -grams in the reference summaries.
sent9: An alternative is to measure the average summary-to-document content similarity where the summary is compared against the documents in the data set using a text similarity measure (e.g., cosine similarity) [20,21].
sent10: We note that these metrics are mostly used with linear representations rather than graph-based models-only three of the reviewed works that extract graphs use summarization metrics [20,21,47].
sent11: This contrasts heavily with the case of traditional information retrieval metrics, where the split was much more balanced between linear (~40%) and graph representations (~60%).
sent12: This might be due to the inability of these metrics to handle complex structures.
sent13: Ranking Metrics Other works rely on ranking-based metrics, like those used in traditional search tasks from information retrieval.
sent14: For example, Wang et al. [116] use a relevance-based approach to evaluate their event phase summaries.
sent15: Liao et al. [61] evaluate the ranking performance of WILSON with the mean reciprocal rank and discounted cumulative gain [6].
sent16: Cai et al. [17] use the normalized discounted cumulative gain [128] to evaluate all their events.
sent17: Clustering Metrics Liu et al. [66,67] used clustering metrics to evaluate the event nodes-which are represented as clusters of articles-in their Story Forest method.
sent18: In particular, they use the homogeneity, completeness, and V-measure scores [93].
sent19: These metrics require labeled data sets to be computed, thus they are supervised despite being designed to evaluate unsupervised clustering methods.
sent20: In particular, homogeneity is larger when each extracted cluster only contains members of a single class.
sent21: In contrast, completeness is maximized when all members of a true class are in the same cluster.
sent22: Finally, the V-measure takes both of these metrics and computes the harmonic mean between them, similar to how the 1 score treats precision and recall in traditional classification metrics.
sent23: We note that none of the other events as clusters methods used these metrics or other similar clustering metrics to evaluate their models.
sent24: Instead, they relied on traditional information retrieval metrics like accuracy, precision, recall, and the 1 score."
256900869,A Survey on Event-based News Narrative Extraction,Computer Science,https://www.semanticscholar.org/paper/0ff9e62181e9eb0817407ecd383b9a4d7263076b,s37,Evaluation Methods,"Benchmark Data Sets In general, most works collect their own data or use a subset of a pre-existing news data repository. For example, some use data sets from TDT literature [20,21,76,117], DUC/TAC conferences [23], or other general news repositories [24]. Most works do not publish their data sets. However, there is a subset of timeline summarization works that have provided evaluation data sets that have been adopted in several works as benchmark data. We present these data sets in Table 4. These data sets are appropriate for the ""events as sentences"" resolution level that timeline summarization uses. However, they do not provide a direct way to evaluate methods that use other resolution levels. Furthermore, we note that there are no such benchmark data sets for the other resolution levels of the narrative extraction tasks considered in this survey. The lack of appropriate benchmark data for the document-level and cluster-level resolutions makes comparing methods harder and makes replicability harder.","[['b22', 'b20', 'b116', 'b23', 'b75', 'b19']]","[['b22', 'b20', 'b116', 'b23', 'b75', 'b19']]",6,"sent1: Benchmark Data Sets In general, most works collect their own data or use a subset of a pre-existing news data repository.
sent2: For example, some use data sets from TDT literature [20,21,76,117], DUC/TAC conferences [23], or other general news repositories [24].
sent3: Most works do not publish their data sets.
sent4: However, there is a subset of timeline summarization works that have provided evaluation data sets that have been adopted in several works as benchmark data.
sent5: We present these data sets in Table 4.
sent6: These data sets are appropriate for the ""events as sentences"" resolution level that timeline summarization uses.
sent7: However, they do not provide a direct way to evaluate methods that use other resolution levels.
sent8: Furthermore, we note that there are no such benchmark data sets for the other resolution levels of the narrative extraction tasks considered in this survey.
sent9: The lack of appropriate benchmark data for the document-level and cluster-level resolutions makes comparing methods harder and makes replicability harder."
257386000,Assessment of the Healthcare Administration of Senior Citizens from Survey Data using Sentiment Analysis,"Computer Science, Medicine",https://www.semanticscholar.org/paper/687f93be9a6c5f9deb053ca8a2fa1875afeb7a5d,s4,C. VADER Sentiment Analysis,"VADER is also known as Valence Aware Lexicon and Sentiment Reasoner. The VADER vocabulary was created using conventional sentiment lexicons. Also, this work offers machine learning techniques for sentiment analysis as well as sentiment intensity and orientation lexicons. In order to better understand how the public feels about different entities, this sentiment analysis methods try to identify the feelings of written reviews. Emotions are linked to many characteristics of a product or service as part of the analysis of consumer feedback data. Moreover, VADER sentiment outperformed seven sentiment analysis lexicons, either better or equally well. [18][19] [20].

According to a study by [21], VADER maintains and even improves on the advantages of conventional sentiment lexicons like LIWC or Linguistic Inquiry and Word Count: it is larger, yet just as easily examined, understood, and swiftly deployed without requiring substantial learning/training), and it is easily extended. VADER differs from LIWC in that it is both more perceptive of sentiment expressions in social media and more tolerant of generalization to other domains. This can be downloaded and used without charge from the website.

Other research confirms the ease of use of VADER's rulebased sentiment analysis. A compilation of lexical features and their corresponding emotion metrics make up this document. Several guidelines are developed based on the language's grammatical and syntactical usage, and these rules are utilized to assess the text's mood. VADER employs a rule-based method and assigns values to each word in the text in order to consider both the sentiment category and the intensity or strength of the text in addition to the sentiment category. It also performs far faster than machine learning algorithms [22] [23].

VADER excels across a range of domain types. Compared to machine learning techniques, VADER has a number of advantages. It is firstly quick and computationally effective. The second advantage is that the terminology and regulations of the VADER are clear and not hidden. Because of this, VADER is easy to understand, build upon, and alter. By setting the threshold at 0.05, VADER is a preferable option if processing the sentiment quickly and if it was the only thing that had been planned. VADER also adheres to grammatical and syntactical rules for expressing and highlighting sentiment intensity. VADER outperforms Text blob and NLTK sentiment analysis technologies in terms of performance. [24].

According to empirical findings, the technique utilized is the best technique for ranking many choices. Additionally, users of the healthcare sector's decision-making processes and healthcare providers' goals for quality improvement can both benefit from ranking information.","[['b21', 'b19'], ['b22'], ['b23', 'b24'], ['b25'], []]","[['b21', 'b19'], ['b22'], ['b23', 'b24'], ['b25'], []]",6,"sent1: VADER is also known as Valence Aware Lexicon and Sentiment Reasoner.
sent2: The VADER vocabulary was created using conventional sentiment lexicons.
sent3: Also, this work offers machine learning techniques for sentiment analysis as well as sentiment intensity and orientation lexicons.
sent4: In order to better understand how the public feels about different entities, this sentiment analysis methods try to identify the feelings of written reviews.
sent5: Emotions are linked to many characteristics of a product or service as part of the analysis of consumer feedback data.
sent6: Moreover, VADER sentiment outperformed seven sentiment analysis lexicons, either better or equally well.
sent7: [18][19] [20]. According to a study by [21], VADER maintains and even improves on the advantages of conventional sentiment lexicons like LIWC or Linguistic Inquiry and Word Count: it is larger, yet just as easily examined, understood, and swiftly deployed without requiring substantial learning/training), and it is easily extended.
sent8: VADER differs from LIWC in that it is both more perceptive of sentiment expressions in social media and more tolerant of generalization to other domains.
sent9: This can be downloaded and used without charge from the website.
sent10: Other research confirms the ease of use of VADER's rulebased sentiment analysis.
sent11: A compilation of lexical features and their corresponding emotion metrics make up this document.
sent12: Several guidelines are developed based on the language's grammatical and syntactical usage, and these rules are utilized to assess the text's mood.
sent13: VADER employs a rule-based method and assigns values to each word in the text in order to consider both the sentiment category and the intensity or strength of the text in addition to the sentiment category.
sent14: It also performs far faster than machine learning algorithms
sent15: [22] [23].VADER excels across a range of domain types.
sent16: Compared to machine learning techniques, VADER has a number of advantages.
sent17: It is firstly quick and computationally effective.
sent18: The second advantage is that the terminology and regulations of the VADER are clear and not hidden.
sent19: Because of this, VADER is easy to understand, build upon, and alter.
sent20: By setting the threshold at 0.05, VADER is a preferable option if processing the sentiment quickly and if it was the only thing that had been planned.
sent21: VADER also adheres to grammatical and syntactical rules for expressing and highlighting sentiment intensity.
sent22: VADER outperforms Text blob and NLTK sentiment analysis technologies in terms of performance.[24].
sent23: According to empirical findings, the technique utilized is the best technique for ranking many choices.
sent24: Additionally, users of the healthcare sector's decision-making processes and healthcare providers' goals for quality improvement can both benefit from ranking information."
257386000,Assessment of the Healthcare Administration of Senior Citizens from Survey Data using Sentiment Analysis,"Computer Science, Medicine",https://www.semanticscholar.org/paper/687f93be9a6c5f9deb053ca8a2fa1875afeb7a5d,s2,A. Healthcare Services for Senior Citizens,"Discussions about how seniors use healthcare services are becoming more and more crucial as the senior population increases. A study being conducted in South Korea aims to create an integrated healthcare service system that is centered on elderly citizens, meeting their needs in daily life and promoting well-being, wellness, and well-dying. A natural structure of regular care, professional care, and rehabilitation for senior members of society in line with the responsibilities of the patients, their families, and caregivers are required for the implementation of the integrated medical care system for elderly users presented in this study [13].

The study [14]'s goal was to determine what senior citizens need from ""embedded retirement facilities (ERFs),"" multipurpose, and community-based care facilities for the elderly in mainland China. This study is based on questionnaire data collected in northeast China. The findings show that senior citizens' healthcare services are deemed to be the most significant. Senior citizens use community-based facilities, but decision-makers and facility administrators frequently fail to consider their needs. Seniors in China also tend to be inactive and largely silent in both formal and informal civic involvement because they typically believe that policymakers would take notice of and accommodate their needs.

The purpose of the [15] study is to evaluate older people's well-being to explore whether the data are consistent with previously announced changes in senior treatment in relation to the real resources provided to their patients. The respondents reported being generally satisfied with their lives. The results show that small-town residents felt substantially worse about their quality of life than seniors from large cities. This shows that the healthcare system continues to utterly fail to meet patients' actual demands, particularly in the elderly sector. Being open to a broader discussion about the diverse needs and resources that elderly people in rural and urban areas face is crucial for doing this.

The study of [6] attempts to assess the potential influences on elderly persons' use of healthcare in Davao City, in the Philippines. Various factors were discovered to be significant predictors of healthcare consumption through the use of multiple regression analysis. The findings demonstrated how socioeconomic demographic, personal characteristics and health insurance knowledge affect the way senior citizens use healthcare. By launching health insurance awareness campaigns and creating health-improving initiatives, policymakers and local government organizations may think about enhancing senior citizens' access to healthcare services.","[['b14'], ['b15'], ['b16'], ['b6']]","[['b14'], ['b15'], ['b16'], ['b6']]",4,"sent1: Discussions about how seniors use healthcare services are becoming more and more crucial as the senior population increases.
sent2: A study being conducted in South Korea aims to create an integrated healthcare service system that is centered on elderly citizens, meeting their needs in daily life and promoting well-being, wellness, and well-dying.
sent3: A natural structure of regular care, professional care, and rehabilitation for senior members of society in line with the responsibilities of the patients, their families, and caregivers are required for the implementation of the integrated medical care system for elderly users presented in this study [13].
sent4: The study [14]'s goal was to determine what senior citizens need from ""embedded retirement facilities (ERFs),"" multipurpose, and community-based care facilities for the elderly in mainland China.
sent5: This study is based on questionnaire data collected in northeast China.
sent6: The findings show that senior citizens' healthcare services are deemed to be the most significant.
sent7: Senior citizens use community-based facilities, but decision-makers and facility administrators frequently fail to consider their needs.
sent8: Seniors in China also tend to be inactive and largely silent in both formal and informal civic involvement because they typically believe that policymakers would take notice of and accommodate their needs.
sent9: The purpose of the [15] study is to evaluate older people's well-being to explore whether the data are consistent with previously announced changes in senior treatment in relation to the real resources provided to their patients.
sent10: The respondents reported being generally satisfied with their lives.
sent11: The results show that small-town residents felt substantially worse about their quality of life than seniors from large cities.
sent12: This shows that the healthcare system continues to utterly fail to meet patients' actual demands, particularly in the elderly sector.
sent13: Being open to a broader discussion about the diverse needs and resources that elderly people in rural and urban areas face is crucial for doing this.
sent14: The study of [6] attempts to assess the potential influences on elderly persons' use of healthcare in Davao City, in the Philippines.
sent15: Various factors were discovered to be significant predictors of healthcare consumption through the use of multiple regression analysis.
sent16: The findings demonstrated how socioeconomic demographic, personal characteristics and health insurance knowledge affect the way senior citizens use healthcare.
sent17: By launching health insurance awareness campaigns and creating health-improving initiatives, policymakers and local government organizations may think about enhancing senior citizens' access to healthcare services."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s69,Remote Sensing,"In remote sensing, different sensors can provide complementary information for earth observations, including hyperspectral data, multispectral data, light detection and ranging (LiDAR), synthetic aperture radar (SAR) data, etc. [165], [166]. For example, hyperspectral images provide information on land-cover categories based on spectral signatures, while SAR images provide dielectric properties and are not affected by clouds. To integrate SAR and optical images, [167] use a multimodal contrastive loss at the image and super-pixel level. To fuse multispectral and SAR images, SSCL [168] presents a unified framework of contrastive learning and masked prediction applicable to images with any number of channels. Change detection is also an important problem in remote sensing, which is used for damage assessment, environmental monitoring, etc. [169] uses a pseudo-Siamese network to regress the output between its two branches for cross-sensor image pairs. The feature distance between the outputs of the two branches is used to define a change measure. Additionally, [170] combines clustering and contrastive learning for change detection in a bi-temporal scene where paired images are captured by the optical sensor and SAR sensor, respectively. Geo-tagged audio recordings can also be used with contrastive learning to learn the correspondence with image data [171].","[['b169', 'b165', 'b170', 'b166', 'b168', 'b167', 'b164']]","[['b169', 'b165', 'b170', 'b166', 'b168', 'b167', 'b164']]",7,"sent1: In remote sensing, different sensors can provide complementary information for earth observations, including hyperspectral data, multispectral data, light detection and ranging (LiDAR), synthetic aperture radar (SAR) data, etc. [165], [166].
sent2: For example, hyperspectral images provide information on land-cover categories based on spectral signatures, while SAR images provide dielectric properties and are not affected by clouds.
sent3: To integrate SAR and optical images, [167] use a multimodal contrastive loss at the image and super-pixel level.
sent4: To fuse multispectral and SAR images, SSCL [168] presents a unified framework of contrastive learning and masked prediction applicable to images with any number of channels.
sent5: Change detection is also an important problem in remote sensing, which is used for damage assessment, environmental monitoring, etc. [169] uses a pseudo-Siamese network to regress the output between its two branches for cross-sensor image pairs.
sent6: The feature distance between the outputs of the two branches is used to define a change measure.
sent7: Additionally, [170] combines clustering and contrastive learning for change detection in a bi-temporal scene where paired images are captured by the optical sensor and SAR sensor, respectively.
sent8: Geo-tagged audio recordings can also be used with contrastive learning to learn the correspondence with image data [171]."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s76,A.1.1 Cross-modal Retrieval,"Image-text Retrieval. Image-text retrieval can take two forms: image-to-text and text-to-image retrieval which correspond to using images and text as the query modality respectively. The evaluation metric used to measure performance is Recall@K, with K typically set to 1, 5, or 10. Popular evaluation datasets include COCO [20] and Flickr30K [204]. Video-text Retrieval. Video-text retrieval mainly focuses on text-to-video retrieval, which has two subtasks: (a) retrieving a relevant video based on a text query, or (b) retrieving a video segment within a given video that matches a specific text description. In both cases, the evaluation metric is recall@K. Popular datasets for (a) include MSRVTT [205], YouCook2 [206], MSVD [207], etc., and for (b) include [208], ActivityNet Captions [209], etc.","[['b208', 'b206', 'b204', 'b19', 'b207', 'b205', 'b203']]","[['b208', 'b206', 'b204', 'b19', 'b207', 'b205', 'b203']]",7,"sent1: Image-text Retrieval. Image-text retrieval can take two forms: image-to-text and text-to-image retrieval which correspond to using images and text as the query modality respectively.
sent2: The evaluation metric used to measure performance is Recall@K, with K typically set to 1, 5, or 10.
sent3: Popular evaluation datasets include COCO [20] and Flickr30K [204].
sent4: Video-text Retrieval. Video-text retrieval mainly focuses on text-to-video retrieval, which has two subtasks: (a) retrieving a relevant video based on a text query, or (b) retrieving a video segment within a given video that matches a specific text description.
sent5: In both cases, the evaluation metric is recall@K. Popular datasets for (a) include MSRVTT [205], YouCook2 [206], MSVD [207], etc., and for (b) include [208], ActivityNet Captions [209], etc."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s11,Self-supervision in Multimodal Learning,"We first delineate the scope of SSML as considered in this survey as this term has been used inconsistently in the literature before. Self-supervision is more straightforward to define in a unimodal context by appealing to the label-free nature of different pretext tasks, e.g., as famously realized by instance discrimination [18] or masked prediction objectives [19]. In contrast, the situation in multimodal learning is more complex as the role of modalities and labels becomes blurred. For example, text is typically considered a label in supervised image captioning [20], but as an input modality in self-supervised multimodal vision and language representation learning [21]. In the multimodal context, the term self-supervision has been used to refer to at least four situations: (1) Labelfree learning from multimodal data that is automatically paired -such as movies with video and audio tracks [22], or images and depth data from RGBD cameras [23]. (2) Learning from multimodal data where one modality has been manually annotated, or two modalities have been manually paired, but this annotation has already been created for a different purpose, and can thus be considered free for the purpose of SSML pretraining. For example, matching image-caption pairs crawled from the web, as used by the seminal CLIP [21], is actually an example of supervised metric-learning [24], [25] where the pairing is the supervision. However, because both the modalities and the pairing are all freely available at scale, it is often described as self-supervised. Such uncurated incidentally created data is often lower-quality and noisier than purposecreated and curated datasets such as COCO [20] and Visual Genome [26]. (3) Learning from high-quality purpose annotated multimodal data (e.g., manually captioned images in the COCO [20]), but with self-supervised style objectives, e.g., Pixel-BERT [27]. (4) Finally, there are 'self-supervised' methods that use a mix of free and manually annotated multimodal data [28], [29].

For the purpose of this survey, we follow the spirit of self-supervision as aiming to scale up by breaking the manual annotation bottleneck. Thus we include methods that fall into the first two and fourth categories above in terms of being able to train on freely available data. We exclude methods only shown to work on manually curated datasets just because they apply typically ""self-supervised"" objectives (e.g., masked prediction) on curated datasets.","[['b27', 'b17', 'b22', 'b20', 'b25', 'b28', 'b21', 'b26', 'b1', 'b23', 'b18', 'b19', 'b24'], []]","[['b27', 'b17', 'b22', 'b20', 'b25', 'b28', 'b21', 'b26', 'b1', 'b23', 'b18', 'b19', 'b24'], []]",13,"sent1: We first delineate the scope of SSML as considered in this survey as this term has been used inconsistently in the literature before.
sent2: Self-supervision is more straightforward to define in a unimodal context by appealing to the label-free nature of different pretext tasks, e.g., as famously realized by instance discrimination [18] or masked prediction objectives [19].
sent3: In contrast, the situation in multimodal learning is more complex as the role of modalities and labels becomes blurred.
sent4: For example, text is typically considered a label in supervised image captioning [20], but as an input modality in self-supervised multimodal vision and language representation learning [21].
sent5: In the multimodal context, the term self-supervision has been used to refer to at least four situations: (1) Labelfree learning from multimodal data that is automatically paired -such as movies with video and audio tracks [22], or images and depth data from RGBD cameras [23].
sent6: (2) Learning from multimodal data where one modality has been manually annotated, or two modalities have been manually paired, but this annotation has already been created for a different purpose, and can thus be considered free for the purpose of SSML pretraining.
sent7: For example, matching image-caption pairs crawled from the web, as used by the seminal CLIP [21], is actually an example of supervised metric-learning [24], [25] where the pairing is the supervision.
sent8: However, because both the modalities and the pairing are all freely available at scale, it is often described as self-supervised.
sent9: Such uncurated incidentally created data is often lower-quality and noisier than purposecreated and curated datasets such as COCO [20] and Visual Genome [26].
sent10: (3) Learning from high-quality purpose annotated multimodal data (e.g., manually captioned images in the COCO [20]), but with self-supervised style objectives, e.g., Pixel-BERT [27].
sent11: (4) Finally, there are 'self-supervised' methods that use a mix of free and manually annotated multimodal data [28], [29].
sent12: For the purpose of this survey, we follow the spirit of self-supervision as aiming to scale up by breaking the manual annotation bottleneck.
sent13: Thus we include methods that fall into the first two and fourth categories above in terms of being able to train on freely available data.
sent14: We exclude methods only shown to work on manually curated datasets just because they apply typically ""self-supervised"" objectives (e.g., masked prediction) on curated datasets."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s12,Multimodal v.s. Multiview,"Multimodal learning and multiview learning are related, but distinct, concepts that are used interchangeably in some literature [30], [31]. Both aim to extract complementary information from more than one data source to improve performance at a task. However they differ in that multimodal learning focuses on learning from data that originates from multiple heterogeneous modalities of data, such as text, image, audio, or gene sequences (e.g., [32], [33], [34]). On the other hand, multiview learning deals with multiple views of data obtained from the same modality. For example, photos of an object from different viewpoints or audio recordings from spatially offset microphones (e.g., [35], [36], [37]). Multiview learning also spans situations where different features are extracted from a single modality of observation -such as amplitude and phase after the Fourier transform of an input. In this survey, we focus on multimodal learning only, where inputs are distinct heterogeneous modalities.","[['b29', 'b35', 'b31', 'b34', 'b36', 'b30', 'b33', 'b32']]","[['b29', 'b35', 'b31', 'b34', 'b36', 'b30', 'b33', 'b32']]",8,"sent1: Multimodal learning and multiview learning are related, but distinct, concepts that are used interchangeably in some literature [30], [31].
sent2: Both aim to extract complementary information from more than one data source to improve performance at a task.
sent3: However they differ in that multimodal learning focuses on learning from data that originates from multiple heterogeneous modalities of data, such as text, image, audio, or gene sequences (e.g., [32], [33], [34]).
sent4: On the other hand, multiview learning deals with multiple views of data obtained from the same modality.
sent5: For example, photos of an object from different viewpoints or audio recordings from spatially offset microphones (e.g., [35], [36], [37]).
sent6: Multiview learning also spans situations where different features are extracted from a single modality of observation -such as amplitude and phase after the Fourier transform of an input.
sent7: In this survey, we focus on multimodal learning only, where inputs are distinct heterogeneous modalities."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s18,Contrastive,"Contrastive methods typically use corresponding samples from different modalities as positive example pairs and noncorresponding samples as negative pairs. These pairs are then used to train a model to accurately distinguish positive and negative pairs using contrastive training objectives.

Given an anchor data point x a drawn from modality k, the other modality from the same instance is selected as positive sample x + , and non-corresponding points are regarded as negative samples x − . After encoding, the extracted representations for anchor, positive, and negative samples can be defined as r a = e(x a ), r + = e(x + ), and r − = e(x − ). Then, a general form of contrastive objective can be written as:

where sim(, ) is a similarity function between two inputs, and m is the number of negative samples. Contrastive Multiview Coding (CMC) [23] is one of the earliest works to explore the application of contrastive learning in the multimodal setting. This framework maximizes the mutual information between representations of different views (modalities) of the same scene while pushing apart the unmatched samples. The idea of maximizing mutual information among different modalities and performing crossmodal instance discrimination has been further developed and extended in various ways. AVTS [43] considers temporally synchronized audio-video pairs as positives and utilizes curriculum Learning to gradually learn hard negatives. To achieve spatial alignment, AVSA [44] samples video and audio clips from different spatial viewing directions and maximizes the mutual information of audio-visual pairs in the same direction. MultiModal Versatile (MMV) networks [45] maximize the mutual information among vision, audio, and text pairs that temporally co-occur. Video-Audio-Text transformer (VATT) [46] uses a similar contrastive objective, but studies a modality-agnostic, single-backbone transformer by sharing weights among the three modalities.

Contrastive learning has also shown great potential for scaling up to larger models and datasets. CLIP [21] achieves strong zero-shot performance when pretraining on a large dataset of 400 million image-language pairs, by simply predicting the pairing. This paradigm has been successfully adopted in various other domains, such as AudioCLIP [47], VideoCLIP [48], CLIP4CLIP [49], and pointCLIP [50]. As collecting paired data may require additional curation steps, ALIGN [51] studies whether pretraining on noisy pairs still achieves strong performance and confirms that it does. As the model and dataset size increase, training becomes more computationally expensive and methods are developed to improve efficiency. CLIPPO [52] unifies CLIP's input modalities by using a pure pixel-based model for image, text, and multimodal tasks. FLIP [53] randomly masks out and removes a large set of image patches during training. Both achieve comparable or better performance than CLIP with improved efficiency.

Besides inter-modal contrastive learning for alignment, conventional intra-modality learning can provide an additional cue. SLIP [54], COOKIE [55], and CrossCLR [56] all add intra-modal contrastive losses alongside cross-modal learning leading to improved performance in image-text and video-text problems. CrossPoint [57] learns cross-modal correspondence from point clouds and rendered images and intra-modal correspondence from different views of a 3D point cloud. However, the addition of within-modality instance discrimination is not always beneficial. For example, AVID [58] shows that naively adding within-modality instance discrimination may harm the overall performance as it is an easier pretext task compared to cross-modal discrimination and can be partially solved by matching lowlevel data statistics.","[[], [], ['b43', 'b22', 'b45', 'b42', 'b44'], ['b52', 'b49', 'b50', 'b20', 'b47', 'b51', 'b46', 'b48'], ['b57', 'b56', 'b55', 'b54', 'b53']]","[[], [], ['b43', 'b22', 'b45', 'b42', 'b44'], ['b52', 'b49', 'b50', 'b20', 'b47', 'b51', 'b46', 'b48'], ['b57', 'b56', 'b55', 'b54', 'b53']]",18,"sent1: Contrastive methods typically use corresponding samples from different modalities as positive example pairs and noncorresponding samples as negative pairs.
sent2: These pairs are then used to train a model to accurately distinguish positive and negative pairs using contrastive training objectives.
sent3: Given an anchor data point x a drawn from modality k, the other modality from the same instance is selected as positive sample x + , and non-corresponding points are regarded as negative samples x − .
sent4: After encoding, the extracted representations for anchor, positive, and negative samples can be defined as r a = e(x a ), r + = e(x + ), and r − = e(x − ).
sent5: Then, a general form of contrastive objective can be written as:where sim(, ) is a similarity function between two inputs, and m is the number of negative samples.
sent6: Contrastive Multiview Coding (CMC)
sent7: [23] is one of the earliest works to explore the application of contrastive learning in the multimodal setting.
sent8: This framework maximizes the mutual information between representations of different views (modalities) of the same scene while pushing apart the unmatched samples.
sent9: The idea of maximizing mutual information among different modalities and performing crossmodal instance discrimination has been further developed and extended in various ways.
sent10: AVTS [43] considers temporally synchronized audio-video pairs as positives and utilizes curriculum Learning to gradually learn hard negatives.
sent11: To achieve spatial alignment, AVSA [44] samples video and audio clips from different spatial viewing directions and maximizes the mutual information of audio-visual pairs in the same direction.
sent12: MultiModal Versatile (MMV) networks [45] maximize the mutual information among vision, audio, and text pairs that temporally co-occur.
sent13: Video-Audio-Text transformer (VATT) [46] uses a similar contrastive objective, but studies a modality-agnostic, single-backbone transformer by sharing weights among the three modalities.
sent14: Contrastive learning has also shown great potential for scaling up to larger models and datasets.
sent15: CLIP [21] achieves strong zero-shot performance when pretraining on a large dataset of 400 million image-language pairs, by simply predicting the pairing.
sent16: This paradigm has been successfully adopted in various other domains, such as AudioCLIP [47], VideoCLIP [48], CLIP4CLIP [49], and pointCLIP [50].
sent17: As collecting paired data may require additional curation steps, ALIGN [51] studies whether pretraining on noisy pairs still achieves strong performance and confirms that it does.
sent18: As the model and dataset size increase, training becomes more computationally expensive and methods are developed to improve efficiency.
sent19: CLIPPO [52] unifies CLIP's input modalities by using a pure pixel-based model for image, text, and multimodal tasks.
sent20: FLIP [53] randomly masks out and removes a large set of image patches during training.
sent21: Both achieve comparable or better performance than CLIP with improved efficiency.
sent22: Besides inter-modal contrastive learning for alignment, conventional intra-modality learning can provide an additional cue.
sent23: SLIP [54], COOKIE [55], and CrossCLR [56] all add intra-modal contrastive losses alongside cross-modal learning leading to improved performance in image-text and video-text problems.
sent24: CrossPoint [57] learns cross-modal correspondence from point clouds and rendered images and intra-modal correspondence from different views of a 3D point cloud.
sent25: However, the addition of within-modality instance discrimination is not always beneficial.
sent26: For example, AVID [58] shows that naively adding within-modality instance discrimination may harm the overall performance as it is an easier pretext task compared to cross-modal discrimination and can be partially solved by matching lowlevel data statistics."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s19,Matching Prediction,"Matching prediction, also known as alignment prediction, aims to predict whether a pair of samples from two input modalities are matched (positive pair) or not (negative pair). For example, if a piece of text corresponds to an image's caption. A key difference between contrastive learning and matching prediction is that in a mini-batch, the former calculates the similarity between a positive pair and all of the other negative pairs, while the latter labels individual tuples as positive or negative. Denoting p to be the twoclass probability of a matched pair, the matching prediction loss minimizes a binary cross-entropy loss (BCE):

where the pseudo-label z i is a one-hot vector representing whether the inputs are matched. Matching prediction is widely used for modeling audiovisual correspondence (AVC). AVC was introduced by L 3 -Net [22], which uses a fused representation from audio and video to make a binary prediction of whether the audioimage pair is from the same video clip. This strategy is adopted by AVE-Net [59] by only using Euclidean distance alignment without fusion, leading to localization of the object that sounds within an image. Owens and Efros [60] also utilize this pretext task, but instead take temporal video frames and audio as input. They construct negative pairs from the same video to increase pretext task difficulty, improving learned representations and localization accuracy.

In order to achieve better audio-visual localization and separation, pixel-wise matching prediction has been proposed as a pretext task. One such example is the mix-andseparate method [61], which combines audio signals from different videos to create an input mixture. The network is trained to separate the audio sources by predicting binary spectrogram masks based on corresponding video frames. Building on this idea, Sound of Motions [62] incorporates motion trajectory modeling, while Music Gesture [63] uses human body and hand movements to guide the separation.

Image-text matching (ITM) is an effective objective for vision-language pretraining first proposed by UNITER [64] before CLIP. It fed the global cross-modal representation to a binary classifier to predict whether the input pair is matched. ITM has been adopted by various algorithms, including ViLBERT [65], BLIP [66], FLAVA [67], etc. ITM can also be complementary to contrastive learning. For example, ALBEF [68] samples hard negatives from the contrastive batch in order to train the ITM objective with more informative negatives. Discussion. Instance discrimination has emerged as an effective and versatile framework for learning representations from multiple modalities. A key aspect of many multimodal instance discrimination methods is the strategy for sampling positive and negative samples across modalities, which can significantly impact the learned representation. For example, two non-corresponding samples will be treated as a negative pair regardless of their semantic similarity. This process can produce both false positive and false negative pairs thus introducing label noise [69], [70]. It also means that most negatives will be very easy to distinguish, leading to hard negative mining being a topic of ongoing study [71], [72]. While contrastive learning is effective, it often requires a large batch size to obtain enough negative samples to avoid mode collapse. This is resource intensive, especially in terms of memory, and has led to active research in more efficient contrastive learning [73]. When it comes to modeling correspondence or interaction between modalities, contrastive models typically take cross-modal dot products after obtaining embeddings from modality-specific encoders. This has the advantage of simplicity, but lacks the ability to model rich interactions between the modalities. On the other hand, matching prediction can be carried out on a joint representation of both modalities. The latter approach enables richer cross-modal interactions. Thus, these two objectives are sometimes combined to achieve a complementary effect.","[[], ['b58', 'b21', 'b59'], ['b61', 'b62', 'b60'], ['b64', 'b67', 'b68', 'b70', 'b71', 'b66', 'b63', 'b65', 'b72', 'b69']]","[[], ['b58', 'b21', 'b59'], ['b61', 'b62', 'b60'], ['b64', 'b67', 'b68', 'b70', 'b71', 'b66', 'b63', 'b65', 'b72', 'b69']]",16,"sent1: Matching prediction, also known as alignment prediction, aims to predict whether a pair of samples from two input modalities are matched (positive pair) or not (negative pair).
sent2: For example, if a piece of text corresponds to an image's caption.
sent3: A key difference between contrastive learning and matching prediction is that in a mini-batch, the former calculates the similarity between a positive pair and all of the other negative pairs, while the latter labels individual tuples as positive or negative.
sent4: Denoting p to be the twoclass probability of a matched pair, the matching prediction loss minimizes a binary cross-entropy loss (BCE):
sent5: where the pseudo-label z i is a one-hot vector representing whether the inputs are matched.
sent6: Matching prediction is widely used for modeling audiovisual correspondence (AVC).
sent7: AVC was introduced by L 3 -Net [22], which uses a fused representation from audio and video to make a binary prediction of whether the audioimage pair is from the same video clip.
sent8: This strategy is adopted by AVE-Net [59] by only using Euclidean distance alignment without fusion, leading to localization of the object that sounds within an image.
sent9: Owens and Efros [60] also utilize this pretext task, but instead take temporal video frames and audio as input.
sent10: They construct negative pairs from the same video to increase pretext task difficulty, improving learned representations and localization accuracy.
sent11: In order to achieve better audio-visual localization and separation, pixel-wise matching prediction has been proposed as a pretext task.
sent12: One such example is the mix-andseparate method [61], which combines audio signals from different videos to create an input mixture.
sent13: The network is trained to separate the audio sources by predicting binary spectrogram masks based on corresponding video frames.
sent14: Building on this idea, Sound of Motions [62] incorporates motion trajectory modeling, while Music Gesture [63] uses human body and hand movements to guide the separation.
sent15: Image-text matching (ITM) is an effective objective for vision-language pretraining first proposed by UNITER [64] before CLIP.
sent16: It fed the global cross-modal representation to a binary classifier to predict whether the input pair is matched.
sent17: ITM has been adopted by various algorithms, including ViLBERT [65], BLIP [66], FLAVA [67], etc.
sent18: ITM can also be complementary to contrastive learning.
sent19: For example, ALBEF [68] samples hard negatives from the contrastive batch in order to train the ITM objective with more informative negatives.
sent20: Discussion. Instance discrimination has emerged as an effective and versatile framework for learning representations from multiple modalities.
sent21: A key aspect of many multimodal instance discrimination methods is the strategy for sampling positive and negative samples across modalities, which can significantly impact the learned representation.
sent22: For example, two non-corresponding samples will be treated as a negative pair regardless of their semantic similarity.
sent23: This process can produce both false positive and false negative pairs thus introducing label noise [69], [70].
sent24: It also means that most negatives will be very easy to distinguish, leading to hard negative mining being a topic of ongoing study [71], [72].
sent25: While contrastive learning is effective, it often requires a large batch size to obtain enough negative samples to avoid mode collapse.
sent26: This is resource intensive, especially in terms of memory, and has led to active research in more efficient contrastive learning [73].
sent27: When it comes to modeling correspondence or interaction between modalities, contrastive models typically take cross-modal dot products after obtaining embeddings from modality-specific encoders.
sent28: This has the advantage of simplicity, but lacks the ability to model rich interactions between the modalities.
sent29: On the other hand, matching prediction can be carried out on a joint representation of both modalities.
sent30: The latter approach enables richer cross-modal interactions.
sent31: Thus, these two objectives are sometimes combined to achieve a complementary effect."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s20,Clustering,"Clustering methods assume that applying end-to-end trained clustering will lead to the grouping of the data by semantically salient characteristics. In practice, these methods iteratively predict the cluster assignments of the encoded representation, and use these predictions, also known as pseudo labels, as supervision signals to update the feature representation. Multimodal clustering provides the opportunity to learn multimodal representations and also improve conventional clustering by using each modality's pseudolabels to supervise the other.

Formally, let the predictive head g be a standard clustering method such as K-means (i.e., with no learnable parameters). It clusters the encoded representations into M distinct clusters based on geometric similarity. We denote the cluster prediction to be C i for input x i . Clusteringbased methods minimize a cross-entropy loss between the predicted cluster assignments C i and the pseudo-labels z i . Proposed by DeepCluster [74], a widely used approach to generate pseudo-labels is to jointly learn a d × M centroid matrix T and the cluster assignments z i of each data sample x i by optimizing the following objective:

This results in a set of optimal assignments (z i * ) n≤N and a centroid matrix T * . We use the assignment as the pseudolabels, while the centroid matrix is discarded. After that, the model can be optimized using:

Then, the clustering process is repeated on the updated representations, and thus the model can be updated iteratively.

Cross-Modal Deep Clustering (XDC) [75] is a representative clustering-based method for video and audio representation learning. It uses pseudo-labels of the cluster assignments of one modality to supervise the training of the other modality. The authors also explore Multi-Head Deep Clustering (MDC) and Concatenation Deep Clustering (CDC), which use cluster assignments from both modalities and joint representations of both modalities as supervision, respectively. All three methods yield representations that achieve good performance on various downstream tasks.

SeLaVi [76] builds on a unimodal SeLa [77], that learns clustering by solving an optimal transport problem, for video labeling. SeLaVi extends it to multimodal data by considering the extracted audio and visual information as different views and then learns view invariance. Nondegenerate clustering is ensured via optimal transport. DMC [78] encodes images and audio spectrograms into separate representations, which are then co-clustered. The model then uses the similarity across modalities as supervision for training.

Alternative pretext tasks for clustering have also been designed. AV-HuBERT [79] uses the encoded masked audio and image-sequence representations to predict a pre-determined sequence of discrete cluster assignments, termed masked cluster prediction. It is more resilient to bad cluster assignments compared to unmasked cluster prediction. u-HuBERT [80] generalizes AV-HuBERT to be compatible with both multimodal and unimodal speech by mapping various inputs to a shared modality-agnostic embedding space for masked cluster prediction.","[[], ['b73'], [], [], ['b74'], ['b76', 'b77', 'b75'], ['b78', 'b79']]","[[], ['b73'], [], [], ['b74'], ['b76', 'b77', 'b75'], ['b78', 'b79']]",7,"sent1: Clustering methods assume that applying end-to-end trained clustering will lead to the grouping of the data by semantically salient characteristics.
sent2: In practice, these methods iteratively predict the cluster assignments of the encoded representation, and use these predictions, also known as pseudo labels, as supervision signals to update the feature representation.
sent3: Multimodal clustering provides the opportunity to learn multimodal representations and also improve conventional clustering by using each modality's pseudolabels to supervise the other.
sent4: Formally, let the predictive head g be a standard clustering method such as K-means (i.e., with no learnable parameters).
sent5: It clusters the encoded representations into M distinct clusters based on geometric similarity.
sent6: We denote the cluster prediction to be C i for input x i .
sent7: Clusteringbased methods minimize a cross-entropy loss between the predicted cluster assignments C i and the pseudo-labels z i .
sent8: Proposed by DeepCluster [74], a widely used approach to generate pseudo-labels is to jointly learn a d × M centroid matrix T and the cluster assignments z i of each data sample
sent9: x i by optimizing the following objective:
sent10: This results in a set of optimal assignments (z i * ) n≤N and a centroid matrix T * .
sent11: We use the assignment as the pseudolabels, while the centroid matrix is discarded.
sent12: After that, the model can be optimized using:Then, the clustering process is repeated on the updated representations, and thus the model can be updated iteratively.
sent13: Cross-Modal Deep Clustering (XDC) [75] is a representative clustering-based method for video and audio representation learning.
sent14: It uses pseudo-labels of the cluster assignments of one modality to supervise the training of the other modality.
sent15: The authors also explore Multi-Head Deep Clustering (MDC) and Concatenation Deep Clustering (CDC), which use cluster assignments from both modalities and joint representations of both modalities as supervision, respectively.
sent16: All three methods yield representations that achieve good performance on various downstream tasks.
sent17: SeLaVi [76] builds on a unimodal SeLa [77], that learns clustering by solving an optimal transport problem, for video labeling.
sent18: SeLaVi extends it to multimodal data by considering the extracted audio and visual information as different views and then learns view invariance.
sent19: Nondegenerate clustering is ensured via optimal transport.
sent20: DMC [78] encodes images and audio spectrograms into separate representations, which are then co-clustered.
sent21: The model then uses the similarity across modalities as supervision for training.
sent22: Alternative pretext tasks for clustering have also been designed.
sent23: AV-HuBERT [79] uses the encoded masked audio and image-sequence representations to predict a pre-determined sequence of discrete cluster assignments, termed masked cluster prediction.
sent24: It is more resilient to bad cluster assignments compared to unmasked cluster prediction.
sent25: u-HuBERT [80] generalizes AV-HuBERT to be compatible with both multimodal and unimodal speech by mapping various inputs to a shared modality-agnostic embedding space for masked cluster prediction."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s23,Auto-encoding Masked Prediction,"Auto-encoding masked predictors pretrain models by providing them with input data where some elements have been randomly masked, and training the model to predict the missing information. This aims to force the model to  understand the relationships between different pieces of data, thus learning rich semantic features. This approach was first introduced for natural language processing with the masked language modeling (MLM) technique proposed by BERT [81], and is now widely used for multimodal tasks.

For multimodal learning, the masked prediction task is often used in a cross-modal context, where the model should predict missing information conditioned on other modalities, as illustrated in Fig. 4. For example, the model may be given an image as context and be trained to predict missing text, or vice versa. This requires the model to understand the relationship and interaction between different modalities. Without loss of generality, consider a pretext task that aims to generate a reconstructed input modality x k conditioning on the other modality x j , (j ̸ = k), where we apply a masking function to the input, i.e.,x = MASK(x):

where L recon is usually a cross-entropy or mean squared error (MSE) loss to measure the difference between the original input and the reconstructed output.

In some cases, intra-modal masked prediction is complementary to cross-modal masked prediction, i.e., the model must also predict intra-modal masked content based solely on the information contained within the same modality. This can help to learn representations that are robust to the absence of other modalities [83], [84].

SelfDoc [85], proposed for document image understanding, introduces a masking function to randomly mask language or vision features for prediction, helping it to infer contextual clues and multimodal information. VideoBERT [84], designed for video and text, transforms both raw inputs into discrete token sequences, allowing them to be used by the same language model. The model can be pretrained on video-only or text-only corpora using intra-modal maskcompletion objectives. VL-BEIT [83] uses a shared transformer to perform masked prediction on both unimodal and multimodal data. It makes use of a simple and effective design that can be learned from scratch with one unified pretraining task, one shared backbone, and one-stage training. BEiT-3 [29] further demonstrates that using masked prediction objectives alone achieves state-of-the-art transfer performance on various downstream tasks. It treats images as a language and performs masked ""language"" modeling on images, text, and image-text pairs in a unified manner. Unified-IO [86] conducts masked language modeling and masked image modeling, which can be trained separately, or together when multiple modalities are present.

Besides performing exact reconstructions, methods have been proposed to instead match distributions over items in order to better model the high-level features. For example, ViLBERT [65] predicts a distribution over semantic classes of the masked text input for the corresponding image region, where the target distribution is obtained by a pretrained object detector. The KL divergence is minimized between the two distributions. Similarly, ActBERT [87] adopts this distribution matching strategy to model video and text.","[['b80'], [], [], ['b83', 'b82'], ['b85', 'b82', 'b28', 'b84', 'b83'], ['b64', 'b86']]","[['b80'], [], [], ['b83', 'b82'], ['b85', 'b82', 'b28', 'b84', 'b83'], ['b64', 'b86']]",10,"sent1: Auto-encoding masked predictors pretrain models by providing them with input data where some elements have been randomly masked, and training the model to predict the missing information.
sent2: This aims to force the model to  understand the relationships between different pieces of data, thus learning rich semantic features.
sent3: This approach was first introduced for natural language processing with the masked language modeling (MLM) technique proposed by BERT [81], and is now widely used for multimodal tasks.
sent4: For multimodal learning, the masked prediction task is often used in a cross-modal context, where the model should predict missing information conditioned on other modalities, as illustrated in Fig. 4.
sent5: For example, the model may be given an image as context and be trained to predict missing text, or vice versa.
sent6: This requires the model to understand the relationship and interaction between different modalities.
sent7: Without loss of generality, consider a pretext task that aims to generate a reconstructed input modality x k conditioning on the other modality x j , (j ̸ = k), where we apply a masking function to the input, i.e.,x = MASK(x):where L recon is usually a cross-entropy or mean squared error (MSE) loss to measure the difference between the original input and the reconstructed output.
sent8: In some cases, intra-modal masked prediction is complementary to cross-modal masked prediction, i.e., the model must also predict intra-modal masked content based solely on the information contained within the same modality.
sent9: This can help to learn representations that are robust to the absence of other modalities [83], [84].
sent10: SelfDoc [85], proposed for document image understanding, introduces a masking function to randomly mask language or vision features for prediction, helping it to infer contextual clues and multimodal information.
sent11: VideoBERT [84], designed for video and text, transforms both raw inputs into discrete token sequences, allowing them to be used by the same language model.
sent12: The model can be pretrained on video-only or text-only corpora using intra-modal maskcompletion objectives.
sent13: VL-BEIT [83] uses a shared transformer to perform masked prediction on both unimodal and multimodal data.
sent14: It makes use of a simple and effective design that can be learned from scratch with one unified pretraining task, one shared backbone, and one-stage training.
sent15: BEiT-3 [29] further demonstrates that using masked prediction objectives alone achieves state-of-the-art transfer performance on various downstream tasks.
sent16: It treats images as a language and performs masked ""language"" modeling on images, text, and image-text pairs in a unified manner.
sent17: Unified-IO [86] conducts masked language modeling and masked image modeling, which can be trained separately, or together when multiple modalities are present.
sent18: Besides performing exact reconstructions, methods have been proposed to instead match distributions over items in order to better model the high-level features.
sent19: For example, ViLBERT [65] predicts a distribution over semantic classes of the masked text input for the corresponding image region, where the target distribution is obtained by a pretrained object detector.
sent20: The KL divergence is minimized between the two distributions.
sent21: Similarly, ActBERT [87] adopts this distribution matching strategy to model video and text."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s78,A.1.3 Visual Captioning,"Captioning is a task to generate a free-form textual caption for a given image or video. The evaluation usually follows standard text generation metrics, including BLEU, METEROR, CIDEr, etc. For image captioning, the common datasets include COCO [20], Vizwiz Caption [219], TextCaps [220], etc. For video, datasets include MSRVTT [205], YouCook2 [206], MSVD [207], etc. Fig. 6. Results of image-text model and multimodal video models on downstream tasks. The size of each scatter corresponds to the number of estimated model parameters. Top row, recall@1 of text-to-image and image-to-text retrieval on COCO dataset, and the accuracy of VQA on VQAv2 dataset. Methods with an asterisk (*) indicate zero-shot performance, while the others show fine-tuning results. Bottom row, recall@10 of video retrieval on MSRVTT and YouCook2 datasets, and accuracy of action recognition on HMDB dataset. For the video retrieval tasks, asterisks denote fine-tuning performance, while non-asterisked methods denote zero-shot performance. The opposite holds true for the action recognition task.","[['b218', 'b206', 'b204', 'b219', 'b205', 'b19']]","[['b218', 'b206', 'b204', 'b219', 'b205', 'b19']]",6,"sent1: Captioning is a task to generate a free-form textual caption for a given image or video.
sent2: The evaluation usually follows standard text generation metrics, including BLEU, METEROR, CIDEr, etc.
sent3: For image captioning, the common datasets include COCO [20], Vizwiz Caption [219], TextCaps [220], etc.
sent4: For video, datasets include MSRVTT [205], YouCook2 [206], MSVD [207], etc.
sent5: Fig. 6. Results of image-text model and multimodal video models on downstream tasks.
sent6: The size of each scatter corresponds to the number of estimated model parameters.
sent7: Top row, recall@1 of text-to-image and image-to-text retrieval on COCO dataset, and the accuracy of VQA on VQAv2 dataset.
sent8: Methods with an asterisk (*) indicate zero-shot performance, while the others show fine-tuning results.
sent9: Bottom row, recall@10 of video retrieval on MSRVTT and YouCook2 datasets, and accuracy of action recognition on HMDB dataset.
sent10: For the video retrieval tasks, asterisks denote fine-tuning performance, while non-asterisked methods denote zero-shot performance.
sent11: The opposite holds true for the action recognition task."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s24,Auto-regressive Masked Prediction,"The auto-regressive pretraining method, popularized by PixelCNN [88] and GPT [82], makes predictions of the next (masked) token one step at a time, from left to right. Formally, considering the specific modality input x k is tokenized by the encoder e θk to a set of tokens U k = {u 1 k , . . . , u n k } (e.g., a set of words in text, or set of patches in images), and similarly for input modality x j , we have tokens U j = {u 1 j , . . . , u n j }. Denoting w to be the context window size, the objective of the auto-regressive process is to maximize the likelihood conditioned on the certain fusion of other modalities:

SimVLM [89] improves auto-regressive (AR) pretraining via PrefixLM objective by enabling bidirectional attention on the prefix sequence, and only conducting AR factorization on the remaining tokens. It effectively utilizes the crossmodal information while reducing the training complexity.

There are also methods that conduct both auto-encoding and auto-regressive reconstruction for pretraining. For example, OPT [90] models audio, vision, and language at different granularities. At the token level, the model is trained in an auto-encoding way while at the modality level, the model performs auto-regression using modalityspecific decoders to improve its generation ability. UNIMO [91] adopts both masked language modeling and seq2seq auto-regressive masked prediction.","[['b81', 'b87'], ['b88'], ['b89', 'b90']]","[['b81', 'b87'], ['b88'], ['b89', 'b90']]",5,"sent1: The auto-regressive pretraining method, popularized by PixelCNN [88] and GPT [82], makes predictions of the next (masked) token one step at a time, from left to right.
sent2: Formally, considering the specific modality input x k is tokenized by the encoder e θk to a set of tokens U k = {u 1 k , . . . , u n k } (e.g., a set of words in text, or set of patches in images), and similarly for input modality x j , we have tokens U j = {u 1 j , . . . , u n j }.
sent3: Denoting w to be the context window size, the objective of the auto-regressive process is to maximize the likelihood conditioned on the certain fusion of other modalities:SimVLM [89] improves auto-regressive (AR) pretraining via PrefixLM objective by enabling bidirectional attention on the prefix sequence, and only conducting AR factorization on the remaining tokens.
sent4: It effectively utilizes the crossmodal information while reducing the training complexity.
sent5: There are also methods that conduct both auto-encoding and auto-regressive reconstruction for pretraining.
sent6: For example, OPT [90] models audio, vision, and language at different granularities.
sent7: At the token level, the model is trained in an auto-encoding way while at the modality level, the model performs auto-regression using modalityspecific decoders to improve its generation ability.
sent8: UNIMO [91] adopts both masked language modeling and seq2seq auto-regressive masked prediction."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s26,Hybrid,"While single objectives already achieve good performance, many methods utilize a combination of the above approaches to take advantage of complementary strengths. This can be seen as a multi-task learning problem with several pretext tasks. A hybrid objective consisting of N separate objectives can then be formulated as the weighted sum:

where λ i is the weighting factor. The combination of contrastive and clustering objectives can be beneficial. As mentioned earlier, contrastive objectives may suffer from false negatives by ignoring semantic similarity between samples. On the other hand, the clustering objective takes semantic similarity into account by grouping semantically similar samples into the same cluster. MCN [92] performs clustering on the joint multimodal representation space (in contrast to XDC [75] that clusters on the separate representation space), and also calculates a contrastive loss pairwise across audio, video, and text. The resulting high-quality embedding space enables effective retrieval of samples even from unseen datasets and domains. Furthermore, inspired by the success of contrastive learning in sound source localization (e.g., [59], [61]), and clustering objectives in identifying classes, Afouras et al. [93] combines both objectives to learn an object detector using pseudolabels from audio-video heatmaps and cluster labels.

Researchers have explored hybrid objectives for vision-language pretraining, especially combining instance discrimination and masked prediction. For example, UNITER [64] employs both masked prediction and matching prediction learning at both the instance and object levels. Contrastive learning is also widely used together with matching prediction, where the matching prediction can utilize the hard negatives calculated from the contrastive objective, enabling more grounded representation learning. ALBEF [68] uses contrastive learning before fusing the image and text representations and using the fused representation for MLM and matching prediction. FLAVA [67] includes a similar combination of objectives, but also employs intramodal masked modeling in order to handle the separately unpaired data. VLMO [94] adopts this objective as well, but employs a mixture-of-modality-experts (MOME) transformer to encode inputs with modality-specific experts. BLIP [66] adopts both contrastive learning and matching prediction and conducts autoregressive masked prediction to enhance generation ability.

In the field of video and language pretraining, hybrid objectives have been explored as well. ActBERT [87] performs masked prediction in global actions, local regional objects, and text description levels. UniVL [95] applies both masked language modeling and masked frame modeling in an auto-encoding way and language reconstruction in an auto-regressive way. It also applies contrastive to align text and video representation. MERLOT Reserve [96] outlines a novel contrastive span objective: given a video with all modalities temporally aligned, a region of text and audio are masked out. The model must maximize the similarity of the predicted masked region only to an independent encoding of the text and audio at the specific time point (positives).

Hybrid objectives are also gaining increasing popularity for video-audio pretraining. CAV-MAE [97] performs contrastive learning of video-audio correspondence and crossmodal masked data modeling. MAViL [98] proposes three multimodal objectives: (1) masked audio-video modeling;

(2) masked inter-/intra-modal contrastive learning; and (3) masked self-training on multimodal features. Discussion. Hybrid objectives aim to combine complementary individual paradigms. During training, different objectives can interact, for example, contrastive learning can enhance negative pair selection for matching prediction. Furthermore, different objectives may benefit different downstream tasks, and combining them can lead to more flexible general-purpose representations [66], [68]. However, using a hybrid objective complicates hyperparameter tuning due to varying importance and differing convergence rates of individual objectives. And there is a potential risk of objective interference, where optimizing one might undermine another. In addition, such an approach could slow down training as it necessitates multiple forward passes to compute different objectives.","[[], ['b60', 'b74', 'b91', 'b58', 'b92'], ['b67', 'b66', 'b63', 'b65', 'b93'], ['b95', 'b86', 'b94'], ['b96', 'b97'], ['b67', 'b65']]","[[], ['b60', 'b74', 'b91', 'b58', 'b92'], ['b67', 'b66', 'b63', 'b65', 'b93'], ['b95', 'b86', 'b94'], ['b96', 'b97'], ['b67', 'b65']]",17,"sent1: While single objectives already achieve good performance, many methods utilize a combination of the above approaches to take advantage of complementary strengths.
sent2: This can be seen as a multi-task learning problem with several pretext tasks.
sent3: A hybrid objective consisting of N separate objectives can then be formulated as the weighted sum:where λ i is the weighting factor.
sent4: The combination of contrastive and clustering objectives can be beneficial.
sent5: As mentioned earlier, contrastive objectives may suffer from false negatives by ignoring semantic similarity between samples.
sent6: On the other hand, the clustering objective takes semantic similarity into account by grouping semantically similar samples into the same cluster.
sent7: MCN [92] performs clustering on the joint multimodal representation space (in contrast to XDC [75] that clusters on the separate representation space), and also calculates a contrastive loss pairwise across audio, video, and text.
sent8: The resulting high-quality embedding space enables effective retrieval of samples even from unseen datasets and domains.
sent9: Furthermore, inspired by the success of contrastive learning in sound source localization (e.g., [59], [61]), and clustering objectives in identifying classes, Afouras et al. [93] combines both objectives to learn an object detector using pseudolabels from audio-video heatmaps and cluster labels.
sent10: Researchers have explored hybrid objectives for vision-language pretraining, especially combining instance discrimination and masked prediction.
sent11: For example, UNITER [64] employs both masked prediction and matching prediction learning at both the instance and object levels.
sent12: Contrastive learning is also widely used together with matching prediction, where the matching prediction can utilize the hard negatives calculated from the contrastive objective, enabling more grounded representation learning.
sent13: ALBEF [68] uses contrastive learning before fusing the image and text representations and using the fused representation for MLM and matching prediction.
sent14: FLAVA [67] includes a similar combination of objectives, but also employs intramodal masked modeling in order to handle the separately unpaired data.
sent15: VLMO [94] adopts this objective as well, but employs a mixture-of-modality-experts (MOME) transformer to encode inputs with modality-specific experts.
sent16: BLIP [66] adopts both contrastive learning and matching prediction and conducts autoregressive masked prediction to enhance generation ability.
sent17: In the field of video and language pretraining, hybrid objectives have been explored as well.
sent18: ActBERT [87] performs masked prediction in global actions, local regional objects, and text description levels.
sent19: UniVL [95] applies both masked language modeling and masked frame modeling in an auto-encoding way and language reconstruction in an auto-regressive way.
sent20: It also applies contrastive to align text and video representation.
sent21: MERLOT Reserve [96] outlines a novel contrastive span objective: given a video with all modalities temporally aligned, a region of text and audio are masked out.
sent22: The model must maximize the similarity of the predicted masked region only to an independent encoding of the text and audio at the specific time point (positives).
sent23: Hybrid objectives are also gaining increasing popularity for video-audio pretraining.
sent24: CAV-MAE [97] performs contrastive learning of video-audio correspondence and crossmodal masked data modeling.
sent25: MAViL [98] proposes three multimodal objectives: (1) masked audio-video modeling;(2) masked inter-/intra-modal contrastive learning; and (3) masked self-training on multimodal features.
sent26: Discussion. Hybrid objectives aim to combine complementary individual paradigms.
sent27: During training, different objectives can interact, for example, contrastive learning can enhance negative pair selection for matching prediction.
sent28: Furthermore, different objectives may benefit different downstream tasks, and combining them can lead to more flexible general-purpose representations [66], [68].
sent29: However, using a hybrid objective complicates hyperparameter tuning due to varying importance and differing convergence rates of individual objectives.
sent30: And there is a potential risk of objective interference, where optimizing one might undermine another.
sent31: In addition, such an approach could slow down training as it necessitates multiple forward passes to compute different objectives."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s32,Unified Encoder with Early Fusion,"An encoder capable of processing multiple modalities is referred to as a unified encoder. Such a model typically employs a transformer-based architecture to handle input tokens from various modalities, allowing it to process different types of data using a single set of parameters:

h(x) = g ϕ ((e θ (x 1 , . . . , x k ))).

Here, we omit the notation of the fusion module f ψ because the fusion occurs across the entire encoder. Similar to the definition of late fusion, early fusion refers to the fusion of representations from the early stages of the unified encoder, although modality-specific tokenizers may still be required. The tokenizer can be either an external model pretrained off-line or a model trained end-to-end jointly. External models were widely used in earlier works to extract detailed features from raw modalities which are then transformed into input token sequences. For example, object detectors have been used for image features [64], [100] and for video features [87]. We refer readers to Section 5.2.2.2 for detailed discussions. End-to-end tokenizers are trained together, and their outputs are the input tokens of the unified encoder. This can be a patch embedding, CNN, or ViT [28], [89] for visual input, and MLP or transformer for text [46], [83]. End-to-end models have become more widely adopted as they do not require additional offline models, simplifying the training process and often yielding better performance.

Unified architectures can implicitly learn modality interactions through shared self-attention, which takes the input tokens from different modalities as input. The input token from different modalities can be distinguished, if needed, by designing specific positional encodings or extra modality type embeddings. This design is flexible, allowing for unimodal input, where only unimodal data is available and thus readily obtainable unimodal datasets can be utilized, as demonstrated in works such as [84], [89]. There have also been attempts to design a unified backbone for videoaudio [101], and video-audio-text [46]. This allows for the mixing of tokens from different modalities, such as replacing certain text tokens with corresponding image object tokens, e.g., [102]. HiP [103] scales the Perceiver-type models [104] to high-resolution raw multimodal input by building back locality into the architecture while preserving its modalityindependence. A variation on the unified architecture is the mixture-of-experts, which has been utilized recently for different modalities including image, text, video, audio, source code, etc. [29], [94], [105], [106]. This design utilizes different experts for different modalities within the same multiway transformer that shares self-attention. It enables specific modality experts to be used for more precise processing, while still benefiting from the unified architecture. Discussion. Generally, unified architectures allow for parameter sharing among different modalities for the major components of the model, which reduces the number of parameters. Also, they are usually more robust to missing modalities compared to modality-specific encoders. However, a major drawback of these methods is their inefficiency when it comes to tasks such as retrieval, since they require encoding all possible cross-modal pairs in order to compute similarity scores for ranking, though they can achieve competitive performance (e.g., [83]).","[[], [], ['b88', 'b27', 'b82', 'b45', 'b99', 'b63', 'b86'], ['b88', 'b104', 'b105', 'b101', 'b28', 'b45', 'b102', 'b82', 'b83', 'b103', 'b93', 'b100']]","[[], [], ['b88', 'b27', 'b82', 'b45', 'b99', 'b63', 'b86'], ['b88', 'b104', 'b105', 'b101', 'b28', 'b45', 'b102', 'b82', 'b83', 'b103', 'b93', 'b100']]",19,"sent1: An encoder capable of processing multiple modalities is referred to as a unified encoder.
sent2: Such a model typically employs a transformer-based architecture to handle input tokens from various modalities, allowing it to process different types of data using a single set of parameters:h(x) = g ϕ ((e θ (x 1 , . . . , x k ))).
sent3: Here, we omit the notation of the fusion module f ψ because the fusion occurs across the entire encoder.
sent4: Similar to the definition of late fusion, early fusion refers to the fusion of representations from the early stages of the unified encoder, although modality-specific tokenizers may still be required.
sent5: The tokenizer can be either an external model pretrained off-line or a model trained end-to-end jointly.
sent6: External models were widely used in earlier works to extract detailed features from raw modalities which are then transformed into input token sequences.
sent7: For example, object detectors have been used for image features [64], [100] and for video features [87].
sent8: We refer readers to Section 5.2.2.2 for detailed discussions.
sent9: End-to-end tokenizers are trained together, and their outputs are the input tokens of the unified encoder.
sent10: This can be a patch embedding, CNN, or ViT [28], [89] for visual input, and MLP or transformer for text [46], [83].
sent11: End-to-end models have become more widely adopted as they do not require additional offline models, simplifying the training process and often yielding better performance.
sent12: Unified architectures can implicitly learn modality interactions through shared self-attention, which takes the input tokens from different modalities as input.
sent13: The input token from different modalities can be distinguished, if needed, by designing specific positional encodings or extra modality type embeddings.
sent14: This design is flexible, allowing for unimodal input, where only unimodal data is available and thus readily obtainable unimodal datasets can be utilized, as demonstrated in works such as [84], [89].
sent15: There have also been attempts to design a unified backbone for videoaudio [101], and video-audio-text [46].
sent16: This allows for the mixing of tokens from different modalities, such as replacing certain text tokens with corresponding image object tokens, e.g., [102].
sent17: HiP [103] scales the Perceiver-type models [104] to high-resolution raw multimodal input by building back locality into the architecture while preserving its modalityindependence.
sent18: A variation on the unified architecture is the mixture-of-experts, which has been utilized recently for different modalities including image, text, video, audio, source code, etc. [29], [94], [105], [106].
sent19: This design utilizes different experts for different modalities within the same multiway transformer that shares self-attention.
sent20: It enables specific modality experts to be used for more precise processing, while still benefiting from the unified architecture.
sent21: Discussion. Generally, unified architectures allow for parameter sharing among different modalities for the major components of the model, which reduces the number of parameters.
sent22: Also, they are usually more robust to missing modalities compared to modality-specific encoders.
sent23: However, a major drawback of these methods is their inefficiency when it comes to tasks such as retrieval, since they require encoding all possible cross-modal pairs in order to compute similarity scores for ranking, though they can achieve competitive performance (e.g., [83])."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s33,Decoders,"While many methods are decoder-free, others require a decoder during the pretraining phase depending on the nature of the pretext tasks. Here, the pretext head g ϕ acts as the decoder. The decoder can then be discarded or retained for other downstream tasks. For example, auto-regressive masked prediction, as used in [86], [89], [90], [95], [107], [108], requires decoders to reconstruct the masked input. Successful generation, conditioned on the multimodal information of the reconstructed input, enforces the fusion of different modalities. Discussion. The additional decoder can enhance the generation ability of the model, benefiting downstream tasks such as image/video captioning and open-ended question answering. With the popularity of large language models, many of which utilize transformer decoder-only architectures [82], [109], decoder architectures are simultaneously finding increased acceptance in multimodal learning. This brings more possibility to pretext design and flexibility for multimodal fusion. However, the inclusion of a decoder can also make training more computationally expensive and potentially less stable compared to decoder-free methods that only use MLPs as the output layer (e.g., [21], [22], [68]).","[['b88', 'b67', 'b85', 'b20', 'b81', 'b21', 'b89', 'b107', 'b106', 'b108', 'b94']]","[['b88', 'b67', 'b85', 'b20', 'b81', 'b21', 'b89', 'b107', 'b106', 'b108', 'b94']]",11,"sent1: While many methods are decoder-free, others require a decoder during the pretraining phase depending on the nature of the pretext tasks.
sent2: Here, the pretext head g ϕ acts as the decoder.
sent3: The decoder can then be discarded or retained for other downstream tasks.
sent4: For example, auto-regressive masked prediction, as used in [86], [89], [90], [95], [107], [108], requires decoders to reconstruct the masked input.
sent5: Successful generation, conditioned on the multimodal information of the reconstructed input, enforces the fusion of different modalities.
sent6: Discussion. The additional decoder can enhance the generation ability of the model, benefiting downstream tasks such as image/video captioning and open-ended question answering.
sent7: With the popularity of large language models, many of which utilize transformer decoder-only architectures [82], [109], decoder architectures are simultaneously finding increased acceptance in multimodal learning.
sent8: This brings more possibility to pretext design and flexibility for multimodal fusion.
sent9: However, the inclusion of a decoder can also make training more computationally expensive and potentially less stable compared to decoder-free methods that only use MLPs as the output layer (e.g., [21], [22], [68])."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s30,Modality-specific Encoder,"These methods adopt modality-specific encoders to encode each type of input, e.g., a CNN for visual, and a transformer for text. Then, the encoded representations can be used for simple dot product alignment or further late fusion. Fusion-free. Fusion-free methods are models that do not include an explicit fusion module f ψ . Instead, they achieve cross-modal alignment by calculating cosine similarity, such as through the use of a contrastive loss, i.e., h(x) = g(e θ1 (x 1 ), . . . , e θ k (x k )).

For example, CLIP [21] and ALIGN [51] use separate image and text encoders to pretrain on large-scale datasets with contrastive learning. MMV [45] encodes visual, audio, and text input with three different encoders and then applies a multimodal contrastive loss for pretraining. This architecture can also be trained using matching prediction, where AVE-Net [59] predicts the alignment of vision and audio based on the Euclidean distance of the separate embeddings.

Additionally, in XDC [75], the cluster assignments of each encoder's embedding are utilized as the supervision signal for the other encoder without further fusion. Late Fusion.

Late fusion refers to models that use modality-specific encoders followed by an explicit fusion module to model the cross-modal interactions, typically via transformer layers or simply fully connected (FC) layers:

Note that this refers to representation fusion, which is different from the traditional definition of fusing prediction probabilities from different models. For instance, embeddings from different encoders can be projected to a shared latent space and then the joint representation can be used to generate cluster assignments [92] or to align predictions after a few FC layers [60]. Using transformer layers can potentially achieve deeper modality interactions due to the use of attention mechanisms. For example, ALBEF [68] and FLAVA [67] use an additional multimodal transformer that takes both visual and textual representations as input and fused with cross-attention, demonstrating that crossattention benefits downstream tasks such as visual grounding/entailment/reasoning that requires in-depth modality interactions. The fusion layers can also take other forms. For instance, Dragon [99] features a fusion layer with a modality interaction module that uses FC layers for information exchange. The fusion layer still retains a language encoder and a graph neural network for encoding text and the knowledge graph, respectively.","[[], ['b20', 'b58', 'b50', 'b44'], ['b74'], [], ['b67', 'b59', 'b66', 'b91', 'b98']]","[[], ['b20', 'b58', 'b50', 'b44'], ['b74'], [], ['b67', 'b59', 'b66', 'b91', 'b98']]",10,"sent1: These methods adopt modality-specific encoders to encode each type of input, e.g., a CNN for visual, and a transformer for text.
sent2: Then, the encoded representations can be used for simple dot product alignment or further late fusion.
sent3: Fusion-free. Fusion-free methods are models that do not include an explicit fusion module f ψ .
sent4: Instead, they achieve cross-modal alignment by calculating cosine similarity, such as through the use of a contrastive loss, i.e., h(x) = g(e θ1 (x 1 ), . . . , e θ k (x k )).
sent5: For example, CLIP [21] and ALIGN [51] use separate image and text encoders to pretrain on large-scale datasets with contrastive learning.
sent6: MMV [45] encodes visual, audio, and text input with three different encoders and then applies a multimodal contrastive loss for pretraining.
sent7: This architecture can also be trained using matching prediction, where AVE-Net [59] predicts the alignment of vision and audio based on the Euclidean distance of the separate embeddings.
sent8: Additionally, in XDC [75], the cluster assignments of each encoder's embedding are utilized as the supervision signal for the other encoder without further fusion.
sent9: Late Fusion. Late fusion refers to models that use modality-specific encoders followed by an explicit fusion module to model the cross-modal interactions, typically via transformer layers or simply fully connected (FC) layers:Note that this refers to representation fusion, which is different from the traditional definition of fusing prediction probabilities from different models.
sent10: For instance, embeddings from different encoders can be projected to a shared latent space and then the joint representation can be used to generate cluster assignments [92] or to align predictions after a few FC layers [60].
sent11: Using transformer layers can potentially achieve deeper modality interactions due to the use of attention mechanisms.
sent12: For example, ALBEF [68] and FLAVA [67] use an additional multimodal transformer that takes both visual and textual representations as input and fused with cross-attention, demonstrating that crossattention benefits downstream tasks such as visual grounding/entailment/reasoning that requires in-depth modality interactions.
sent13: The fusion layers can also take other forms.
sent14: For instance, Dragon [99] features a fusion layer with a modality interaction module that uses FC layers for information exchange.
sent15: The fusion layer still retains a language encoder and a graph neural network for encoding text and the knowledge graph, respectively."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s36,Stitching with Deep Interactions,"Researchers have also explored stitching models with deeper interactions. Instead of stitching only at the input layer, deep stitching fuses inside frozen models. This is typically achieved by incorporating additional adapters, and it encourages deep fusion with rich interactions. CLIPCap [117] proposes to use a transformer to map the visual tokens as the prefix embeddings for the language model, and append the prefix at each layer. Similar to Frozen, MAGMA [118] introduces additional adapter modules in the form of scaled residual bottleneck MLPs between each block of the language model to better interpret visual tokens. Flamingo [119] stitches frozen vision and language models with a perceiver-based [104] module and interleaves the visual tokens with the language model across each layer with cross-attention. Remarkably, without using any purposelyannotated data, it achieves strong performance and even outperforms the fine-tuned state of the art in some tasks. However, it still requires billion-scale trainable parameters.","[['b116', 'b118', 'b117', 'b103']]","[['b116', 'b118', 'b117', 'b103']]",4,"sent1: Researchers have also explored stitching models with deeper interactions.
sent2: Instead of stitching only at the input layer, deep stitching fuses inside frozen models.
sent3: This is typically achieved by incorporating additional adapters, and it encourages deep fusion with rich interactions.
sent4: CLIPCap [117] proposes to use a transformer to map the visual tokens as the prefix embeddings for the language model, and append the prefix at each layer.
sent5: Similar to Frozen, MAGMA [118] introduces additional adapter modules in the form of scaled residual bottleneck MLPs between each block of the language model to better interpret visual tokens.
sent6: Flamingo [119] stitches frozen vision and language models with a perceiver-based [104] module and interleaves the visual tokens with the language model across each layer with cross-attention.
sent7: Remarkably, without using any purposelyannotated data, it achieves strong performance and even outperforms the fine-tuned state of the art in some tasks.
sent8: However, it still requires billion-scale trainable parameters."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s58,Noisy-Paired,"Assuming paired data, with known correspondence between samples in each modality, is the most popular setting in multimodal learning. Paired data occurs naturally in cases where the modalities are recorded synchronously (e.g., audio and video modalities in a video clip), but can also be created incidentally in other cases (e.g., alt text tags of web images). However, the data pairing can be noisy in both cases. For example, in naturally paired data like audiovideo, a speaker might discuss a topic before physically demonstrating it, or they might neglect to describe a visible action. Similarly, in web-crawled image-text pairs, the text might only partially cover image content.

To learn representations from videos where noisy pairs can occur, methods have been developed to exploit temporal alignment among visual, audio, or text modalities using different strategies. For example, negative pairs can be selected from different videos [22], [45], [59], from the same video but using modalities from different frames [60], or by selecting multiple correct positives while downweighting incorrect ones [122]. Image-text pairs are a special case as captions must be manually created from images. However, as a large amount of noisy image-text pairs can be easily crawled from the internet, researchers have achieved promising results with various objectives as described in Section 3 even with noisy-paired data, e.g., [21], [29], [52], [66]. Discussion. While the use of approximately paired data in multimodal learning can benefit from straightforward pretext tasks that assume known pairing, it remains to be seen if such alignment noise ultimately limits the efficacy of these models. Furthermore, in many domains obtaining paired data is still a bottleneck. For example, in healthcare, obtaining paired data can be impossible due to privacy reasons. Even in the image-text setting, where substantial paired data exists, the volume of unpaired images and text vastly dwarfs the volume of paired examples. Thus it is desirable to develop methods capable of learning where some or all of the data is unpaired.","[[], ['b59', 'b28', 'b20', 'b51', 'b21', 'b58', 'b65', 'b121', 'b44']]","[[], ['b59', 'b28', 'b20', 'b51', 'b21', 'b58', 'b65', 'b121', 'b44']]",9,"sent1: Assuming paired data, with known correspondence between samples in each modality, is the most popular setting in multimodal learning.
sent2: Paired data occurs naturally in cases where the modalities are recorded synchronously (e.g., audio and video modalities in a video clip), but can also be created incidentally in other cases (e.g., alt text tags of web images).
sent3: However, the data pairing can be noisy in both cases.
sent4: For example, in naturally paired data like audiovideo, a speaker might discuss a topic before physically demonstrating it, or they might neglect to describe a visible action.
sent5: Similarly, in web-crawled image-text pairs, the text might only partially cover image content.
sent6: To learn representations from videos where noisy pairs can occur, methods have been developed to exploit temporal alignment among visual, audio, or text modalities using different strategies.
sent7: For example, negative pairs can be selected from different videos [22], [45], [59], from the same video but using modalities from different frames [60], or by selecting multiple correct positives while downweighting incorrect ones [122].
sent8: Image-text pairs are a special case as captions must be manually created from images.
sent9: However, as a large amount of noisy image-text pairs can be easily crawled from the internet, researchers have achieved promising results with various objectives as described in Section 3 even with noisy-paired data, e.g., [21], [29], [52], [66].
sent10: Discussion. While the use of approximately paired data in multimodal learning can benefit from straightforward pretext tasks that assume known pairing, it remains to be seen if such alignment noise ultimately limits the efficacy of these models.
sent11: Furthermore, in many domains obtaining paired data is still a bottleneck.
sent12: For example, in healthcare, obtaining paired data can be impossible due to privacy reasons.
sent13: Even in the image-text setting, where substantial paired data exists, the volume of unpaired images and text vastly dwarfs the volume of paired examples.
sent14: Thus it is desirable to develop methods capable of learning where some or all of the data is unpaired."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s59,Unpaired,"Unpaired learning aims to reduce the dependency on wellaligned multimodal data pairs and directly leverage largescale unimodal data corpora for multimodal learning. The key goal of unpaired multimodal algorithms is to find ways to induce alignment in unaligned data. Existing approaches mainly achieve this in two ways: (1) using external models, or (2) enforcing cycle-consistency.

External models are often used to detect concepts that enable connections between instances in one modality with similar instances in another, thus creating noisy coarsegrained pairs to provide extra supervision for multimodal learning. Object detectors, sometimes called concept detectors, are often used to extract region features and object tags to align vision and language data. For example, U-VisualBERT [123], VLMixer [102], and µ-VLA [124] utilize object detectors to extract the object tags from the images and then connect to the text with the concept words. Other tools such as scene graph detectors have also been used to extract the mutual relationship. For example, Graph-Align [125] enables unsupervised image captioning by extracting both scene graphs from both text and images. We discuss in detail the use of external models for explicitly extracting fine-grained pairing in Section 5.2.2.2.

Enforcing cycle consistency [126] is another approach to aligning the representation of different modalities. In the multimodal context, consider two modalities A and B, where we want to learn a mapping G AB : A → B and G BA : B → A. The concept of cycle consistency encodes the intuition that these mappings should be the reverse of each other and that both mappings should be bijections. Specifically, a cycle consistency loss encourages G AB (G BA (b)) ≈ b, and vice versa. DM2C [127] leverages cycle consistency to learn the unpaired cross-modal mappings of the latent representation with a special inter-modal auto-encoder, which is first pretrained on the single-modal data. MACK [128] collects a set of conceptual words and their related image regions from publicly available datasets, and then computes prototypical region representations to obtain the pretrained general knowledge. To further finetune the model for specific datasets a region-level cycleconsistency loss can be applied. Similarly, Graph-Align [125] also adopts this loss for modality alignment. Discussion. Learning with unpaired multimodal data is challenging but of great practical value as it enables the use of large-scale unimodal data corpora. Although this has gained growing attention in recent years, it is still immature. Approaches that rely on external pre-trained models are limited by the external model's quality, and these may not generalize well to the specific task or dataset being used. Furthermore, external models limit generality and scalability, as pretrained models covering all concepts or domains of interest may not exist. Meanwhile, cycle-consistency losses typically require additional model components, e.g., decoders. This adds additional complexity to the model and increases the amount of computation required.","[[], ['b123', 'b101', 'b124', 'b122'], ['b124', 'b125', 'b126', 'b127']]","[[], ['b123', 'b101', 'b124', 'b122'], ['b124', 'b125', 'b126', 'b127']]",8,"sent1: Unpaired learning aims to reduce the dependency on wellaligned multimodal data pairs and directly leverage largescale unimodal data corpora for multimodal learning.
sent2: The key goal of unpaired multimodal algorithms is to find ways to induce alignment in unaligned data.
sent3: Existing approaches mainly achieve this in two ways: (1) using external models, or (2) enforcing cycle-consistency.
sent4: External models are often used to detect concepts that enable connections between instances in one modality with similar instances in another, thus creating noisy coarsegrained pairs to provide extra supervision for multimodal learning.
sent5: Object detectors, sometimes called concept detectors, are often used to extract region features and object tags to align vision and language data.
sent6: For example, U-VisualBERT [123], VLMixer [102], and µ-VLA [124] utilize object detectors to extract the object tags from the images and then connect to the text with the concept words.
sent7: Other tools such as scene graph detectors have also been used to extract the mutual relationship.
sent8: For example, Graph-Align [125] enables unsupervised image captioning by extracting both scene graphs from both text and images.
sent9: We discuss in detail the use of external models for explicitly extracting fine-grained pairing in Section 5.2.2.2.
sent10: Enforcing cycle consistency [126] is another approach to aligning the representation of different modalities.
sent11: In the multimodal context, consider two modalities A and B, where we want to learn a mapping G AB : A → B and G BA : B → A. The concept of cycle consistency encodes the intuition that these mappings should be the reverse of each other and that both mappings should be bijections.
sent12: Specifically, a cycle consistency loss encourages G AB (G BA (b))
sent13: ≈ b, and vice versa. DM2C [127] leverages cycle consistency to learn the unpaired cross-modal mappings of the latent representation with a special inter-modal auto-encoder, which is first pretrained on the single-modal data.
sent14: MACK [128] collects a set of conceptual words and their related image regions from publicly available datasets, and then computes prototypical region representations to obtain the pretrained general knowledge.
sent15: To further finetune the model for specific datasets a region-level cycleconsistency loss can be applied.
sent16: Similarly, Graph-Align [125] also adopts this loss for modality alignment.
sent17: Discussion. Learning with unpaired multimodal data is challenging but of great practical value as it enables the use of large-scale unimodal data corpora.
sent18: Although this has gained growing attention in recent years, it is still immature.
sent19: Approaches that rely on external pre-trained models are limited by the external model's quality, and these may not generalize well to the specific task or dataset being used.
sent20: Furthermore, external models limit generality and scalability, as pretrained models covering all concepts or domains of interest may not exist.
sent21: Meanwhile, cycle-consistency losses typically require additional model components, e.g., decoders.
sent22: This adds additional complexity to the model and increases the amount of computation required."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s60,Mixed,"Training with a mix of paired and unpaired multimodal data is often the most realistic scenario as it reflects the reality that there is often some paired data and a larger amount of unpaired data available. Methods for dealing with mixedpairing data usually apply separate pretext tasks for unpaired data and paired data pretext tasks for paired inputs in a multi-task manner.

Masked prediction is widely used for dealing with mixed-paired data due to its flexibility. VATLM [129] uses a unified masked prediction objective, with within-modality masked prediction for text and multimodal masked prediction from combinations of visual-audio-text pairs. BEiT-3 [29] pretrains a multiway transformer by performing masked data modeling on image, text, and image-text pairs.

Clustering methods have also been utilized. For example, u-HuBERT [80] adopts separate audio and video encoders, and predicts the cluster assignments of the masked input frames after they are concatenated by a fusion module. If any of the modalities are missing, a dummy concatenation (i.e., by adding zero) is performed, and the same objective function can then be used as usual.

In terms of using different objectives for paired and unpaired data, VideoBERT [84] features a unified architecture and performs masked prediction for both text-only and video-only data. When paired data is available, it utilizes matching prediction to learn cross-modal correspondence. FLAVA [67] employs masked image modeling and masked language modeling for image-only and text-only data, while it utilizes masked multimodal modeling and contrastive learning over paired data. UNIMO [91] applies masked image modeling for image-only data, both AE-based and AR-based masked prediction for text-only data. It utilizes both unimodal and multimodal data, retrieving similar unimodal samples as positive pairs for cross-modal contrastive learning. VLMO [94] adopts a stagewise training approach that starts by training image-only and text-only modality experts using masked prediction, followed by instance discrimination on image-text pairs. Notably, SkillNet [105] utilizes mixture-of-experts for five different modalities, and can be trained on paired image-text and video-text using contrastive learning, and on unpaired sound and code using clustering and masked prediction. Discussion. Methods of mixed-pair learning can effectively leverage readily available unimodal datasets, leading to improved scalability and thus better downstream performance. However, some limitations do exist. For example, if the volume of unimodal data is imbalanced, the trained models may suffer from imbalanced performance on different downstream tasks or can overfit certain modalities.","[[], ['b128', 'b28'], ['b79'], ['b104', 'b66', 'b93', 'b83', 'b90']]","[[], ['b128', 'b28'], ['b79'], ['b104', 'b66', 'b93', 'b83', 'b90']]",8,"sent1: Training with a mix of paired and unpaired multimodal data is often the most realistic scenario as it reflects the reality that there is often some paired data and a larger amount of unpaired data available.
sent2: Methods for dealing with mixedpairing data usually apply separate pretext tasks for unpaired data and paired data pretext tasks for paired inputs in a multi-task manner.
sent3: Masked prediction is widely used for dealing with mixed-paired data due to its flexibility.
sent4: VATLM [129] uses a unified masked prediction objective, with within-modality masked prediction for text and multimodal masked prediction from combinations of visual-audio-text pairs.
sent5: BEiT-3 [29] pretrains a multiway transformer by performing masked data modeling on image, text, and image-text pairs.
sent6: Clustering methods have also been utilized.
sent7: For example, u-HuBERT [80] adopts separate audio and video encoders, and predicts the cluster assignments of the masked input frames after they are concatenated by a fusion module.
sent8: If any of the modalities are missing, a dummy concatenation (i.e., by adding zero) is performed, and the same objective function can then be used as usual.
sent9: In terms of using different objectives for paired and unpaired data, VideoBERT [84] features a unified architecture and performs masked prediction for both text-only and video-only data.
sent10: When paired data is available, it utilizes matching prediction to learn cross-modal correspondence.
sent11: FLAVA [67] employs masked image modeling and masked language modeling for image-only and text-only data, while it utilizes masked multimodal modeling and contrastive learning over paired data.
sent12: UNIMO [91] applies masked image modeling for image-only data, both AE-based and AR-based masked prediction for text-only data.
sent13: It utilizes both unimodal and multimodal data, retrieving similar unimodal samples as positive pairs for cross-modal contrastive learning.
sent14: VLMO [94] adopts a stagewise training approach that starts by training image-only and text-only modality experts using masked prediction, followed by instance discrimination on image-text pairs.
sent15: Notably, SkillNet [105] utilizes mixture-of-experts for five different modalities, and can be trained on paired image-text and video-text using contrastive learning, and on unpaired sound and code using clustering and masked prediction.
sent16: Discussion. Methods of mixed-pair learning can effectively leverage readily available unimodal datasets, leading to improved scalability and thus better downstream performance.
sent17: However, some limitations do exist.
sent18: For example, if the volume of unimodal data is imbalanced, the trained models may suffer from imbalanced performance on different downstream tasks or can overfit certain modalities."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s73,Unification.,"The trend towards unification in SSML has emerged across three principal axes: architectures, objective functions, and tasks. The first facet, architectural unification, entails the use of a specific modality encoder to process all input modalities (e.g., a vision model to encode language [196]) or a specifically designed architecture to encode multiple modalities [105], [197]. This unification facilitates an adaptable architecture that can cater to various modalities with increased efficacy. Similarly, the consolidation of pretraining objectives into a common target, as seen in approaches like data2vec [198] and BEiT-v3 [29], has enabled an easier integration of modalities. By employing a common objective across different modalities, the learning process becomes more streamlined and enables more straightforward adaptation to a wide range of data types, such as language models for proteins [199]. Lastly, unification in SSML also refers to the presentation of various tasks in a consistent form. For example, image classification, segmentation, and detection can be unified by text generation. This harmonization of tasks allows for zero-shot performance on previously unseen tasks, markedly enhancing multimodal models' generalizability across various domains. Emergent Abilities. Large language models exhibit emergent abilities, where meaningful performance on certain tasks can only be observed when the model size reaches a certain scale [200]. However, emergent abilities in multimodal models remain relatively unexplored. SSL allows for the scaling of multimodal models, which may lead to emergent abilities in challenging tasks such as multimodal reasoning, bringing us closer to general-purpose models. Thorough analysis is necessary to determine the tasks, architectures, model sizes, and dataset sizes at which SSML models demonstrate emergent abilities. Notably, emergent abilities may manifest differently between modalities. For instance, language models appear to gain common sense and logic abilities from scaling [120] while a multimodal model may showcase emergent zero-shot recognition [201]. Analyzing how and when these properties emerge across modalities can provide insights into the representations learned. Additionally, emergent multimodal abilities may arise from model architectures uniquely suited for fusing modalities or the alignment mechanism. Identifying fusion and alignment techniques that unlock new abilities will underpin future multimodal model designs. Rethinking Self-supervision in the Multimodal Context and Beyond. The core appeal of SSL is utilizing widely available unlabeled data [202]. However, applying this principle to multimodal data raises open questions. First, multimodal data differs from unimodal data, for which SSL paradigms are well-established. As discussed in Sec 2.3, should we consider freely available co-occurring webcrawled data pairs as fulfilling the self-supervision criteria? Second, new paradigms are emerging, especially with large language models, the outputs of which could potentially offer unlimited sources of supervision at scale. For instance, can text generated by a model like ChatGPT [203] be considered a valid form of self-supervision given the manuallydesigned template? On one hand, these outputs do require a modicum of manually-provided prompt templates. On the other hand, the design of prompt templates can be analogous to manually-defined rules for web data crawling or the design of pretext tasks (e.g., prompt template: rewriting image caption v.s. pretext task: contrastive learning with different augmentations). As SSML research progresses, our understanding of what constitutes a ""self-supervision"" is likely to evolve. Careful consideration of the spirit and aims of SSL will be important for developing methods that learn rich multimodal representations from abundant data.","[['b200', 'b196', 'b104', 'b119', 'b28', 'b199', 'b198', 'b202', 'b195', 'b201', 'b197']]","[['b200', 'b196', 'b104', 'b119', 'b28', 'b199', 'b198', 'b202', 'b195', 'b201', 'b197']]",11,"sent1: The trend towards unification in SSML has emerged across three principal axes: architectures, objective functions, and tasks.
sent2: The first facet, architectural unification, entails the use of a specific modality encoder to process all input modalities (e.g., a vision model to encode language [196]) or a specifically designed architecture to encode multiple modalities [105], [197].
sent3: This unification facilitates an adaptable architecture that can cater to various modalities with increased efficacy.
sent4: Similarly, the consolidation of pretraining objectives into a common target, as seen in approaches like data2vec [198] and BEiT-v3 [29], has enabled an easier integration of modalities.
sent5: By employing a common objective across different modalities, the learning process becomes more streamlined and enables more straightforward adaptation to a wide range of data types, such as language models for proteins [199].
sent6: Lastly, unification in SSML also refers to the presentation of various tasks in a consistent form.
sent7: For example, image classification, segmentation, and detection can be unified by text generation.
sent8: This harmonization of tasks allows for zero-shot performance on previously unseen tasks, markedly enhancing multimodal models' generalizability across various domains.
sent9: Emergent Abilities. Large language models exhibit emergent abilities, where meaningful performance on certain tasks can only be observed when the model size reaches a certain scale [200].
sent10: However, emergent abilities in multimodal models remain relatively unexplored.
sent11: SSL allows for the scaling of multimodal models, which may lead to emergent abilities in challenging tasks such as multimodal reasoning, bringing us closer to general-purpose models.
sent12: Thorough analysis is necessary to determine the tasks, architectures, model sizes, and dataset sizes at which SSML models demonstrate emergent abilities.
sent13: Notably, emergent abilities may manifest differently between modalities.
sent14: For instance, language models appear to gain common sense and logic abilities from scaling [120] while a multimodal model may showcase emergent zero-shot recognition [201].
sent15: Analyzing how and when these properties emerge across modalities can provide insights into the representations learned.
sent16: Additionally, emergent multimodal abilities may arise from model architectures uniquely suited for fusing modalities or the alignment mechanism.
sent17: Identifying fusion and alignment techniques that unlock new abilities will underpin future multimodal model designs.
sent18: Rethinking Self-supervision in the Multimodal Context and Beyond.
sent19: The core appeal of SSL is utilizing widely available unlabeled data [202].
sent20: However, applying this principle to multimodal data raises open questions.
sent21: First, multimodal data differs from unimodal data, for which SSL paradigms are well-established.
sent22: As discussed in Sec 2.3, should we consider freely available co-occurring webcrawled data pairs as fulfilling the self-supervision criteria?
sent23: Second, new paradigms are emerging, especially with large language models, the outputs of which could potentially offer unlimited sources of supervision at scale.
sent24: For instance, can text generated by a model like ChatGPT [203] be considered a valid form of self-supervision given the manuallydesigned template?
sent25: On one hand, these outputs do require a modicum of manually-provided prompt templates.
sent26: On the other hand, the design of prompt templates can be analogous to manually-defined rules for web data crawling or the design of pretext tasks (e.g., prompt template: rewriting image caption v.s. pretext task: contrastive learning with different augmentations).
sent27: As SSML research progresses, our understanding of what constitutes a ""self-supervision"" is likely to evolve.
sent28: Careful consideration of the spirit and aims of SSL will be important for developing methods that learn rich multimodal representations from abundant data."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s62,Implicit,"Implicit alignment is often achieved by enforcing crossmodal connections in the embedding space. Cross-modal attention and optimal transport are two commonly used techniques (i.e., realizations of the permutation function) for achieving such connections. Self-attention is a powerful mechanism that allows elements of input set to interact [109]. In multimodal learning, self-attention is extended to cross-attention, and the inferred attention map induces fine-grained correspondence between the modalities. LLA-CMA's [130] co-attention module consists of audio-guided attention and visual-guided attention, allowing the model to exploit audio-visual cooccurrence. ViLBERT [65] introduces a co-attentional layer to produce attention-pooled features for both image and text modalities conditioned on each other, thus enabling sparse interactions between them. Similarly, co-attention is also used in FLAVA [67] and SelfDoc [85] to uncover inter-modal relationships. FG-MMSSL [131] uses attention mechanisms to compute importance scores from finer-granularity embeddings across modalities and weights a contrastive loss using these scores to reduce potential noise.

Cross-attention can also model global-local interactions. For instance, ActBERT [87] incorporates additional globallocal correspondences into its design by stacking original key-value pairs with values from the other modality, thus ensuring that the joint video-text representation is aware of both fine-grained objects and global information. COOT [132] optimizes representations with respect to interactions between local features (clips/words) and global context (frames/sentences) by inputting local representations as key-value pairs and the global representation as the query.

Cross-modal attention can also be conducted in a directed manner, where one modality attends to another, but not vice versa. This approach is designed with the idea that some modalities require more complex modeling, while other modalities can be adequately encoded with a shallower model. For instance, ALBEF [68] fuses the image representation into the multimodal encoder to align the unimodal text representation. Similarly, BLIP [66] uses an image-grounded text encoder and decoder to fuse the visual representation with texts through cross-attention.

Recent methods such as [29], [94] employ shared selfattention for different modalities encoded by a mixture of experts. Although not explicitly studied, the shared selfattention weights have the potential to establish connections between different modalities. To demonstrate the finegrained alignment between modalities, several methods such as Oscar [100], UNITER [64], Hero [133], and µ-VLA [124] have used visualized the learned attention weights. They show that attention can learn cross-modal alignment, such as mapping words to image regions.

Optimal transport (OT) [134], [135], which defines distances between probability measures, is also used for the cross-domain fine-grained alignment. OT for cross-domain alignment aims to match the distributions by minimizing the cost of transforming one distribution into another. UNITER [64] employs OT to minimize the cost of transporting representations from image regions to words in a sentence (and vice versa), resulting in improved crossmodal alignment. The fast inexact proximal point method for optimal transports (IPOT) [136] is used to approximate the OT distance to overcome the challenge of intractable computation. Similarly, ViLT [28] adopts this approach to align textual subsets and visual subsets, which are not extracted by external models, unlike in UNITER [64].

Canonical correlation analysis (CCA) [137] is a classic approach to finding linear relationships between two sets of variables, while ensuring orthogonality. Its objective is to maximize the correlation between the corresponding dimensions of the representations from different modalities. While deep extensions of CCA exist [138], [139], and have been applied to tasks such as fine-grained audio-visual correlation [140], it has not been widely used in SSML overall.","[['b64', 'b129', 'b84', 'b66', 'b108', 'b130'], ['b86', 'b131'], ['b67', 'b65'], ['b132', 'b28', 'b99', 'b63', 'b123', 'b93'], ['b27', 'b133', 'b63', 'b134', 'b135'], ['b138', 'b137', 'b136', 'b139']]","[['b64', 'b129', 'b84', 'b66', 'b108', 'b130'], ['b86', 'b131'], ['b67', 'b65'], ['b132', 'b28', 'b99', 'b63', 'b123', 'b93'], ['b27', 'b133', 'b63', 'b134', 'b135'], ['b138', 'b137', 'b136', 'b139']]",25,"sent1: Implicit alignment is often achieved by enforcing crossmodal connections in the embedding space.
sent2: Cross-modal attention and optimal transport are two commonly used techniques (i.e., realizations of the permutation function) for achieving such connections.
sent3: Self-attention is a powerful mechanism that allows elements of input set to interact [109].
sent4: In multimodal learning, self-attention is extended to cross-attention, and the inferred attention map induces fine-grained correspondence between the modalities.
sent5: LLA-CMA's [130] co-attention module consists of audio-guided attention and visual-guided attention, allowing the model to exploit audio-visual cooccurrence.
sent6: ViLBERT [65] introduces a co-attentional layer to produce attention-pooled features for both image and text modalities conditioned on each other, thus enabling sparse interactions between them.
sent7: Similarly, co-attention is also used in FLAVA [67] and SelfDoc [85] to uncover inter-modal relationships.
sent8: FG-MMSSL [131] uses attention mechanisms to compute importance scores from finer-granularity embeddings across modalities and weights a contrastive loss using these scores to reduce potential noise.
sent9: Cross-attention can also model global-local interactions.
sent10: For instance, ActBERT [87] incorporates additional globallocal correspondences into its design by stacking original key-value pairs with values from the other modality, thus ensuring that the joint video-text representation is aware of both fine-grained objects and global information.
sent11: COOT [132] optimizes representations with respect to interactions between local features (clips/words) and global context (frames/sentences) by inputting local representations as key-value pairs and the global representation as the query.
sent12: Cross-modal attention can also be conducted in a directed manner, where one modality attends to another, but not vice versa.
sent13: This approach is designed with the idea that some modalities require more complex modeling, while other modalities can be adequately encoded with a shallower model.
sent14: For instance, ALBEF [68] fuses the image representation into the multimodal encoder to align the unimodal text representation.
sent15: Similarly, BLIP [66] uses an image-grounded text encoder and decoder to fuse the visual representation with texts through cross-attention.
sent16: Recent methods such as [29], [94] employ shared selfattention for different modalities encoded by a mixture of experts.
sent17: Although not explicitly studied, the shared selfattention weights have the potential to establish connections between different modalities.
sent18: To demonstrate the finegrained alignment between modalities, several methods such as Oscar [100], UNITER [64], Hero [133], and µ-VLA [124] have used visualized the learned attention weights.
sent19: They show that attention can learn cross-modal alignment, such as mapping words to image regions.
sent20: Optimal transport (OT) [134], [135], which defines distances between probability measures, is also used for the cross-domain fine-grained alignment.
sent21: OT for cross-domain alignment aims to match the distributions by minimizing the cost of transforming one distribution into another.
sent22: UNITER [64] employs OT to minimize the cost of transporting representations from image regions to words in a sentence (and vice versa), resulting in improved crossmodal alignment.
sent23: The fast inexact proximal point method for optimal transports (IPOT) [136] is used to approximate the OT distance to overcome the challenge of intractable computation.
sent24: Similarly, ViLT [28] adopts this approach to align textual subsets and visual subsets, which are not extracted by external models, unlike in UNITER [64].
sent25: Canonical correlation analysis (CCA) [137] is a classic approach to finding linear relationships between two sets of variables, while ensuring orthogonality.
sent26: Its objective is to maximize the correlation between the corresponding dimensions of the representations from different modalities.
sent27: While deep extensions of CCA exist [138], [139], and have been applied to tasks such as fine-grained audio-visual correlation [140], it has not been widely used in SSML overall."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s72,Mutual and Complementary Information.,"Prior work [186] has investigated the influence of mutual information on multiview contrastive learning and found that there is an optimal degree of mutual information that enables the model to learn the most effective representation. This is an even more salient problem in SSML, yet it is understudied. Different modalities can encode both shared and complementary information, e.g., a caption can describe the main content of an image, but not all image pixels are directly explained by the caption. Therefore, understanding how to represent features within each modality that have cross-modal correspondence, and those that do not, is still an open question. Some methods introduce intra-modal self-supervision in addition to intermodal learning, but it is unclear to what extent intra-modal information is truly useful for learning the representation, given that the relationship between mutual information and representation quality can follow a U-shaped curve [186]. While the answer may depend on the specific downstream tasks, more theoretical and empirical studies are needed. Robustness and Fairness. As SSML models become more widely used, it is crucial to ensure their reliability prior to deployment, in order for society to trust AI. However state-of-the-art models are still limited in this regard. For example, vision-language models have been shown to have limited compositional understanding and struggle with simple concepts like ""behind"" [187]. Additionally, researchers have observed that some modalities may be less robust than others when faced with perturbations in the multimodal setting [188]. Another significant challenge in robustness is to maintain stable performance when arbitrary combinations of modalities are added or removed at inference time. SSL has shown itself to be promising in improving robust multimodal representations [189].

Also, despite the growing interest in fairness issues in generative models [190], [191], discriminative SSML models are not immune to bias. For instance, they have been found to exhibit bias based on gender [192] or race [193] by aggregating bias from multimodal data [194], the model architecture and objective function [195], which can reinforce harmful stereotypes. Future work on identifying and eliminating the sources of bias is required.","[['b188', 'b187', 'b185', 'b186'], ['b189', 'b190', 'b191', 'b192', 'b193', 'b194']]","[['b188', 'b187', 'b185', 'b186'], ['b189', 'b190', 'b191', 'b192', 'b193', 'b194']]",10,"sent1: Prior work [186] has investigated the influence of mutual information on multiview contrastive learning and found that there is an optimal degree of mutual information that enables the model to learn the most effective representation.
sent2: This is an even more salient problem in SSML, yet it is understudied.
sent3: Different modalities can encode both shared and complementary information, e.g., a caption can describe the main content of an image, but not all image pixels are directly explained by the caption.
sent4: Therefore, understanding how to represent features within each modality that have cross-modal correspondence, and those that do not, is still an open question.
sent5: Some methods introduce intra-modal self-supervision in addition to intermodal learning, but it is unclear to what extent intra-modal information is truly useful for learning the representation, given that the relationship between mutual information and representation quality can follow a U-shaped curve [186].
sent6: While the answer may depend on the specific downstream tasks, more theoretical and empirical studies are needed.
sent7: Robustness and Fairness. As SSML models become more widely used, it is crucial to ensure their reliability prior to deployment, in order for society to trust AI.
sent8: However state-of-the-art models are still limited in this regard.
sent9: For example, vision-language models have been shown to have limited compositional understanding and struggle with simple concepts like ""behind"" [187].
sent10: Additionally, researchers have observed that some modalities may be less robust than others when faced with perturbations in the multimodal setting [188].
sent11: Another significant challenge in robustness is to maintain stable performance when arbitrary combinations of modalities are added or removed at inference time.
sent12: SSL has shown itself to be promising in improving robust multimodal representations [189].
sent13: Also, despite the growing interest in fairness issues in generative models [190], [191], discriminative SSML models are not immune to bias.
sent14: For instance, they have been found to exhibit bias based on gender [192] or race [193]
sent15: by aggregating bias from multimodal data [194], the model architecture and objective function [195], which can reinforce harmful stereotypes.
sent16: Future work on identifying and eliminating the sources of bias is required."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s35,Shallow Stitching at the Input Layer,"Shallow stitching takes place at the early stage of fusion. The output representation from a pretrained unimodal model of one modality is repurposed as input for another unimodal model of a different modality, with translation usually achieved by a learned projection network.

Frozen [112] is one of the first attempts to ground the visual modality to linguistic contexts. By treating the visual representation as soft prompts, it effectively leverages the semantic knowledge of a pretrained language model, showcasing the potential of transferring the language-only abilities to multimodal tasks in a zero-shot manner. A step further, LiMBeR [113] shows that visual semantic representations can be mapped to a language space as soft prompts simply by linear layers, with both models remaining frozen. FROMAGe [114] demonstrates that the model can learn strong few-shot multimodal abilities with linear projections even with small-scale training data. BLIP-2 [115] proposes to bridge the modality gap with a Querying Transformer using the same objectives as BLIP [66]. It achieves state-of-the-art performance by bootstrapping the pretrained image model and language model in two stages. To alleviate the reliance on paired training data, ESPER [116] aligns unpaired multimodal inputs to language model generations utilizing CLIP to obtain reward signals for reinforcement learning with all models frozen. These advancements reveal a promising trajectory for lightweight and efficient multimodal learning.","[[], ['b111', 'b115', 'b113', 'b112', 'b114', 'b65']]","[[], ['b111', 'b115', 'b113', 'b112', 'b114', 'b65']]",6,"sent1: Shallow stitching takes place at the early stage of fusion.
sent2: The output representation from a pretrained unimodal model of one modality is repurposed as input for another unimodal model of a different modality, with translation usually achieved by a learned projection network.
sent3: Frozen [112] is one of the first attempts to ground the visual modality to linguistic contexts.
sent4: By treating the visual representation as soft prompts, it effectively leverages the semantic knowledge of a pretrained language model, showcasing the potential of transferring the language-only abilities to multimodal tasks in a zero-shot manner.
sent5: A step further, LiMBeR [113] shows that visual semantic representations can be mapped to a language space as soft prompts simply by linear layers, with both models remaining frozen.
sent6: FROMAGe [114] demonstrates that the model can learn strong few-shot multimodal abilities with linear projections even with small-scale training data.
sent7: BLIP-2 [115] proposes to bridge the modality gap with a Querying Transformer using the same objectives as BLIP [66].
sent8: It achieves state-of-the-art performance by bootstrapping the pretrained image model and language model in two stages.
sent9: To alleviate the reliance on paired training data, ESPER [116] aligns unpaired multimodal inputs to language model generations utilizing CLIP to obtain reward signals for reinforcement learning with all models frozen.
sent10: These advancements reveal a promising trajectory for lightweight and efficient multimodal learning."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s65,External Models,"Explicit fine-grained input pairing can help the model learn detailed relationships between different modalities. In this survey, we define SSML methods as those that do not require manual annotations. Thus these methods may not strictly be considered self-supervised as the external models may be trained on datasets using supervision. However, they are included due to their ease of access through opensource communities and for completeness.

For visual data, object detectors (e.g., Faster R-CNN [142]) are frequently used to extract regions of interest (ROI) and the object classes. They can then be used to align with the corresponding parts in the other modalities. For imagetext pretraining, extracted ROIs can be used for word-region alignment [64], [65], [91], [102], [143], masked object classification [143], [144], and feature regression [145]. It can also be used to extract ROI from videos in every static frame at a set framerate, e.g., ActBERT [87]. For document understanding, SelfDoc [85] extracts document object proposals and applies OCR to obtain words for each proposal.

Object detectors can also be used to align unpaired data. U-VisualBERT [123] is the first to study unpaired imagetext pretraining. To achieve noisy alignment, a pretrained detector is used to extract the object tags from the images, which are then appended to the token embeddings with spatial coordinates. A masked prediction objective is then also applied to the detected tags to provide a noisy grounding signal via reconstruction. VLMixer [102] proposes to randomly wipe off some concept words in the sentence and then paste the visual patches with the same concept labels generated by detectors to obtain mixed sentences, serving as a cross-modal representation of the original sentence. Then, masked language modeling and contrastive learning are used to learn cross-modal alignment. µ-VLA [124] achieves effective self-supervised learning without paired data by weakly aligning an image-text corpus with a retrieval-based method and employing multi-granular alignment pretext tasks including masked prediction, contrastive learning, and the classification of the detected object labels.

To extract various levels of features from language, pretrained semantic parsers are used to obtain an explicitly factorized semantic space. For example, UNIMO [91] applies a scene graph parser to collate objects, attributes, and relations into vocabularies, which then aid in data augmentation through text rewriting at various levels. The extracted scene graph can also be applied to tasks such as unpaired image captioning [125] by aligning language and image information through parsing sentences syntactically.","[[], ['b64', 'b101', 'b142', 'b143', 'b84', 'b141', 'b63', 'b90', 'b144', 'b86'], ['b123', 'b101', 'b122'], ['b90', 'b124']]","[[], ['b64', 'b101', 'b142', 'b143', 'b84', 'b141', 'b63', 'b90', 'b144', 'b86'], ['b123', 'b101', 'b122'], ['b90', 'b124']]",15,"sent1: Explicit fine-grained input pairing can help the model learn detailed relationships between different modalities.
sent2: In this survey, we define SSML methods as those that do not require manual annotations.
sent3: Thus these methods may not strictly be considered self-supervised as the external models may be trained on datasets using supervision.
sent4: However, they are included due to their ease of access through opensource communities and for completeness.
sent5: For visual data, object detectors (e.g., Faster R-CNN [142]) are frequently used to extract regions of interest (ROI) and the object classes.
sent6: They can then be used to align with the corresponding parts in the other modalities.
sent7: For imagetext pretraining, extracted ROIs can be used for word-region alignment [64], [65], [91], [102], [143], masked object classification [143], [144], and feature regression [145].
sent8: It can also be used to extract ROI from videos in every static frame at a set framerate, e.g., ActBERT [87].
sent9: For document understanding, SelfDoc [85] extracts document object proposals and applies OCR to obtain words for each proposal.
sent10: Object detectors can also be used to align unpaired data.
sent11: U-VisualBERT [123] is the first to study unpaired imagetext pretraining.
sent12: To achieve noisy alignment, a pretrained detector is used to extract the object tags from the images, which are then appended to the token embeddings with spatial coordinates.
sent13: A masked prediction objective is then also applied to the detected tags to provide a noisy grounding signal via reconstruction.
sent14: VLMixer [102] proposes to randomly wipe off some concept words in the sentence and then paste the visual patches with the same concept labels generated by detectors to obtain mixed sentences, serving as a cross-modal representation of the original sentence.
sent15: Then, masked language modeling and contrastive learning are used to learn cross-modal alignment.
sent16: µ-VLA [124] achieves effective self-supervised learning without paired data by weakly aligning an image-text corpus with a retrieval-based method and employing multi-granular alignment pretext tasks including masked prediction, contrastive learning, and the classification of the detected object labels.
sent17: To extract various levels of features from language, pretrained semantic parsers are used to obtain an explicitly factorized semantic space.
sent18: For example, UNIMO [91] applies a scene graph parser to collate objects, attributes, and relations into vocabularies, which then aid in data augmentation through text rewriting at various levels.
sent19: The extracted scene graph can also be applied to tasks such as unpaired image captioning [125] by aligning language and image information through parsing sentences syntactically."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s67,State Representation Learning for Control,"State representation learning (SRL) is a special type of multimodal representation learning that captures the interaction between environmental observation modalities and an agent's action modality. SRL need not be task-specific and can be solved with SSML objectives. The learned representation can then be transferred to benefit downstream reinforcement learning and control tasks.

A common approach to SRL is masked prediction. Forward model and inverse models [149], [150] in control can be considered a special form of auto-regressive masked prediction. In the classic reinforcement learning framework [151], the observation is the raw sensor information and the state is a compressed depiction of this information that contains the necessary information for action selection. A forward model predicts the future state s t+1 from current action a t and current observation or state o t /s t ; while an inverse model predicts action a t given observations o t and o t+1 or states s t and s t+1 . The unobserved state/action can be considered masked and SRL corresponds to SSML masked prediction. For example, [152] proposes an action-conditional video forward model that predicts the next k states in the future. The World Model [153] trains an RNN to predict the future representation given the past visual observations and actions. Then an agent decides what actions to take based only on the learned representations to get rewards. Similarly, PlaNet [154] is a model-based agent that learns the environment dynamics from images and chooses actions through planning in the latent space, where the dynamics model, containing both deterministic and stochastic transition components, learns to predict the rewards for multiple time steps ahead. [155] trains a neural network by having a robot randomly poke objects and recording before/after visual states, estimating forward and inverse models of dynamics. Masked prediction objectives can also be used together with instance discrimination. For instance, Contrastive Forward Modeling [156] combines contrastive predictive coding [157] with predicting future state to maximize the mutual information between predictions and positives. [158] propose to fuse representations of images, force, and proprioception and then learn to predict the next control cycle's optical flow and potential environmental contact, using auto-encoding and matching prediction, respectively.","[[], ['b153', 'b157', 'b151', 'b155', 'b156', 'b148', 'b152', 'b149', 'b150', 'b154']]","[[], ['b153', 'b157', 'b151', 'b155', 'b156', 'b148', 'b152', 'b149', 'b150', 'b154']]",10,"sent1: State representation learning (SRL) is a special type of multimodal representation learning that captures the interaction between environmental observation modalities and an agent's action modality.
sent2: SRL need not be task-specific and can be solved with SSML objectives.
sent3: The learned representation can then be transferred to benefit downstream reinforcement learning and control tasks.
sent4: A common approach to SRL is masked prediction.
sent5: Forward model and inverse models [149], [150] in control can be considered a special form of auto-regressive masked prediction.
sent6: In the classic reinforcement learning framework [151], the observation is the raw sensor information and the state is a compressed depiction of this information that contains the necessary information for action selection.
sent7: A forward model predicts the future state s t+1 from current action a t and current observation or state o t /s t ; while an inverse model predicts action a t given observations o t and o t+1 or states s t and s t+1 .
sent8: The unobserved state/action can be considered masked and SRL corresponds to SSML masked prediction.
sent9: For example, [152] proposes an action-conditional video forward model that predicts the next k states in the future.
sent10: The World Model [153] trains an RNN to predict the future representation given the past visual observations and actions.
sent11: Then an agent decides what actions to take based only on the learned representations to get rewards.
sent12: Similarly, PlaNet [154] is a model-based agent that learns the environment dynamics from images and chooses actions through planning in the latent space, where the dynamics model, containing both deterministic and stochastic transition components, learns to predict the rewards for multiple time steps ahead.
sent13: [155] trains a neural network by having a robot randomly poke objects and recording before/after visual states, estimating forward and inverse models of dynamics.
sent14: Masked prediction objectives can also be used together with instance discrimination.
sent15: For instance, Contrastive Forward Modeling [156] combines contrastive predictive coding [157] with predicting future state to maximize the mutual information between predictions and positives.
sent16: [158] propose to fuse representations of images, force, and proprioception and then learn to predict the next control cycle's optical flow and potential environmental contact, using auto-encoding and matching prediction, respectively."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s68,Healthcare,"Clinicians often rely on information from multiple sources and modalities in diagnosis, prognosis, and treatment planning. Representation learning on diverse data is thus important for accurate diagnoses and effective patient care. Medical imaging is widely used for automatic diagnosis, and various methods have been proposed to improve downstream diagnosis with imaging and other modalities. For example, ConVIRT [159] and GLoRIA [160] (with fine-grained alignment) adopt contrastive learning to learn representation from medical imaging and medical reports. CheXzero [161] employs a CLIP-style approach using chest X-rays and corresponding reports, allowing for zero-shot classification of unseen pathologies. MEDCLIP [162] decouples images and text, enabling low-cost utilization of readily available imageonly and text-only training data. Additionally, ContIG [163] aligns retinal images and multiple genetic modalities in the representation space with a contrastive loss. CoMIR [164] enables registration with a contrastive loss by enforcing rotational equivariance.","[['b158', 'b161', 'b159', 'b163', 'b162', 'b160']]","[['b158', 'b161', 'b159', 'b163', 'b162', 'b160']]",6,"sent1: Clinicians often rely on information from multiple sources and modalities in diagnosis, prognosis, and treatment planning.
sent2: Representation learning on diverse data is thus important for accurate diagnoses and effective patient care.
sent3: Medical imaging is widely used for automatic diagnosis, and various methods have been proposed to improve downstream diagnosis with imaging and other modalities.
sent4: For example, ConVIRT [159] and GLoRIA [160] (with fine-grained alignment) adopt contrastive learning to learn representation from medical imaging and medical reports.
sent5: CheXzero [161] employs a CLIP-style approach using chest X-rays and corresponding reports, allowing for zero-shot classification of unseen pathologies.
sent6: MEDCLIP [162] decouples images and text, enabling low-cost utilization of readily available imageonly and text-only training data.
sent7: Additionally, ContIG [163] aligns retinal images and multiple genetic modalities in the representation space with a contrastive loss.
sent8: CoMIR [164] enables registration with a contrastive loss by enforcing rotational equivariance."
257913422,Self-Supervised Multimodal Learning: A Survey,Computer Science,https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359,s70,Machine Translation (MT),"MT considers different languages as different modalities, as multi-lingual features are n-gram dictionaries, which can be heterogeneous across languages. Due to the scarcity of parallel corpora for most language pairs, SSML algorithms offer a promising solution for unpaired machine translation. Lample et al [172] propose to map unpaired sentences from two different languages into a shared latent space, allowing it to learn translation by reconstructing in both languages. To further enhance performance, researchers have proposed methods such as denoising auto-encoders and back translation [173] or training with a large number of languages [174]. Incorporating visual cues, such as embeddings from unpaired instructional videos in the native language, has also been shown to improve unsupervised word mapping between languages [175]. For a comprehensive review of this field, we refer readers to the survey from [176].","[['b173', 'b172', 'b174', 'b171', 'b175']]","[['b173', 'b172', 'b174', 'b171', 'b175']]",5,"sent1: MT considers different languages as different modalities, as multi-lingual features are n-gram dictionaries, which can be heterogeneous across languages.
sent2: Due to the scarcity of parallel corpora for most language pairs, SSML algorithms offer a promising solution for unpaired machine translation.
sent3: Lample et al [172] propose to map unpaired sentences from two different languages into a shared latent space, allowing it to learn translation by reconstructing in both languages.
sent4: To further enhance performance, researchers have proposed methods such as denoising auto-encoders and back translation [173] or training with a large number of languages [174].
sent5: Incorporating visual cues, such as embeddings from unpaired instructional videos in the native language, has also been shown to improve unsupervised word mapping between languages [175].
sent6: For a comprehensive review of this field, we refer readers to the survey from [176]."
259089180,A Survey on Learning Objects' Relationship for Image Captioning,"Computer Science, Medicine",https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be,s15,Motion Relationship.,"Te action relationship between objects is more specifc than the positional relationship, which refects the relationship at a higher semantic level. With the diferent data structures, the motion relation can also be divided into the following two forms: (1) tensor and (2) graph. Te frst method is more intuitive. Te complexity of the motion relation makes it difcult to represent by the geometric feature. Terefore, many studies [73,74,[78][79][80][81] begin to directly mine the information from the image content, extract the features of relevant image regions, and represent them in the form of tensor. Te second method uses the graph pretrained by the upstream tasks to generate a suitable graph.","[['b79', 'b77', 'b78', 'b80', 'b73', 'b72']]","[['b79', 'b77', 'b78', 'b80', 'b73', 'b72']]",6,"sent1: Te action relationship between objects is more specifc than the positional relationship, which refects the relationship at a higher semantic level.
sent2: With the diferent data structures, the motion relation can also be divided into the following two forms: (1) tensor and (2) graph.
sent3: Te frst method is more intuitive.
sent4: Te complexity of the motion relation makes it difcult to represent by the geometric feature.
sent5: Terefore, many studies [73,74,[78][79][80][81] begin to directly mine the information from the image content, extract the features of relevant image regions, and represent them in the form of tensor.
sent6: Te second method uses the graph pretrained by the upstream tasks to generate a suitable graph."
259089180,A Survey on Learning Objects' Relationship for Image Captioning,"Computer Science, Medicine",https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be,s17,Semantic Graph.,"Te graph method use pretrained relationship detection networks in visual relation detection to extract action relations between objects and construct corresponding scene graphs. Specifcally, Yao et al. [72] used the abovementioned method to build the graph, as shown in Figure 5. Te pretrained model predicts the action relationship and uses the relationship category as the edge label. In each relational tuple <subject-predicate-object>, the subject and object are the 2048-dimensional attribute feature from the object detection network's RoI pooling. Te image region feature corresponding initializes the feature of the predicate to the minimum circumscribing moment of two bounding boxes belonging to the subject and object. Te above features are concatenated together and then input to the subsequent classifcation layer for obtaining the relationship category of the predicate. Te N × (N − 1) relational tuples are input into (excluding self-relations) the Computational Intelligence and Neuroscience 7 relational classifcation network. Edges with a probability larger than 0.5 are kept to form an action graph, as shown in Figure 6(b). Yang et al. [73] constructed scene graphs based on reference sentences in the training phase to reconstruct the sentence to accomplish the auto-encode training. Te scene graph divides its nodes into three categories: object nodes, relational nodes, and attribute nodes. For each <subjectpredicate-object> tuple, the subject and object correspond to the object node o i and o j . Te l attribute of the object corresponds to the attribute node a i,l , and the relationship between the two objects i, j corresponds to the relationship node r ij . Each node in the scene graph is represented by a feature vector of e o , e a , e r ∈ R d , respectively. Te object node o i and all of its attribute nodes a i,l have connections by an edge from the object node to the attribute node. If there is a relationship node, the subject-object node o i will frst connect to the relationship node r ij , and then the relationship node r ij will connect to the object object node o j . Te constructed graph is shown in Figure 6(c). In terms of implementation, they adopt the scene graph constructor used in [83] frst to convert sentences into syntactically independent trees and then convert the trees into scene graphs according to the rules mentioned in [75].

Chen et al. [74] designed a customized captioning model to generate sentences according to an abstract graph. Te abstract graph is a scene graph customized according to the user's wish. Te diferent forms of description graphs determine the level of detail in the generated caption. Specifcally, the abstract graph is constructed by the combination of three types of nodes: (1) object nodes, (2) attribute nodes (representing a specifc attribute of an object node), and (3) relationship nodes. Te construction of the abstract graph is to add the nodes and edges into the graph according to the user's interests. Specifcally, given all N object boxes of an image, if the user wants to know the content of the i object box, the object node o i is added to the abstract graph. At the same time, if the user wants to know about the attribute characteristics contained in the object node o i , l attribute nodes are added, and each attribute node corresponds to a path from o i to a i,l directed edges. If the user wants to describe the relationship between two objects, add the corresponding relationship node r i,j in the abstract graph, and build the edge connection between the subject and the object. Te subject-object node o i points to the relationship node r i,j , and then the relationship node r i,j points to the object object node o j . Te features corresponding to the object nodes and attribute nodes in the abstract graph adopt the visual features of the corresponding object bounding box. Te extraction method for the relational node is mainly used to extract the union frame features of two objects. Te result of its construction is shown in Figure 6(d).

In summary, the graph method represents more complex action relationships between objects than the tensor method. At the same time, some unnecessary relationship information is also eliminated, which can better retain important relationship content. Tere has also been a more signifcant improvement in computational cost and model performance. But the disadvantage is that it depends on the efectiveness of the relationship detection network and relies on training additional relationship information, which increases the complexity of the entire process. In the geometric graph, each edge represents a certain orientation. But in the semantic graph, each edge directly corresponds to a relational category. Tis more detailed representation of the relationship makes the semantic graph more efective to model the alignment of relational words. However, the limited number of relational categories also limits the variety of generated relational words. At the same time, the semantic similarity between diferent categories is also eliminated due to the classifcation operation.","[['b74', 'b72', 'b71', 'b82'], ['b73'], []]","[['b74', 'b72', 'b71', 'b82'], ['b73'], []]",5,"sent1: Te graph method use pretrained relationship detection networks in visual relation detection to extract action relations between objects and construct corresponding scene graphs.
sent2: Specifcally, Yao et al. [72] used the abovementioned method to build the graph, as shown in Figure 5.
sent3: Te pretrained model predicts the action relationship and uses the relationship category as the edge label.
sent4: In each relational tuple <subject-predicate-object>, the subject and object are the 2048-dimensional attribute feature from the object detection network's RoI pooling.
sent5: Te image region feature corresponding initializes the feature of the predicate to the minimum circumscribing moment of two bounding boxes belonging to the subject and object.
sent6: Te above features are concatenated together and then input to the subsequent classifcation layer for obtaining the relationship category of the predicate.
sent7: Te N × (N − 1) relational tuples are input into (excluding self-relations) the Computational Intelligence and Neuroscience 7 relational classifcation network.
sent8: Edges with a probability larger than 0.5 are kept to form an action graph, as shown in Figure 6(b).
sent9: Yang et al. [73] constructed scene graphs based on reference sentences in the training phase to reconstruct the sentence to accomplish the auto-encode training.
sent10: Te scene graph divides its nodes into three categories: object nodes, relational nodes, and attribute nodes.
sent11: For each <subjectpredicate-object> tuple, the subject and object correspond to the object node o i and o j .
sent12: Te l attribute of the object corresponds to the attribute node a i,l , and the relationship between the two objects i, j corresponds to the relationship node r ij .
sent13: Each node in the scene graph is represented by a feature vector of e o , e a , e r ∈ R d , respectively.
sent14: Te object node o i and all of its attribute nodes a i,l have connections by an edge from the object node to the attribute node.
sent15: If there is a relationship node, the subject-object node o i will frst connect to the relationship node r ij , and then the relationship node r ij will connect to the object object node o j .
sent16: Te constructed graph is shown in Figure 6(c).
sent17: In terms of implementation, they adopt the scene graph constructor used in [83] frst to convert sentences into syntactically independent trees and then convert the trees into scene graphs according to the rules mentioned in [75].
sent18: Chen et al. [74] designed a customized captioning model to generate sentences according to an abstract graph.
sent19: Te abstract graph is a scene graph customized according to the user's wish.
sent20: Te diferent forms of description graphs determine the level of detail in the generated caption.
sent21: Specifcally, the abstract graph is constructed by the combination of three types of nodes: (1) object nodes, (2) attribute nodes (representing a specifc attribute of an object node), and (3) relationship nodes.
sent22: Te construction of the abstract graph is to add the nodes and edges into the graph according to the user's interests.
sent23: Specifcally, given all N object boxes of an image, if the user wants to know the content of the i object box, the object node o i is added to the abstract graph.
sent24: At the same time, if the user wants to know about the attribute characteristics contained in the object node o
sent25: i , l attribute nodes are added, and each attribute node corresponds to a path from o i to a i,l directed edges.
sent26: If the user wants to describe the relationship between two objects, add the corresponding relationship node r i,j in the abstract graph, and build the edge connection between the subject and the object.
sent27: Te subject-object node o i points to the relationship node r i,j , and then the relationship node r i,j points to the object object node o j .
sent28: Te features corresponding to the object nodes and attribute nodes in the abstract graph adopt the visual features of the corresponding object bounding box.
sent29: Te extraction method for the relational node is mainly used to extract the union frame features of two objects.
sent30: Te result of its construction is shown in Figure 6(d).
sent31: In summary, the graph method represents more complex action relationships between objects than the tensor method.
sent32: At the same time, some unnecessary relationship information is also eliminated, which can better retain important relationship content.
sent33: Tere has also been a more signifcant improvement in computational cost and model performance.
sent34: But the disadvantage is that it depends on the efectiveness of the relationship detection network and relies on training additional relationship information, which increases the complexity of the entire process.
sent35: In the geometric graph, each edge represents a certain orientation.
sent36: But in the semantic graph, each edge directly corresponds to a relational category.
sent37: Tis more detailed representation of the relationship makes the semantic graph more efective to model the alignment of relational words.
sent38: However, the limited number of relational categories also limits the variety of generated relational words.
sent39: At the same time, the semantic similarity between diferent categories is also eliminated due to the classifcation operation."
259089180,A Survey on Learning Objects' Relationship for Image Captioning,"Computer Science, Medicine",https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be,s29,Evaluation.,"Te evaluation standard of relational captioning is consistent with the standard evaluation used in natural language processing to evaluate the similarity between the generated sentence and the ground-truth sentence. Te evaluation metrics: BLEU [88], METEOR [89], ROUGE [90], CIDEr [91], and SPICE [92]. For the fve metrics, BLEU and METEOR are used for machine translation, ROUGE for automatic translation summaries, and CIDEr and SPICE for image captioning. In principle, the abovementioned evaluation metrics measure the n-gram consistency between generated sentences and reference sentences and are also afected by the importance and rarity of n-grams in the corpus.","[['b88', 'b91', 'b89', 'b90', 'b87']]","[['b88', 'b91', 'b89', 'b90', 'b87']]",5,"sent1: Te evaluation standard of relational captioning is consistent with the standard evaluation used in natural language processing to evaluate the similarity between the generated sentence and the ground-truth sentence.
sent2: Te evaluation metrics: BLEU [88], METEOR [89], ROUGE [90], CIDEr [91], and SPICE [92].
sent3: For the fve metrics, BLEU and METEOR are used for machine translation, ROUGE for automatic translation summaries, and CIDEr and SPICE for image captioning.
sent4: In principle, the abovementioned evaluation metrics measure the n-gram consistency between generated sentences and reference sentences and are also afected by the importance and rarity of n-grams in the corpus."
259089180,A Survey on Learning Objects' Relationship for Image Captioning,"Computer Science, Medicine",https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be,s9,Transformer Decoder.,"Te transformer decoder proposed by Vaswani et al. [64] is also widely used in image captioning, which consists of multiple sublayers. Te textual features in each sublayer frst learn the interaction within its modality through self-attention, then align specifc object features through the cross attention between the textual features and X. Tey fnally pass the fully connected layer to generate the representation w t of the word at the current moment. w t fnally generates the corresponding word through the mapping matrix and the softmax function.

In summary, relational image description's overall process is generating sentences through the visual branch. At the same time, the relational branch processes the objectlevel relational features to be integrated into the visual branch. In the vision branch, given an image I, the object feature sequence V obtained by target detection is used as input, and then X is obtained by encoder learning. Te commonly used models in encoders are mainly transformer encoders or graph convolutional networks [72][73][74][75][76]. Ten, V e is input to the transformer decoder or double LSTM to generate natural sentences word-by-word.","[['b63'], ['b71', 'b74', 'b75', 'b73', 'b72']]","[['b63'], ['b71', 'b74', 'b75', 'b73', 'b72']]",6,"sent1: Te transformer decoder proposed by Vaswani et al. [64] is also widely used in image captioning, which consists of multiple sublayers.
sent2: Te textual features in each sublayer frst learn the interaction within its modality through self-attention, then align specifc object features through the cross attention between the textual features and X. Tey fnally pass the fully connected layer to generate the representation w t of the word at the current moment.
sent3: w t fnally generates the corresponding word through the mapping matrix and the softmax function.
sent4: In summary, relational image description's overall process is generating sentences through the visual branch.
sent5: At the same time, the relational branch processes the objectlevel relational features to be integrated into the visual branch.
sent6: In the vision branch, given an image I, the object feature sequence V obtained by target detection is used as input, and then X is obtained by encoder learning.
sent7: Te commonly used models in encoders are mainly transformer encoders or graph convolutional networks [72][73][74][75][76].
sent8: Ten, V e is input to the transformer decoder or double LSTM to generate natural sentences word-by-word."
259089180,A Survey on Learning Objects' Relationship for Image Captioning,"Computer Science, Medicine",https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be,s24,Scene Graph Auto-Encoder.,"Yang et al. [73] proposed the Scene Graph Auto-Encoder (SGAE) model to learn a recoder to optimize the original visual features through reconstruction of the sentence in training. Te scene graph is constructed from the ground-true sentence, and each visual feature further fuses features according to the connection in the graph. It is shown in Figure 6(c), which includes object nodes, relational nodes, and attribute nodes.

x r ij � g r e o i , e r ij , e o j ,

g a e o i , e a il ,

where x r ij is the node feature of the relation node r ij , and its neighbor node features e o i , e r ij , and e o j belong to the corresponding node in the relation tuple <o i -r ij -o j >. x a i represents the attribute information of the i object node, and its neighbor e o i and e a il belong to the object node i and l-th attribute feature. An object may have multiple attributes, each attribute corresponds to an attribute node. N is the total number of all attributes. x o i represents the feature of the i-th object node, <o i -r i * -o * > represents all the tuples whose i -th object as the subject. <o * -r * i -o i > represents all the tuples whose i-th node is the object. After passing the abovementioned embedding, they use the form of a memory network to set up a dictionary matrix D ∈ R d×V to optimize the input node feature x. Te calculation formula is as follows:

Te optimized feature x is input to the subsequent decoder to regenerate the sentence and compare with the real input sentence. Te error is fed back to the network for self-encoding training. Te auto-encoder method uses the reconstruction to learn the semantic knowledge which begins from the sentence and regenerates it. Te semantic knowledge refects in the scene graph and assists the inference process. Te whole framework is shown in Figure 8 3.4.3. Multirelational GCN. Chen et al. [74] proposed a customized abstract graph to generate specifc captions. For representing each node, the features of the object nodes and attribute nodes adopt the visual features of the corresponding object bounding box, which are reasoned from the object detection network. Te union bounding box's feature of two objects is used for the relational node. At the same time, Chen et al. made various types of nodes corresponding to diferent transformation matrices in feature embedding to further distinguish diferent kinds of nodes. Te formula is shown as follows:

where W r [k] is the transformation matrix and its three matrices corresponding to three types of nodes. pos[i] adds the order information for diferent attribute nodes a i,l . According to the abovementioned embedding methods, the features of each node in the abstract graph are fused with their adjacency nodes. Meanwhile, the directed abstract graph is converted into an undirected graph which fts with the GCN. Chen et al. [74] designed a multirelational GCN ( Figure 8) so that graph convolution learns diferent sets of parameters according to the edge types. Tere are six different types of edges: (1) object node to attribute node, (2) subject node to relational node, and (3) object node to relational node point and their inverse edges. Te transformation transforms the direct graph into a unidirected graph and feeds into the multi-relational GCN to refne each node's feature. Diferent transformation matrices in each layer of the graph convolutional network are used to map the edges of diferent categories. Specifcally, each layer is calculated as follows:

where l represents the diferent layers in the graph convolutional network, the parameters for diferent classes of edges in each layer are shared. Trough stacking encoders, each node feature is learned according to the connection between the nodes in the graph. Te multirelational GCN is based on the abstract graph, which the user designs for generating the customized caption. Te controllable ability has been improved, and the abstract graph determines the attribute, object, and relationship feature fed into the model. In summary, Table 1 summarizes the methods used in relational feature construction and relational encoding by current methods in relational captioning. Tere are 108K images in total and many object annotations, attribute information annotations, and relationship annotations between objects for tasks such as object detection and visual relationship detection. In relational captioning, it is mainly used as a pretraining dataset to pretrain the object detection or the visual relationship detection network. In the pretraining stage, the training, validation, and test dataset split is followed by Anderson et al. [5]. Specifcally, 98K images are used for training, and the remaining 10K images are divided into validation and test sets, respectively. When Yao et al. [72] Computational Intelligence and Neuroscience pretrained the target detection network, the dataset was fltered to retain 1600 object categories and 400 attribute categories. When dealing with pretrained object detection networks, it mainly selects the top 50 standard action relationships and artifcially classifes them into 20 categories. [85] is developed by Microsoft Team with the goal of scene understanding, capturing images from complex scenes, and can perform multiple tasks such as image recognition, segmentation, and captioning. Te dataset uses Amazon's ""Mechanical Turk"" service to manually generate at least fve sentences for each image. It contains more than 1.5 million sentences. Te training set contains 82,783 images, the validation set contains 40,504 images, and the test set contains 40,775 images. In captioning tasks, the ""Karpathy"" split [5] is the standard data split method, which takes 5000 images in the validation set for evaluation and 5000 images for testing. Te rest of the training and validation datasets are used for training.","[['b72'], [], [], [], ['b73'], ['b73'], ['b4', 'b84', 'b71']]","[['b72'], [], [], [], ['b73'], ['b73'], ['b4', 'b84', 'b71']]",6,"sent1: Yang et al. [73] proposed the Scene Graph Auto-Encoder (SGAE) model to learn a recoder to optimize the original visual features through reconstruction of the sentence in training.
sent2: Te scene graph is constructed from the ground-true sentence, and each visual feature further fuses features according to the connection in the graph.
sent3: It is shown in Figure 6(c), which includes object nodes, relational nodes, and attribute nodes.
sent4: x r ij � g r e o i , e r ij , e o j ,g a e o
sent5: i , e a il ,where x r ij is the node feature of the relation node r ij , and its neighbor node features
sent6: e o i , e r ij , and e o j belong to the corresponding node in the relation tuple <o
sent7: i -r ij -o j >. x a i represents the attribute information of the i object node, and its neighbor e o i and e a il belong to the object node i and l-th attribute feature.
sent8: An object may have multiple attributes, each attribute corresponds to an attribute node.
sent9: N is the total number of all attributes.
sent10: x o i represents the feature of the i-th object node, <o
sent11: i -r i * -o * > represents all the tuples whose i -th object as the subject.
sent12: <o * -r * i -o i > represents all the tuples whose i-th node is the object.
sent13: After passing the abovementioned embedding, they use the form of a memory network to set up a dictionary matrix D ∈ R d×V to optimize the input node feature x.
sent14: Te calculation formula is as follows:Te optimized feature x is input to the subsequent decoder to regenerate the sentence and compare with the real input sentence.
sent15: Te error is fed back to the network for self-encoding training.
sent16: Te auto-encoder method uses the reconstruction to learn the semantic knowledge which begins from the sentence and regenerates it.
sent17: Te semantic knowledge refects in the scene graph and assists the inference process.
sent18: Te whole framework is shown in Figure 8 3.4.3.
sent19: Multirelational GCN. Chen et al. [74] proposed a customized abstract graph to generate specifc captions.
sent20: For representing each node, the features of the object nodes and attribute nodes adopt the visual features of the corresponding object bounding box, which are reasoned from the object detection network.
sent21: Te union bounding box's feature of two objects is used for the relational node.
sent22: At the same time, Chen et al. made various types of nodes corresponding to diferent transformation matrices in feature embedding to further distinguish diferent kinds of nodes.
sent23: Te formula is shown as follows:where W r [k] is the transformation matrix and its three matrices corresponding to three types of nodes.
sent24: pos[i] adds the order information for diferent attribute nodes a i,l .
sent25: According to the abovementioned embedding methods, the features of each node in the abstract graph are fused with their adjacency nodes.
sent26: Meanwhile, the directed abstract graph is converted into an undirected graph which fts with the GCN.
sent27: Chen et al. [74] designed a multirelational GCN ( Figure 8)
sent28: so that graph convolution learns diferent sets of parameters according to the edge types.
sent29: Tere are six different types of edges: (1) object node to attribute node, (2) subject node to relational node, and (3) object node to relational node point and their inverse edges.
sent30: Te transformation transforms the direct graph into a unidirected graph and feeds into the multi-relational GCN to refne each node's feature.
sent31: Diferent transformation matrices in each layer of the graph convolutional network are used to map the edges of diferent categories.
sent32: Specifcally, each layer is calculated as follows:where l represents the diferent layers in the graph convolutional network, the parameters for diferent classes of edges in each layer are shared.
sent33: Trough stacking encoders, each node feature is learned according to the connection between the nodes in the graph.
sent34: Te multirelational GCN is based on the abstract graph, which the user designs for generating the customized caption.
sent35: Te controllable ability has been improved, and the abstract graph determines the attribute, object, and relationship feature fed into the model.
sent36: In summary, Table 1 summarizes the methods used in relational feature construction and relational encoding by current methods in relational captioning.
sent37: Tere are 108K images in total and many object annotations, attribute information annotations, and relationship annotations between objects for tasks such as object detection and visual relationship detection.
sent38: In relational captioning, it is mainly used as a pretraining dataset to pretrain the object detection or the visual relationship detection network.
sent39: In the pretraining stage, the training, validation, and test dataset split is followed by Anderson et al. [5].
sent40: Specifcally, 98K images are used for training, and the remaining 10K images are divided into validation and test sets, respectively.
sent41: When Yao et al. [72] Computational Intelligence and Neuroscience pretrained the target detection network, the dataset was fltered to retain 1600 object categories and 400 attribute categories.
sent42: When dealing with pretrained object detection networks, it mainly selects the top 50 standard action relationships and artifcially classifes them into 20 categories.
sent43: [85] is developed by Microsoft Team with the goal of scene understanding, capturing images from complex scenes, and can perform multiple tasks such as image recognition, segmentation, and captioning.
sent44: Te dataset uses Amazon's ""Mechanical Turk"" service to manually generate at least fve sentences for each image.
sent45: It contains more than 1.5 million sentences.
sent46: Te training set contains 82,783 images, the validation set contains 40,504 images, and the test set contains 40,775 images.
sent47: In captioning tasks, the ""Karpathy"" split [5] is the standard data split method, which takes 5000 images in the validation set for evaluation and 5000 images for testing.
sent48: Te rest of the training and validation datasets are used for training."
259089180,A Survey on Learning Objects' Relationship for Image Captioning,"Computer Science, Medicine",https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be,s2,Backbone,"Te backbone of relational captioning is the standard encoder-decoder framework [2][3][4] as the common captioning task. It is irrelevant to the relationship but is necessary to discuss for constructing the whole procedure. As shown in Figure 1, the backbone consists of two parts: encoder and decoder. Given an image I, relational captioning begins with objects detected from the object detector [38]. Te encoder refnes each element in the visual sequence and further feed it into the decoder for generating a natural sentence.","[['b3', 'b1', 'b2', 'b37']]","[['b3', 'b1', 'b2', 'b37']]",4,"sent1: Te backbone of relational captioning is the standard encoder-decoder framework [2][3][4] as the common captioning task.
sent2: It is irrelevant to the relationship but is necessary to discuss for constructing the whole procedure.
sent3: As shown in Figure 1, the backbone consists of two parts: encoder and decoder.
sent4: Given an image I, relational captioning begins with objects detected from the object detector [38].
sent5: Te encoder refnes each element in the visual sequence and further feed it into the decoder for generating a natural sentence."
259089180,A Survey on Learning Objects' Relationship for Image Captioning,"Computer Science, Medicine",https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be,s34,Methods,"Feature construction Relational encoding Decoder GCN-LSTM [72] Positional relation: directed graph with label Convolutional graph network Two-LSTMs decoder Motional relation: directed scene graph SGAE [73] Motional relation: directed scene graph Auto-encoder Two-LSTMs decoder ORT [62] Positional relation: directed graph with label Attention multiplier Transformer NG-SAN [39] Positional relation: directed graph with label Attention bias Transformer DLCT [83] Positional relation: directed graph with label Attention bias Transformer equally. Meanwhile, it considers the importance of the information of each word itself. Likewise, the higher the CIDEr score, the better the performance. [92] is a semantic evaluation metric for image captions, which measures how efectively image captions recover objects, attributes, and relationships between them. On the image captioning dataset, SPICE can better capture human judgments of model captions than existing n-gram metrics. Table 2 shows the scoring index ranking of the models used in the current relational image description on the MSCOCO dataset.","[['b71', 'b82', 'b61', 'b91', 'b38', 'b72']]","[['b71', 'b82', 'b61', 'b91', 'b38', 'b72']]",6,"sent1: Feature construction Relational encoding Decoder GCN-LSTM [72] Positional relation: directed graph with label Convolutional graph network Two-LSTMs decoder Motional relation: directed scene graph SGAE [73]
sent2: Motional relation: directed scene graph Auto-encoder Two-LSTMs decoder ORT [62]
sent3: Positional relation: directed graph with label Attention multiplier
sent4: Transformer NG-SAN [39] Positional relation: directed graph with label Attention bias Transformer DLCT [83]
sent5: Positional relation: directed graph with label Attention bias Transformer equally.
sent6: Meanwhile, it considers the importance of the information of each word itself.
sent7: Likewise, the higher the CIDEr score, the better the performance.
sent8: [92] is a semantic evaluation metric for image captions, which measures how efectively image captions recover objects, attributes, and relationships between them.
sent9: On the image captioning dataset, SPICE can better capture human judgments of model captions than existing n-gram metrics.
sent10: Table 2 shows the scoring index ranking of the models used in the current relational image description on the MSCOCO dataset."
259089180,A Survey on Learning Objects' Relationship for Image Captioning,"Computer Science, Medicine",https://www.semanticscholar.org/paper/d7d32386953eb6e4dda42c237ff27e3953bfb3be,s30,BLEU.,"As a widely used and essential evaluation metric in machine translation, BLEU [88] mainly measures the degree of the repetition between the generated sentence and the reference sentence. Te number of identical n-grams in both generated and reference sentences determines the BLEU score. With the more signifcant number, the BLEU score is higher, meaning the generated sentences are closer to the reference sentences. With the increase of the n in ngram, BLEU considers the correlation no longer limited to several words but prefers the correlation between contents. Te higher the BLEU score, the better the generated sentences. [89] mainly considers the infuence of synonyms and word forms in comparing generated sentences with all reference sentences. When evaluating the fuency of the sentence, METEOR is computed based on the chunks, which are constructed by considering the combination of semantically consecutive words. Te word's consistency between the candidate and reference sentences is measured by the chunk. At the same time, METEOR is calculated by combining the precision, recall, and F-values of matching various cases. Te higher the METEOR score, the better the sentence performance. [90] is a set of evaluation metrics designed to evaluate text summarization. ROUGE-L is used in relational captioning. It is calculated using the longest common subsequence between the generated and reference sentences. Te score is calculated by summing the recall and precision of the longest common subsequence. Te higher the ROUGE score, the better the sentence performance. [91] is an evaluation metric specially designed for captioning. It measures the consistency of image annotations by performing a term frequency-inverse document frequency (TF-IDF) weight calculation for each n-gram. Tis metric treats each sentence as a ""document,"" represented as a TF-IDF vector, and then computes the cosine similarity between the generated sentence and the reference sentence. Tis indicator makes up for a shortcoming of BLEU, in which all words on the match are treated Table 1: Summary of the various methods in the relational captioning.","[['b88', 'b90', 'b89', 'b87']]","[['b88', 'b90', 'b89', 'b87']]",4,"sent1: As a widely used and essential evaluation metric in machine translation, BLEU [88] mainly measures the degree of the repetition between the generated sentence and the reference sentence.
sent2: Te number of identical n-grams in both generated and reference sentences determines the BLEU score.
sent3: With the more signifcant number, the BLEU score is higher, meaning the generated sentences are closer to the reference sentences.
sent4: With the increase of the n in ngram, BLEU considers the correlation no longer limited to several words but prefers the correlation between contents.
sent5: Te higher the BLEU score, the better the generated sentences.
sent6: [89] mainly considers the infuence of synonyms and word forms in comparing generated sentences with all reference sentences.
sent7: When evaluating the fuency of the sentence, METEOR is computed based on the chunks, which are constructed by considering the combination of semantically consecutive words.
sent8: Te word's consistency between the candidate and reference sentences is measured by the chunk.
sent9: At the same time, METEOR is calculated by combining the precision, recall, and F-values of matching various cases.
sent10: Te higher the METEOR score, the better the sentence performance.
sent11: [90] is a set of evaluation metrics designed to evaluate text summarization.
sent12: ROUGE-L is used in relational captioning.
sent13: It is calculated using the longest common subsequence between the generated and reference sentences.
sent14: Te score is calculated by summing the recall and precision of the longest common subsequence.
sent15: Te higher the ROUGE score, the better the sentence performance.
sent16: [91] is an evaluation metric specially designed for captioning.
sent17: It measures the consistency of image annotations by performing a term frequency-inverse document frequency (TF-IDF) weight calculation for each n-gram.
sent18: Tis metric treats each sentence as a ""document,"" represented as a TF-IDF vector, and then computes the cosine similarity between the generated sentence and the reference sentence.
sent19: Tis indicator makes up for a shortcoming of BLEU, in which all words on the match are treated Table 1: Summary of the various methods in the relational captioning."
259376518,Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of Biomedical Research Articles,"Computer Science, Biology",https://www.semanticscholar.org/paper/3bef44ef3aacddaae56d6efc9f6767e64bc65b31,s13,Systems Overview,"Three teams have made in total 7 attempts for Subtask 2. A brief description of their respecitve approaches are as following: LHS712EE (Liu et al., 2023) The team carried on with the LED (Beltagy et al., 2020) model trained on the PLOS dataset from Subtask 1 to test the generalizability of their approach in generating lay summaries coupled with a pre-trained LED model for abstractive summaries.They later retrained the model using the abstract section of the dataset to improve performance in generating technical abstracts.

Pathology Dynamics (Al-Hussaini et al., 2023) As the abstract with the most salient information is no longer present in the input, to tackle the long context input, the team trained a base LSG model (Condevaux and Harispe, 2022) and truncated each article to the first 4096 tokens for generating both abstracts and lay summaries.The model was then trained on a merged dataset that uses each article twice, with one output having the lay summaries and the other having the abstract.They also reported using simplification procedures such as MUSS (Martin et al., 2022) to enhance the lay summary or other instruction-following models such as T5 with different prefix for summarisation.(Chen et al., 2023) This team made use of different models for each submission, including Primera, a PEGASUS model pre-trained on PubMed, and a BART-large Longformer model.","[['b1', 'b12'], ['b3', 'b2', 'b16', 'b0']]","[['b1', 'b12'], ['b3', 'b2', 'b16', 'b0']]",6,"sent1: Three teams have made in total 7 attempts for Subtask 2.
sent2: A brief description of their respecitve approaches are as following: LHS712EE (Liu et al., 2023)
sent3: The team carried on with the LED (Beltagy et al., 2020) model trained on the PLOS dataset from Subtask 1 to test the generalizability of their approach in generating lay summaries coupled with a pre-trained LED model for abstractive summaries.
sent4: They later retrained the model using the abstract section of the dataset to improve performance in generating technical abstracts.
sent5: Pathology Dynamics (Al-Hussaini et al., 2023)
sent6: As the abstract with the most salient information is no longer present in the input, to tackle the long context input, the team trained a base LSG model (Condevaux and Harispe, 2022) and truncated each article to the first 4096 tokens for generating both abstracts and lay summaries.
sent7: The model was then trained on a merged dataset that uses each article twice, with one output having the lay summaries and the other having the abstract.
sent8: They also reported using simplification procedures such as MUSS (Martin et al., 2022) to enhance the lay summary or other instruction-following models such as T5 with different prefix for summarisation.(Chen et al., 2023)
sent9: This team made use of different models for each submission, including Primera, a PEGASUS model pre-trained on PubMed, and a BART-large Longformer model."
259376518,Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of Biomedical Research Articles,"Computer Science, Biology",https://www.semanticscholar.org/paper/3bef44ef3aacddaae56d6efc9f6767e64bc65b31,s10,Pos. Team,"Relevance  Arizona Sky This team first truncate input documents, before using them to train two separate BART base models.

IKM_Lab (Wu et al., 2023) This team experimented with the use of a LED model trained on both datasets, as well as the adoption of different formats for including additional article information, such as keywords and section headings, in the input.

NCUEE-NLP (Chen et al., 2023) This team also made use of different models for each submission, including Primera (Xiao et al., 2022), a PEGASUS model (Zhang et al., 2020a) pretrained on PubMed, and a BART-large Longformer model.

himil The team experimented with both BERT (Devlin et al., 2019) and Longformer-based models, trained individually on each dataset.","[[], ['b25'], ['b29', 'b2', 'b26'], ['b4']]","[[], ['b25'], ['b29', 'b2', 'b26'], ['b4']]",5,"sent1: Relevance  Arizona Sky This team first truncate input documents, before using them to train two separate BART base models.
sent2: IKM_Lab (Wu et al., 2023) This team experimented with the use of a LED model trained on both datasets, as well as the adoption of different formats for including additional article information, such as keywords and section headings, in the input.
sent3: NCUEE-NLP (Chen et al., 2023) This team also made use of different models for each submission, including Primera (Xiao et al., 2022), a PEGASUS model (Zhang et al., 2020a) pretrained on PubMed, and a BART-large Longformer model.
sent4: himil The team experimented with both BERT (Devlin et al., 2019) and Longformer-based models, trained individually on each dataset."
259376518,Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of Biomedical Research Articles,"Computer Science, Biology",https://www.semanticscholar.org/paper/3bef44ef3aacddaae56d6efc9f6767e64bc65b31,s5,Evaluation,"For both subtasks, we evaluate summary quality according to three criteria -Relevance, Readability, and Factuality -where each criterion is composed of one or more automatic metrics:

• Relevance: ROUGE-1, 2, and L (Lin, 2004) and BERTScore (Zhang et al., 2020b).

• Readability: Flesch-Kincaid Grade Level (FKGL) and Dale-Chall Readability Score (DCRS).

• Factuality: BARTScore (Yuan et al., 2021), fine-tuned on our respective datasets (as has proven effective in recent work (Koh et al., 2022)).2

For Subtask 1, the scores calculated for each metric are the average of those calculated independently for the generated lay summaries of PLOS and eLife.The aim is to maximise the scores for Relevance and Factuality metrics and minimise scores for Readability metrics.

For Subtask 2, the scores presented for each metric are the average of those calculated independently for the generated abstracts and lay summaries.Notably, for Readability metrics in this subtask, we calculate the absolute difference between the scores of generated summary and target summary pairs (rather than simply using the scores obtained for generated summaries, as in subtask 1).The aim is to maximise the scores for Relevance and Factuality metrics and minimise the absolute difference scores calculated for Readability metrics.

Following the submission deadline for each subtask, an overall ranking is calculated based on the cumulative rank of the evaluation criteria, where a lower overall ranking equates to better overall performance).To produce a criterion ranking, we apply min-max normalisation to the scores of each metric, before averaging across metrics within each evaluation criterion.","[[], ['b30', 'b11'], [], ['b8', 'b28'], [], [], []]","[[], ['b30', 'b11'], [], ['b8', 'b28'], [], [], []]",4,"sent1: For both subtasks, we evaluate summary quality according to three criteria -Relevance, Readability, and Factuality -where each criterion is composed of one or more automatic metrics:• Relevance: ROUGE-1, 2, and L (Lin, 2004) and BERTScore (Zhang et al., 2020b).• Readability: Flesch-Kincaid Grade Level (FKGL) and Dale-Chall Readability Score (DCRS).
sent2: • Factuality: BARTScore (Yuan et al., 2021), fine-tuned on our respective datasets (as has proven effective in recent work (Koh et al., 2022)).2For Subtask 1, the scores calculated for each metric are the average of those calculated independently for the generated lay summaries of PLOS and eLife.
sent3: The aim is to maximise the scores for Relevance and Factuality metrics and minimise scores for Readability metrics.
sent4: For Subtask 2, the scores presented for each metric are the average of those calculated independently for the generated abstracts and lay summaries.
sent5: Notably, for Readability metrics in this subtask, we calculate the absolute difference between the scores of generated summary and target summary pairs (rather than simply using the scores obtained for generated summaries, as in subtask 1).The aim is to maximise the scores for Relevance and Factuality metrics and minimise the absolute difference scores calculated for Readability metrics.
sent6: Following the submission deadline for each subtask, an overall ranking is calculated based on the cumulative rank of the evaluation criteria, where a lower overall ranking equates to better overall performance).To produce a criterion ranking, we apply min-max normalisation to the scores of each metric, before averaging across metrics within each evaluation criterion."
259376518,Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of Biomedical Research Articles,"Computer Science, Biology",https://www.semanticscholar.org/paper/3bef44ef3aacddaae56d6efc9f6767e64bc65b31,s8,Systems Overview,"Subtask 1 attracted a total of 20 participating teams, between them making a total of 49 submissions.A brief explanation of the modelling approach taken by each team is given below:3 LHS712EE (Liu et al., 2023) The team employed a BART model for eLife and a Longformer Encoder-Decoder (LED) model (Beltagy et al., 2020) for PLOS, whilst also experimenting with optimising memory usage.

GRASUM (Rosati, 2023) Standing for Grounded, Referenced, and Annotated SUMmaries, this team's method combines approaches from retrieval augmentation, offline RL, and controlled generation, using a LED model with 16k input limit as the base model.A ""grounding"" step enhances each document with content retrieved from scientific abstracts, Wikipedia, simple Wikipedia, and UMLS that is appended to input, in addition the bibliographic reference string of the source document (obtained from CrossRef).An additional ""annotation"" step annotates each source document with control tokens that indicate whether the corresponding summary achieves higher or lower than the median score for each of the task evaluation aspects (given in §4).

BITSpi This team's method involves fine-tuning two separate BART models on pre-processed versions of each dataset.Specifically, stopwords are removed from the input data, and abbreviations are substituted for their full forms using a medical dictionary.

APTSum (Poornash et al., 2023) A three-step approach is adopted by this team, leveraging the SimCLS contrasting learning framework (Liu and Liu, 2021).Specifically, they first perform content selection, identifying the Abstract and Introduction as best model input, before generating candidate summaries using BART, followed section-wise reranking using a RoBERTa-base model to capture section-based salience information.

LaSTUS-FBK This team used a multi-stage unified approach, first cleaning the data via reference  VBD-NLP (Phan et al., 2023) This team's method is based on the combined use of sequenceto-sequence model BioBART (Yuan et al., 2022) and FACTORSUM (Fonseca et al., 2022), a factorized energy-based model that aims to identify the most important input content, enabling more effective processing of long documents.Additional experimentation with handling length as well as utilising other Pretrained Language Models (PLMs) was also carried out.

MDC (Turbitt and Bevan, 2023) This team focused on comparing the performance of generalpurpose GPT models (e.g., ChatGPT) with indomain GPT models (e.g., BioGPT (Luo et al., 2022a)).Additionally, they experimented with zero-shot and few-shot prompting, as well as finetuning different models.","[['b1', 'b12'], ['b21'], [], ['b13', 'b19'], ['b27', 'b5', 'b18'], ['b23', 'b14']]","[['b1', 'b12'], ['b21'], [], ['b13', 'b19'], ['b27', 'b5', 'b18'], ['b23', 'b14']]",10,"sent1: Subtask 1 attracted a total of 20 participating teams, between them making a total of 49 submissions.
sent2: A brief explanation of the modelling approach taken by each team is given below:3 LHS712EE (Liu et al., 2023)
sent3: The team employed a BART model for eLife and a Longformer Encoder-Decoder (LED) model (Beltagy et al., 2020) for PLOS, whilst also experimenting with optimising memory usage.
sent4: GRASUM (Rosati, 2023) Standing for Grounded, Referenced, and Annotated SUMmaries, this team's method combines approaches from retrieval augmentation, offline RL, and controlled generation, using a LED model with 16k input limit as the base model.
sent5: A ""grounding"" step enhances each document with content retrieved from scientific abstracts, Wikipedia, simple Wikipedia, and UMLS that is appended to input, in addition the bibliographic reference string of the source document (obtained from CrossRef).An additional ""annotation"" step annotates each source document with control tokens that indicate whether the corresponding summary achieves higher or lower than the median score for each of the task evaluation aspects (given in §4).
sent6: BITSpi This team's method involves fine-tuning two separate BART models on pre-processed versions of each dataset.
sent7: Specifically, stopwords are removed from the input data, and abbreviations are substituted for their full forms using a medical dictionary.
sent8: APTSum (Poornash et al., 2023) A three-step approach is adopted by this team, leveraging the SimCLS contrasting learning framework (Liu and Liu, 2021).Specifically, they first perform content selection, identifying the Abstract and Introduction as best model input, before generating candidate summaries using BART, followed section-wise reranking using a RoBERTa-base model to capture section-based salience information.
sent9: LaSTUS-FBK This team used a multi-stage unified approach, first cleaning the data via reference  VBD-NLP (Phan et al., 2023)
sent10: This team's method is based on the combined use of sequenceto-sequence model BioBART (Yuan et al., 2022) and FACTORSUM (Fonseca et al., 2022), a factorized energy-based model that aims to identify the most important input content, enabling more effective processing of long documents.
sent11: Additional experimentation with handling length as well as utilising other Pretrained Language Models (PLMs) was also carried out.
sent12: MDC (Turbitt and Bevan, 2023) This team focused on comparing the performance of generalpurpose GPT models (e.g., ChatGPT) with indomain GPT models (e.g., BioGPT (Luo et al., 2022a)).Additionally, they experimented with zero-shot and few-shot prompting, as well as finetuning different models."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s38,Explainable Model Structure,"The explainable model structure is to analyze and understand the internal structure of the model through explainable technology and to understand the working principle and working mechanism of the model.Structural analysis involves comprehending the operating mechanism and fundamental principles of the model structure.Only by fully understanding the working mechanism and working principle of the model structure can researchers and developers determine what problems exist in the model and when it is difficult to continue improving its performance.Only then, on the premise of understanding the characteristics of the model structure, can they point out the next optimization direction of the model.This enables them to improve the performance of the model in a better and faster way.Most of these explainable models use deep learning methods such as knowledge graphs and attention mechanisms.

Chien et al. [139] proposed the Explainable AI (XAI) framework XFlag, used LSTM [140] to carry out the fake news detection model and used the Layered Relevance Propagation (LRP) [141] algorithm to explain the model.Wu et al. [142] used knowledge graphs to enhance the embedded representation learning framework to detect fake news while providing interpretations of relationships.In this study, an external dataset was used to extract a knowledge graph, and a graph neural network was utilized to pre-train structured features for entities and relationships.The pre-trained features and semantic features were then combined to integrate explainable structured knowledge for recognizing fake news.Chen et al. [143] designed an explainable modular structure for automatically detecting rumors on social media.They utilized a two-level attention mechanism to capture the relative importance both between features and between feature classes.Furthermore, they highlighted the most significant features in the news to explain the algorithmic results.In addition, based on multidisciplinary explainable fake news detection, Qiao et al. [144] used multidisciplinary language synthesis methods to train features that are understandable to humans and then used these features to train a deep learning classifier with a bidirectional recurrent neural network (BRNN) structure [145], so that the classifier can obtain more explainable detection results in news data.

Silva et al. [146] proposed a novel fake news early detection technology called Propa-gation2Vec, as shown in Figure 7.The technology assigns different levels of importance to nodes and cascades in the propagation network and reconstructs knowledge of the complete propagation network based on their partial propagation network during the early detection phase.The study further presents a comprehensive explanation of the underlying logic of Propagation2Vec according to the attention weights assigned to different nodes and cascades.This enhances the applicability of the method and stimulates future research in the domain of fake news detection utilizing propagation networks.We summarize the explainable model structure methods in Table 7, including the main techniques they use, datasets and accuracy performance.We summarize the explainable model structure methods in Table 7, including the main techniques they use, datasets and accuracy performance.","[[], ['b142', 'b143', 'b141', 'b140', 'b145', 'b144', 'b139'], ['b146']]","[[], ['b142', 'b143', 'b141', 'b140', 'b145', 'b144', 'b139'], ['b146']]",8,"sent1: The explainable model structure is to analyze and understand the internal structure of the model through explainable technology and to understand the working principle and working mechanism of the model.
sent2: Structural analysis involves comprehending the operating mechanism and fundamental principles of the model structure.
sent3: Only by fully understanding the working mechanism and working principle of the model structure can researchers and developers determine what problems exist in the model and when it is difficult to continue improving its performance.
sent4: Only then, on the premise of understanding the characteristics of the model structure, can they point out the next optimization direction of the model.
sent5: This enables them to improve the performance of the model in a better and faster way.
sent6: Most of these explainable models use deep learning methods such as knowledge graphs and attention mechanisms.
sent7: Chien et al. [139] proposed the Explainable AI (XAI) framework XFlag, used LSTM [140] to carry out the fake news detection model and used the Layered Relevance Propagation (LRP) [141] algorithm to explain the model.
sent8: Wu et al. [142] used knowledge graphs to enhance the embedded representation learning framework to detect fake news while providing interpretations of relationships.
sent9: In this study, an external dataset was used to extract a knowledge graph, and a graph neural network was utilized to pre-train structured features for entities and relationships.
sent10: The pre-trained features and semantic features were then combined to integrate explainable structured knowledge for recognizing fake news.
sent11: Chen et al. [143] designed an explainable modular structure for automatically detecting rumors on social media.
sent12: They utilized a two-level attention mechanism to capture the relative importance both between features and between feature classes.
sent13: Furthermore, they highlighted the most significant features in the news to explain the algorithmic results.
sent14: In addition, based on multidisciplinary explainable fake news detection, Qiao et al. [144] used multidisciplinary language synthesis methods to train features that are understandable to humans and then used these features to train a deep learning classifier with a bidirectional recurrent neural network (BRNN) structure [145], so that the classifier can obtain more explainable detection results in news data.
sent15: Silva et al. [146] proposed a novel fake news early detection technology called Propa-gation2Vec, as shown in Figure 7.The technology assigns different levels of importance to nodes and cascades in the propagation network and reconstructs knowledge of the complete propagation network based on their partial propagation network during the early detection phase.
sent16: The study further presents a comprehensive explanation of the underlying logic of Propagation2Vec according to the attention weights assigned to different nodes and cascades.
sent17: This enhances the applicability of the method and stimulates future research in the domain of fake news detection utilizing propagation networks.
sent18: We summarize the explainable model structure methods in Table 7, including the main techniques they use, datasets and accuracy performance.
sent19: We summarize the explainable model structure methods in Table 7, including the main techniques they use, datasets and accuracy performance."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s28,Dataset,"In the detection of fake news, the dataset used can be divided into single-modal and multi-modal data, as shown in Figure 5.We gathered prevalent fake news datasets from the past five years based on citations.Multimodal data has a more diverse form, typically comprising a combination of images or video text, as demonstrated in Table 3. Abbreviations for technical terms are defined upon first use.In contrast, unimodal datasets exclusively comprise text, providing a more extensive characterization.According to the dataset-construction method, data characteristics and adaptation tasks, it can be divided into three categories:

BERT model [18] has significantly improved the performance of many natural language tasks.For fake news detection in natural language processing tasks of text data, the BERT pre-training model and related improved models gradually replace the original language model, which has become the basis of current research.Jwa et al. [87] combined news data in the pre-training phase to improve fake news recognition skills; Kaliyar et al. [92] proposed a BERT-based deep convolution method (fakeBERT) to detect fake news.The advantage of the pre-trained model is that the BERT method is unique in identifying and capturing contextual meanings in sentences or texts.In the model learning process, it does not need to go deep into the details of fake news to achieve good detection performance.

After several years of development, the BERT pre-training model based on transformer structure has gradually produced many related models [93][94][95] after structural adjustment, performance optimization and retraining.These models are collectively referred to as BERTology series models and have achieved good performance in various tasks.In summary, the fake news detection method based on the pre-trained model is already a research trend in this field.However, despite the complex characteristics of fake news, fake news detection based on pre-trained models still cannot achieve good performance in practical applications, like other practical tasks.How to extract features from more complex semantic information about fake news and establish a more effective fake news detection model for the 'pre-training + fine-tuning' paradigm of the pre-training model [96] is still an urgent problem to be solved.","[[], ['b92', 'b87', 'b17'], ['b93', 'b95', 'b96', 'b94']]","[[], ['b92', 'b87', 'b17'], ['b93', 'b95', 'b96', 'b94']]",7,"sent1: In the detection of fake news, the dataset used can be divided into single-modal and multi-modal data, as shown in Figure 5.We gathered prevalent fake news datasets from the past five years based on citations.
sent2: Multimodal data has a more diverse form, typically comprising a combination of images or video text, as demonstrated in Table 3.
sent3: Abbreviations for technical terms are defined upon first use.
sent4: In contrast, unimodal datasets exclusively comprise text, providing a more extensive characterization.
sent5: According to the dataset-construction method, data characteristics and adaptation tasks, it can be divided into three categories:BERT model [18] has significantly improved the performance of many natural language tasks.
sent6: For fake news detection in natural language processing tasks of text data, the BERT pre-training model and related improved models gradually replace the original language model, which has become the basis of current research.
sent7: Jwa et al. [87] combined news data in the pre-training phase to improve fake news recognition skills; Kaliyar et al. [92] proposed a BERT-based deep convolution method (fakeBERT) to detect fake news.
sent8: The advantage of the pre-trained model is that the BERT method is unique in identifying and capturing contextual meanings in sentences or texts.
sent9: In the model learning process, it does not need to go deep into the details of fake news to achieve good detection performance.
sent10: After several years of development, the BERT pre-training model based on transformer structure has gradually produced many related models [93][94][95] after structural adjustment, performance optimization and retraining.
sent11: These models are collectively referred to as BERTology series models and have achieved good performance in various tasks.
sent12: In summary, the fake news detection method based on the pre-trained model is already a research trend in this field.
sent13: However, despite the complex characteristics of fake news, fake news detection based on pre-trained models still cannot achieve good performance in practical applications, like other practical tasks.
sent14: How to extract features from more complex semantic information about fake news and establish a more effective fake news detection model for the 'pre-training + fine-tuning' paradigm of the pre-training model [96] is still an urgent problem to be solved."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s39,Explainable Model Behavior,"Explainable model behavior, that is, explainable analysis of the results of model prediction behavior, provides the basis for prediction results.Behavioral analysis typically involves comprehending the foundation of a model's anticipated behavior.Since deep learning algorithms consist of nonlinear structures, these successful models are commonly obscure and have difficulty revealing the rationale of their forecasts in a format that humans can grasp.The absence of transparency and intelligibility regarding a model's forecasts can lead to grave consequences.Shu et al. [55] used the sentence-comment joint attention sub-network to improve the performance of fake news detection, aiming to capture the inherent interpretability of news phrases and user comments.The dEFEND algorithm module facilitates search functionalities for searching news dissemination networks, trending news, top statements and related news.Moreover, it presents test results and explanations.In a similar vein, Lu et al. [151] utilized the graph-aware common attention network (GCAN) to assess the authenticity of source tweets on social media while providing explanations for the results.GCAN uses the attention mechanism to capture three aspects of the algorithm results: highlighting key words in source tweets, identifying characteristics of retweet propagation paths and understanding the behavior of retweeters.Chi et al. [152] proposed an automated explainable decision-making system (QA-AXDS) based on quantitative argumentation.This system can detect fake news and explain the results to users.It automatically captures human-level knowledge, constructs an interpretation model based on a dialogue tree and employs natural language to help users understand the reasoning process within the system.Notably, QA-AXDS is fully automated and does not require expert experience as pre-input, which enhances the robustness of the system.Ni et al. [153] studied the use of a multi-view attention mechanism network (MVAN) [154] to detect fake news in social networks and provide explanations for the results.MVAN incorporates a dual attention mechanism, encompassing text semantic attention and propagation structure attention, to capture clues in source tweets and propagation structures.It identifies crucial keywords and generates explainable detection results.Raha et al. [155] proposed a neural model for factual inconsistency classification with explanations.By training four neural models, they can predict the inconsistency type and provide explanations for a given sentence.However, Bhattarai et al. [156] introduced an explainable fake news detection framework based on the Tsetlin Machine (TM) [157].By capturing lexical and semantic features of true and fake news texts, this framework achieves accurate detection of fake news and the credibility score is used to provide interpretability.

Fu et al. [158] introduced a comprehensive and explainable false information detection framework called DISCO, as depicted in Figure 8.This framework addresses the challenge of detecting false information by leveraging the heterogeneity of false information and offering explanations for the detection results.Their approach demonstrates commendable accuracy and interpretability in a real-world fake news detection task.

within the system.Notably, QA-AXDS is fully automated and does not require expert experience as pre-input, which enhances the robustness of the system.Ni et al. [153] studied the use of a multi-view attention mechanism network (MVAN) [154] to detect fake news in social networks and provide explanations for the results.MVAN incorporates a dual attention mechanism, encompassing text semantic attention and propagation structure attention, to capture clues in source tweets and propagation structures.It identifies crucial keywords and generates explainable detection results.Raha et al. [155] proposed a neural model for factual inconsistency classification with explanations.By training four neural models, they can predict the inconsistency type and provide explanations for a given sentence.However, Bhattarai et al. [156] introduced an explainable fake news detection framework based on the Tsetlin Machine (TM) [157].By capturing lexical and semantic features of true and fake news texts, this framework achieves accurate detection of fake news and the credibility score is used to provide interpretability.

Fu et al. [158] introduced a comprehensive and explainable false information detection framework called DISCO, as depicted in Figure 8.This framework addresses the challenge of detecting false information by leveraging the heterogeneity of false information and offering explanations for the detection results.Their approach demonstrates commendable accuracy and interpretability in a real-world fake news detection task.We summarize the explainable model behavior analysis methods in Table 8, including the main techniques they use, datasets and accuracy performance.We summarize the explainable model behavior analysis methods in Table 8, including the main techniques they use, datasets and accuracy performance.","[['b153', 'b157', 'b151', 'b155', 'b156', 'b54', 'b152', 'b154'], ['b158'], ['b153', 'b157', 'b156', 'b155', 'b154'], ['b158']]","[['b153', 'b157', 'b151', 'b155', 'b156', 'b54', 'b152', 'b154'], ['b158'], ['b153', 'b157', 'b156', 'b155', 'b154'], ['b158']]",15,"sent1: Explainable model behavior, that is, explainable analysis of the results of model prediction behavior, provides the basis for prediction results.
sent2: Behavioral analysis typically involves comprehending the foundation of a model's anticipated behavior.
sent3: Since deep learning algorithms consist of nonlinear structures, these successful models are commonly obscure and have difficulty revealing the rationale of their forecasts in a format that humans can grasp.
sent4: The absence of transparency and intelligibility regarding a model's forecasts can lead to grave consequences.
sent5: Shu et al. [55] used the sentence-comment joint attention sub-network to improve the performance of fake news detection, aiming to capture the inherent interpretability of news phrases and user comments.
sent6: The dEFEND algorithm module facilitates search functionalities for searching news dissemination networks, trending news, top statements and related news.
sent7: Moreover, it presents test results and explanations.
sent8: In a similar vein, Lu et al. [151] utilized the graph-aware common attention network (GCAN) to assess the authenticity of source tweets on social media while providing explanations for the results.
sent9: GCAN uses the attention mechanism to capture three aspects of the algorithm results: highlighting key words in source tweets, identifying characteristics of retweet propagation paths and understanding the behavior of retweeters.
sent10: Chi et al. [152] proposed an automated explainable decision-making system (QA-AXDS) based on quantitative argumentation.
sent11: This system can detect fake news and explain the results to users.
sent12: It automatically captures human-level knowledge, constructs an interpretation model based on a dialogue tree and employs natural language to help users understand the reasoning process within the system.
sent13: Notably, QA-AXDS is fully automated and does not require expert experience as pre-input, which enhances the robustness of the system.
sent14: Ni et al. [153] studied the use of a multi-view attention mechanism network (MVAN) [154] to detect fake news in social networks and provide explanations for the results.
sent15: MVAN incorporates a dual attention mechanism, encompassing text semantic attention and propagation structure attention, to capture clues in source tweets and propagation structures.
sent16: It identifies crucial keywords and generates explainable detection results.
sent17: Raha et al. [155] proposed a neural model for factual inconsistency classification with explanations.
sent18: By training four neural models, they can predict the inconsistency type and provide explanations for a given sentence.
sent19: However, Bhattarai et al. [156] introduced an explainable fake news detection framework based on the Tsetlin Machine (TM) [157].By capturing lexical and semantic features of true and fake news texts, this framework achieves accurate detection of fake news and the credibility score is used to provide interpretability.
sent20: Fu et al. [158] introduced a comprehensive and explainable false information detection framework called DISCO, as depicted in Figure
sent21: 8.This framework addresses the challenge of detecting false information by leveraging the heterogeneity of false information and offering explanations for the detection results.
sent22: Their approach demonstrates commendable accuracy and interpretability in a real-world fake news detection task.within the system.
sent23: Notably, QA-AXDS is fully automated and does not require expert experience as pre-input, which enhances the robustness of the system.
sent24: Ni et al. [153] studied the use of a multi-view attention mechanism network (MVAN) [154] to detect fake news in social networks and provide explanations for the results.
sent25: MVAN incorporates a dual attention mechanism, encompassing text semantic attention and propagation structure attention, to capture clues in source tweets and propagation structures.
sent26: It identifies crucial keywords and generates explainable detection results.
sent27: Raha et al. [155] proposed a neural model for factual inconsistency classification with explanations.
sent28: By training four neural models, they can predict the inconsistency type and provide explanations for a given sentence.
sent29: However, Bhattarai et al. [156] introduced an explainable fake news detection framework based on the Tsetlin Machine (TM) [157].By capturing lexical and semantic features of true and fake news texts, this framework achieves accurate detection of fake news and the credibility score is used to provide interpretability.
sent30: Fu et al. [158] introduced a comprehensive and explainable false information detection framework called DISCO, as depicted in Figure
sent31: 8.This framework addresses the challenge of detecting false information by leveraging the heterogeneity of false information and offering explanations for the detection results.
sent32: Their approach demonstrates commendable accuracy and interpretability in a real-world fake news detection task.
sent33: We summarize the explainable model behavior analysis methods in Table 8, including the main techniques they use, datasets and accuracy performance.
sent34: We summarize the explainable model behavior analysis methods in Table 8, including the main techniques they use, datasets and accuracy performance."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s27,Fake News Detection Based on Pre-Training Model,"Traditional word embeddings may be difficult to capture complex contextual relationships, and they regard words as independent entities without considering the entire sentence structure.Vaswani et al. [17] introduced the Transformer, a deep learning model architecture, in 2017, which yielded exceptional outcomes in natural language processing tasks.The transformer enables the model to better capture the context and semantic information in the text so as to more accurately identify malicious content.Transformer introduces position encoding to process the position information of words in the input sequence.This allows the model to distinguish words in different locations, thereby avoiding the loss of location information.Its encoder-decoder structure can understand text at different levels and provide effective detection methods for multiple types of malicious behavior.Research [32,[86][87][88] and others have achieved good performance in detecting fake news using the transformer architecture.In practice, the choice of embedding depends on factors such as the size of the dataset, computing resources and the complexity of fake news detection tasks.Combining word embedding and transformer embedding may produce better results because word embedding captures the meaning of a single word while transformer embedding captures complex sentence-level semantics.Truică et al. [89] propose a new document embedding (DocEmb) constructed using word embeddings and transformers that achieves better results than more complex deep neural network models.In addition, Truică et al. [90] also proposed two bidirectional long short-term memory (BiLSTM) architectures, incorporating sentence transformers, to address two tasks: (1) a multi-class monolingual task of detecting fake news and (2) a multi-class cross-lingual task of detecting fake news.Using multiple transformer models may also achieve good performance.Truică et al. [91] proposed a new deep neural integration architecture based on transformers for false information detection (MisRoBAERTa), which uses RoBERTa-Bart sentences to embed error information and is superior to other transformer models in false information detection tasks.

In 2018, with the emergence and development of pre-training models, natural language processing tasks entered the era of pre-training models.Fine-tuning based on the BERT model [18] has significantly improved the performance of many natural language tasks.For fake news detection in natural language processing tasks of text data, the BERT pretraining model and related improved models gradually replace the original language model, which has become the basis of current research.Jwa et al. [87] combined news data in the pre-training phase to improve fake news recognition skills; Kaliyar et al. [92] proposed a BERT-based deep convolution method (fakeBERT) to detect fake news.The advantage of the pre-trained model is that the BERT method is unique in identifying and capturing contextual meanings in sentences or texts.In the model learning process, it does not need to go deep into the details of fake news to achieve good detection performance.

After several years of development, the BERT pre-training model based on transformer structure has gradually produced many related models [93][94][95] after structural adjustment, performance optimization and retraining.These models are collectively referred to as BERTology series models and have achieved good performance in various tasks.In summary, the fake news detection method based on the pre-trained model is already a research trend in this field.However, despite the complex characteristics of fake news, fake news detection based on pre-trained models still cannot achieve good performance in practical applications, like other practical tasks.How to extract features from more complex semantic information about fake news and establish a more effective fake news detection model for the 'pre-training + fine-tuning' paradigm of the pre-training model [96] is still an urgent problem to be solved.","[['b88', 'b16', 'b91', 'b89', 'b31', 'b90', 'b87', 'b86'], ['b92', 'b87', 'b17'], ['b93', 'b95', 'b96', 'b94']]","[['b88', 'b16', 'b91', 'b89', 'b31', 'b90', 'b87', 'b86'], ['b92', 'b87', 'b17'], ['b93', 'b95', 'b96', 'b94']]",15,"sent1: Traditional word embeddings may be difficult to capture complex contextual relationships, and they regard words as independent entities without considering the entire sentence structure.
sent2: Vaswani et al. [17] introduced the Transformer, a deep learning model architecture, in 2017, which yielded exceptional outcomes in natural language processing tasks.
sent3: The transformer enables the model to better capture the context and semantic information in the text so as to more accurately identify malicious content.
sent4: Transformer introduces position encoding to process the position information of words in the input sequence.
sent5: This allows the model to distinguish words in different locations, thereby avoiding the loss of location information.
sent6: Its encoder-decoder structure can understand text at different levels and provide effective detection methods for multiple types of malicious behavior.
sent7: Research [32,[86][87][88] and others have achieved good performance in detecting fake news using the transformer architecture.
sent8: In practice, the choice of embedding depends on factors such as the size of the dataset, computing resources and the complexity of fake news detection tasks.
sent9: Combining word embedding and transformer embedding may produce better results because word embedding captures the meaning of a single word while transformer embedding captures complex sentence-level semantics.
sent10: Truică et al. [89] propose a new document embedding (DocEmb) constructed using word embeddings and transformers that achieves better results than more complex deep neural network models.
sent11: In addition, Truică et al. [90] also proposed two bidirectional long short-term memory (BiLSTM) architectures, incorporating sentence transformers, to address two tasks: (1) a multi-class monolingual task of detecting fake news and (2) a multi-class cross-lingual task of detecting fake news.
sent12: Using multiple transformer models may also achieve good performance.
sent13: Truică et al. [91] proposed a new deep neural integration architecture based on transformers for false information detection (MisRoBAERTa), which uses RoBERTa-Bart sentences to embed error information and is superior to other transformer models in false information detection tasks.
sent14: In 2018, with the emergence and development of pre-training models, natural language processing tasks entered the era of pre-training models.
sent15: Fine-tuning based on the BERT model [18] has significantly improved the performance of many natural language tasks.
sent16: For fake news detection in natural language processing tasks of text data, the BERT pretraining model and related improved models gradually replace the original language model, which has become the basis of current research.
sent17: Jwa et al. [87] combined news data in the pre-training phase to improve fake news recognition skills; Kaliyar et al. [92] proposed a BERT-based deep convolution method (fakeBERT) to detect fake news.
sent18: The advantage of the pre-trained model is that the BERT method is unique in identifying and capturing contextual meanings in sentences or texts.
sent19: In the model learning process, it does not need to go deep into the details of fake news to achieve good detection performance.
sent20: After several years of development, the BERT pre-training model based on transformer structure has gradually produced many related models [93][94][95] after structural adjustment, performance optimization and retraining.
sent21: These models are collectively referred to as BERTology series models and have achieved good performance in various tasks.
sent22: In summary, the fake news detection method based on the pre-trained model is already a research trend in this field.
sent23: However, despite the complex characteristics of fake news, fake news detection based on pre-trained models still cannot achieve good performance in practical applications, like other practical tasks.
sent24: How to extract features from more complex semantic information about fake news and establish a more effective fake news detection model for the 'pre-training + fine-tuning' paradigm of the pre-training model [96] is still an urgent problem to be solved."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s26,Fake News Detection based on Deep Learning,"Because machine learning is based on manual feature extraction, there will be deviations, and it performs poorly in feature extraction speed.In addition, machine learning produces high-dimensional representations of language information, resulting in dimensional disasters.In contrast, deep learning has more advantages than machine learning, showing higher accuracy and precision in fake news detection.Lai et al. [79] compared several machine learning and deep learning models based on pure content features.They found that the performance of the neural network model is better than the traditional ML model, and the accuracy of the neural network model is about 6% higher than the ML model.The essence of the neural network model is to use the method of word embedding [80] to combine the language model and feature learning to detect fake news.The fake news detection model based on a neural network has achieved relevant results in the research of fake news detection in many languages by using the word embedding method.A word embedding is a numerical representation of a word that captures its semantics based on the context in a given corpus.Word embedding can help understand semantics and find contextual clues, which is helpful for false news detection.Ilie et al. [81] used three word embeddings, Word2Vec, FastText and GloVe, to preserve word context, trained multiple deep learning architectures for classification and compared their performance in detecting the authenticity of news articles, ultimately obtaining the best results using a recursive convolutional neural network-based architecture.

The most typical recurrent neural network and convolutional neural network in deep learning can be used to solve the problem of fake news detection.Ma et al. [82] first used the hidden layer of a recurrent neural network to represent fake news information and proved that the model is superior to the good performance of artificial features.Since then, a model called FNDNet (Deep CNN) [83] has been proposed to learn the discriminative features of detecting fake news using multiple hidden layers.In addition, Huang et al. [84] used a graph convolutional neural network (GCN) [85] to learn user representations from graphs created by user behavior information.","[['b80', 'b79', 'b81'], ['b84', 'b85', 'b83', 'b82']]","[['b80', 'b79', 'b81'], ['b84', 'b85', 'b83', 'b82']]",7,"sent1: Because machine learning is based on manual feature extraction, there will be deviations, and it performs poorly in feature extraction speed.
sent2: In addition, machine learning produces high-dimensional representations of language information, resulting in dimensional disasters.
sent3: In contrast, deep learning has more advantages than machine learning, showing higher accuracy and precision in fake news detection.
sent4: Lai et al. [79] compared several machine learning and deep learning models based on pure content features.
sent5: They found that the performance of the neural network model is better than the traditional ML model, and the accuracy of the neural network model is about 6% higher than the ML model.
sent6: The essence of the neural network model is to use the method of word embedding [80] to combine the language model and feature learning to detect fake news.
sent7: The fake news detection model based on a neural network has achieved relevant results in the research of fake news detection in many languages by using the word embedding method.
sent8: A word embedding is a numerical representation of a word that captures its semantics based on the context in a given corpus.
sent9: Word embedding can help understand semantics and find contextual clues, which is helpful for false news detection.
sent10: Ilie et al. [81] used three word embeddings, Word2Vec, FastText and GloVe, to preserve word context, trained multiple deep learning architectures for classification and compared their performance in detecting the authenticity of news articles, ultimately obtaining the best results using a recursive convolutional neural network-based architecture.
sent11: The most typical recurrent neural network and convolutional neural network in deep learning can be used to solve the problem of fake news detection.
sent12: Ma et al. [82] first used the hidden layer of a recurrent neural network to represent fake news information and proved that the model is superior to the good performance of artificial features.
sent13: Since then, a model called FNDNet (Deep CNN) [83] has been proposed to learn the discriminative features of detecting fake news using multiple hidden layers.
sent14: In addition, Huang et al. [84] used a graph convolutional neural network (GCN) [85] to learn user representations from graphs created by user behavior information."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s7,Literature Search,"This paper uses the Google Scholar database as a reliable source to assess recent trends in fake news detection research over the past five years.Using ""fake news"", ""fake news detection"", ""multimodal fake news detection"", ""multidisciplinary + fake news"" and ""explainable fake news detection"" as relevant keywords, the database was queried.The search yielded a total of 18,100 references published in the fake news detection field between 2018 and 2023.The results obtained were carefully analyzed to identify the prominent research directions in the field.This review article provides a comprehensive overview of the current state of research in fake news detection and highlights the most promising avenues for future investigation.With the help of deep insights into the various concerns in the field of fake news detection research, we are able to better appreciate the potential of this technique to develop more efficient and credible fake news detection techniques.

The specific research on fake news detection includes fact verification, position detection, topic detection and other tasks involving text classification, text clustering, image understanding, speech recognition and other research directions.The related research on fake news detection uses technologies such as text mining [10], machine learning [11], deep learning [12], natural language processing [13], machine vision [14] and other technologies to extract and identify key information from subjective visual perceptions of text or news pages.According to the summary and classification of a large number of references, the entire fake news detection method can be divided into three stages: (1) Machine learning stage.(2) Compared with machine learning algorithms, deep learning is not limited by manual feature extraction.It can extract text features from language texts through the self-learning ability of the network layer, which greatly improves the system performance of natural language processing tasks [12].Deep learning networks, including convolutional neural networks [15] and recurrent neural networks [16], are applied to fake news detection tasks.They can effectively learn complex semantic features and high-level semantic representations from text and have been shown to improve the performance of fake news detection tasks.(3) However, the manual annotation of large text data is very complex, and the data used for natural language processing tasks is very limited.The application of deep learning models with strong dependency data in natural language processing is also very challenging.In order to avoid the problems of overfitting and insufficient generalization ability caused by insufficient data volume, researchers began to explore the pre-trained model for semantic representation.So far, the pre-training model based on the transformer structure [17] has been vigorously developed.The representative BERT pre-training model [18], the GPT model [19][20][21], etc., have made rapid progress in the development of natural language processing.At the same time, the relevant studies on fake news detection have also been further developed.","[[], ['b15', 'b14', 'b10', 'b16', 'b17', 'b20', 'b9', 'b18', 'b11', 'b13', 'b12', 'b19']]","[[], ['b15', 'b14', 'b10', 'b16', 'b17', 'b20', 'b9', 'b18', 'b11', 'b13', 'b12', 'b19']]",12,"sent1: This paper uses the Google Scholar database as a reliable source to assess recent trends in fake news detection research over the past five years.
sent2: Using ""fake news"", ""fake news detection"", ""multimodal fake news detection"", ""multidisciplinary + fake news"" and ""explainable fake news detection"" as relevant keywords, the database was queried.
sent3: The search yielded a total of 18,100 references published in the fake news detection field between 2018 and 2023.The results obtained were carefully analyzed to identify the prominent research directions in the field.
sent4: This review article provides a comprehensive overview of the current state of research in fake news detection and highlights the most promising avenues for future investigation.
sent5: With the help of deep insights into the various concerns in the field of fake news detection research, we are able to better appreciate the potential of this technique to develop more efficient and credible fake news detection techniques.
sent6: The specific research on fake news detection includes fact verification, position detection, topic detection and other tasks involving text classification, text clustering, image understanding, speech recognition and other research directions.
sent7: The related research on fake news detection uses technologies such as text mining [10], machine learning [11], deep learning [12], natural language processing [13], machine vision [14] and other technologies to extract and identify key information from subjective visual perceptions of text or news pages.
sent8: According to the summary and classification of a large number of references, the entire fake news detection method can be divided into three stages: (1) Machine learning stage.(2)
sent9: Compared with machine learning algorithms, deep learning is not limited by manual feature extraction.
sent10: It can extract text features from language texts through the self-learning ability of the network layer, which greatly improves the system performance of natural language processing tasks [12].Deep learning networks, including convolutional neural networks [15] and recurrent neural networks [16], are applied to fake news detection tasks.
sent11: They can effectively learn complex semantic features and high-level semantic representations from text and have been shown to improve the performance of fake news detection tasks.(3)
sent12: However, the manual annotation of large text data is very complex, and the data used for natural language processing tasks is very limited.
sent13: The application of deep learning models with strong dependency data in natural language processing is also very challenging.
sent14: In order to avoid the problems of overfitting and insufficient generalization ability caused by insufficient data volume, researchers began to explore the pre-trained model for semantic representation.
sent15: So far, the pre-training model based on the transformer structure [17] has been vigorously developed.
sent16: The representative BERT pre-training model [18], the GPT model [19][20][21], etc., have made rapid progress in the development of natural language processing.
sent17: At the same time, the relevant studies on fake news detection have also been further developed."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s14,Detection Method Based on Social Network,"The content-based approach can discover the linguistic features of true and fake news.However, sometimes fake news will mislead readers by deliberately imitating the writing techniques of real news.The content-based approach cannot distinguish the feature differences between such fake news and true news.In order to solve this problem, we can make full use of hidden information as auxiliary data, such as social background information and propagation paths in social networks.Social background information is one of the research directions.Shu et al. [29] explored the relationship between user data and fake news on social media and used the user's social participation as auxiliary Systems 2023, 11, 458 5 of 26 information for detection.Furthermore, Shu et al. [30] proposed a framework to simulate the triadic relationship between news publishers, news articles and users, extract effective features from the participation behavior of news publishers and readers, and then capture the interaction between them.Studies have shown that the use of social background information can not only improve the effect of fake news detection but also effectively predict it early.Another research direction detects fake news by simulating the propagation path of fake news in the network.Through experiments, Monti et al. [31] found that the mode of transmission is an important feature of fake news that exceeds other aspects such as news content, user data and social behavior.Raza et al. [32] proposed a fake news detection framework based on the Transformer architecture, which includes encoder and decoder parts.The encoder part is used to learn the representation of fake news data, and the decoder part is used to predict future behavior based on past observations.The model uses the characteristics of news content and social background to improve classification accuracy.","[['b31', 'b29', 'b30', 'b28']]","[['b31', 'b29', 'b30', 'b28']]",4,"sent1: The content-based approach can discover the linguistic features of true and fake news.
sent2: However, sometimes fake news will mislead readers by deliberately imitating the writing techniques of real news.
sent3: The content-based approach cannot distinguish the feature differences between such fake news and true news.
sent4: In order to solve this problem, we can make full use of hidden information as auxiliary data, such as social background information and propagation paths in social networks.
sent5: Social background information is one of the research directions.
sent6: Shu et al. [29] explored the relationship between user data and fake news on social media and used the user's social participation as auxiliary Systems 2023, 11, 458 5 of 26 information for detection.
sent7: Furthermore, Shu et al. [30] proposed a framework to simulate the triadic relationship between news publishers, news articles and users, extract effective features from the participation behavior of news publishers and readers, and then capture the interaction between them.
sent8: Studies have shown that the use of social background information can not only improve the effect of fake news detection but also effectively predict it early.
sent9: Another research direction detects fake news by simulating the propagation path of fake news in the network.
sent10: Through experiments, Monti et al. [31] found that the mode of transmission is an important feature of fake news that exceeds other aspects such as news content, user data and social behavior.
sent11: Raza et al. [32] proposed a fake news detection framework based on the Transformer architecture, which includes encoder and decoder parts.
sent12: The encoder part is used to learn the representation of fake news data, and the decoder part is used to predict future behavior based on past observations.
sent13: The model uses the characteristics of news content and social background to improve classification accuracy."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s15,Knowledge-Based Detection Method,"Knowledge-based (KB) fake news detection detects the authenticity of news by verifying fake news and facts, so this is also called fact checking.Fact checking can be divided into two categories: manual verification and automatic verification [6].The manual method uses domain expert knowledge or the crowdsourcing method.It has high accuracy but low efficiency, which cannot meet the needs of the era of big data.The automatic verification method using natural language processing and machine learning technology has become a hot research field.Fact checking first needs to construct a knowledge base or knowledge graph from the network through knowledge extraction.Then it compares and verifies the fake news with the knowledge base or knowledge graph to judge the authenticity of the news.Pan et al. [33] used knowledge graphs to detect fake news based on news content.They solved the problem that computational fact checking is not comprehensive enough.By extracting the triples from news articles, their method's F0 score exceeds 80.1.Hu et al. [34] developed a heterogeneous graph attention network to learn the context of news representation and encode the semantics of news content.By using an entity comparison network, they compare the context entity representation with the derived representation from the knowledge base (KB).This comparison aims to capture the consistency between the news content and the KB.

Based on the above analysis and the content fake news detection method, we try to extract effective features from text information and locate the key information about fake news.Fake news based on social networks and knowledge requires not only news information itself but also vast external resources, such as stance information, knowledge information and multi-modal feature information.In text-based fake news detection, we try to analyze the style and content characteristics of the news, capture specific features and judge the authenticity of the news.At the same time, there are also studies [35] that combine content features and environmental features as the input of the classifier.Additionally, they integrate user data, social behavior, propagation paths and other features to optimize the detection method [31].The existing methods are all different aspects of fake news detection methods, but they also have limitations.How to combine the existing methods to improve them and effectively improve the performance of fake news detection has become an urgent problem to be solved.","[[None, 'b33', 'b32'], ['b30', 'b34']]","[[None, 'b33', 'b32'], ['b30', 'b34']]",5,"sent1: Knowledge-based (KB) fake news detection detects the authenticity of news by verifying fake news and facts, so this is also called fact checking.
sent2: Fact checking can be divided into two categories: manual verification and automatic verification [6].The manual method uses domain expert knowledge or the crowdsourcing method.
sent3: It has high accuracy but low efficiency, which cannot meet the needs of the era of big data.
sent4: The automatic verification method using natural language processing and machine learning technology has become a hot research field.
sent5: Fact checking first needs to construct a knowledge base or knowledge graph from the network through knowledge extraction.
sent6: Then it compares and verifies the fake news with the knowledge base or knowledge graph to judge the authenticity of the news.
sent7: Pan et al. [33] used knowledge graphs to detect fake news based on news content.
sent8: They solved the problem that computational fact checking is not comprehensive enough.
sent9: By extracting the triples from news articles, their method's F0 score exceeds 80.1.Hu et al. [34] developed a heterogeneous graph attention network to learn the context of news representation and encode the semantics of news content.
sent10: By using an entity comparison network, they compare the context entity representation with the derived representation from the knowledge base (KB).This comparison aims to capture the consistency between the news content and the KB.
sent11: Based on the above analysis and the content fake news detection method, we try to extract effective features from text information and locate the key information about fake news.
sent12: Fake news based on social networks and knowledge requires not only news information itself but also vast external resources, such as stance information, knowledge information and multi-modal feature information.
sent13: In text-based fake news detection, we try to analyze the style and content characteristics of the news, capture specific features and judge the authenticity of the news.
sent14: At the same time, there are also studies [35] that combine content features and environmental features as the input of the classifier.
sent15: Additionally, they integrate user data, social behavior, propagation paths and other features to optimize the detection method [31].The existing methods are all different aspects of fake news detection methods, but they also have limitations.
sent16: How to combine the existing methods to improve them and effectively improve the performance of fake news detection has become an urgent problem to be solved."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s16,Multimodal Fake News Detection,"In addition to the detection method based on a single feature source, it can also combine multiple features for fake news detection.In recent years, the data used in fake news detection is no longer limited to text information, and there has been an increasing focus on visual features.Multimodal fake news detection refers to the use of multiple types (such as text, images, etc.) of data to determine whether a news report contains misleading or inaccurate content [36][37][38].Cao et al. [39] found that visual content has become an important part of fake news.Fake news often uses unverified visual content (video, images, etc.) to mislead readers and deepen their trust in false information.Pictures, videos and other media information can also be applied to fake news detection.Figure 2 shows some examples of fake news that we collected on the network, with both textual and visual features.Fortunately, many multimodal datasets have been made available.For example, Shu et al. [23] proposed FakeNewsNet, a fake news resource library that covers news content, social environments and spatio-temporal information.It greatly enhances the capability of multi-feature fusion for detecting fake news.

In addition to the detection method based on a single feature source, it can also combine multiple features for fake news detection.In recent years, the data used in fake news detection is no longer limited to text information, and there has been an increasing focus on visual features.Multimodal fake news detection refers to the use of multiple types (such as text, images, etc.) of data to determine whether a news report contains misleading or inaccurate content [36][37][38].Cao et al. [39] found that visual content has become an important part of fake news.Fake news often uses unverified visual content (video, images, etc.) to mislead readers and deepen their trust in false information.Pictures, videos and other media information can also be applied to fake news detection.Figure 2 shows some examples of fake news that we collected on the network, with both textual and visual features.Fortunately, many multimodal datasets have been made available.For example, Shu et al. [23] proposed FakeNewsNet, a fake news resource library that covers news content, social environments and spatio-temporal information.It greatly enhances the capability of multi-feature fusion for detecting fake news.The main idea of the multimodal method is to train features from different modalities and then fuse them.Some fake news detection methods have been integrated into methods based on cross-modal comparative learning.For example, Qi et al. [40] mapped the pictures in fake news to the frequency and pixel domains and then fused the visual information in the two domains through a multi-domain visual neural network.Singhal et al.The main idea of the multimodal method is to train features from different modalities and then fuse them.Some fake news detection methods have been integrated into methods based on cross-modal comparative learning.For example, Qi et al. [40] mapped the pictures in fake news to the frequency and pixel domains and then fused the visual information in the two domains through a multi-domain visual neural network.Singhal et al. [41] obtained text features and visual features through pre-training models and fused them into new feature representations.Their simple, unified framework is shown in Figure 3 (the fake news image in Figure 3 comes from the network).

In addition, research on multimodal fake news detection has gradually increased in recent years.Qian et al. [42] proposed a hierarchical multi-modal context attention network for fake news detection, which includes two modules: a multi-modal context attention module and a hierarchical coding module.To model the multi-modal context of news posts, the multi-modal context attention module uses pre-trained BERT [18] for text representation and pre-trained ResNet [43] for image representation, ensuring a seamless integration of both textual and visual information.It combines inter-modal and intra-modal relationships to enhance fake news detection.The hierarchical coding module captures the rich hierarchical semantics of the text to improve the representation of multimodal news.In addition, research on multimodal fake news detection has gradually increased in recent years.Qian et al. [42] proposed a hierarchical multi-modal context attention network for fake news detection, which includes two modules: a multi-modal context attention module and a hierarchical coding module.To model the multi-modal context of news posts, the multi-modal context attention module uses pre-trained BERT [18] for text representation and pre-trained ResNet [43] for image representation, ensuring a seamless integration of both textual and visual information.It combines inter-modal and intra-modal relationships to enhance fake news detection.The hierarchical coding module captures the rich hierarchical semantics of the text to improve the representation of multimodal news.

The MCAN model proposed by Wu et al. [36] aims to learn multi-modal fusion representations by considering the dependencies between different modalities.The model includes three main steps: feature extraction, feature fusion and fake news detection.In the feature extraction step, three sub-models are used to extract features from the spatial domain, frequency domain and text.The VGG-19 [44] network is used to extract visual features from the spatial domain, and the ACNN-based sub-network is designed to extract features from the frequency domain, especially for re-compressed or tampered images.Furthermore, the BERT model is used to obtain the text features of the text content.In the feature fusion step, the deep common attention model is used to fuse multimodal features.The fusion process simulates the way that humans first see the image and then read the text.The common attention model is composed of multiple common attention layers, which capture the interdependence between different features.Finally, the fusion feature is used to detect fake news, and the output of the common attention model is used to judge the authenticity of the input news.

Wang et al. [37] proposed a cross-modal contrastive learning framework, COOLANT, for multimodal fake news detection.The framework consists of three main components: a cross-modal contrastive learning module for alignment, a cross-modal fusion module for learning cross-modal correction and a cross-modal aggregation module with an attention mechanism and guidance to improve the performance of multimodal fake news detection.The cross-modal contrast learning module aligns features by converting singlemodal embedding into a shared space.It uses auxiliary cross-modal consistency learning tasks to measure the semantic similarity between images and texts and provides soft targets for the contrast learning module.The contrastive learning module uses the contrast loss to predict the actual image-text pairing in the batch.The MCAN model proposed by Wu et al. [36] aims to learn multi-modal fusion representations by considering the dependencies between different modalities.The model includes three main steps: feature extraction, feature fusion and fake news detection.In the feature extraction step, three sub-models are used to extract features from the spatial domain, frequency domain and text.The VGG-19 [44] network is used to extract visual features from the spatial domain, and the ACNN-based sub-network is designed to extract features from the frequency domain, especially for re-compressed or tampered images.Furthermore, the BERT model is used to obtain the text features of the text content.In the feature fusion step, the deep common attention model is used to fuse multimodal features.The fusion process simulates the way that humans first see the image and then read the text.The common attention model is composed of multiple common attention layers, which capture the interdependence between different features.Finally, the fusion feature is used to detect fake news, and the output of the common attention model is used to judge the authenticity of the input news.

Wang et al. [37] proposed a cross-modal contrastive learning framework, COOLANT, for multimodal fake news detection.The framework consists of three main components: a cross-modal contrastive learning module for alignment, a cross-modal fusion module for learning cross-modal correction and a cross-modal aggregation module with an attention mechanism and guidance to improve the performance of multimodal fake news detection.The cross-modal contrast learning module aligns features by converting single-modal embedding into a shared space.It uses auxiliary cross-modal consistency learning tasks to measure the semantic similarity between images and texts and provides soft targets for the contrast learning module.The contrastive learning module uses the contrast loss to predict the actual image-text pairing in the batch.

In Table 2, we summarize several multimodal fake news detection methods, including the main techniques they use, datasets and extracted features and the corresponding accuracy performance.","[['b22', 'b37', 'b35', 'b38', 'b36'], ['b22', 'b37', 'b35', 'b40', 'b38', 'b36', 'b39'], ['b41', 'b17', 'b42'], ['b43', 'b35'], ['b43', 'b36', 'b35'], ['b36'], []]","[['b22', 'b37', 'b35', 'b38', 'b36'], ['b22', 'b37', 'b35', 'b40', 'b38', 'b36', 'b39'], ['b41', 'b17', 'b42'], ['b43', 'b35'], ['b43', 'b36', 'b35'], ['b36'], []]",21,"sent1: In addition to the detection method based on a single feature source, it can also combine multiple features for fake news detection.
sent2: In recent years, the data used in fake news detection is no longer limited to text information, and there has been an increasing focus on visual features.
sent3: Multimodal fake news detection refers to the use of multiple types (such as text, images, etc.) of data to determine whether a news report contains misleading or inaccurate content [36][37][38].Cao et al. [39] found that visual content has become an important part of fake news.
sent4: Fake news often uses unverified visual content (video, images, etc.) to mislead readers and deepen their trust in false information.
sent5: Pictures, videos and other media information can also be applied to fake news detection.
sent6: Figure 2 shows some examples of fake news that we collected on the network, with both textual and visual features.
sent7: Fortunately, many multimodal datasets have been made available.
sent8: For example, Shu et al. [23] proposed FakeNewsNet, a fake news resource library that covers news content, social environments and spatio-temporal information.
sent9: It greatly enhances the capability of multi-feature fusion for detecting fake news.
sent10: In addition to the detection method based on a single feature source, it can also combine multiple features for fake news detection.
sent11: In recent years, the data used in fake news detection is no longer limited to text information, and there has been an increasing focus on visual features.
sent12: Multimodal fake news detection refers to the use of multiple types (such as text, images, etc.) of data to determine whether a news report contains misleading or inaccurate content [36][37][38].Cao et al. [39] found that visual content has become an important part of fake news.
sent13: Fake news often uses unverified visual content (video, images, etc.) to mislead readers and deepen their trust in false information.
sent14: Pictures, videos and other media information can also be applied to fake news detection.
sent15: Figure 2 shows some examples of fake news that we collected on the network, with both textual and visual features.
sent16: Fortunately, many multimodal datasets have been made available.
sent17: For example, Shu et al. [23] proposed FakeNewsNet, a fake news resource library that covers news content, social environments and spatio-temporal information.
sent18: It greatly enhances the capability of multi-feature fusion for detecting fake news.
sent19: The main idea of the multimodal method is to train features from different modalities and then fuse them.
sent20: Some fake news detection methods have been integrated into methods based on cross-modal comparative learning.
sent21: For example, Qi et al. [40] mapped the pictures in fake news to the frequency and pixel domains and then fused the visual information in the two domains through a multi-domain visual neural network.
sent22: Singhal et al.The main idea of the multimodal method is to train features from different modalities and then fuse them.
sent23: Some fake news detection methods have been integrated into methods based on cross-modal comparative learning.
sent24: For example, Qi et al. [40] mapped the pictures in fake news to the frequency and pixel domains and then fused the visual information in the two domains through a multi-domain visual neural network.
sent25: Singhal et al. [41] obtained text features and visual features through pre-training models and fused them into new feature representations.
sent26: Their simple, unified framework is shown in Figure 3 (the fake news image in Figure 3 comes from the network).
sent27: In addition, research on multimodal fake news detection has gradually increased in recent years.
sent28: Qian et al. [42] proposed a hierarchical multi-modal context attention network for fake news detection, which includes two modules: a multi-modal context attention module and a hierarchical coding module.
sent29: To model the multi-modal context of news posts, the multi-modal context attention module uses pre-trained BERT [18] for text representation and pre-trained ResNet [43] for image representation, ensuring a seamless integration of both textual and visual information.
sent30: It combines inter-modal and intra-modal relationships to enhance fake news detection.
sent31: The hierarchical coding module captures the rich hierarchical semantics of the text to improve the representation of multimodal news.
sent32: In addition, research on multimodal fake news detection has gradually increased in recent years.
sent33: Qian et al. [42] proposed a hierarchical multi-modal context attention network for fake news detection, which includes two modules: a multi-modal context attention module and a hierarchical coding module.
sent34: To model the multi-modal context of news posts, the multi-modal context attention module uses pre-trained BERT [18] for text representation and pre-trained ResNet [43] for image representation, ensuring a seamless integration of both textual and visual information.
sent35: It combines inter-modal and intra-modal relationships to enhance fake news detection.
sent36: The hierarchical coding module captures the rich hierarchical semantics of the text to improve the representation of multimodal news.
sent37: The MCAN model proposed by Wu et al. [36] aims to learn multi-modal fusion representations by considering the dependencies between different modalities.
sent38: The model includes three main steps: feature extraction, feature fusion and fake news detection.
sent39: In the feature extraction step, three sub-models are used to extract features from the spatial domain, frequency domain and text.
sent40: The VGG-19 [44] network is used to extract visual features from the spatial domain, and the ACNN-based sub-network is designed to extract features from the frequency domain, especially for re-compressed or tampered images.
sent41: Furthermore, the BERT model is used to obtain the text features of the text content.
sent42: In the feature fusion step, the deep common attention model is used to fuse multimodal features.
sent43: The fusion process simulates the way that humans first see the image and then read the text.
sent44: The common attention model is composed of multiple common attention layers, which capture the interdependence between different features.
sent45: Finally, the fusion feature is used to detect fake news, and the output of the common attention model is used to judge the authenticity of the input news.
sent46: Wang et al. [37] proposed a cross-modal contrastive learning framework, COOLANT, for multimodal fake news detection.
sent47: The framework consists of three main components: a cross-modal contrastive learning module for alignment, a cross-modal fusion module for learning cross-modal correction and a cross-modal aggregation module with an attention mechanism and guidance to improve the performance of multimodal fake news detection.
sent48: The cross-modal contrast learning module aligns features by converting singlemodal embedding into a shared space.
sent49: It uses auxiliary cross-modal consistency learning tasks to measure the semantic similarity between images and texts and provides soft targets for the contrast learning module.
sent50: The contrastive learning module uses the contrast loss to predict the actual image-text pairing in the batch.
sent51: The MCAN model proposed by Wu et al. [36] aims to learn multi-modal fusion representations by considering the dependencies between different modalities.
sent52: The model includes three main steps: feature extraction, feature fusion and fake news detection.
sent53: In the feature extraction step, three sub-models are used to extract features from the spatial domain, frequency domain and text.
sent54: The VGG-19 [44] network is used to extract visual features from the spatial domain, and the ACNN-based sub-network is designed to extract features from the frequency domain, especially for re-compressed or tampered images.
sent55: Furthermore, the BERT model is used to obtain the text features of the text content.
sent56: In the feature fusion step, the deep common attention model is used to fuse multimodal features.
sent57: The fusion process simulates the way that humans first see the image and then read the text.
sent58: The common attention model is composed of multiple common attention layers, which capture the interdependence between different features.
sent59: Finally, the fusion feature is used to detect fake news, and the output of the common attention model is used to judge the authenticity of the input news.
sent60: Wang et al. [37] proposed a cross-modal contrastive learning framework, COOLANT, for multimodal fake news detection.
sent61: The framework consists of three main components: a cross-modal contrastive learning module for alignment, a cross-modal fusion module for learning cross-modal correction and a cross-modal aggregation module with an attention mechanism and guidance to improve the performance of multimodal fake news detection.
sent62: The cross-modal contrast learning module aligns features by converting single-modal embedding into a shared space.
sent63: It uses auxiliary cross-modal consistency learning tasks to measure the semantic similarity between images and texts and provides soft targets for the contrast learning module.
sent64: The contrastive learning module uses the contrast loss to predict the actual image-text pairing in the batch.
sent65: In Table 2, we summarize several multimodal fake news detection methods, including the main techniques they use, datasets and extracted features and the corresponding accuracy performance."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s17,Multidisciplinary Research Progress,"In the past few decades, there have been studies on fake news and its detection in many disciplines, including computer science, sociology, psychology, linguistics, communication and neurocognitive science [30,[45][46][47].Each field has its own research content and research methods for fake news.There are also studies [46] that combine the knowledge of these fields and use interdisciplinary methods to detect fake news.Multidisciplinary false news theory research can help the natural sciences achieve fake news detection.Figure 4 summarizes the basic problems of fake news detection in various disciplines, and we also hope that future research can combine more interdisciplinary knowledge.

In Table 2, we summarize several multimodal fake news detection methods, including the main techniques they use, datasets and extracted features and the corresponding accuracy performance.","[['b45', 'b29', 'b46', 'b44'], []]","[['b45', 'b29', 'b46', 'b44'], []]",4,"sent1: In the past few decades, there have been studies on fake news and its detection in many disciplines, including computer science, sociology, psychology, linguistics, communication and neurocognitive science [30,[45][46][47].Each field has its own research content and research methods for fake news.
sent2: There are also studies [46] that combine the knowledge of these fields and use interdisciplinary methods to detect fake news.
sent3: Multidisciplinary false news theory research can help the natural sciences achieve fake news detection.
sent4: Figure 4 summarizes the basic problems of fake news detection in various disciplines, and we also hope that future research can combine more interdisciplinary knowledge.
sent5: In Table 2, we summarize several multimodal fake news detection methods, including the main techniques they use, datasets and extracted features and the corresponding accuracy performance."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s8,Fake News Classification,"The categorization of fake news is multifaceted and diverse, encompassing everything from unverified hearsay circulating on social media to deceitful propaganda deliberately spread by its creators.According to references [22][23][24][25], fake news is classified into five categories, illustrated in Table 1: (1) deceptive fake news; (2) false information of rumor nature; (3) false comment information; (4) headline party-type fake news; (5) fact-based recombination of false information.","[['b23', 'b21', 'b24', 'b22']]","[['b23', 'b21', 'b24', 'b22']]",4,"sent1: The categorization of fake news is multifaceted and diverse, encompassing everything from unverified hearsay circulating on social media to deceitful propaganda deliberately spread by its creators.
sent2: According to references [22][23][24][25], fake news is classified into five categories, illustrated in Table 1: (1) deceptive fake news; (2) false information of rumor nature; (3) false comment information; (4) headline party-type fake news; (5) fact-based recombination of false information."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s19,Psychology,"Psychological researchers mainly explore the cognitive behavior of fake news and study the psychological mechanisms of fake news dissemination [48,49].Bordia et al. [50] found that the interactive behavior of fake news is largely based on people's psychological Systems 2023, 11, 458 9 of 26 needs for the truth of the facts after analyzing the content of fake news on the Internet.The psychological factors that promote the dissemination of fake news include uncertainty, anxiety, etc. Pennycook et al. [51] investigated why people believe and share fake or highly misleading news online.They believe that it is the strong causal effect of political motivation on beliefs that makes people believe fake news.","[['b50', 'b47', 'b49', 'b48']]","[['b50', 'b47', 'b49', 'b48']]",4,"sent1: Psychological researchers mainly explore the cognitive behavior of fake news and study the psychological mechanisms of fake news dissemination [48,49].Bordia et al. [50] found that the interactive behavior of fake news is largely based on people's psychological Systems 2023, 11, 458 9 of 26 needs for the truth of the facts after analyzing the content of fake news on the Internet.
sent2: The psychological factors that promote the dissemination of fake news include uncertainty, anxiety, etc.
sent3: Pennycook et al. [51] investigated why people believe and share fake or highly misleading news online.
sent4: They believe that it is the strong causal effect of political motivation on beliefs that makes people believe fake news."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s24,General Technical Model of Fake News Detection,"From the perspective of technical methods, the artificial intelligence technology used in fake news detection [65] involves many research fields such as natural language processing, computer vision and data mining [66].Fake news is divided into three categories: false text news, false picture news and false video news.For false text news detection, natural language processing has gradually become an important technical means in social science and information dissemination research [6].Its primary applications encompass sentiment analysis, which centers around text classification techniques; news summarization generation, which focuses on text summarization techniques; and opinion mining, which relies on topic modeling techniques [67].Therefore, for the detection of false information in text data, research on the application of natural language processing technology [68] is also constantly developing.For picture news, video news, etc., in terms of false picture news detection, researchers use computer vision technology [69-71] to detect false pictures synthesized.In addition, the continuous development of deep synthesis technology has led to the proliferation of fake videos.Researchers use deep learning technology to detect face tampering in videos [72].

It is generally believed that false text detection has gone through three stages: the first stage, the artificial feature design stage, started in 2011, which is mainly manual feature extraction based on expert knowledge; the second stage, the data-driven stage, started in 2016, is based on the research of various methods based on deep learning.The third stage, so far, is the research and exploration of the integration of knowledge and data.This stage is based on the pre-training model.Based on the above analysis, this paper believes that the specific technical methods of fake news detection research have the following aspects:","[['b67', 'b71', 'b66', None, 'b65'], []]","[['b67', 'b71', 'b66', None, 'b65'], []]",5,"sent1: From the perspective of technical methods, the artificial intelligence technology used in fake news detection [65] involves many research fields such as natural language processing, computer vision and data mining [66].Fake news is divided into three categories: false text news, false picture news and false video news.
sent2: For false text news detection, natural language processing has gradually become an important technical means in social science and information dissemination research [6].Its primary applications encompass sentiment analysis, which centers around text classification techniques; news summarization generation, which focuses on text summarization techniques; and opinion mining, which relies on topic modeling techniques [67].Therefore, for the detection of false information in text data, research on the application of natural language processing technology [68] is also constantly developing.
sent3: For picture news, video news, etc., in terms of false picture news detection, researchers use computer vision technology [69-71] to detect false pictures synthesized.
sent4: In addition, the continuous development of deep synthesis technology has led to the proliferation of fake videos.
sent5: Researchers use deep learning technology to detect face tampering in videos [72].
sent6: It is generally believed that false text detection has gone through three stages: the first stage, the artificial feature design stage, started in 2011, which is mainly manual feature extraction based on expert knowledge; the second stage, the data-driven stage, started in 2016, is based on the research of various methods based on deep learning.
sent7: The third stage, so far, is the research and exploration of the integration of knowledge and data.
sent8: This stage is based on the pre-training model.
sent9: Based on the above analysis, this paper believes that the specific technical methods of fake news detection research have the following aspects:"
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s25,Fake News Detection based on Machine Learning,"Commonly used classification models for fake news detection based on machine learning include support vector machine [73] and naive bayes [74].In addition, logistic regression [75] and decision trees [76], such as random forest classifiers, can also be used in fake news detection tasks [77].The basic principle of these models is to detect text based on the manual features of expert knowledge.Specific features include: linguistic features, theme features, user features and communication features.Eldesoky et al. [78] presented a classification model with the capability to detect fake news by utilizing Doc2vec and Word2vec embeddings as feature extraction techniques.The combination of the Doc2vec model and support vector machines achieved 95.5% accuracy on a real-world dataset.","[['b74', 'b77', 'b76', 'b78', 'b73', 'b72']]","[['b74', 'b77', 'b76', 'b78', 'b73', 'b72']]",6,"sent1: Commonly used classification models for fake news detection based on machine learning include support vector machine [73] and naive bayes [74].In addition, logistic regression [75] and decision trees [76], such as random forest classifiers, can also be used in fake news detection tasks [77].The basic principle of these models is to detect text based on the manual features of expert knowledge.
sent2: Specific features include: linguistic features, theme features, user features and communication features.
sent3: Eldesoky et al. [78] presented a classification model with the capability to detect fake news by utilizing Doc2vec and Word2vec embeddings as feature extraction techniques.
sent4: The combination of the Doc2vec model and support vector machines achieved 95.5% accuracy on a real-world dataset."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s18,Multidisciplinary Research Progress,"In the past few decades, there have been studies on fake news and its detection in many disciplines, including computer science, sociology, psychology, linguistics, communication and neurocognitive science [30,[45][46][47].Each field has its own research content and research methods for fake news.There are also studies [46] that combine the knowledge of these fields and use interdisciplinary methods to detect fake news.Multidisciplinary false news theory research can help the natural sciences achieve fake news detection.Figure 4 summarizes the basic problems of fake news detection in various disciplines, and we also hope that future research can combine more interdisciplinary knowledge.","[['b45', 'b29', 'b46', 'b44']]","[['b45', 'b29', 'b46', 'b44']]",4,"sent1: In the past few decades, there have been studies on fake news and its detection in many disciplines, including computer science, sociology, psychology, linguistics, communication and neurocognitive science [30,[45][46][47].Each field has its own research content and research methods for fake news.
sent2: There are also studies [46] that combine the knowledge of these fields and use interdisciplinary methods to detect fake news.
sent3: Multidisciplinary false news theory research can help the natural sciences achieve fake news detection.
sent4: Figure 4 summarizes the basic problems of fake news detection in various disciplines, and we also hope that future research can combine more interdisciplinary knowledge."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s23,Mitigation of the Spread of Malicious Content,"Halting the distribution of malicious content online demands a blend of diverse approaches and strategies.Present studies employ deep learning architectures integrating social networks, propagation trees and other techniques to establish systems that automatically classify and screen malicious content, thereby preventing its entry into online forums or mitigating its distribution.By studying the problem of detecting geolocated content communities on Twitter, Apostol et al. [59] propose a new distributed system that offers nearly real-time information on hazard-related events and their development.Furthermore, they introduce a novel deep learning model to identify fake news, and misguided tweets will be eliminated from the display.In order to alleviate the spread of real-time fake news in social media, Truică et al. [60] proposed a real-time network awareness strategy that constructs a minimum cost-weighted directed spanning tree for the detected nodes and immunizes the nodes in the tree by using a novel ranking function to score the harmfulness of the nodes.In addition, Coban et al. [61] propose a novel COmmuNiTy-based Algorithm for network ImmuNization that uses network information to detect harmful content distributors as well as generate partitions and immunize them using subgraphs induced by each distributor.

The diffusion-based method [62][63][64] can also alleviate the spread of malicious content.By using the propagation mechanism in social networks, it can guide the propagation path of information in a targeted manner, thereby reducing the impact of malicious content.This method emphasizes active intervention and the influence of network communication structure in order to achieve the purpose of reducing the spread of malicious content.

In short, by researching how to stop the spread of malicious content, working with governments, civil society organizations and technology companies to develop relevant regulations and guidelines can be effective in combating the spread of malicious content.","[['b58', 'b59', 'b60'], ['b62', 'b63', 'b61'], []]","[['b58', 'b59', 'b60'], ['b62', 'b63', 'b61'], []]",6,"sent1: Halting the distribution of malicious content online demands a blend of diverse approaches and strategies.
sent2: Present studies employ deep learning architectures integrating social networks, propagation trees and other techniques to establish systems that automatically classify and screen malicious content, thereby preventing its entry into online forums or mitigating its distribution.
sent3: By studying the problem of detecting geolocated content communities on Twitter, Apostol et al. [59] propose a new distributed system that offers nearly real-time information on hazard-related events and their development.
sent4: Furthermore, they introduce a novel deep learning model to identify fake news, and misguided tweets will be eliminated from the display.
sent5: In order to alleviate the spread of real-time fake news in social media, Truică et al. [60] proposed a real-time network awareness strategy that constructs a minimum cost-weighted directed spanning tree for the detected nodes and immunizes the nodes in the tree by using a novel ranking function to score the harmfulness of the nodes.
sent6: In addition, Coban et al. [61] propose a novel COmmuNiTy-based Algorithm for network ImmuNization that uses network information to detect harmful content distributors as well as generate partitions and immunize them using subgraphs induced by each distributor.
sent7: The diffusion-based method [62][63][64] can also alleviate the spread of malicious content.
sent8: By using the propagation mechanism in social networks, it can guide the propagation path of information in a targeted manner, thereby reducing the impact of malicious content.
sent9: This method emphasizes active intervention and the influence of network communication structure in order to achieve the purpose of reducing the spread of malicious content.
sent10: In short, by researching how to stop the spread of malicious content, working with governments, civil society organizations and technology companies to develop relevant regulations and guidelines can be effective in combating the spread of malicious content."
261539442,Sustainable Development of Information Dissemination: A Review of Current Fake News Detection Research and Practice,Computer Science,https://www.semanticscholar.org/paper/540f6cd3e634f30781c6ab3ce46408cd3db0556f,s22,Communication Science,"Communication researchers mainly analyze the concept of fake news [55] and try to find out the information dissemination mechanism, prevention and governance model of fake news in the context of the continuous use of social media [56].Jana et al. [57] proposed that the current concept of fake news is more extensive.The study believes that the essence of fake news is a two-dimensional phenomenon of public communication, which puts forward the theoretical framework of fake news research.Di et al. [58] revealed the sharing motivation related to fake news: benign online users may not share fake news in pursuit of financial or political/ideological goals but seek social recognition of the desired group by informing other members of specific related topics, which also strengthens the unity of the group.","[['b54', 'b57', 'b56', 'b55']]","[['b54', 'b57', 'b56', 'b55']]",4,"sent1: Communication researchers mainly analyze the concept of fake news [55] and try to find out the information dissemination mechanism, prevention and governance model of fake news in the context of the continuous use of social media [56].Jana et al. [57] proposed that the current concept of fake news is more extensive.
sent2: The study believes that the essence of fake news is a two-dimensional phenomenon of public communication, which puts forward the theoretical framework of fake news research.
sent3: Di et al. [58] revealed the sharing motivation related to fake news: benign online users may not share fake news in pursuit of financial or political/ideological goals but seek social recognition of the desired group by informing other members of specific related topics, which also strengthens the unity of the group."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s2,Large Language Models,"Large Language Models (LLM) refer to huge transformer architecture models (typically above ten billions of parameters) pre-trained with massive corpus (Zhao et al., 2023).With the increment of model size and training corpus, it starts to emerge some new abilities (Wei et al., 2022a).Recent years, LLM have achieved remarkable progress in many NLP fields (Brown et al., 2020;Chowdhery et al., 2022;Chung et al., 2022).","[['b4', None, 'b9', 'b81']]","[['b4', None, 'b9', 'b81']]",4,"sent1: Large Language Models (LLM) refer to huge transformer architecture models (typically above ten billions of parameters) pre-trained with massive corpus (Zhao et al., 2023).With the increment of model size and training corpus, it starts to emerge some new abilities (Wei et al., 2022a).Recent years, LLM have achieved remarkable progress in many NLP fields (Brown et al., 2020;Chowdhery et al., 2022;Chung et al., 2022)."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s14,Textual Instructions,"LLM show ability to follow explicit instructions even in zero-shot scenarios (Ouyang et al., 2022;Sanh et al., 2022).Inspired by this, some work finds explicitly prompting LLM with an active tex-tual instruction like ""Let's think step by step"" can guide a progressive reasoning (Kojima et al., 2022;Zhou et al., 2022d).Without any demonstrations, this simple zero-shot strategy shows impressive result comparing to non-CoT methods, implying these textual instructions can similarly elicit the reasoning ability of LLM.Some work also finds combining these textual instruction with few-shot CoT can achieve a further performance increment (Kojima et al., 2022).","[['b102', 'b51', 'b34', 'b61']]","[['b102', 'b51', 'b34', 'b61']]",4,"sent1: LLM show ability to follow explicit instructions even in zero-shot scenarios (Ouyang et al., 2022;Sanh et al., 2022).Inspired by this, some work finds explicitly prompting LLM with an active tex-tual instruction like ""Let's think step by step"" can guide a progressive reasoning (Kojima et al., 2022;Zhou et al., 2022d).Without any demonstrations, this simple zero-shot strategy shows impressive result comparing to non-CoT methods, implying these textual instructions can similarly elicit the reasoning ability of LLM.Some work also finds combining these textual instruction with few-shot CoT can achieve a further performance increment (Kojima et al., 2022)."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s16,Ensemble,"Ensemble learning is an effective strategy which combines diverse learners, enhancing the model performance comparing to a single learner (Zhou, 2012).Recent work achieved superior performance when using ensemble strategy on CoT prompting (Wang et al., 2022d;Li et al., 2022b;Wang et al., 2022e), which can help to correct errors made by individual reasoning process and integrate diverse prompts and demonstrations into a single prediction.Wang et al. (2022e) point out ensemble methods can even bring performance increment on tasks where vanilla CoT fails.However, unnecessary ensembles on problems which vanilla CoT can already effectively solve may inject noise to a confident prediction and instead do harm to model performance (Wang et al., 2022d).

To go a step further, what elements should be embraced into ensemble also matters a lot.According to different ensemble materials, we categorize these methods into prompts ensemble method and predictions ensemble method.Prompts ensemble focuses on the ensemble of results generated with various prompts.This method construct diverse CoT prompts by repeating sampling different demonstrations from exemplars set (Li et al., 2022b).Predictions ensemble focuses on integrating output space materials including rationales and answers.This method generates various predictions given a fixed input query by LLM sampling algorithms (Wang et al., 2022e;Fu et al., 2022;Yoran et al., 2023).It is found predictions ensemble may lead to more performance gain comparing to prompts ensemble (Wang et al., 2022d), but mul-tiple decoding for predictions ensemble may lead to higher computation cost.How to choose the ensemble strategy depends on the access of demonstrations number and computing resource.It's also possible to jointly combine two ensemble methods (Wang et al., 2022d).","[['b80', 'b79', 'b37', 'b103'], ['b37', 'b79', 'b91', 'b80', 'b19']]","[['b80', 'b79', 'b37', 'b103'], ['b37', 'b79', 'b91', 'b80', 'b19']]",9,"sent1: Ensemble learning is an effective strategy which combines diverse learners, enhancing the model performance comparing to a single learner (Zhou, 2012).Recent work achieved superior performance when using ensemble strategy on CoT prompting (Wang et al., 2022d;Li et al., 2022b;Wang et al., 2022e), which can help to correct errors made by individual reasoning process and integrate diverse prompts and demonstrations into a single prediction.
sent2: Wang et al. (2022e) point out ensemble methods can even bring performance increment on tasks where vanilla CoT fails.
sent3: However, unnecessary ensembles on problems which vanilla CoT can already effectively solve may inject noise to a confident prediction and instead do harm to model performance (Wang et al., 2022d).
sent4: To go a step further, what elements should be embraced into ensemble also matters a lot.
sent5: According to different ensemble materials, we categorize these methods into prompts ensemble method and predictions ensemble method.
sent6: Prompts ensemble focuses on the ensemble of results generated with various prompts.
sent7: This method construct diverse CoT prompts by repeating sampling different demonstrations from exemplars set (Li et al., 2022b).Predictions ensemble focuses on integrating output space materials including rationales and answers.
sent8: This method generates various predictions given a fixed input query by LLM sampling algorithms (Wang et al., 2022e;Fu et al., 2022;Yoran et al., 2023).It is found predictions ensemble may lead to more performance gain comparing to prompts ensemble (Wang et al., 2022d), but mul-tiple decoding for predictions ensemble may lead to higher computation cost.
sent9: How to choose the ensemble strategy depends on the access of demonstrations number and computing resource.
sent10: It's also possible to jointly combine two ensemble methods (Wang et al., 2022d)."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s4,Reasoning on LLM,"Reasoning is a complex process which involves using evidence, logically thinking and making arguments (Huang and Chang, 2022).It has been a long journey to explore the way to make neural network a reasoning machine (Peng et al., 2015;Barrett et al., 2018;Qu and Tang, 2019;Angelov and Soares, 2020).Recently, combined with incontext-learning strategy, LLM showed the prominent progress in reasoning tasks (Brown et al., 2020;Chowdhery et al., 2022;Chung et al., 2022).Especially, with assistance of Chain-of-Thought prompt (Wei et al., 2022b), neural networks made an unprecedented breakthrough on many reasoning benchmarks (Wang et al., 2022e;Lyu et al., 2023;Fu et al., 2022).Some work showed that the reasoning ability may emerge when language models are at a certain scale (Cobbe et al., 2021;Wei et al., 2022a,b;Huang and Chang, 2022).

3 What is Chain-of-Thought Prompting?Vanilla CoT (Wei et al., 2022b), Zeroshot-CoT (Kojima et al., 2022), Synthetic (Shao et al., 2023), Auto-CoT (Zhang et al., 2022b) ART (Paranjape et al., 2023), Active-Prompt (Diao et al., 2023), APE (Zhou et al., 2022d), Faithfull-CoT (Lyu et al., 2023) Explanation Selection (Ye and Durrett, 2023) Complexity-based prompting (Fu et al., 2022), SC (Wang et al., 2022e) DIVERSE (Li et al., 2022b), Rationale-augmented (Wang et al., 2022d), Table-CoT(Chen, 2023), Dater (Ye et al., 2023) SOLIS (Zhou et al., 2022b), LtM (Zhou et al., 2022a), Decomposed (Khot et al., 2022), PAL (Gao et al., 2022) PoT (Chen et al., 2022) Selectioninference (Creswell et al., 2022), Algorithmic prompt (Zhou et al., 2022c), LP (Guo et al., 2023), MoT (Li and Qiu, 2023) SV (Weng et al., 2022), MathPrompter (Imani et al., 2023), Automate-CoT (Shum et al., 2023),PHP (Zheng et al., 2023) Open Domain Reasoning and QA § 4.2

Vanilla CoT (Wei et al., 2022b), Zeroshot-CoT (Kojima et al., 2022), Auto-CoT (Zhang et al., 2022b), ART (Paranjape et al., 2023) Active-Prompt (Diao et al., 2023), APE (Zhou et al., 2022d), Explanation Selection(Ye and Durrett, 2023) iCAP (Wang et al., 2022a), Multimodal-CoT (Zhang et al., 2023b), Complexity-based prompting (Fu et al., 2022) Self-ask (Press et al., 2022), SC (Wang et al., 2022e), DIVERSE (Li et al., 2022b), SV (Weng et al., 2022) Rationale-augmented (Wang et al., 2022d), IRCoT (Trivedi et al., 2022), LtM (Zhou et al., 2022a), MCR(Yoran et al., 2023) Decomposed (Khot et al., 2022), Maieutic (Jung et al., 2022), PINTO (Wang et al., 2022c), MoT (Li and Qiu, 2023) Chameleon (Lu et al., 2023) , Automate-CoT (Shum et al., 2023), RR (He et al., 2023) Code Generation § 4.3

Self-planning (Jiang et al., 2023), XRICL (Shi et al., 2022b), DIN-SQL (Pourreza and Rafiei, 2023) Imitation Attack CoT (Li et al., 2023c) Prompts Design § 5 Demonstrations § 5.1 Self-ask (Press et al., 2022), Auto-CoT (Zhang et al., 2022b), Synthetic (Shao et al., 2023), MMRSelect (Fu et al., 2022) Complexity-based prompting (Fu et al., 2022), Active-Prompt (Diao et al., 2023), Explanation Selection (Ye and Durrett, 2023) Algorithmic prompt (Zhou et al., 2022c), Concise-CoT (Madaan and Yazdanbakhsh, 2022) , MoT (Li and Qiu, 2023) Automate-CoT (Shum et al., 2023) Textual Instructions § 5.2

Zeroshot-CoT (Kojima et al., 2022), APE (Zhou et al., 2022d), Auto-CoT (Zhang et al., 2022b) Extension Strategies § 6 Ensemble § 6.1 SC (Wang et al., 2022e), SV (Weng et al., 2022), Complexity-based prompting (Fu et al., 2022) , MCR(Yoran et al., 2023) DIVERSE (Li et al., 2022b), Rationale-augmented (Wang et al., 2022d) Sub-problems Division § 6.2

LtM (Zhou et al., 2022a), Self-ask (Press et al., 2022), Selectioninference (Creswell et al., 2022), Decomposed (Khot et al., 2022) HuggingGPT (Shen et al., 2023), Toolformer (Schick et al., 2023), Dater (Ye et al., 2023), Dynamix-LtM (Drozdov et al., 2022) RR (He et al., 2023) External Assistance § 6.3 iCAP (Wang et al., 2022a), PoT (Chen et al., 2022), PAL (Gao et al., 2022), Decomposed (Khot et al., 2022) HuggingGPT (Shen et al., 2023), Toolformer (Schick et al., 2023), Faithfull-CoT (Lyu et al., 2023), IRCoT (Trivedi et al., 2022) ART (Paranjape et al., 2023), Synthetic (Shao et al., 2023), Chameleon (Lu et al., 2023) Rationalization § 6.4

STaR (Zelikman et al., 2022), LP (Guo et al., 2023), PINTO (Wang et al., 2022c), PHP (Zheng et al., 2023) Figure 2: Taxonomy of Chain-of-Thought Prompting Strategies herent series of intermediate reasoning steps (Wei et al., 2022b).Specially, what refers to the ""intermediate reasoning steps"" shows discrepancy in different work.The scope of our survey is a more general covering of present work on prompts for multi-step tasks, which ranges from common stepwise reasoning to multi-step tasks deployment, so we encompass various non-programmatic problems division process including sub-problems decomposition (Zhou et al., 2022a;Press et al., 2022) and multi-step deployment (Khot et al., 2022;Shen et al., 2023;Schick et al., 2023) into our definition.

CoT prompting is a strategy to utilize CoT prompts, which works as following pipeline.As shown in Figure 1, given a specific task, CoT prompts are designed on it.With the assistance of optional extension strategies, models predict the answers (typically as well as CoT rationales) based on the input prompts.Above four elements in bold are key factors of CoT prompting pipeline which make significant impact on the final performance.We will separately discuss how these factors influence the prompting effect and introduce the work motivated by these factors in remaining survey.We present the taxonomy of various CoT prompting strategies in Figure 2.","[['b4', 'b3', 'b10', 'b82', 'b60', 'b46', 'b26', 'b53', None, 'b9', 'b80', 'b19', 'b0'], ['b102', 'b20', 'b69', 'b22', 'b80', 'b11', 'b32', 'b19', 'b101', 'b37', 'b79', 'b96', 'b46', 'b65', 'b15', 'b88', 'b27', 'b8', 'b52', 'b82', 'b99', None, 'b36', 'b34', 'b83', 'b100'], ['b102', 'b97', 'b69', 'b80', 'b32', 'b19', 'b37', 'b76', 'b96', 'b79', 'b15', 'b44', 'b52', 'b82', 'b55', 'b74', 'b78', None, 'b23', 'b31', 'b34', 'b83', 'b36', 'b100'], ['b88', 'b68', 'b55', 'b102', 'b47', 'b96', 'b54', 'b36', 'b65', 'b40', 'b30', 'b15', 'b19', 'b69'], ['b37', 'b102', 'b79', 'b96', 'b80', None, 'b83', 'b34', 'b19'], ['b8', 'b52', 'b17', 'b55', 'b20', 'b74', 'b76', 'b66', 'b46', 'b63', None, 'b23', 'b44', 'b65', 'b11', 'b32', 'b100'], ['b82', 'b22', 'b55', 'b99', 'b66', 'b63', 'b78', 'b93', 'b32', 'b100'], []]","[['b4', 'b3', 'b10', 'b82', 'b60', 'b46', 'b26', 'b53', None, 'b9', 'b80', 'b19', 'b0'], ['b102', 'b20', 'b69', 'b22', 'b80', 'b11', 'b32', 'b19', 'b101', 'b37', 'b79', 'b96', 'b46', 'b65', 'b15', 'b88', 'b27', 'b8', 'b52', 'b82', 'b99', None, 'b36', 'b34', 'b83', 'b100'], ['b102', 'b97', 'b69', 'b80', 'b32', 'b19', 'b37', 'b76', 'b96', 'b79', 'b15', 'b44', 'b52', 'b82', 'b55', 'b74', 'b78', None, 'b23', 'b31', 'b34', 'b83', 'b36', 'b100'], ['b88', 'b68', 'b55', 'b102', 'b47', 'b96', 'b54', 'b36', 'b65', 'b40', 'b30', 'b15', 'b19', 'b69'], ['b37', 'b102', 'b79', 'b96', 'b80', None, 'b83', 'b34', 'b19'], ['b8', 'b52', 'b17', 'b55', 'b20', 'b74', 'b76', 'b66', 'b46', 'b63', None, 'b23', 'b44', 'b65', 'b11', 'b32', 'b100'], ['b82', 'b22', 'b55', 'b99', 'b66', 'b63', 'b78', 'b93', 'b32', 'b100'], []]",113,"sent1: Reasoning is a complex process which involves using evidence, logically thinking and making arguments (Huang and Chang, 2022).It has been a long journey to explore the way to make neural network a reasoning machine (Peng et al., 2015;Barrett et al., 2018;Qu and Tang, 2019;Angelov and Soares, 2020).Recently, combined with incontext-learning strategy, LLM showed the prominent progress in reasoning tasks (Brown et al., 2020;Chowdhery et al., 2022;Chung et al., 2022).Especially, with assistance of Chain-of-Thought prompt (Wei et al., 2022b), neural networks made an unprecedented breakthrough on many reasoning benchmarks (Wang et al., 2022e;Lyu et al., 2023;Fu et al., 2022).Some work showed that the reasoning ability may emerge when language models are at a certain scale (Cobbe et al., 2021;Wei et al., 2022a,b;Huang and Chang, 2022).
sent2: 3 What is Chain-of-Thought Prompting?Vanilla CoT (Wei et al., 2022b), Zeroshot-CoT (Kojima et al., 2022), Synthetic (Shao et al., 2023), Auto-CoT (Zhang et al., 2022b) ART (Paranjape et al., 2023), Active-Prompt (Diao et al., 2023), APE (Zhou et al., 2022d), Faithfull-CoT (Lyu et al., 2023) Explanation Selection (Ye and Durrett, 2023) Complexity-based prompting (Fu et al., 2022), SC (Wang et al., 2022e) DIVERSE (Li et al., 2022b), Rationale-augmented (Wang et al., 2022d), Table-CoT(Chen, 2023), Dater (Ye et al., 2023) SOLIS (Zhou et al., 2022b), LtM (Zhou et al., 2022a), Decomposed (Khot et al., 2022), PAL (Gao et al., 2022) PoT (Chen et al., 2022) Selectioninference (Creswell et al., 2022), Algorithmic prompt (Zhou et al., 2022c), LP (Guo et al., 2023), MoT (Li and Qiu, 2023) SV (Weng et al., 2022), MathPrompter (Imani et al., 2023), Automate-CoT (Shum et al., 2023),PHP (Zheng et al., 2023)
sent3: Open Domain Reasoning and QA § 4.2Vanilla CoT (Wei et al., 2022b), Zeroshot-CoT (Kojima et al., 2022), Auto-CoT (Zhang et al., 2022b), ART (Paranjape et al., 2023)
sent4: Active-Prompt (Diao et al., 2023), APE (Zhou et al., 2022d), Explanation Selection(Ye and Durrett, 2023) iCAP (Wang et al., 2022a), Multimodal-CoT (Zhang et al., 2023b), Complexity-based prompting (Fu et al., 2022) Self-ask (Press et al., 2022), SC (Wang et al., 2022e), DIVERSE (Li et al., 2022b), SV (Weng et al., 2022)
sent5: Rationale-augmented (Wang et al., 2022d), IRCoT (Trivedi et al., 2022), LtM (Zhou et al., 2022a), MCR(Yoran et al., 2023) Decomposed (Khot et al., 2022), Maieutic (Jung et al., 2022), PINTO (Wang et al., 2022c), MoT (Li and Qiu, 2023) Chameleon (Lu et al., 2023) , Automate-CoT (Shum et al., 2023), RR (He et al., 2023) Code Generation § 4.3Self-planning (Jiang et al., 2023), XRICL (Shi et al., 2022b), DIN-SQL (Pourreza and Rafiei, 2023) Imitation Attack CoT (Li et al., 2023c) Prompts Design § 5 Demonstrations § 5.1 Self-ask (Press et al., 2022), Auto-CoT (Zhang et al., 2022b), Synthetic (Shao et al., 2023), MMRSelect (Fu et al., 2022) Complexity-based prompting (Fu et al., 2022), Active-Prompt (Diao et al., 2023), Explanation Selection (Ye and Durrett, 2023) Algorithmic prompt (Zhou et al., 2022c), Concise-CoT (Madaan and Yazdanbakhsh, 2022) , MoT (Li and Qiu, 2023) Automate-CoT (Shum et al., 2023) Textual Instructions § 5.2Zeroshot-CoT (Kojima et al., 2022), APE (Zhou et al., 2022d), Auto-CoT (Zhang et al., 2022b) Extension Strategies § 6 Ensemble § 6.1 SC (Wang et al., 2022e), SV (Weng et al., 2022), Complexity-based prompting (Fu et al., 2022) , MCR(Yoran et al., 2023) DIVERSE (Li et al., 2022b), Rationale-augmented (Wang et al., 2022d) Sub-problems Division § 6.2LtM (Zhou et al., 2022a), Self-ask (Press et al., 2022), Selectioninference (Creswell et al., 2022), Decomposed (Khot et al., 2022) HuggingGPT (Shen et al., 2023), Toolformer (Schick et al., 2023), Dater (Ye et al., 2023), Dynamix-LtM (Drozdov et al., 2022) RR (He et al., 2023) External Assistance § 6.3 iCAP (Wang et al., 2022a), PoT (Chen et al., 2022), PAL (Gao et al., 2022), Decomposed (Khot et al., 2022) HuggingGPT (Shen et al., 2023), Toolformer (Schick et al., 2023), Faithfull-CoT (Lyu et al., 2023), IRCoT (Trivedi et al., 2022) ART (Paranjape et al., 2023), Synthetic (Shao et al., 2023), Chameleon (Lu et al., 2023)
sent6: Rationalization § 6.4STaR (Zelikman et al., 2022), LP (Guo et al., 2023), PINTO (Wang et al., 2022c), PHP (Zheng et al., 2023) Figure 2: Taxonomy of Chain-of-Thought Prompting Strategies herent series of intermediate reasoning steps (Wei et al., 2022b).Specially, what refers to the ""intermediate reasoning steps"" shows discrepancy in different work.
sent7: The scope of our survey is a more general covering of present work on prompts for multi-step tasks, which ranges from common stepwise reasoning to multi-step tasks deployment, so we encompass various non-programmatic problems division process including sub-problems decomposition (Zhou et al., 2022a;Press et al., 2022) and multi-step deployment (Khot et al., 2022;Shen et al., 2023;Schick et al., 2023) into our definition.
sent8: CoT prompting is a strategy to utilize CoT prompts, which works as following pipeline.
sent9: As shown in Figure 1, given a specific task, CoT prompts are designed on it.
sent10: With the assistance of optional extension strategies, models predict the answers (typically as well as CoT rationales) based on the input prompts.
sent11: Above four elements in bold are key factors of CoT prompting pipeline which make significant impact on the final performance.
sent12: We will separately discuss how these factors influence the prompting effect and introduce the work motivated by these factors in remaining survey.
sent13: We present the taxonomy of various CoT prompting strategies in Figure 2."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s18,External Assistance,"In order to expand the ability of LLM and assist LLM to perform on broader applications, it's useful to introduce external sources including knowledge, tools or codes interpreters into reasoning process.

Knowledge injection is especially helpful in tasks which need external knowledge like commonsense QA (Wang et al., 2022a).Tools and codes assisted strategies show preponderance in problems which need abilities beyond LLM capacities such as accurate numerical calculation or search engine (Chen et al., 2022;Khot et al., 2022;Schick et al., 2023;Paranjape et al., 2023).With proper prompts, LLM can generate task deployment chains to instruct when and where to call external tools (Khot et al., 2022;Schick et al., 2023), codes interpreter (Gao et al., 2022;Chen et al., 2022;Lyu et al., 2023) or even other models (Shen et al., 2023), to solve more complex problems.

Codes interpreters can also serve as external verifiers to check the validity of generated rationales by checking whether they can be interpreted and lead to a correct answer (Shao et al., 2023;Gao et al., 2022;Chen et al., 2022).Additionally, Lyu et al. (2023) point out executing reasoning chains with programmatic modules can enhance the faithfulness of CoT (We will discuss in § 8).","[[], ['b8', 'b52', 'b20', 'b76', 'b46', 'b66', 'b63', 'b32'], ['b20', 'b8', 'b46', 'b65']]","[[], ['b8', 'b52', 'b20', 'b76', 'b46', 'b66', 'b63', 'b32'], ['b20', 'b8', 'b46', 'b65']]",12,"sent1: In order to expand the ability of LLM and assist LLM to perform on broader applications, it's useful to introduce external sources including knowledge, tools or codes interpreters into reasoning process.
sent2: Knowledge injection is especially helpful in tasks which need external knowledge like commonsense QA (Wang et al., 2022a).Tools and codes assisted strategies show preponderance in problems which need abilities beyond LLM capacities such as accurate numerical calculation or search engine (Chen et al., 2022;Khot et al., 2022;Schick et al., 2023;Paranjape et al., 2023).With proper prompts, LLM can generate task deployment chains to instruct when and where to call external tools (Khot et al., 2022;Schick et al., 2023), codes interpreter (Gao et al., 2022;Chen et al., 2022;Lyu et al., 2023) or even other models (Shen et al., 2023), to solve more complex problems.
sent3: Codes interpreters can also serve as external verifiers to check the validity of generated rationales by checking whether they can be interpreted and lead to a correct answer (Shao et al., 2023;Gao et al., 2022;Chen et al., 2022).Additionally, Lyu et al. (2023) point out executing reasoning chains with programmatic modules can enhance the faithfulness of CoT (We will discuss in § 8)."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s6,Close Domain Reasoning and QA,"These kinds of tasks include all necessary conditions and background knowledge in the problems.Models need to select informative materials and conduct reasoning on these materials.CoT prompts could provide a reasoning pattern to instruct how to select key materials and reason on them, showing superiority on these tasks like mathematical reasoning (Zhang et al., 2022b;Shao et al., 2023;Wang et al., 2022e), symbolic reasoning (Wei et al., 2022b;Anil et al., 2022;Suzgun et al., 2022) and table QA (Ye et al., 2023;Chen, 2023).","[['b82', 'b96', 'b7', 'b80', 'b1', None, 'b65', 'b73']]","[['b82', 'b96', 'b7', 'b80', 'b1', None, 'b65', 'b73']]",8,"sent1: These kinds of tasks include all necessary conditions and background knowledge in the problems.
sent2: Models need to select informative materials and conduct reasoning on these materials.
sent3: CoT prompts could provide a reasoning pattern to instruct how to select key materials and reason on them, showing superiority on these tasks like mathematical reasoning (Zhang et al., 2022b;Shao et al., 2023;Wang et al., 2022e), symbolic reasoning (Wei et al., 2022b;Anil et al., 2022;Suzgun et al., 2022) and table QA (Ye et al., 2023;Chen, 2023)."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s21,Model Size,"Many researches have found as the model size is relatively small (typically below ten billions parameters), CoT doesn't remain a positive impact.But as the model size increases to a certain size (above ten billions parameters), it will exhibit a sudden performance breakout (Wei et al., 2022b;Suzgun et al., 2022;Magister et al., 2022;Fu et al., 2023).This implies CoT is an emergent ability (Wei et al., 2022a) of LLM.Prompting small models with CoT will commonly lead to hallucination (Ji et al., 2023), which often presents as fluent but illogical generation (Wei et al., 2022b).But it's still possible to enhance small models reasoning ability by CoT.Some work fine-tuned a small-scaled model with self-constructed CoT dataset (Zelikman et al., 2022) or knowledge distillation (Magister et al., 2022;Ho et al., 2022;Wang et al., 2022c;Fu et al., 2023), making small models compatible to perform step-by-step reasoning even on few-shot scenarios.However, small models will forget general abilities on other tasks except stepby-step reasoning after CoT tuning (Fu et al., 2023) and still lag behind large models on tasks which demands substantial knowledge to conduct reasoning (Magister et al., 2022).","[['b82', 'b28', 'b81', 'b78', 'b93', 'b18', 'b48', 'b73', 'b24']]","[['b82', 'b28', 'b81', 'b78', 'b93', 'b18', 'b48', 'b73', 'b24']]",9,"sent1: Many researches have found as the model size is relatively small (typically below ten billions parameters), CoT doesn't remain a positive impact.
sent2: But as the model size increases to a certain size (above ten billions parameters), it will exhibit a sudden performance breakout (Wei et al., 2022b;Suzgun et al., 2022;Magister et al., 2022;Fu et al., 2023).This implies CoT is an emergent ability (Wei et al., 2022a) of LLM.Prompting small models with CoT will commonly lead to hallucination (Ji et al., 2023), which often presents as fluent but illogical generation
sent3: (Wei et al., 2022b).But it's still possible to enhance small models reasoning ability by CoT.Some work fine-tuned a small-scaled model with self-constructed CoT dataset (Zelikman et al., 2022) or knowledge distillation (Magister et al., 2022;Ho et al., 2022;Wang et al., 2022c;Fu et al., 2023), making small models compatible to perform step-by-step reasoning even on few-shot scenarios.
sent4: However, small models will forget general abilities on other tasks except stepby-step reasoning after CoT tuning (Fu et al., 2023) and still lag behind large models on tasks which demands substantial knowledge to conduct reasoning (Magister et al., 2022)."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s22,Training Corpus,"It is believed that the abilities LLM exhibit originate from training corpus.Some work finds models pre-trained with codes could acquire more performance gain when prompted with CoT (Diao et al., 2023).Instruction tuning also shows relevance to the CoT prompting and zero-shot learning performance (Chung et al., 2022;Fu et al., 2023), which may be illustrated by the presence of CoT-like samples in the training corpus of instruction tuning.Recent work even tries to explicitly involve CoT samples into training corpus to enhance the stepby-step reasoning ability and avoid over-fitting to monotonous sample templates (Chung et al., 2022;Ho et al., 2022;Yu et al., 2022).In a word, embracing aforementioned contents into training corpus could introduce more reasoning materials and necessary knowledge for LLM, leading to a profound influence on CoT reasoning ability.","[['b15', None, 'b18', 'b92', 'b24']]","[['b15', None, 'b18', 'b92', 'b24']]",5,"sent1: It is believed that the abilities LLM exhibit originate from training corpus.
sent2: Some work finds models pre-trained with codes could acquire more performance gain when prompted with CoT (Diao et al., 2023).Instruction tuning also shows relevance to the CoT prompting and zero-shot learning performance (Chung et al., 2022;Fu et al., 2023), which may be illustrated by the presence of CoT-like samples in the training corpus of instruction tuning.
sent3: Recent work even tries to explicitly involve CoT samples into training corpus to enhance the stepby-step reasoning ability and avoid over-fitting to monotonous sample templates (Chung et al., 2022;Ho et al., 2022;Yu et al., 2022).In a word, embracing aforementioned contents into training corpus could introduce more reasoning materials and necessary knowledge for LLM, leading to a profound influence on CoT reasoning ability."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s8,Code Generation,"Code generation aims to generate codes according to an input instruction.Due to the internal logical form of codes, the step-by-step reasoning chain of CoT is coherent with abilities needed for code generation (Shi et al., 2022b;Chen et al., 2022;Gao et al., 2022;Jiang et al., 2023;Pourreza and Rafiei, 2023).","[['b8', 'b68', 'b20', 'b54', 'b30']]","[['b8', 'b68', 'b20', 'b54', 'b30']]",5,"sent1: Code generation aims to generate codes according to an input instruction.
sent2: Due to the internal logical form of codes, the step-by-step reasoning chain of CoT is coherent with abilities needed for code generation (Shi et al., 2022b;Chen et al., 2022;Gao et al., 2022;Jiang et al., 2023;Pourreza and Rafiei, 2023)."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s13,Holistic Perspective,"Number and Order: When prompted with multiple CoT demonstrations, the model accesses the intersection information of each demonstration (Ye et al., 2022).The number and order of demonstrations leave a impact on the final performance (Wei et al., 2022b;Lu et al., 2022a;Chen et al., 2023) Some work finds model performance would gain prominently when the CoT demonstrations number is gradually increasing from zero to two while the improvement remains slowly and negligibly if demonstrations number continually increases (Shi et al., 2022a;Lu et al., 2022a;Chen, 2023;Chen et al., 2023).Too many demonstrations can lead to a mass of computation cost while insufficient demonstrations may make LLM more sensitive to a single demonstration and increase the variance (Chen et al., 2022).

Changing the order of demonstrations may result in a non-trivial effect, but we still can't draw a conclusion for a universal order strategy since the impact of order may vary according to the models, tasks and dataset (Lu et al., 2022b;Liu et al., 2022).An compromised method is to use some prompting searching strategies (Lu et al., 2022b;Ye and Durrett, 2023) or some heuristic measurements like complexity and relevance order Liu et al. (2022).","[['b67', 'b43', 'b8', 'b82', 'b7', 'b87', 'b6'], ['b45', 'b41', 'b88']]","[['b67', 'b43', 'b8', 'b82', 'b7', 'b87', 'b6'], ['b45', 'b41', 'b88']]",10,"sent1: Number and Order: When prompted with multiple CoT demonstrations, the model accesses the intersection information of each demonstration (Ye et al., 2022).The number and order of demonstrations leave a impact on the final performance (Wei et al., 2022b;Lu et al., 2022a;Chen et al., 2023)
sent2: Some work finds model performance would gain prominently when the CoT demonstrations number is gradually increasing from zero to two while the improvement remains slowly and negligibly if demonstrations number continually increases (Shi et al., 2022a;Lu et al., 2022a;Chen, 2023;Chen et al., 2023).Too many demonstrations can lead to a mass of computation cost while insufficient demonstrations may make LLM more sensitive to a single demonstration and increase the variance (Chen et al., 2022).
sent3: Changing the order of demonstrations may result in a non-trivial effect, but we still can't draw a conclusion for a universal order strategy since the impact of order may vary according to the models, tasks and dataset (Lu et al., 2022b;Liu et al., 2022).An compromised method is to use some prompting searching strategies (Lu et al., 2022b;Ye and Durrett, 2023) or some heuristic measurements like complexity and relevance order Liu et al. (2022)."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s11,Problem Perspective,"Complexity: Complexity measures the difficulty of a problem, which can be reflected as the number, the length and the logical difficulty of reasoning steps to solve this problem.

Empirically, more complex demonstration problems usually guarantee longer reasoning steps, which can provide more reasoning context and induce the model to generate a longer rationale, avoiding a short-cut prediction (Shao et al., 2023;Anil et al., 2022;Saparov and He, 2022).To get more complex demonstration problems, a direct way is to select the samples with most reasoning steps (Shao et al., 2023;Fu et al., 2022).Diao et al. (2023) select the demonstrations with higher uncertainty, which represents the extent a problem is able to confuse the model.

Relevance and Diversity: Relevance measures how similar the demonstration problems are to the query problem.Relevant demonstration problems could provide similar knowledge and reasoning pattern for the query, which make LLM easier to imitate (Ye et al., 2022).

Diversity measures how different a demonstration problem is from other demonstration problems within a single prompt.Diverse demonstrations could fuse different reasoning process and make model less sensitive to a specific reasoning process, increasing the prompt robustness (Shao et al., 2023;Zhang et al., 2022b).

Both relevance and diversity can be ensured by representative features of problems (Ye et al., 2022;Zhang et al., 2022b) or manual topic informing (Shao et al., 2023).Specially, we need to point out that diversity and relevance are a trade-off process (Ye et al., 2022).Too monotonous demonstrations may lead to a less robust model performance while involving too many irrelevant demonstrations may inject more noise.","[[], ['b62', 'b1', 'b65', 'b15', 'b19'], ['b87'], ['b96', 'b65'], ['b96', 'b87', 'b65']]","[[], ['b62', 'b1', 'b65', 'b15', 'b19'], ['b87'], ['b96', 'b65'], ['b96', 'b87', 'b65']]",11,"sent1: Complexity: Complexity measures the difficulty of a problem, which can be reflected as the number, the length and the logical difficulty of reasoning steps to solve this problem.
sent2: Empirically, more complex demonstration problems usually guarantee longer reasoning steps, which can provide more reasoning context and induce the model to generate a longer rationale, avoiding a short-cut prediction (Shao et al., 2023;Anil et al., 2022;Saparov and He, 2022).To get more complex demonstration problems, a direct way is to select the samples with most reasoning steps (Shao et al., 2023;Fu et al., 2022).Diao et al. (2023) select the demonstrations with higher uncertainty, which represents the extent a problem is able to confuse the model.
sent3: Relevance and Diversity: Relevance measures how similar the demonstration problems are to the query problem.
sent4: Relevant demonstration problems could provide similar knowledge and reasoning pattern for the query, which make LLM easier to imitate (Ye et al., 2022).Diversity measures how different a demonstration problem is from other demonstration problems within a single prompt.
sent5: Diverse demonstrations could fuse different reasoning process and make model less sensitive to a specific reasoning process, increasing the prompt robustness (Shao et al., 2023;Zhang et al., 2022b).
sent6: Both relevance and diversity can be ensured by representative features of problems (Ye et al., 2022;Zhang et al., 2022b) or manual topic informing (Shao et al., 2023).Specially, we need to point out that diversity and relevance are a trade-off process (Ye et al., 2022).Too monotonous demonstrations may lead to a less robust model performance while involving too many irrelevant demonstrations may inject more noise."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s3,Prompting and In-context-learning,"Prompting is a strategy to better elicit the knowledge and ability acquired during training stage of language models by modifying the input samples in a specific manner (Liu et al., 2023;Brown et al., 2020;Schick and Schütze, 2021).In-contextlearning is a special designed prompting strategy, which prefixed the query sample with a few example demonstrations including both queries and answers (Wei et al., 2022a), enabling the model to analogously make predictions (Dong et al., 2023).In-context-learning is a training-free paradigm and can significantly boost LLM performance and dataefficiency across many NLP benchmarks on fewshot scenarios (Sun et al., 2022).","[['b4', 'b64', 'b16', 'b81', 'b42', 'b72']]","[['b4', 'b64', 'b16', 'b81', 'b42', 'b72']]",6,"sent1: Prompting is a strategy to better elicit the knowledge and ability acquired during training stage of language models by modifying the input samples in a specific manner (Liu et al., 2023;Brown et al., 2020;Schick and Schütze, 2021).In-contextlearning is a special designed prompting strategy, which prefixed the query sample with a few example demonstrations including both queries and answers (Wei et al., 2022a), enabling the model to analogously make predictions (Dong et al., 2023).In-context-learning is a training-free paradigm and can significantly boost LLM performance and dataefficiency across many NLP benchmarks on fewshot scenarios (Sun et al., 2022)."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s17,Sub-problems Division,"When confronting a problem needs to be recursively inferred or harder than demonstrations, dividing a problem into several sub-problems could be a better option (Zhou et al., 2022a;Press et al., 2022;Khot et al., 2022;Schick et al., 2023).Comparing to vanilla CoT, sub-problems division strategy decomposes a complex problem into a series of simple sub-problems, which are much easier to solve, enabling models to accomplish query problems harder than demonstrations (Zhou et al., 2022a;Press et al., 2022).Also, when dealing with each sub-problem, model is free from information which is irrelevant to current sub-problem and more informative information is prone to guiding a valid reasoning (Creswell et al., 2022;Zhou et al., 2022a).Additionally, the required abilities for separate subproblems are different.This strategy makes it more convenient to deploy each sub-problem with different modules and inject external assistance (Khot et al., 2022;Schick et al., 2023;Shen et al., 2023), which we will introduce in § 6.3.","[['b55', 'b66', 'b63', 'b11', 'b32', 'b100']]","[['b55', 'b66', 'b63', 'b11', 'b32', 'b100']]",6,"sent1: When confronting a problem needs to be recursively inferred or harder than demonstrations, dividing a problem into several sub-problems could be a better option (Zhou et al., 2022a;Press et al., 2022;Khot et al., 2022;Schick et al., 2023).Comparing to vanilla CoT, sub-problems division strategy decomposes a complex problem into a series of simple sub-problems, which are much easier to solve, enabling models to accomplish query problems harder than demonstrations (Zhou et al., 2022a;Press et al., 2022).Also, when dealing with each sub-problem, model is free from information which is irrelevant to current sub-problem and more informative information is prone to guiding a valid reasoning (Creswell et al., 2022;Zhou et al., 2022a).Additionally, the required abilities for separate subproblems are different.
sent2: This strategy makes it more convenient to deploy each sub-problem with different modules and inject external assistance (Khot et al., 2022;Schick et al., 2023;Shen et al., 2023), which we will introduce in § 6.3."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s19,Rationalization,"Usually, the rationales predicted by LLM would make some mistakes and lead to wrong answers.If these mistakes can be corrected, it is possible to rationalize the reasoning process and boost the performance.

Manual rationalization would be effective but sometimes too costly (Wang et al., 2022c;Kim et al., 2023).A simple way is to use some hints to guide the model to rethink (Zelikman et al., 2022;Guo et al., 2023).When the model produces a wrong answer, we can tell model the correct answer and ask it to self-revise illogical reasoning and regenerate a rationale based on the golden answer.This process can be regarded as a self-learning process, where the model can progressively improve its reasoning ability just with answers supervised.However, it's still hard to rationalize imperfect rationales which lead to correct answers.","[[], ['b93', 'b78', 'b33', 'b22']]","[[], ['b93', 'b78', 'b33', 'b22']]",4,"sent1: Usually, the rationales predicted by LLM would make some mistakes and lead to wrong answers.
sent2: If these mistakes can be corrected, it is possible to rationalize the reasoning process and boost the performance.
sent3: Manual rationalization would be effective but sometimes too costly (Wang et al., 2022c;Kim et al., 2023).A simple way is to use some hints to guide the model to rethink (Zelikman et al., 2022;Guo et al., 2023).When
sent4: the model produces a wrong answer, we can tell model the correct answer and ask it to self-revise illogical reasoning and regenerate a rationale based on the golden answer.
sent5: This process can be regarded as a self-learning process, where the model can progressively improve its reasoning ability just with answers supervised.
sent6: However, it's still hard to rationalize imperfect rationales which lead to correct answers."
263829198,Towards Better Chain-of-Thought Prompting Strategies: A Survey,Computer Science,https://www.semanticscholar.org/paper/12a4c41b087629548b07d0dadb9da05147fa4f81,s12,Rationale Perspective,"Structural Completeness: A CoT rationale can be divided into two fine-grained components2 : bridging objects and language templates (Wang et al., 2022b;Ye et al., 2022).A structural complete rationale should contain both of two components.

Bridging objects refer to the critical elements that convey the logical process when solving problems, which are meant to be traversed in a single prediction.Language templates refer to complementary textual parts that connect the bridging objects, which involve complementary knowledge for the problems and construct logical connections with bridging objects (Wang et al., 2022b).Figure 3 presents some examples about each component of CoT rationales on different tasks.For a sound reasoning, bridging objects provide the reasoning materials and lead to logical language templates, while language templates provide the essential knowledge and help to better organize bridging objects in back (Madaan and Yazdanbakhsh, 2022).In a word, bridging objects and language templates are two codependent and indispensable components for an effective CoT rationale.More detailed two components can also jointly reduce the ambiguity of demonstration rationales (Zhou et al., 2022c).

Validity: Validity measures the correctness and coherence of a reasoning chain.The validity of a CoT rationale can be measured from different metrics like accuracy of answers (Ye and Durrett, 2022;Saparov and He, 2022), perplexity (Press et al., 2022) and more detailed measurement on each reasoning step (Saparov and He, 2022).It's also possible to train a verifier model to evaluate the validity of CoT rationales (Li et al., 2022b).

Some work is conducted to explore how the validity plays a role in CoT prompt.In a nutshell, a valid CoT rationale does promote a better prompting performance while an effective CoT prompt does not necessarily demand a totally valid rationale (Wang et al., 2022b;Madaan and Yazdanbakhsh, 2022;Ye et al., 2022;Chen et al., 2023).Wang et al. (2022b) found that a completely wrong rationale may bring a similar reasoning boost as vanilla CoT as long as it is coherent, i.e., latter steps should not contain any condition or text information which doesn't present in previous steps.It seems that a coherent CoT rationale demonstrates a relative logical manner of reasoning, which help to elicit the LLM ability of progressive reasoning.Thus, the coherence should be primarily ensured if we have to compromise to incorrect CoT rationales.","[['b77', 'b87'], ['b102', 'b47', 'b77'], ['b37', 'b87', 'b62', 'b55'], ['b47', 'b77', 'b87', 'b6']]","[['b77', 'b87'], ['b102', 'b47', 'b77'], ['b37', 'b87', 'b62', 'b55'], ['b47', 'b77', 'b87', 'b6']]",13,"sent1: Structural Completeness: A CoT rationale can be divided into two fine-grained components2 : bridging objects and language templates (Wang et al., 2022b;Ye et al., 2022).A structural complete rationale should contain both of two components.
sent2: Bridging objects refer to the critical elements that convey the logical process when solving problems, which are meant to be traversed in a single prediction.
sent3: Language templates refer to complementary textual parts that connect the bridging objects, which involve complementary knowledge for the problems and construct logical connections with bridging objects (Wang et al., 2022b).Figure 3 presents some examples about each component of CoT rationales on different tasks.
sent4: For a sound reasoning, bridging objects provide the reasoning materials and lead to logical language templates, while language templates provide the essential knowledge and help to better organize bridging objects in back (Madaan and Yazdanbakhsh, 2022).In a word, bridging objects and language templates are two codependent and indispensable components for an effective CoT rationale.
sent5: More detailed two components can also jointly reduce the ambiguity of demonstration rationales (Zhou et al., 2022c).Validity: Validity measures the correctness and coherence of a reasoning chain.
sent6: The validity of a CoT rationale can be measured from different metrics like accuracy of answers (Ye and Durrett, 2022;Saparov and He, 2022), perplexity (Press et al., 2022) and more detailed measurement on each reasoning step (Saparov and He, 2022).It's also possible to train a verifier model to evaluate the validity of CoT rationales (Li et al., 2022b).
sent7: Some work is conducted to explore how the validity plays a role in CoT prompt.
sent8: In a nutshell, a valid CoT rationale does promote a better prompting performance while an effective CoT prompt does not necessarily demand a totally valid rationale (Wang et al., 2022b;Madaan and Yazdanbakhsh, 2022;Ye et al., 2022;Chen et al., 2023).Wang et al. (2022b) found that a completely wrong rationale may bring a similar reasoning boost as vanilla CoT as long as it is coherent, i.e., latter steps should not contain any condition or text information which doesn't present in previous steps.
sent9: It seems that a coherent CoT rationale demonstrates a relative logical manner of reasoning, which help to elicit the LLM ability of progressive reasoning.
sent10: Thus, the coherence should be primarily ensured if we have to compromise to incorrect CoT rationales."
264832783,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825,s7,Bayesian Inference,"In their work, Xie et al. ( 2022) first provided an interpretation of ICL through the lens of Bayesian inference, proposing that LLMs have the capability to perform implicit Bayesian inference via ICL.Specifically, they synthesized a small-scale dataset to examine how ICL emerges in LSTM and Transformer models during pretraining on text with extended coherence.Their findings revealed that both models are capable of inferring latent concepts to generate coherent subsequent tokens during pretraining.Additionally, these models were shown to perform ICL by identifying a shared latent concept among examples during the inference process.Their theoretical analysis confirms that this phenomenon persists even when there is a distribution mismatch between the examples and the data used for pretraining, particularly in settings where the pretraining distribution is derived from a mixture of Hidden Markov Models (HMMs) (Baum and Petrie, 1966).Furthermore, Xie et al. ( 2022) observed that the ICL error decreases as the length of each example increases, emphasizing the significance of the inherent information within inputs.This goes beyond mere input-label correlations and highlights the roles of intrinsic input characteristics in facilitating ICL.

Following on, Wang et al. (2023b) expanded the investigation of ICL by relaxing the assumptions made by Xie et al. (2022) and posited that ICL in LLMs essentially operates as a form of topic modeling that implicitly extracts task-relevant information from examples to aid in inference.Wang et al. (2023b) grounded their theoretical analysis in a setting with a finite number of demonstrations, and under a more general language generation process.Specifically, they characterized the data generation process using a causal graph with three variables and imposed no constraints on the distribution or quantity of samples.Their empirical and theoretical investigations revealed that ICL can approximate the Bayes optimal predictor when a finite number of samples are chosen based on the latent concept variable.Moreover, Wang et al. (2023b) devised an effective practical algorithm for demonstration selection tailored to real-world LLMs.

At the same time, Jiang (2023) also introduced a novel latent space theory extending the idea of Xie et al. ( 2022) to explain emergent abilities in LLMs.Instead of focusing on specific data distributions generated by HMMs, they delved into general sparse data distributions and employed LLMs as a universal density approximator for the marginal distribution, allowing them to probe these sparse structures more broadly.Jiang (2023) demonstrated that ICL, CoT, and instruction-following abilities in LLMs can be ascribed to Bayesian inference operating on the broader sparse joint distribution of languages.To shed light on the significance of the attention mechanism for ICL from a Bayesian view, Zhang et al. (2023) defined ICL as the task of predicting a response that aligns with a given covariate based on examples derived from a latent variable model.They established that ICL implicitly implements the Bayesian Model Averaging (BMA) algorithm, which is approximated by the attention mechanism.Furthermore, they demonstrated that certain attention mechanisms converge towards the conventional softmax attention as the number of examples goes to infinity.These attentions, due to their encoding of BMA within their structure, empower the Transformer model to perform ICL.

Although their conclusions are insightful, there is a room for improvement.Their findings might be influenced by various factors, such as the formats of the examples, the nature of tasks, and the choice of evaluation metrics.Additionally, many of these studies are based on analyses conducted using small synthetic datasets, potentially restricting their relevance and applicability to real-world scenarios.","[['b4'], [None, 'b60'], ['b70', 'b22'], []]","[['b4'], [None, 'b60'], ['b70', 'b22'], []]",5,"sent1: In their work, Xie et al. ( 2022) first provided an interpretation of ICL through the lens of Bayesian inference, proposing that LLMs have the capability to perform implicit Bayesian inference via ICL.Specifically, they synthesized a small-scale dataset to examine how ICL emerges in LSTM and Transformer models during pretraining on text with extended coherence.
sent2: Their findings revealed that both models are capable of inferring latent concepts to generate coherent subsequent tokens during pretraining.
sent3: Additionally, these models were shown to perform ICL by identifying a shared latent concept among examples during the inference process.
sent4: Their theoretical analysis confirms that this phenomenon persists even when there is a distribution mismatch between the examples and the data used for pretraining, particularly in settings where the pretraining distribution is derived from a mixture of Hidden Markov Models (HMMs) (Baum and Petrie, 1966).Furthermore, Xie et al. ( 2022) observed that the ICL error decreases as the length of each example increases, emphasizing the significance of the inherent information within inputs.
sent5: This goes beyond mere input-label correlations and highlights the roles of intrinsic input characteristics in facilitating ICL.
sent6: Following on, Wang et al. (2023b) expanded the investigation of ICL by relaxing the assumptions made by Xie et al. (2022) and posited that ICL in LLMs essentially operates as a form of topic modeling that implicitly extracts task-relevant information from examples to aid in inference.
sent7: Wang et al. (2023b) grounded their theoretical analysis in a setting with a finite number of demonstrations, and under a more general language generation process.
sent8: Specifically, they characterized the data generation process using a causal graph with three variables and imposed no constraints on the distribution or quantity of samples.
sent9: Their empirical and theoretical investigations revealed that ICL can approximate the Bayes optimal predictor when a finite number of samples are chosen based on the latent concept variable.
sent10: Moreover, Wang et al. (2023b) devised an effective practical algorithm for demonstration selection tailored to real-world LLMs.
sent11: At the same time, Jiang (2023) also introduced a novel latent space theory extending the idea of Xie et al. ( 2022) to explain emergent abilities in LLMs.
sent12: Instead of focusing on specific data distributions generated by HMMs, they delved into general sparse data distributions and employed LLMs as a universal density approximator for the marginal distribution, allowing them to probe these sparse structures more broadly.
sent13: Jiang (2023) demonstrated that ICL, CoT, and instruction-following abilities in LLMs can be ascribed to Bayesian inference operating on the broader sparse joint distribution of languages.
sent14: To shed light on the significance of the attention mechanism for ICL from a Bayesian view, Zhang et al. (2023) defined ICL as the task of predicting a response that aligns with a given covariate based on examples derived from a latent variable model.
sent15: They established that ICL implicitly implements the Bayesian Model Averaging (BMA) algorithm, which is approximated by the attention mechanism.
sent16: Furthermore, they demonstrated that certain attention mechanisms converge towards the conventional softmax attention as the number of examples goes to infinity.
sent17: These attentions, due to their encoding of BMA within their structure, empower the Transformer model to perform ICL.
sent18: Although their conclusions are insightful, there is a room for improvement.
sent19: Their findings might be influenced by various factors, such as the formats of the examples, the nature of tasks, and the choice of evaluation metrics.
sent20: Additionally, many of these studies are based on analyses conducted using small synthetic datasets, potentially restricting their relevance and applicability to real-world scenarios."
264832783,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825,s8,Interpreting Emergent Abilities from Micro Perspective,"From a micro perspective, research predominantly emphasizes empirical probing, focusing on the factors that influence the emergent abilities of LLMs.These factors encompass variations in results across downstream tasks, driven by aspects such as the quality of pre-training data (Chan et al., 2022;Razeghi et al., 2022;Shin et al., 2022;Razeghi et al., 2022;Power et al., 2022), the quality of the provided examples (Lu et al., 2022;Liu et al., 2022;Wang et al., 2022;Turpin et al., 2023), and mappings of demonstration labels (Min et al., 2022;Kossen et al., 2023;Wei et al., 2023;Yoo et al., 2022).","[['b64', 'b27', 'b68', 'b49', 'b56', 'b37', 'b45', 'b58', 'b38', 'b48', 'b11', 'b33']]","[['b64', 'b27', 'b68', 'b49', 'b56', 'b37', 'b45', 'b58', 'b38', 'b48', 'b11', 'b33']]",12,"sent1: From a micro perspective, research predominantly emphasizes empirical probing, focusing on the factors that influence the emergent abilities of LLMs.
sent2: These factors encompass variations in results across downstream tasks, driven by aspects such as the quality of pre-training data (Chan et al., 2022;Razeghi et al., 2022;Shin et al., 2022;Razeghi et al., 2022;Power et al., 2022), the quality of the provided examples (Lu et al., 2022;Liu et al., 2022;Wang et al., 2022;Turpin et al., 2023), and mappings of demonstration labels (Min et al., 2022;Kossen et al., 2023;Wei et al., 2023;Yoo et al., 2022)."
264832783,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825,s9,Pre-training Data,"Some studies have suggested that factors related to pre-traning data such as data domain, data term frequency, and data distribution (Chan et al., 2022;Razeghi et al., 2022), are crucial elements influencing the development of emergent abilities.

Data Domain Shin et al. (2022) conducted a study to explore the variations of ICL performance concerning the domain source and the size of the pre-training corpus, focusing primarily on the Korean lexicon.They utilized seven subcorpora from the HyperCLOVA corpus (Kim et al., 2021) to pretrain various language models and evaluated these models on Korean downstream tasks.Interestingly, Shin et al. (2022) found that the size of the pretraining corpus does not always determine the emergence of ICL.Instead, the domain source of the corpus significantly influences ICL performance.For example, language models trained with subcorpora constructed from blog posts exhibited the best ICL capability.This phenomenon may be attributed to the greater token diversity presented in the blog posts corpus compared with other sources like news.Moreover, their experiments highlighted that combining multiple corpora can lead to the emergence of ICL, even if individual corpora did not produce such learning on their own.Surprisingly, Shin et al. (2022) also found that a language model pre-trained with a corpus related to a downstream task did not always guarantee competitive ICL performance.For instance, a model trained on a news-related dataset (Park et al., 2021) showed superior performance in zero-shot news topic classification, but its few-shot performance was not superior.In a similar vein, The authors focused particularly on a crucial type of reasoning in LLMs -numerical reasoning in fewshot settings; and examined the extend to which the frequency of terms from the pre-training data correlates with model performance in these situations.Their analysis focused on the prevalence of numerical reasoning tasks within the training instances and established a connection between frequencies and reasoning performance.This connection is quantified by introducing the ""performance gap"", which is defined as the accuracy of terms appearing more than 90% of the time minus the accuracy of terms appearing less than 10% of the time.They conducted their experiments using GPTbased language models trained on the Pile dataset (Gao et al., 2021), ranging in size from 1.3B to 6B parameters.Evaluation was carried out on 11 datasets spanning three types of mathematical reasoning tasks: Arithmetic, Operation Inference and Time-Unit Conversion.The findings consistently show that models perform better in instances where terms from the pre-training data are more prevalent (Razeghi et al., 2022).In some scenarios, there is a substantial performance gap of over 70% between the most frequently occurring terms and the least frequently occurring terms.The significant performance difference raises questions about the actual generalization capabilities of these models beyond their pre-training data.Razeghi et al. (2022)'s observations suggest that the more prevalent content included in the pre-training data may exert an influ-ence on the emergent abilities, and it is possible that these language models are not actually reasoning to solve arithmetic tasks.In line with this research, Kandpal et al. (2023) (2023) theoretically demonstrated that unseen tasks can be efficiently learned via ICL when the pretraining data distribution comprises a mixture of latent tasks.","[['b11', 'b48'], ['b17', 'b49', 'b24', 'b25', None, 'b48', 'b44']]","[['b11', 'b48'], ['b17', 'b49', 'b24', 'b25', None, 'b48', 'b44']]",9,"sent1: Some studies have suggested that factors related to pre-traning data such as data domain, data term frequency, and data distribution (Chan et al., 2022;Razeghi et al., 2022), are crucial elements influencing the development of emergent abilities.
sent2: Data Domain Shin et al. (2022) conducted a study to explore the variations of ICL performance concerning the domain source and the size of the pre-training corpus, focusing primarily on the Korean lexicon.
sent3: They utilized seven subcorpora from the HyperCLOVA corpus (Kim et al., 2021) to pretrain various language models and evaluated these models on Korean downstream tasks.
sent4: Interestingly, Shin et al. (2022) found that the size of the pretraining corpus does not always determine the emergence of ICL.Instead, the domain source of the corpus significantly influences ICL performance.
sent5: For example, language models trained with subcorpora constructed from blog posts exhibited the best ICL capability.
sent6: This phenomenon may be attributed to the greater token diversity presented in the blog posts corpus compared with other sources like news.
sent7: Moreover, their experiments highlighted that combining multiple corpora can lead to the emergence of ICL, even if individual corpora did not produce such learning on their own.
sent8: Surprisingly, Shin et al. (2022) also found that a language model pre-trained with a corpus related to a downstream task did not always guarantee competitive ICL performance.
sent9: For instance, a model trained on a news-related dataset (Park et al., 2021) showed superior performance in zero-shot news topic classification, but its few-shot performance was not superior.
sent10: In a similar vein, The authors focused particularly on a crucial type of reasoning in LLMs -numerical reasoning in fewshot settings; and examined the extend to which the frequency of terms from the pre-training data correlates with model performance in these situations.
sent11: Their analysis focused on the prevalence of numerical reasoning tasks within the training instances and established a connection between frequencies and reasoning performance.
sent12: This connection is quantified by introducing the ""performance gap"", which is defined as the accuracy of terms appearing more than 90% of the time minus the accuracy of terms appearing less than 10% of the time.
sent13: They conducted their experiments using GPTbased language models trained on the Pile dataset (Gao et al., 2021), ranging in size from 1.3B to 6B parameters.
sent14: Evaluation was carried out on 11 datasets spanning three types of mathematical reasoning tasks: Arithmetic, Operation Inference and Time-Unit Conversion.
sent15: The findings consistently show that models perform better in instances where terms from the pre-training data are more prevalent (Razeghi et al., 2022).In some scenarios, there is a substantial performance gap of over 70% between the most frequently occurring terms and the least frequently occurring terms.
sent16: The significant performance difference raises questions about the actual generalization capabilities of these models beyond their pre-training data.
sent17: Razeghi et al. (2022)'s observations suggest that the more prevalent content included in the pre-training data may exert an influ-ence on the emergent abilities, and it is possible that these language models are not actually reasoning to solve arithmetic tasks.
sent18: In line with this research, Kandpal et al. (2023) (2023) theoretically demonstrated that unseen tasks can be efficiently learned via ICL when the pretraining data distribution comprises a mixture of latent tasks."
264832783,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825,s10,Pre-training Model,"Wei et al. (2022b) embarked on an investigation into the emergent abilities of LLMs.Their ap-proach to interpreting this phenomenon involved conducting a comprehensive survey of existing literature and analyzing the unpredictable nature of how certain abilities manifest as these models scale (Brown et al., 2020).Wei et al. (2022b) emphasized that while model scale has been correlated with LLM performance, it is not the sole determinant.Task-specific abilities can also be examined by considering a language model's performance (perplexity) on general text corpora, such as WikiText103.Their experiments showed that, despite having fewer parameters, the PaLM 62B model outperformed LaMDA 137B and GPT-3 175B in certain tasks.This suggests that other factors, like high-quality data and architectural differences, also play a role.Moreover, continued pre-training on different objectives, like the mixture-of-denoisers objective, has shown potential in enabling emergent abilities (Tay et al., 2022).

Research is also advancing to make these discovered abilities accessible for smaller-scale models.

For instance, instruction-based fine-tuning showed potential in smaller models with different architectures.Additionally, the emergence of syntactic rulelearning can be triggered by threshold frequencies in training data, similar to ""aha"" moments in human learning (Abend et al., 2017;Zhang et al., 2021).While the majority of research agrees that model scale is a key factor for emergent abilities.Kirsch et al. (2022) presented an interesting perspective.They found that among the factors determining the inductive bias of the model, the state-size (such as the hidden state size in a recurrent network) is a more crucial parameter than the overall model size for the emergence of ICL ability.","[['b54', None, 'b7', 'b62'], [], ['b69', 'b26', 'b0']]","[['b54', None, 'b7', 'b62'], [], ['b69', 'b26', 'b0']]",7,"sent1: Wei et al. (2022b) embarked on an investigation into the emergent abilities of LLMs.
sent2: Their ap-proach to interpreting this phenomenon involved conducting a comprehensive survey of existing literature and analyzing the unpredictable nature of how certain abilities manifest as these models scale (Brown et al., 2020).Wei et al. (2022b) emphasized that while model scale has been correlated with LLM performance, it is not the sole determinant.
sent3: Task-specific abilities can also be examined by considering a language model's performance (perplexity) on general text corpora, such as WikiText103.Their experiments showed that, despite having fewer parameters, the PaLM 62B model outperformed LaMDA 137B and GPT-3 175B in certain tasks.
sent4: This suggests that other factors, like high-quality data and architectural differences, also play a role.
sent5: Moreover, continued pre-training on different objectives, like the mixture-of-denoisers objective, has shown potential in enabling emergent abilities (Tay et al., 2022).
sent6: Research is also advancing to make these discovered abilities accessible for smaller-scale models.
sent7: For instance, instruction-based fine-tuning showed potential in smaller models with different architectures.
sent8: Additionally, the emergence of syntactic rulelearning can be triggered by threshold frequencies in training data, similar to ""aha"" moments in human learning (Abend et al., 2017;Zhang et al., 2021).While the majority of research agrees that model scale is a key factor for emergent abilities.
sent9: Kirsch et al. (2022) presented an interesting perspective.
sent10: They found that among the factors determining the inductive bias of the model, the state-size (such as the hidden state size in a recurrent network) is a more crucial parameter than the overall model size for the emergence of ICL ability."
264832783,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825,s12,Demonstration Order,"The order of the demonstrations has a significant impact on downstream task performance.Lu et al. (2022) showed that it be the deciding factor between achieving near stateof-the-art and random guessing.They designed demonstrations containing four samples with a balanced label distribution and conducted experiments involving all 24 possible permutations of sample orders.The experimental results showed that the performance variations among different permutations exist across various model sizes, especially for smaller models.Besides, is was observed that effective prompts are not transferrable across models, indicating that the optimal order is model-dependent, and what works well for one model does not guarantee good results for another model.Zhao et al. (2021) identified a phenomenon that LLMs tend to repeat answers found at the end of demonstrations, which they termed ""recency bias"".Similarly, in multi-document question answering and key-value retrieval tasks, Liu et al. ( 2023) made analogous observations.These tasks involve identifying relevant information within lengthy input contexts.The results showed that LLMs performed best when the relevant information is located at the beginning or end of their input contexts.However, their performance degraded when they are forced to use information from the middle of their input.In addition, they noted that model performance declines as the input context length increases, suggesting that current models struggle to effectively reason over their entire context window.Although these studies offer insights into how demonstration order influences emergent abilities, they do not delve into the underlying reasons of these obesrvations.In an effort to investigate the impact of semantic similarity between ICL examples and test examples on downstream task, Liu et al. (2022) proposed retrieving examples semantically similar to a test example for creating its demonstration.They utilized the CLS embeddings from a pre-trained RoBERTa-large (Liu et al., 2019) model to represent sentences and assessed the semantic similarity between two sentences by computing the cosine similarity of their respective representations.For each test example, they identified the nearest K neighbors from the training set and concatenated them in descending order of semantic similarity to create the demonstration.Their experiments on Web Questions (Berant et al., 2013) and Trivia Question Answering (Joshi et al., 2017) benchmarks showed that the default order performed slightly better than the reverse order.However, the reverse order performed better on the Natural Questions (Kwiatkowski et al., 2019) benchmark.Consequently, the choice of order appears to be dependent on the specific dataset in use.

Input-Label Mapping Some studies have been conducted to investigate how input-label mappings influence the performance of ICL.Min et al. (2022) revealed that substituting the correct labels of incontext examples in demonstrations with random labels only leads to a marginal decrease in performance across a variety of classification and multichoice tasks.They also conducted ablation experiments to investigate the impact of the number of correct labels on performance.Surprisingly, the results showed that the performance was not sensitive to the number of correct labels in demonstrations.This led to the counter-intuitive conclusion that LLMs do not heavily rely on input-label mappings to perform tasks.

However, Yoo et al. ( 2022), Wei et al. (2023), and Kossen et al. (2023) disagreed with the claim put forth by Min et al. (2022).Yoo et al. (2022) pointed out that the claim exhibited overgeneralization in two aspects: (1) Aggregating the mean performance across various datasets was found to be inadequate in capturing the insensitivity behavior observed within individual datasets.

(2) The experimental setting lacked generalizability, and the results were sensitive to minor adjustments to the experimental setup.To delve deeper into the topic of input-label mapping, Yoo et al. (2022) introduced two novel metrics.The first metric, Label-Correctness Sensitivity, quantifies the impact on downstream classification performance when a fixed amount of label corruption is introduced into the demonstration.The second metric, Ground-Truth Label Effect Ratio, assesses how much the presence of ground-truth labels improves the performance compared to a baseline with random labels.Their experimental results showed that the sensitivity exhibited significant variation across 17 datasets, with the aggregate sensitivity considerably high.This indicated that label correctness does indeed affect downstream task performance.Furthermore, Yoo et al. (2022) suggested a strong correlation between sensitivity and task difficulty, revealing that LLMs displayed low sensitivity on challenging tasks.Wei et al. (2023) further explored how semantic priors and input-label mappings affect ICL.They suggested that LLMs possess the ability to override semantic priors from pre-training in favour of inputlabel mappings from demonstrations.This explains why the performance of LLMs drops below random guessing when all the labels in the demonstrations are flipped.They also found that smaller models experienced a less severe decline in performance because they lack the capacity to override semantic priors to the same extent.More specifically, Wei et al. (2023) conducted experiments where they replaced the labels with semantically unrelated labels.The results showed that the performance drop was more significant for small models compared to LLMs.This led them to suggest that small models rely heavily on the semantic meanings of labels rather than learning the input-label mappings provided in the demonstrations.Kossen et al. (2023) also found that larger models are more sensitive to randomized labels, and they highlighted that LLMs can learn new input-label mappings from demonstrations.

However, the ability to learn new input-label mappings can, at times, have adverse effect on performance.Tang et al. (2023) revealed that LLMs sometimes tend to exploit shortcuts within demonstrations for downstream tasks.These shortcuts represent spurious correlations between in-context examples and their corresponding labels.Tang et al. (2023) designed several types of shortcuts, and their experimental results showed that LLMs are ""lazy reasoners"".They relied heavily on the shortcuts within demonstrations to deduce the final answers.Furthermore, Si et al. (2023) discovered that when presented with a set of non-specific demonstrations (For example, the labels are semantically unrelated), LLMs exhibited feature bias.This indicated that LLMs tend to favour one feature over another, even when both features are equally capable of predicting the label, as mentioned in the prompt.For example, in a sentiment analysis setting, LLMs showed a significant bias towards predicting labels based on sentiment rather than shallow lexical features.Nevertheless, feature bias has the potential to detrimentally affect performance when the model's feature bias does not align with the intended task.Si et al. (2023) suggested that certain interventions could help mitigate feature bias, such as employing natural-language instructions and incorporating label words that have semantic relevance to the intended feature.

To further investigate the underlying mechanism of how LLMs learn from input-label mappings, Wang et al. (2023a) conducted an extensive study into the workings of ICL from the perspective of information flow.They computed saliency scores for each element within the attention matrix to unveil the significant token interactions.The experimental results demonstrated that label words within demonstrations play a crucial role in this process.

Specifically: (1) During the processing of shallow computation layers, semantic information becomes concentrated within the representations of label words.

(2) The aggregated information contained within label words serves as a reference for the final predictions made by LLMs.Their findings confirmed that label words can indeed have a substantial impact on the performance of the final task.

Chain-of-Thought Prompting Some studies have focused on exploring the impact of COT prompting on LLM performance.Wang et al. (2022) found that the validity of the reasoning process in demonstrations has only a minimal impact on performance.To assess this, they constructed invalid reasoning processes manually for all incontext examples.Surprisingly, the experimental results showed that LLMs can retain 80-90% of their performance even when presented with invalid reasoning steps in demonstrations.They also found that the coherence of the reasoning process and its relevance to the query are significantly more crucial factors for the effectiveness of CoT.

Regarding the explanations generated by LLMs, Turpin et al. (2023) found that CoT explanations produced by LLMs can occasionally misrepresent the true underlying rationales behind their predictions.They introduced two types of bias in the prompt design to investigate this phenomenon.The first bias involves consistently reordering the multiple-choice options of in-context examples to make the answer 'A'.The second bias entails including the suggested answers directly in the prompt.The experimental results indicated that, in both bias scenarios, LLMs tend to provide answers aligned with stereotypes and generate explanations that do not faithfully support the answer.Furthermore, there was a large drop in performance when comparing biased demonstrations to unbiased demonstrations.","[['b37', 'b5', 'b35', 'b23', None, 'b72', 'b33'], ['b38'], ['b64', 'b27', 'b38', 'b68'], ['b64', 'b27', 'b68'], ['b53', 'b50'], ['b59'], [], [], ['b58'], ['b56']]","[['b37', 'b5', 'b35', 'b23', None, 'b72', 'b33'], ['b38'], ['b64', 'b27', 'b38', 'b68'], ['b64', 'b27', 'b68'], ['b53', 'b50'], ['b59'], [], [], ['b58'], ['b56']]",20,"sent1: The order of the demonstrations has a significant impact on downstream task performance.
sent2: Lu et al. (2022) showed that it be the deciding factor between achieving near stateof-the-art and random guessing.
sent3: They designed demonstrations containing four samples with a balanced label distribution and conducted experiments involving all 24 possible permutations of sample orders.
sent4: The experimental results showed that the performance variations among different permutations exist across various model sizes, especially for smaller models.
sent5: Besides, is was observed that effective prompts are not transferrable across models, indicating that the optimal order is model-dependent, and what works well for one model does not guarantee good results for another model.
sent6: Zhao et al. (2021) identified a phenomenon that LLMs tend to repeat answers found at the end of demonstrations, which they termed ""recency bias"".
sent7: Similarly, in multi-document question answering and key-value retrieval tasks, Liu et al. ( 2023) made analogous observations.
sent8: These tasks involve identifying relevant information within lengthy input contexts.
sent9: The results showed that LLMs performed best when the relevant information is located at the beginning or end of their input contexts.
sent10: However, their performance degraded when they are forced to use information from the middle of their input.
sent11: In addition, they noted that model performance declines as the input context length increases, suggesting that current models struggle to effectively reason over their entire context window.
sent12: Although these studies offer insights into how demonstration order influences emergent abilities, they do not delve into the underlying reasons of these obesrvations.
sent13: In an effort to investigate the impact of semantic similarity between ICL examples and test examples on downstream task, Liu et al. (2022) proposed retrieving examples semantically similar to a test example for creating its demonstration.
sent14: They utilized the CLS embeddings from a pre-trained RoBERTa-large (Liu et al., 2019) model to represent sentences and assessed the semantic similarity between two sentences by computing the cosine similarity of their respective representations.
sent15: For each test example, they identified the nearest K neighbors from the training set and concatenated them in descending order of semantic similarity to create the demonstration.
sent16: Their experiments on Web Questions (Berant et al., 2013) and Trivia Question Answering (Joshi et al., 2017) benchmarks showed that the default order performed slightly better than the reverse order.
sent17: However, the reverse order performed better on the Natural Questions (Kwiatkowski et al., 2019) benchmark.
sent18: Consequently, the choice of order appears to be dependent on the specific dataset in use.Input-Label Mapping Some studies have been conducted to investigate how input-label mappings influence the performance of ICL.Min et al. (2022) revealed that substituting the correct labels of incontext examples in demonstrations with random labels only leads to a marginal decrease in performance across a variety of classification and multichoice tasks.
sent19: They also conducted ablation experiments to investigate the impact of the number of correct labels on performance.
sent20: Surprisingly, the results showed that the performance was not sensitive to the number of correct labels in demonstrations.
sent21: This led to the counter-intuitive conclusion that LLMs do not heavily rely on input-label mappings to perform tasks.
sent22: However, Yoo et al. ( 2022), Wei et al. (2023), and Kossen et al. (2023) disagreed with the claim put forth by Min et al. (2022).Yoo et al. (2022) pointed out that the claim exhibited overgeneralization in two aspects: (1) Aggregating the mean performance across various datasets was found to be inadequate in capturing the insensitivity behavior observed within individual datasets.
sent23: (2) The experimental setting lacked generalizability, and the results were sensitive to minor adjustments to the experimental setup.
sent24: To delve deeper into the topic of input-label mapping, Yoo et al. (2022) introduced two novel metrics.
sent25: The first metric, Label-Correctness Sensitivity, quantifies the impact on downstream classification performance when a fixed amount of label corruption is introduced into the demonstration.
sent26: The second metric, Ground-Truth Label Effect Ratio, assesses how much the presence of ground-truth labels improves the performance compared to a baseline with random labels.
sent27: Their experimental results showed that the sensitivity exhibited significant variation across 17 datasets, with the aggregate sensitivity considerably high.
sent28: This indicated that label correctness does indeed affect downstream task performance.
sent29: Furthermore, Yoo et al. (2022) suggested a strong correlation between sensitivity and task difficulty, revealing that LLMs displayed low sensitivity on challenging tasks.
sent30: Wei et al. (2023) further explored how semantic priors and input-label mappings affect ICL.They suggested that LLMs possess the ability to override semantic priors from pre-training in favour of inputlabel mappings from demonstrations.
sent31: This explains why the performance of LLMs drops below random guessing when all the labels in the demonstrations are flipped.
sent32: They also found that smaller models experienced a less severe decline in performance because they lack the capacity to override semantic priors to the same extent.
sent33: More specifically, Wei et al. (2023) conducted experiments where they replaced the labels with semantically unrelated labels.
sent34: The results showed that the performance drop was more significant for small models compared to LLMs.
sent35: This led them to suggest that small models rely heavily on the semantic meanings of labels rather than learning the input-label mappings provided in the demonstrations.
sent36: Kossen et al. (2023) also found that larger models are more sensitive to randomized labels, and they highlighted that LLMs can learn new input-label mappings from demonstrations.
sent37: However, the ability to learn new input-label mappings can, at times, have adverse effect on performance.
sent38: Tang et al. (2023) revealed that LLMs sometimes tend to exploit shortcuts within demonstrations for downstream tasks.
sent39: These shortcuts represent spurious correlations between in-context examples and their corresponding labels.
sent40: Tang et al. (2023) designed several types of shortcuts, and their experimental results showed that LLMs are ""lazy reasoners"".
sent41: They relied heavily on the shortcuts within demonstrations to deduce the final answers.
sent42: Furthermore, Si et al. (2023) discovered that when presented with a set of non-specific demonstrations (For example, the labels are semantically unrelated), LLMs exhibited feature bias.
sent43: This indicated that LLMs tend to favour one feature over another, even when both features are equally capable of predicting the label, as mentioned in the prompt.
sent44: For example, in a sentiment analysis setting, LLMs showed a significant bias towards predicting labels based on sentiment rather than shallow lexical features.
sent45: Nevertheless, feature bias has the potential to detrimentally affect performance when the model's feature bias does not align with the intended task.
sent46: Si et al. (2023) suggested that certain interventions could help mitigate feature bias, such as employing natural-language instructions and incorporating label words that have semantic relevance to the intended feature.
sent47: To further investigate the underlying mechanism of how LLMs learn from input-label mappings, Wang et al. (2023a) conducted an extensive study into the workings of ICL from the perspective of information flow.
sent48: They computed saliency scores for each element within the attention matrix to unveil the significant token interactions.
sent49: The experimental results demonstrated that label words within demonstrations play a crucial role in this process.
sent50: Specifically: (1) During the processing of shallow computation layers, semantic information becomes concentrated within the representations of label words.
sent51: (2) The aggregated information contained within label words serves as a reference for the final predictions made by LLMs.
sent52: Their findings confirmed that label words can indeed have a substantial impact on the performance of the final task.
sent53: Chain-of-Thought Prompting Some studies have focused on exploring the impact of COT prompting on LLM performance.
sent54: Wang et al. (2022) found that the validity of the reasoning process in demonstrations has only a minimal impact on performance.
sent55: To assess this, they constructed invalid reasoning processes manually for all incontext examples.
sent56: Surprisingly, the experimental results showed that LLMs can retain 80-90% of their performance even when presented with invalid reasoning steps in demonstrations.
sent57: They also found that the coherence of the reasoning process and its relevance to the query are significantly more crucial factors for the effectiveness of CoT.Regarding the explanations generated by LLMs, Turpin et al.
sent58: (2023) found that CoT explanations produced by LLMs can occasionally misrepresent the true underlying rationales behind their predictions.
sent59: They introduced two types of bias in the prompt design to investigate this phenomenon.
sent60: The first bias involves consistently reordering the multiple-choice options of in-context examples to make the answer 'A'.
sent61: The second bias entails including the suggested answers directly in the prompt.
sent62: The experimental results indicated that, in both bias scenarios, LLMs tend to provide answers aligned with stereotypes and generate explanations that do not faithfully support the answer.
sent63: Furthermore, there was a large drop in performance when comparing biased demonstrations to unbiased demonstrations."
264832783,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825,s14,Unified Framework,"There is currently no standardized framework available for understanding or interpreting emergent abilities.While researchers often investigate factors contributing to emergent abilities based on empirical insights, the resulting conclusion may not always be robust or broadly applicable to realworld applications.The challenge lie in the multitude of factors that influence emergent abilities, many of which may not be directly modifiable with respect to the abilities themselves, as noted by Wei et al. (2022b).For instance, apart from the attention mechanism, Li et al. (2023a) found that softmax unit plays a pivotal role in understanding ICL through function regression problems (Garg et al., 2022;Akyürek et al., 2023;von Oswald et al., 2022).From the micro-perspective, when examining how the extent of pre-training impacts emergent abilities, data quality serves a crucial role alongside factors like data scale and training time.","[['b41', 'b62', 'b28', 'b18', 'b2']]","[['b41', 'b62', 'b28', 'b18', 'b2']]",5,"sent1: There is currently no standardized framework available for understanding or interpreting emergent abilities.
sent2: While researchers often investigate factors contributing to emergent abilities based on empirical insights, the resulting conclusion may not always be robust or broadly applicable to realworld applications.
sent3: The challenge lie in the multitude of factors that influence emergent abilities, many of which may not be directly modifiable with respect to the abilities themselves, as noted by Wei et al. (2022b).For instance, apart from the attention mechanism, Li et al. (2023a) found that softmax unit plays a pivotal role in understanding ICL through function regression problems (Garg et al., 2022;Akyürek et al., 2023;von Oswald et al., 2022).From the micro-perspective, when examining how the extent of pre-training impacts emergent abilities, data quality serves a crucial role alongside factors like data scale and training time."
264832783,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825,s15,Evaluation Metrics,"Current research efforts typically measure emergent abilities by assessing task performance or optimizing criteria such as gradient (von Oswald et al., 2022) and token loss (Olsson et al., 2022) during the pre-training stage.Another line of research (Shin et al., 2022;Razeghi et al., 2022) has discovered that the relationship between the evaluation measures of language models during training does not strongly correlate with the conventional evaluation metrics, such as F1-score, that have been used to measure performance of emergent abilities under most experimental setups.However, a dedicated criterion explicitly designed for the assessment of emergent abilities is currently lacking.In addition, assessing emergent abilities often becomes complicated due to the interwined emergence of other competencies (Lu et al., 2023).In this work, we postulate that the assessment of emergent ability can be based on its capability to produce satisfactory results in comparison to a finetuned model.This approach provides a preliminary framework for devising evaluation criteria.However, it is important to note that this methodology is preliminary and not yet comprehensive or definitive.Further refinement and development of formal criteria are necessary to establish a robust and universally applicable evaluation metric for emergent ability itself.","[['b41', 'b49', 'b36', 'b48', 'b40']]","[['b41', 'b49', 'b36', 'b48', 'b40']]",5,"sent1: Current research efforts typically measure emergent abilities by assessing task performance or optimizing criteria such as gradient (von Oswald et al., 2022) and token loss (Olsson et al., 2022) during the pre-training stage.
sent2: Another line of research (Shin et al., 2022;Razeghi et al., 2022) has discovered that the relationship between the evaluation measures of language models during training does not strongly correlate with the conventional evaluation metrics, such as F1-score, that have been used to measure performance of emergent abilities under most experimental setups.
sent3: However, a dedicated criterion explicitly designed for the assessment of emergent abilities is currently lacking.
sent4: In addition, assessing emergent abilities often becomes complicated due to the interwined emergence of other competencies (Lu et al., 2023).In this work, we postulate that the assessment of emergent ability can be based on its capability to produce satisfactory results in comparison to a finetuned model.
sent5: This approach provides a preliminary framework for devising evaluation criteria.
sent6: However, it is important to note that this methodology is preliminary and not yet comprehensive or definitive.
sent7: Further refinement and development of formal criteria are necessary to establish a robust and universally applicable evaluation metric for emergent ability itself."
264832783,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825,s16,Cost and Computational Resources,"LLMs faced constraints related to their token capacity, which can lead to deficiency in coherence when dealing with longer demonstration examples or text generation.This limitation can result in challenges for emergent abilities, making it difficult to maintain a consistent and extended logical flow.What's more, there are some experimental limitations that have hindered the exploration of this type of research.The pre-training stage of these models demands a huge amount of computational resources, which could become a barrier for researchers who lack the necessary resources (Shin et al., 2022;Brown et al., 2020;Wei et al., 2022b;Berglund et al., 2023).This limitation has restricted investigations into the sources of emergent abilities in commonly used LLMs.Furthermore, the limited knowledge of the detailed lexical resources utilized during the pre-training stage adds complexity to the examination of their abilities (Berglund et al., 2023).","[['b62', 'b7', 'b49', 'b6']]","[['b62', 'b7', 'b49', 'b6']]",4,"sent1: LLMs faced constraints related to their token capacity, which can lead to deficiency in coherence when dealing with longer demonstration examples or text generation.
sent2: This limitation can result in challenges for emergent abilities, making it difficult to maintain a consistent and extended logical flow.
sent3: What's more, there are some experimental limitations that have hindered the exploration of this type of research.
sent4: The pre-training stage of these models demands a huge amount of computational resources, which could become a barrier for researchers who lack the necessary resources (Shin et al., 2022;Brown et al., 2020;Wei et al., 2022b;Berglund et al., 2023).This limitation has restricted investigations into the sources of emergent abilities in commonly used LLMs.
sent5: Furthermore, the limited knowledge of the detailed lexical resources utilized during the pre-training stage adds complexity to the examination of their abilities (Berglund et al., 2023)."
264832783,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825,s17,Transparency of Training Data,"Some studies (Chan et al., 2022;Razeghi et al., 2022;Shin et al., 2022;Power et al., 2022) have emphasized the connection between emergent abilities and the training data.It is clear that diverse, clearly structured pre-training data can facilitate the emergence of abilities, such as reasoning.Consequently, understanding how to better evaluate the data quality and how to construct high-quality training data may enable future research to better study the emergent abilities at the pre-training stage.Hence, our community should refrain from treating the pre-training data of LLMs as black boxes.Neglecting the role of pre-training data can lead to misinterpretations when assessing the emergent abilities.","[['b45', 'b11', 'b49', 'b48']]","[['b45', 'b11', 'b49', 'b48']]",4,"sent1: Some studies (Chan et al., 2022;Razeghi et al., 2022;Shin et al., 2022;Power et al., 2022) have emphasized the connection between emergent abilities and the training data.
sent2: It is clear that diverse, clearly structured pre-training data can facilitate the emergence of abilities, such as reasoning.
sent3: Consequently, understanding how to better evaluate the data quality and how to construct high-quality training data may enable future research to better study the emergent abilities at the pre-training stage.
sent4: Hence, our community should refrain from treating the pre-training data of LLMs as black boxes.
sent5: Neglecting the role of pre-training data can lead to misinterpretations when assessing the emergent abilities."
264832783,The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/7f48fbb13c5a31529ab4bc2bf53adeb4bd213825,s3,Mechanistic Interpretability,"With the goal of reverse-engineering components of frontier models into more understandable algorithms, Elhage et al. (2021) developed a mathematical framework for decomposing operations within transformers (Vaswani et al., 2017).They initially introduced the concept of ""induction heads"" in a two-layer attention-only model to explain the functioning of ICL within transformers with Circuits (Cammarata et al., 2020).They found that one-layer attention-only models perform relatively basic ICL in a crude manner, whereas two-layer models perform very general ICL using very different algorithms.Specifically, they discovered that one-layer models essentially function as an ensemble of bigram and ""skip-trigram"" models that can be accessed directly from the model weights without running the entire model.Most attention heads in these models allocate significant capacity to copying mechanisms, resulting in very simple ICL.In contrast, the two-layer models manifest a significantly powerful mechanism that employs more advanced, qualitative algorithms at inference time, referred to as ""induction heads"".This allows them to perform ICL in a manner that resembles a computer program executing an algorithm, rather than merely referencing skip-trigrams.Building on this foundation, Olsson et al. (2022) later investigated the internal structures responsible for ICL by extending the concept of ""induction head"" (Elhage et al., 2021).They implemented circuits consist of two attention heads: the ""previous token head"", which copies information from one token to its successor, and the actual ""induction head"", which uses this information to target tokens that precede the current one.Their study revealed a phase change occurring early in the training of LLMs of various sizes.This phase change involves circuits that perform ""fuzzy"" or ""nearest neighbor"" pattern completion in a mechanism similar to the two-layer induction heads.These circuits play a crucial role in implementating most ICL in large models.One pivotal insight from (Olsson et al., 2022) presented six arguments supporting their hypothesis that induction heads may serve as the primary mechanistic source of ICL in a significant portion of LLMs, particularly those based on transformer architectures.

While Elhage et al. (2021) and Olsson et al. (2022) contribute to our understanding of ICL by probing the internal architecture of LLMs, it is important to note that their findings represent initial steps towards the comprehensive reverseengineering of LLMs.It becomes particularly intricate when dealing with LLMs characterized by complex structures comprising hundreds of layers and spanning billions to trillions of parameters.This complexity introduces significant challenges.Moreover, a substantial portion of their conclusions relies primarily on empirical correlations, which might be susceptible to confounding from various factors, thereby introducing potential vulnerabilities into their findings.","[['b40', None, 'b9', 'b57'], ['b40', None]]","[['b40', None, 'b9', 'b57'], ['b40', None]]",6,"sent1: With the goal of reverse-engineering components of frontier models into more understandable algorithms, Elhage et al. (2021) developed a mathematical framework for decomposing operations within transformers (Vaswani et al., 2017).They initially introduced the concept of ""induction heads"" in a two-layer attention-only model to explain the functioning of ICL within transformers with Circuits (Cammarata et al., 2020).They found that one-layer attention-only models perform relatively basic ICL in a crude manner, whereas two-layer models perform very general ICL using very different algorithms.
sent2: Specifically, they discovered that one-layer models essentially function as an ensemble of bigram and ""skip-trigram"" models that can be accessed directly from the model weights without running the entire model.
sent3: Most attention heads in these models allocate significant capacity to copying mechanisms, resulting in very simple ICL.In contrast, the two-layer models manifest a significantly powerful mechanism that employs more advanced, qualitative algorithms at inference time, referred to as ""induction heads"".
sent4: This allows them to perform ICL in a manner that resembles a computer program executing an algorithm, rather than merely referencing skip-trigrams.
sent5: Building on this foundation, Olsson et al. (2022) later investigated the internal structures responsible for ICL by extending the concept of ""induction head"" (Elhage et al., 2021).They implemented circuits consist of two attention heads: the ""previous token head"", which copies information from one token to its successor, and the actual ""induction head"", which uses this information to target tokens that precede the current one.
sent6: Their study revealed a phase change occurring early in the training of LLMs of various sizes.
sent7: This phase change involves circuits that perform ""fuzzy"" or ""nearest neighbor"" pattern completion in a mechanism similar to the two-layer induction heads.
sent8: These circuits play a crucial role in implementating most ICL in large models.
sent9: One pivotal insight from (Olsson et al., 2022) presented six arguments supporting their hypothesis that induction heads may serve as the primary mechanistic source of ICL in a significant portion of LLMs, particularly those based on transformer architectures.
sent10: While Elhage et al. (2021) and Olsson et al. (2022) contribute to our understanding of ICL by probing the internal architecture of LLMs, it is important to note that their findings represent initial steps towards the comprehensive reverseengineering of LLMs.
sent11: It becomes particularly intricate when dealing with LLMs characterized by complex structures comprising hundreds of layers and spanning billions to trillions of parameters.
sent12: This complexity introduces significant challenges.
sent13: Moreover, a substantial portion of their conclusions relies primarily on empirical correlations, which might be susceptible to confounding from various factors, thereby introducing potential vulnerabilities into their findings."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s22,Multimodality,"Although rumors and fake news were traditionally spread through face-to-face communication, the emergence of social media resulted in their primary means of dissemination switching to text.However, continual advances in technology have led to an increasing shift towards multimodality.For example, people now frequently augment textual post content with images or videos, while on platforms like YouTube or TikTok, videos are the predominant means of sharing information.Accordingly, it is becoming increasingly important to explore methods that can address the challenges of multimodality [216], and that are able to adapt to the ever-changing characteristics of social media communication.While we have reviewed a number of approaches that combine text and image-based information, recent advanced multi-modal models that integrate language and visual understanding provide considerable scope for further research in this area.For example, GPT-4 has a certain level of visual understanding capability, although its implementation details have not been publicly disclosed.Inspired by the success of LLMs, some studies have started to focus on large multi-modal models, such as LLaVA [217,218], an end-to-end large multimodal model that connects a visual encoder and a large language model to achieve general visual and language understanding.Additionally, MiniGPT-5 [219] introduces a novel interleaved vision-and-language generation technique, with a focus on non-descriptive multimodal generation.Exploring the integration of these large multimodal models within misinformation detection methods is an interesting and promising research direction.","[['b217', 'b215', 'b216', 'b218']]","[['b217', 'b215', 'b216', 'b218']]",4,"sent1: Although rumors and fake news were traditionally spread through face-to-face communication, the emergence of social media resulted in their primary means of dissemination switching to text.
sent2: However, continual advances in technology have led to an increasing shift towards multimodality.
sent3: For example, people now frequently augment textual post content with images or videos, while on platforms like YouTube or TikTok, videos are the predominant means of sharing information.
sent4: Accordingly, it is becoming increasingly important to explore methods that can address the challenges of multimodality [216], and that are able to adapt to the ever-changing characteristics of social media communication.
sent5: While we have reviewed a number of approaches that combine text and image-based information, recent advanced multi-modal models that integrate language and visual understanding provide considerable scope for further research in this area.
sent6: For example, GPT-4 has a certain level of visual understanding capability, although its implementation details have not been publicly disclosed.
sent7: Inspired by the success of LLMs, some studies have started to focus on large multi-modal models, such as LLaVA [217,218], an end-to-end large multimodal model that connects a visual encoder and a large language model to achieve general visual and language understanding.
sent8: Additionally, MiniGPT-5 [219] introduces a novel interleaved vision-and-language generation technique, with a focus on non-descriptive multimodal generation.
sent9: Exploring the integration of these large multimodal models within misinformation detection methods is an interesting and promising research direction."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s24,Interpretability,"Understanding how and why misinformation detection models have arrived at their decision about whether or not a post or news article represents true or fake information can be important to make their reasoning processes more transparent and make it easier to understand why errors occur.However, despite the high levels of performance achieved by many DL approaches, their black-box nature means that no such reasoning information is available, and that their decisions are hard to justify.Although it remains a challenge to develop models that are both sufficiently accurate and whose results are interpretable, several studies have proposed possible solutions for explainable misinformation detection.These include the use of topic-based features for classification [221], Explainable Artificial Intelligence (XAI) techniques [222] and Commonsense Knowledge Graphs [223].Recent research has also begun to focus on the development of interpretable LLMs [224], such as MentalLLaMA [225], which is an interpretable mental health analysis model based on LLaMA-2.Accordingly, it is hoped that researchers working in misinformation detection will begin to place greater emphasis on exploring the increasing range of options that could be used to improve the interpretability of their models.","[['b223', 'b220', 'b224', 'b222', 'b221']]","[['b223', 'b220', 'b224', 'b222', 'b221']]",5,"sent1: Understanding how and why misinformation detection models have arrived at their decision about whether or not a post or news article represents true or fake information can be important to make their reasoning processes more transparent and make it easier to understand why errors occur.
sent2: However, despite the high levels of performance achieved by many DL approaches, their black-box nature means that no such reasoning information is available, and that their decisions are hard to justify.
sent3: Although it remains a challenge to develop models that are both sufficiently accurate and whose results are interpretable, several studies have proposed possible solutions for explainable misinformation detection.
sent4: These include the use of topic-based features for classification [221], Explainable Artificial Intelligence (XAI) techniques [222] and Commonsense Knowledge Graphs [223].Recent research has also begun to focus on the development of interpretable LLMs [224], such as MentalLLaMA [225], which is an interpretable mental health analysis model based on LLaMA-2.Accordingly, it is hoped that researchers working in misinformation detection will begin to place greater emphasis on exploring the increasing range of options that could be used to improve the interpretability of their models."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s25,Large Language Models,"The popularity of ChatGPT and GPT-4 [9] has resulted in the powerful capabilities of LLMs becoming widely known [226].As mentioned above, there is potential for LLMs to be employed in misinformation detection in multiple ways, including sentiment and emotion detection, multimodal analysis, and to enhance the interpretability of detection models.Some studies have additionally begun to explore the use of LLMs for rumor and fake news prediction.For example, Hu et al. [25] designed a framework for fake news detection in which a small language model (i.e., BERT) is complemented by an LLM, which provides multi-perspective guiding principles to improve prediction accuracy.Meanwhile, Pavlyshenko et al. [26] designed prompts to fine-tune LLaMA for rumor and fake news detection.Cheung et al [27] used external knowledge to bridge the gap between knowledge encoded in the LLM and the most up-to-date information available on the Internet, in order to enhance fake news detection performance.The promising results achieved by these approaches, combined with the indisputable power and advanced capabilities of LLMs, motivate further exploration of how they can be best exploited to further improve the accuracy of rumor and fake news detection.","[['b8', 'b225', 'b25', 'b26', 'b24']]","[['b8', 'b225', 'b25', 'b26', 'b24']]",5,"sent1: The popularity of ChatGPT and GPT-4 [9] has resulted in the powerful capabilities of LLMs becoming widely known [226].As mentioned above, there is potential for LLMs to be employed in misinformation detection in multiple ways, including sentiment and emotion detection, multimodal analysis, and to enhance the interpretability of detection models.
sent2: Some studies have additionally begun to explore the use of LLMs for rumor and fake news prediction.
sent3: For example, Hu et al. [25] designed a framework for fake news detection in which a small language model (i.e., BERT) is complemented by an LLM, which provides multi-perspective guiding principles to improve prediction accuracy.
sent4: Meanwhile, Pavlyshenko et al. [26] designed prompts to fine-tune LLaMA for rumor and fake news detection.
sent5: Cheung et al [27] used external knowledge to bridge the gap between knowledge encoded in the LLM and the most up-to-date information available on the Internet, in order to enhance fake news detection performance.
sent6: The promising results achieved by these approaches, combined with the indisputable power and advanced capabilities of LLMs, motivate further exploration of how they can be best exploited to further improve the accuracy of rumor and fake news detection."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s27,A. Specific types of content-based features B. Emotion Detection Tools,"Various tools and resources are used for the detection of emotion and sentiment features, the most commonly used of which are summarized in Table 7.In addition to the methods in Table 7, there are also some title-text similarity, word similarity, sentence similarity, cosine similarity between source post and related comments Cluster Features word-cluster feature, brown cluster feature [199], SDQC depth-based clusters [199] Semantic Feature word vector features (Glove [227], BERT [228], GoogleW2V [229], Word2vec [230]) Grammatical Features part-of-speech tags, noun, verbs, adjectives, and pronouns Lexical Features bad sexual words, cue words, multilingual hate lexicon, linguistic words, specific categories, denial term, support words, negation words, swear words, surprise and doubt words Linguisticinformed Features tf-idf, n-gram, named entity recognition, text language, bag-of-characters, bag of words (BoW) Stylistic Features [43] question marks, exclamation marks, punctuation marks, length of a sentence, uppercase ratio, consecutive characters and letters, presence of URLs, number of stop words, number of upper case letters, number of lower case letters, number of numeric values, word count, character count, sentence count, average sentence length, ease of comprehension, lexical diversity Syntactic Features ratio of negation, bag of relations (all tokens, list of words, verbs) Conversation based Features text similarity to source tweet, text similarity to replied tweet, tweet depth Twitter Metadata [28,204] the number of characters in a tweet, the number of retweets, favorites, presence of hashtags, URLs, mentions, existence of photos, creating time gaps for posts, Twitter verification.etc. Reddit Metadata [139] karma, gold status, Reddit employment status (if any), verified e-mail, reply count, upvotes, and whether the user is the submission submitter.Reddit commenting syntax: sarcasm ('/s'), edited ('edit:'), and quote count ('>') Others Topics, term features, textual novelty other efficient sentiment analysis tools such as Emojis Dictionary8 , Emoticons list9 , Affect-Br [242], SemEval10 , MPQA 11 , ENGAR [243], Hespress Facebook12 , Offense lexicon 13 , Sarcasm lexicon [244], Named entities lexicon (Religion lexicon, Nationality lexicon, Named entities) [158], Baidu sentiment API 14 , NLTK sentiment module 15 , SEO Scout's analysis tool 16 , IBM Watson's Natural Language Understanding (NLU) 17 , MeaningCloud18 , ParallelDots 19 , Empath [245], EffectWordNet [246], Hu&Liu opinion lexicon 20 , SSWE [247], NRC-Canada [248], Stanford sentiment Tree [249], Dictionary of Affect in Language (DAL) [250], Affective Norms for English Words (ANEW) [241], Meta-Mind sentimentclassifier API 21 .","[['b227', 'b198', 'b138', 'b203', 'b228', 'b157', 'b229', 'b243', 'b248', 'b42', 'b247', 'b242', 'b240', 'b244', 'b226', 'b27', 'b245', 'b249', 'b241', 'b246']]","[['b227', 'b198', 'b138', 'b203', 'b228', 'b157', 'b229', 'b243', 'b248', 'b42', 'b247', 'b242', 'b240', 'b244', 'b226', 'b27', 'b245', 'b249', 'b241', 'b246']]",20,"sent1: Various tools and resources are used for the detection of emotion and sentiment features, the most commonly used of which are summarized in Table 7.In addition to the methods in Table 7, there are also some title-text similarity, word similarity, sentence similarity, cosine similarity between source post and related comments Cluster Features word-cluster feature, brown cluster feature [199], SDQC depth-based clusters [199]
sent2: Semantic Feature word vector features (Glove [227], BERT [228], GoogleW2V [229], Word2vec [230]) Grammatical Features part-of-speech tags, noun, verbs, adjectives, and pronouns Lexical Features bad sexual words, cue words, multilingual hate lexicon, linguistic words, specific categories, denial term, support words, negation words, swear words, surprise and doubt words Linguisticinformed Features tf-idf, n-gram, named entity recognition, text language, bag-of-characters, bag of words (BoW) Stylistic Features [43] question marks, exclamation marks, punctuation marks, length of a sentence, uppercase ratio, consecutive characters and letters, presence of URLs, number of stop words, number of upper case letters, number of lower case letters, number of numeric values, word count, character count, sentence count, average sentence length, ease of comprehension, lexical diversity Syntactic Features ratio of negation, bag of relations
sent3: (all tokens, list of words, verbs) Conversation based Features text similarity to source tweet, text similarity to replied tweet, tweet depth Twitter Metadata [28,204] the number of characters in a tweet, the number of retweets, favorites, presence of hashtags, URLs, mentions, existence of photos, creating time gaps for posts, Twitter verification.etc.
sent4: Reddit Metadata [139] karma, gold status, Reddit employment status (if any), verified e-mail, reply count, upvotes, and whether the user is the submission submitter.
sent5: Reddit commenting syntax: sarcasm ('/s'), edited ('edit:'), and quote count ('>') Others Topics, term features, textual novelty other efficient sentiment analysis tools such as Emojis Dictionary8 , Emoticons list9 , Affect-Br [242], SemEval10 , MPQA 11 , ENGAR [243], Hespress Facebook12 , Offense lexicon 13 , Sarcasm lexicon [244], Named entities lexicon (Religion lexicon, Nationality lexicon, Named entities) [158], Baidu sentiment API 14 , NLTK sentiment module 15 , SEO Scout's analysis tool 16 , IBM Watson's Natural Language Understanding (NLU) 17 , MeaningCloud18 , ParallelDots 19 , Empath [245], EffectWordNet [246], Hu&Liu opinion lexicon 20 , SSWE [247], NRC-Canada [248], Stanford sentiment Tree [249], Dictionary of Affect in Language (DAL) [250], Affective Norms for English Words (ANEW) [241], Meta-Mind sentimentclassifier API 21 ."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s29,C.1. Misinformation Detection Evaluation,"A variety of techniques has been used to evaluate the output of misinformation detection methods, including accuracy, recall, precision, and F1-score, Macro F1, class-wise F1-score, AUC [172,176,177], and RMSE [29].These are calculated on the basis of a number of basic concepts, which are defined as follows: TP (True Positive) refers to the number of samples that the model correctly predicts as positive; TN (True Negative) refers to the number of samples that the model correctly predicts as negative; FP (False Positive) refers to the number of samples that the model incorrectly predicts as positive; FN (False Negative) refers to the number of samples that the model incorrectly predicts as negative.

The accuracy indicates the overall classification correctness of a model:   [234] Arabic An open-source package consisting of a set of Python APIs for NLP with accompanying command-line tools that thin-wrap these APIs sentiment Affective Lexicon Ontology (ALO) [193] Chinese Lexicon in which each entry is with an emotion and sentiment polarity sentiment, emotion TextBlob a English A Python sentiment analysis library that uses the Natural Language ToolKit (NLTK) sentiment scores with subject and polarity LIWC [235] Multilingual Text analysis software to conduct various calculations related to emotions, social dynamics, and cognitive processes by counting relevant words.Root Mean Squared Error (RMSE) represents the expected value of the squared error.","[['b171', 'b175', 'b28', 'b176'], ['b233', 'b234', 'b192']]","[['b171', 'b175', 'b28', 'b176'], ['b233', 'b234', 'b192']]",7,"sent1: A variety of techniques has been used to evaluate the output of misinformation detection methods, including accuracy, recall, precision, and F1-score, Macro F1, class-wise F1-score, AUC [172,176,177], and RMSE [29].These are calculated on the basis of a number of basic concepts, which are defined as follows: TP (True Positive) refers to the number of samples that the model correctly predicts as positive; TN (True Negative) refers to the number of samples that the model correctly predicts as negative; FP (False Positive) refers to the number of samples that the model incorrectly predicts as positive; FN (False Negative) refers to the number of samples that the model incorrectly predicts as negative.
sent2: The accuracy indicates the overall classification correctness of a model:   [234]
sent3: Arabic An open-source package consisting of a set of Python APIs for NLP with accompanying command-line tools that thin-wrap these APIs sentiment Affective Lexicon Ontology
sent4: (ALO) [193] Chinese Lexicon in which each entry is with an emotion and sentiment polarity sentiment, emotion TextBlob a English A Python sentiment analysis library that uses the Natural Language ToolKit (NLTK) sentiment scores with subject and polarity LIWC [235] Multilingual Text analysis software to conduct various calculations related to emotions, social dynamics, and cognitive processes by counting relevant words.
sent5: Root Mean Squared Error (RMSE) represents the expected value of the squared error."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s31,C.2. Stance Detection Evaluation Measurements,"Accuracy, recall, precision, and F1-score, Macro F1, class-wise F1-score, FNC1-Score [202,203], weighted accuracy [207] are used in stance detection.

The FNC-1 weighted accuracy score is used as the final evaluation metric for the FNC-1 dataset.  − 1  = 0.25 *    + 0.75 *  ,, (6) Weighted Accuracy is a performance metric that takes into account the weight of each class in an imbalanced dataset.It calculates the overall performance of the model by taking a weighted average of the accuracy for each category.","[['b206', 'b201', 'b202'], ['b5']]","[['b206', 'b201', 'b202'], ['b5']]",4,"sent1: Accuracy, recall, precision, and F1-score, Macro F1, class-wise F1-score, FNC1-Score [202,203], weighted accuracy [207] are used in stance detection.
sent2: The FNC-1 weighted accuracy score is used as the final evaluation metric for the FNC-1 dataset.
sent3: − 1  = 0.25 *    + 0.75 *  ,, (6) Weighted Accuracy is a performance metric that takes into account the weight of each class in an imbalanced dataset.
sent4: It calculates the overall performance of the model by taking a weighted average of the accuracy for each category."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s4,Sentiments and Emotions,"Sentiments and emotions are important and fundamental aspects of our lives.What we do and say reflects our emotions in some way.Emotion detection (ED) and sentiment analysis (SA) are two types of NLP techniques for analyzing human expressions that can help us to understand people's feelings towards specific topics [67].SA [68] aims to capture the overall emotional tone conveyed by a data source (usually positive, negative, or neutral), along with the strength of this tone [69].ED is the process of classifying data at a finer-grained level, according to the emotions that it conveys.

Compared to sentiment, the term emotion refers to more specific and stronger feelings [70].For example, positive sentiment encompasses a range of different emotions, such as happiness and joy, while negative sentiment includes the emotions of sadness and anger, among others.

A number of theoretical emotion classification models has been proposed, which can be divided into two categories, i.e., categorical and dimensional [71].Categorical models define a single discrete set of emotional states; examples include Shaver [72] (sadness, love, joy, anger, surprise and fear), Ekman [73] (joy, anger, fear, disgust, sadness and surprise), and Plutchik [74] (anticipation, surprise, anger, fear, trust, disgust, joy and sadness).In contrast, dimensional models posit that emotions can be decomposed into a number of distinct dimensions.One of the best-known examples is Plutchick's wheel of emotions [74,75], in which emotions are defined in a two-dimensional space of valence and arousal.The wheel is divided into 24 primary, secondary, and tertiary dyads based on eight basic emotions.Other popular dimensional emotion models include the PAD model [76], which is based on three dimensions, i.e., Pleasure (the pleasantness of the emotion), Arousal (the level of physiological activation or intensity of the emotion), and Domination (the degree of control or dominance experienced in the emotion); and the VAD model [77], in which Arousal and Dominance are supplemented by Valence (the positivity or negativity of the emotion).

A range of automated methods has been developed to detect both sentiment and emotions in text, which may be broadly categorized into dictionary-based, conventional machine learning [78], and DL [79] methods.The dictionarybased approach involves constructing an inventory of words that denote specific sentiments and/or emotions, and matching them against words appearing in the text to be processed to obtain information about the sentiments and emotions conveyed.Meanwhile, methods based on conventional machine learning and DL apply learning algorithms to datasets annotated with sentiment or emotion labels to teach them how to detect the different ways in which these types of emotions may be conveyed in text.Recently, there has also been a growing interest in exploring how LLMs can be exploited to enhance the accuracy of sentiment analysis and emotion detection [80,81,82].","[['b67', 'b68', 'b66'], ['b69'], ['b70', 'b71', 'b74', 'b76', 'b75', 'b73', 'b72'], ['b79', 'b81', 'b78', 'b80', 'b77']]","[['b67', 'b68', 'b66'], ['b69'], ['b70', 'b71', 'b74', 'b76', 'b75', 'b73', 'b72'], ['b79', 'b81', 'b78', 'b80', 'b77']]",16,"sent1: Sentiments and emotions are important and fundamental aspects of our lives.
sent2: What we do and say reflects our emotions in some way.
sent3: Emotion detection (ED) and sentiment analysis (SA) are two types of NLP techniques for analyzing human expressions that can help us to understand people's feelings towards specific topics [67].SA [68] aims to capture the overall emotional tone conveyed by a data source (usually positive, negative, or neutral), along with the strength of this tone [69].ED is the process of classifying data at a finer-grained level, according to the emotions that it conveys.
sent4: Compared to sentiment, the term emotion refers to more specific and stronger feelings [70].For example, positive sentiment encompasses a range of different emotions, such as happiness and joy, while negative sentiment includes the emotions of sadness and anger, among others.
sent5: A number of theoretical emotion classification models has been proposed, which can be divided into two categories, i.e., categorical and dimensional [71].Categorical models define a single discrete set of emotional states; examples include Shaver [72]
sent6: (sadness, love, joy, anger, surprise and fear), Ekman [73] (joy, anger, fear, disgust, sadness and surprise), and Plutchik [74] (anticipation, surprise, anger, fear, trust, disgust, joy and sadness).In contrast, dimensional models posit that emotions can be decomposed into a number of distinct dimensions.
sent7: One of the best-known examples is Plutchick's wheel of emotions [74,75], in which emotions are defined in a two-dimensional space of valence and arousal.
sent8: The wheel is divided into 24 primary, secondary, and tertiary dyads based on eight basic emotions.
sent9: Other popular dimensional emotion models include the PAD model [76], which is based on three dimensions, i.e., Pleasure (the pleasantness of the emotion), Arousal (the level of physiological activation or intensity of the emotion), and Domination (the degree of control or dominance experienced in the emotion); and the VAD model [77], in which Arousal and Dominance are supplemented by Valence (the positivity or negativity of the emotion).
sent10: A range of automated methods has been developed to detect both sentiment and emotions in text, which may be broadly categorized into dictionary-based, conventional machine learning [78], and DL [79] methods.
sent11: The dictionarybased approach involves constructing an inventory of words that denote specific sentiments and/or emotions, and matching them against words appearing in the text to be processed to obtain information about the sentiments and emotions conveyed.
sent12: Meanwhile, methods based on conventional machine learning and DL apply learning algorithms to datasets annotated with sentiment or emotion labels to teach them how to detect the different ways in which these types of emotions may be conveyed in text.
sent13: Recently, there has also been a growing interest in exploring how LLMs can be exploited to enhance the accuracy of sentiment analysis and emotion detection [80,81,82]."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s3,Fake News and Rumors Detection,"The convenience of accessing social media platforms on various electronic devices means that people can easily post or access large amounts of information on the Internet.This can lead to the rampant spread of misinformation.Certain individuals intentionally spread rumors to gain attention, mislead readers, or make a profit, even though such rumors can pose significant harm to society [64].Therefore, there is an urgent need to detect misinformation in an efficient and effective manner.A large body of research has aimed to respond to this need, which has been summarized in various reviews, which cover both methods for detecting rumors and fake news [48,49,50,51,53] and potential applications of these methods, including source detection [57,58], bot detection [57,65,66] and stance detection [55,56].

Misinformation detection approaches consist of three main components, i.e., the datasets used to support their development, the methods used to perform detection, and the features used within these methods.The majority of the datasets are obtained from social media platforms such as Twitter, Facebook and Sina Weibo, or from fact-checking websites, such as Snopes2 , Factcheck3 , PolitiFact 4 .Detection methods may be divided into those based on conventional machine learning [51] or DL [50,53].Figure 4 presents a range of features that have been employed for misinformation detection.Among these features, contentbased features constitute the most diverse class;  types of features that fall under each of the content-based sub-classes shown in Figure 4. Within the Affective group of features, dual emotion features aim to account for the importance of considering different emotional perspectives when identifying misinformation, i.e., both publisher emotion, which refers to the emotions conveyed in an original post that starts a thread on social media, and social emotion, which refers to the emotions expressed in follow-up posts that respond to and/or comment on the original post.","[['b64', 'b52', 'b50', 'b49', 'b56', 'b57', 'b47', 'b55', 'b63', 'b54', 'b65', 'b48'], ['b49', 'b52', 'b50']]","[['b64', 'b52', 'b50', 'b49', 'b56', 'b57', 'b47', 'b55', 'b63', 'b54', 'b65', 'b48'], ['b49', 'b52', 'b50']]",15,"sent1: The convenience of accessing social media platforms on various electronic devices means that people can easily post or access large amounts of information on the Internet.
sent2: This can lead to the rampant spread of misinformation.
sent3: Certain individuals intentionally spread rumors to gain attention, mislead readers, or make a profit, even though such rumors can pose significant harm to society [64].Therefore, there is an urgent need to detect misinformation in an efficient and effective manner.
sent4: A large body of research has aimed to respond to this need, which has been summarized in various reviews, which cover both methods for detecting rumors and fake news [48,49,50,51,53] and potential applications of these methods, including source detection [57,58], bot detection [57,65,66] and stance detection [55,56].
sent5: Misinformation detection approaches consist of three main components, i.e., the datasets used to support their development, the methods used to perform detection, and the features used within these methods.
sent6: The majority of the datasets are obtained from social media platforms such as Twitter, Facebook and Sina Weibo, or from fact-checking websites, such as Snopes2 , Factcheck3 , PolitiFact 4 .Detection methods may be divided into those based on conventional machine learning [51] or DL [50,53].Figure 4 presents a range of features that have been employed for misinformation detection.
sent7: Among these features, contentbased features constitute the most diverse class;  types of features that fall under each of the content-based sub-classes shown in Figure 4.
sent8: Within the Affective group of features, dual emotion features aim to account for the importance of considering different emotional perspectives when identifying misinformation, i.e., both publisher emotion, which refers to the emotions conveyed in an original post that starts a thread on social media, and social emotion, which refers to the emotions expressed in follow-up posts that respond to and/or comment on the original post."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s14,Methods Based on Temporal Information,"Various temporal features have been explored to enhance the performance of misinformation detection, based on the time-sensitive patterns that are frequently observed in social media, e.g., rumors initially spread quickly but gradually disappear, while reader emotions tend to change over time.The TDEI model [22] (Figure 9 (a)) integrates emotion features with information concerning time-sensitive dynamic changes in the topological propagation structure of tweets, which is considered to be a better predictor of rumors than the final, static propagation structure.The graph representing the propagation structure of a publisher post and its associated social comments is firstly divided into a sequence of temporal snapshot graphs.Stacked GCNs and a readout function are used to learn structural features of the snapshots.A GRU with self-attention is then applied to learn the diffusion process of structures.Meanwhile, emotion vectors are extracted from each post using a pre-trained, finetuned BERT model.A self-attention mechanism is then used to merge the emotion vectors for each post corresponding to a rumor event into a single vector, whose dimensionality is adjusted using a fully connected layer.The temporal dynamic structure and emotion vector are then concatenated and fed into multi-layer perception with softmax function to make predictions.

The SD-TsDTS-CGRU fusion rumor detection method [153,154] (Figure 9 (b)) focuses on detecting rumors at the event level, i.e., by considering all information expressed in the complete set of sequential posts related to the same topic or event.Posts are firstly automatically partitioned into sets covering distinct events by dividing them into intervals using a two-step dynamic time series division algorithm, based on fuzzy clustering and information granules [194].The latter step helps to ensure that each batch of posts covers information at an appropriate level of granularity and has a consistent semantic interpretation.The calculation of information granularity takes into account the number of sentiment words belonging to each fine-grained sentiment category in each interval, obtained using a novel sentiment dictionary containing sentiment words and emoticons.Following the division, word embeddings and sentiment information extracted from the posts in each event-specific set are fed to two different GRUs, whose outputs are combined and fed into a dense layer with Softmax function to predict whether or not each set of event-related posts constitutes a rumor.

Temporal sentiment features of rumors are employed in [169] (Figure 9 (c)) to account for changes in sentiment over the lifetime of an original publisher post and its associated social posts in both Chinese and English social media datasets.The Baidu sentiment API 6 and NLTK sentiment module 7 are used to obtain sentiment scores for Chinese and English posts, respectively.Temporal features are characterized using a one-hot vector, whose length is modified by normalizing the number of posts in the reply series.Temporal sentiment features are obtained by multiplying the modified one-hot vector with the sentiment score.Textual features of posts are obtained using pre-trained word embeddings and a mean pooling layer, which are combined with the sentiment vector to derive a microblog representation.An RvNN and max pooling layer are then used to obtain a comprehensive representation of an event as it propagates through the path of social replies, which is passed to a MLP with ReLU to determine whether or not the publisher post is a rumor.","[['b21'], ['b193', 'b153', 'b152'], ['b168']]","[['b21'], ['b193', 'b153', 'b152'], ['b168']]",5,"sent1: Various temporal features have been explored to enhance the performance of misinformation detection, based on the time-sensitive patterns that are frequently observed in social media, e.g., rumors initially spread quickly but gradually disappear, while reader emotions tend to change over time.
sent2: The TDEI model [22] (Figure 9 (a)) integrates emotion features with information concerning time-sensitive dynamic changes in the topological propagation structure of tweets, which is considered to be a better predictor of rumors than the final, static propagation structure.
sent3: The graph representing the propagation structure of a publisher post and its associated social comments is firstly divided into a sequence of temporal snapshot graphs.
sent4: Stacked GCNs and a readout function are used to learn structural features of the snapshots.
sent5: A GRU with self-attention is then applied to learn the diffusion process of structures.
sent6: Meanwhile, emotion vectors are extracted from each post using a pre-trained, finetuned BERT model.
sent7: A self-attention mechanism is then used to merge the emotion vectors for each post corresponding to a rumor event into a single vector, whose dimensionality is adjusted using a fully connected layer.
sent8: The temporal dynamic structure and emotion vector are then concatenated and fed into multi-layer perception with softmax function to make predictions.
sent9: The SD-TsDTS-CGRU fusion rumor detection method [153,154] (Figure 9 (b)) focuses on detecting rumors at the event level, i.e., by considering all information expressed in the complete set of sequential posts related to the same topic or event.
sent10: Posts are firstly automatically partitioned into sets covering distinct events by dividing them into intervals using a two-step dynamic time series division algorithm, based on fuzzy clustering and information granules [194].The latter step helps to ensure that each batch of posts covers information at an appropriate level of granularity and has a consistent semantic interpretation.
sent11: The calculation of information granularity takes into account the number of sentiment words belonging to each fine-grained sentiment category in each interval, obtained using a novel sentiment dictionary containing sentiment words and emoticons.
sent12: Following the division, word embeddings and sentiment information extracted from the posts in each event-specific set are fed to two different GRUs, whose outputs are combined and fed into a dense layer with Softmax function to predict whether or not each set of event-related posts constitutes a rumor.
sent13: Temporal sentiment features of rumors are employed in [169] (Figure 9 (c)) to account for changes in sentiment over the lifetime of an original publisher post and its associated social posts in both Chinese and English social media datasets.
sent14: The Baidu sentiment API 6 and NLTK sentiment module 7 are used to obtain sentiment scores for Chinese and English posts, respectively.
sent15: Temporal features are characterized using a one-hot vector, whose length is modified by normalizing the number of posts in the reply series.
sent16: Temporal sentiment features are obtained by multiplying the modified one-hot vector with the sentiment score.
sent17: Textual features of posts are obtained using pre-trained word embeddings and a mean pooling layer, which are combined with the sentiment vector to derive a microblog representation.
sent18: An RvNN and max pooling layer are then used to obtain a comprehensive representation of an event as it propagates through the path of social replies, which is passed to a MLP with ReLU to determine whether or not the publisher post is a rumor."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s5,Relationships between emotions and misinformation,"Although emotions are regarded as a dominant driver of human behavior, the exploration of their role in the online diffusion of misinformation has only recently begun.Misinformation can evoke emotional responses in readers, which in turn can lead to specific behaviors, such as belief in the information, resharing or liking it, etc [13].

Table 1 lists a range of recent studies that has investigated the relationships between emotions and misinformation, e.g., how the expression of particular emotions can indicate that a data source is likely to contain misinformation and/or predict the likely response of readers.For each study, we list the dataset used, the ED and relationship analysis methods (RAM) methods employed, and details of the most important relationships identified.The most commonly explored topic is COVID-19, according to the explosion of rumors and fake news generated by the pandemic.To perform ED, the majority of researchers applied dictionary-based methods (Table 7 for details) or traditional machine learning methods, while Wu et al. [83] manually annotated discrete emotions based on the Pleasure-Arousal-Dominance (PAD) emotional state model.In [13,12,84,85,86,87], questionnaires were designed to ask participants to directly report their emotions.Among these approaches, Zhang et al. [12] and Martel et al. [84] use the Positive-Negative Emotional Scale (PANAS) to further quantify the emotional state of participants.The analysis of Li et al. [88] was based on the results obtained from their novel Multi-EmoBERT multilabel emotion recognition tool.Wan et al. [89] used a mixture of existing NLP tools and lexicons for ED, enhanced using rules and automated weighting.They extracted Emotion Triple Elements to study potentially different responses to emotional triggers.For relationship analysis, a range of commonly used statistical analysis methods has been applied, including Logistic Regression (LR) [83,90], Linear Regression [91], and T-Test [92,93].

Various indicators have been used to judge the impact of emotions on the spread of rumors, the degree of outbreak, etc.For example, the questionnaires of [12,85,87] directly asked participants what actions they would take when faced with certain types of news, such as sharing intention or ""likes"".Other studies used cascade size, cascade lifetime, and structural virality [15,16,94] to analyze the patterns of misinformation spread.Cascade size corresponds to the number of forwardings generated by a cascade; cascade lifetime is the length of time that a rumor cascade remains active, i.e., the time elapsed between the root broadcast and the final forwarding; structural virality [95] provides an aggregated metric combining the depth and breadth of a cascade.In addition, many studies have analyzed relationships by investigating the number of rumors that occur over time, or by comparing the number of rumors that convey different emotions.

The analyses detailed in Table 1 reveal a number of important relationships between emotions and misinformation, which can sometimes depend on the types of topics being discussed.Misinformation is generally associated with a significant level of high-arousal emotions such as anger, sadness, anxiety, surprise, and fear.Rumors conveying anger, sadness, anxiety, and fear are likely to generate a large number of shares, and to be long-lived and viral [15,17,83,90], while emotional appeals (like anger and disgust) can increase users' engagement with fake posts [96].Fake news expresses higher overall emotion, negative sentiment, and lower positive sentiment than genuine news [92,93].In general, it may be concluded that sentiment, emotions, and misinformation are inextricably intertwined, thus confirming that sentiment and emotion both have important parts to play in the automated detection of fake news and rumors.","[['b12'], ['b88', 'b85', 'b82', 'b84', 'b92', 'b91', 'b89', 'b83', 'b90', 'b11', 'b87', 'b12', 'b86'], ['b15', 'b14', 'b84', 'b93', 'b11', 'b86', 'b94'], ['b14', 'b16', 'b82', 'b95', 'b91', 'b89', 'b92']]","[['b12'], ['b88', 'b85', 'b82', 'b84', 'b92', 'b91', 'b89', 'b83', 'b90', 'b11', 'b87', 'b12', 'b86'], ['b15', 'b14', 'b84', 'b93', 'b11', 'b86', 'b94'], ['b14', 'b16', 'b82', 'b95', 'b91', 'b89', 'b92']]",28,"sent1: Although emotions are regarded as a dominant driver of human behavior, the exploration of their role in the online diffusion of misinformation has only recently begun.
sent2: Misinformation can evoke emotional responses in readers, which in turn can lead to specific behaviors, such as belief in the information, resharing or liking it, etc [13].
sent3: Table 1 lists a range of recent studies that has investigated the relationships between emotions and misinformation, e.g., how the expression of particular emotions can indicate that a data source is likely to contain misinformation and/or predict the likely response of readers.
sent4: For each study, we list the dataset used, the ED and relationship analysis methods (RAM) methods employed, and details of the most important relationships identified.
sent5: The most commonly explored topic is COVID-19, according to the explosion of rumors and fake news generated by the pandemic.
sent6: To perform ED, the majority of researchers applied dictionary-based methods (Table 7 for details) or traditional machine learning methods, while Wu et al. [83] manually annotated discrete emotions based on the Pleasure-Arousal-Dominance (PAD) emotional state model.
sent7: In [13,12,84,85,86,87], questionnaires were designed to ask participants to directly report their emotions.
sent8: Among these approaches, Zhang et al. [12] and Martel et al. [84] use the Positive-Negative Emotional Scale (PANAS) to further quantify the emotional state of participants.
sent9: The analysis of Li et al. [88] was based on the results obtained from their novel Multi-EmoBERT multilabel emotion recognition tool.
sent10: Wan et al. [89] used a mixture of existing NLP tools and lexicons for ED, enhanced using rules and automated weighting.
sent11: They extracted Emotion Triple Elements to study potentially different responses to emotional triggers.
sent12: For relationship analysis, a range of commonly used statistical analysis methods has been applied, including Logistic Regression (LR) [83,90], Linear Regression [91], and T-Test [92,93].
sent13: Various indicators have been used to judge the impact of emotions on the spread of rumors, the degree of outbreak, etc.
sent14: For example, the questionnaires of [12,85,87] directly asked participants what actions they would take when faced with certain types of news, such as sharing intention or ""likes"".
sent15: Other studies used cascade size, cascade lifetime, and structural virality [15,16,94] to analyze the patterns of misinformation spread.
sent16: Cascade size corresponds to the number of forwardings generated by a cascade; cascade lifetime is the length of time that a rumor cascade remains active, i.e., the time elapsed between the root broadcast and the final forwarding; structural virality [95] provides an aggregated metric combining the depth and breadth of a cascade.
sent17: In addition, many studies have analyzed relationships by investigating the number of rumors that occur over time, or by comparing the number of rumors that convey different emotions.
sent18: The analyses detailed in Table 1 reveal a number of important relationships between emotions and misinformation, which can sometimes depend on the types of topics being discussed.
sent19: Misinformation is generally associated with a significant level of high-arousal emotions such as anger, sadness, anxiety, surprise, and fear.
sent20: Rumors conveying anger, sadness, anxiety, and fear are likely to generate a large number of shares, and to be long-lived and viral [15,17,83,90], while emotional appeals (like anger and disgust) can increase users' engagement with fake posts [96].Fake news expresses higher overall emotion, negative sentiment, and lower positive sentiment than genuine news [92,93].In general, it may be concluded that sentiment, emotions, and misinformation are inextricably intertwined, thus confirming that sentiment and emotion both have important parts to play in the automated detection of fake news and rumors."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s7,Datasets,"Table lists a range of publicly available datasets aimed at facilitating the development and/or evaluation of misinformation detection methods.The majority of these datasets consist of data obtained from popular social media platforms and fact-checking websites, such as Twitter, Weibo, Reddit, politifact.com,gossipcop.com,etc.For each dataset, we provide its commonly used name and reference, its source, a description of its size and main characteristics, its level of availability, and notes.The latter is used to indicate datasets that cover languages other than English, those that are multimodal, those specifically concerning COVID-19, and those annotated with stance information.Datasets without notes consist of textual English data that is labeled according to whether or not it represents misinformation.

As may be observed in Table 2, the majority of datasets is publicly available, which is highly advantageous to promote research in the field of misinformation.Due to the prevalence of rumors relating to the COVID-19 pandemic on social media, there has been a trend towards collecting misinformation datasets relating to this topic, as a means to explore rumor detection in the field of health disease transmission [102,103,104,105].Another important feature of several of the datasets listed (including PHEME [106], the Twitter series [107], and the Weibo series [29,108,109]) is that they include comments/replies relating to original news stories or tweets.Such datasets allow the exploration of methods that take into account the dual publisher and social emotions, and the possible interactions between them, to improve the accuracy of misinformation detection.We can furthermore observe that several datasets are multimodal, i.e., they consist of both text and images.These include FakeNewsNet [110], Fakeddit [111] and MediaEval2016 [112].The inclusion of images in these datasets provides scope to explore methods that take advantage of visual clues to complement textbased information in identifying misinformation.Although the majority of datasets only contain English text, there is a growing number of corpora that cover other single and sometimes multiple languages, including Chinese, Portuguese, Spanish and Danish, thus providing opportunities to develop methods that are multilingual and/or which target lesser resourced languages.While most datasets are annotated according to whether or not their constituent text constitutes fake news or rumor, there is also a number of corpora annotated with stance-related labels, which can facilitate investigations into how stance information can contribute towards the detection of misinformation.","[[], ['b111', 'b104', 'b105', 'b101', 'b28', 'b102', 'b107', 'b110', 'b109', 'b106', 'b103', 'b108']]","[[], ['b111', 'b104', 'b105', 'b101', 'b28', 'b102', 'b107', 'b110', 'b109', 'b106', 'b103', 'b108']]",12,"sent1: Table lists a range of publicly available datasets aimed at facilitating the development and/or evaluation of misinformation detection methods.
sent2: The majority of these datasets consist of data obtained from popular social media platforms and fact-checking websites, such as Twitter, Weibo, Reddit, politifact.com,gossipcop.com,etc.
sent3: For each dataset, we provide its commonly used name and reference, its source, a description of its size and main characteristics, its level of availability, and notes.
sent4: The latter is used to indicate datasets that cover languages other than English, those that are multimodal, those specifically concerning COVID-19, and those annotated with stance information.
sent5: Datasets without notes consist of textual English data that is labeled according to whether or not it represents misinformation.
sent6: As may be observed in Table 2, the majority of datasets is publicly available, which is highly advantageous to promote research in the field of misinformation.
sent7: Due to the prevalence of rumors relating to the COVID-19 pandemic on social media, there has been a trend towards collecting misinformation datasets relating to this topic, as a means to explore rumor detection in the field of health disease transmission [102,103,104,105].Another important feature of several of the datasets listed (including PHEME [106], the Twitter series [107], and the Weibo series [29,108,109]) is that they include comments/replies relating to original news stories or tweets.
sent8: Such datasets allow the exploration of methods that take into account the dual publisher and social emotions, and the possible interactions between them, to improve the accuracy of misinformation detection.
sent9: We can furthermore observe that several datasets are multimodal, i.e., they consist of both text and images.
sent10: These include FakeNewsNet [110], Fakeddit [111] and MediaEval2016 [112].The inclusion of images in these datasets provides scope to explore methods that take advantage of visual clues to complement textbased information in identifying misinformation.
sent11: Although the majority of datasets only contain English text, there is a growing number of corpora that cover other single and sometimes multiple languages, including Chinese, Portuguese, Spanish and Danish, thus providing opportunities to develop methods that are multilingual and/or which target lesser resourced languages.
sent12: While most datasets are annotated according to whether or not their constituent text constitutes fake news or rumor, there is also a number of corpora annotated with stance-related labels, which can facilitate investigations into how stance information can contribute towards the detection of misinformation."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s8,Conventional Machine Learning Methods,"Machine learning is a branch of AI that uses algorithms and statistical models to teach computers how to make predictions and decisions automatically.As shown in Table 3, a variety of machine learning algorithms has been used to develop misinformation classifiers.These include both supervised methods, such as passive-aggressive [171], Naive Bayes (NB) [151], k-Nearest Neighbour (KNN) [135,161], Support Vector Machine (SVM) [159], Random Forests (RF) [24], Decision Tree (DT) [157], AdaBoost (AB) [127], LOGIT, Grad Boosting, XG-Boost, Gradient Boost (GB) [152], and unsupervised methods like K-Means and DB-SCAN [128].","[['b158', 'b126', 'b156', 'b151', 'b170', 'b23', 'b134', 'b150', 'b160', 'b127']]","[['b158', 'b126', 'b156', 'b151', 'b170', 'b23', 'b134', 'b150', 'b160', 'b127']]",10,"sent1: Machine learning is a branch of AI that uses algorithms and statistical models to teach computers how to make predictions and decisions automatically.
sent2: As shown in Table 3, a variety of machine learning algorithms has been used to develop misinformation classifiers.
sent3: These include both supervised methods, such as passive-aggressive [171], Naive Bayes (NB) [151], k-Nearest Neighbour (KNN) [135,161], Support Vector Machine (SVM) [159], Random Forests (RF) [24], Decision Tree (DT) [157], AdaBoost (AB) [127], LOGIT, Grad Boosting, XG-Boost, Gradient Boost (GB) [152], and unsupervised methods like K-Means and DB-SCAN [128]."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s21,Annotation (Emotion),"The development of emotion-based misinformation detection methods with optimal performance requires that supporting misinformation datasets are annotated with reliable emotion and/or sentiment labels, since inaccurate labels are likely to impact negatively on the overall performance of the methods.While this is most often carried out using dictionary lookup, some studies have employed transfer learning methods, by applying models trained on other sentiment analysis or emotion-labeled datasets to automatically annotate the emotions expressed in misinformation datasets.Examples include [30,181,182], which use a previously developed Unison model, and [162], which utilizes a finetuned RoBERTa model.However, the emotion labels obtained in these ways are not sufficiently accurate.Compared to time-consuming manual annotation, a more promising approach is to use LLMs to annotate emotion and/or sentiment [80,81,82], given their advanced capabilities and transferability.Both [81] and [82] have demonstrated that LLMs can compete with or exceed the state-of-the-art (SOTA) in recognising emotions in dialogue.In particular [81] showed that the LLaMA-7B [215] model can achieve performance levels close to those of SOTA supervised methods, but using only half as much training data for fine-tuning.Zhang et al. [80] developed an instruction-tuned LLM for financial sentiment analysis which, augmented with additional context from external sources, is able to outperform LLM baselines such as ChatGPT and LLaMA by margins of between 15% and 48%.The above studies all highlight the tremendous potential of LLMs in the field of sentiment analysis.","[['b29', 'b161', 'b214', 'b79', 'b81', 'b181', 'b80', 'b180']]","[['b29', 'b161', 'b214', 'b79', 'b81', 'b181', 'b80', 'b180']]",8,"sent1: The development of emotion-based misinformation detection methods with optimal performance requires that supporting misinformation datasets are annotated with reliable emotion and/or sentiment labels, since inaccurate labels are likely to impact negatively on the overall performance of the methods.
sent2: While this is most often carried out using dictionary lookup, some studies have employed transfer learning methods, by applying models trained on other sentiment analysis or emotion-labeled datasets to automatically annotate the emotions expressed in misinformation datasets.
sent3: Examples include [30,181,182], which use a previously developed Unison model, and [162], which utilizes a finetuned RoBERTa model.
sent4: However, the emotion labels obtained in these ways are not sufficiently accurate.
sent5: Compared to time-consuming manual annotation, a more promising approach is to use LLMs to annotate emotion and/or sentiment [80,81,82], given their advanced capabilities and transferability.
sent6: Both [81] and [82] have demonstrated that LLMs can compete with or exceed the state-of-the-art (SOTA) in recognising emotions in dialogue.
sent7: In particular [81] showed that the LLaMA-7B [215] model can achieve performance levels close to those of SOTA supervised methods, but using only half as much training data for fine-tuning.
sent8: Zhang et al. [80] developed an instruction-tuned LLM for financial sentiment analysis which, augmented with additional context from external sources, is able to outperform LLM baselines such as ChatGPT and LLaMA by margins of between 15% and 48%.The above studies all highlight the tremendous potential of LLMs in the field of sentiment analysis."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s17,Emotion-based stance detection in misinformation,"In addition to emotions and sentiment, the stance of readers is also an important factor in affecting rumor diffusion.If somebody supports a piece of fake news, he/she is more likely to reshare it.Emotions can impact upon a person's thinking, judgment, and decision-making, which in turn can influence their stance toward a particular topic.This section introduces methods that use emotion as a feature for stance detection in misinformation.

Most work in this area has been driven by shared tasks, in which a number of teams compete with each other to produce the best results for a given task and dataset.Examples of relevant tasks include SemEval-2017-Task8 [35], SemEval-2019-Task7 [35], and the FNC dataset [142,143].Lillie et al. [139] constructed a Danish stance-annotated dataset (DAST), consisting of Reddit posts.A number of other publicly available stance-annotated datasets has also been used in various studies [47,149,207].Further details about these datasets are provided in Table 2. Most stance detection methods detect emotion features using simple dictionaries or tools, and use conventional machine learning approaches, based on sentiment features and a variety of other features.Further details are provided in Table 4.","[[], ['b206', 'b141', 'b46', None, 'b148', 'b34', 'b138']]","[[], ['b206', 'b141', 'b46', None, 'b148', 'b34', 'b138']]",7,"sent1: In addition to emotions and sentiment, the stance of readers is also an important factor in affecting rumor diffusion.
sent2: If somebody supports a piece of fake news, he/she is more likely to reshare it.
sent3: Emotions can impact upon a person's thinking, judgment, and decision-making, which in turn can influence their stance toward a particular topic.
sent4: This section introduces methods that use emotion as a feature for stance detection in misinformation.
sent5: Most work in this area has been driven by shared tasks, in which a number of teams compete with each other to produce the best results for a given task and dataset.
sent6: Examples of relevant tasks include SemEval-2017-Task8 [35], SemEval-2019-Task7 [35], and the FNC dataset [142,143].Lillie et al.
sent7: [139] constructed a Danish stance-annotated dataset (DAST), consisting of Reddit posts.
sent8: A number of other publicly available stance-annotated datasets has also been used in various studies [47,149,207].Further details about these datasets are provided in Table 2.
sent9: Most stance detection methods detect emotion features using simple dictionaries or tools, and use conventional machine learning approaches, based on sentiment features and a variety of other features.
sent10: Further details are provided in Table 4."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s16,Multimodal Methods,"On platforms like Twitter or Weibo, people often attach images to their textual posts to better express their opinions or emotions.Several studies have thus attempted to exploit information from these images to improve the detection of rumors, mainly based on two different frameworks, both of which involve combining features from text and images, but which differ in terms of whether emotion features are extracted from text (Figure 11 (a)) [150,173] or images (Figure 11 (b)) [131,132,167].The Title-Text Similarity and Emotion-Aware Fake News Detection method [173] applies BERT with a fully connected layer and ResNet-50 to obtain textual and visual features, respectively.The publisher emotion extractor from [29] (described above in Section 4.4.2) is used to obtain a range of emotion-based feature values from textual news content.The scaled dot-product attention mechanism is also used to capture the similarity between the title and textual features, based on the observation that authors of fake news may attempt to catch the reader's attention by using titles that are not relevant to the news content.All features are subsequently combined and fed into a FC layer with softmax to make predictions.

The SAME multi-modal embedding model [150] incorporates user sentiment for fake news detection.Firstly, VGGNet and CNN are used to represent images, while text is represented using Glove and MLP, and profiles (i.e., source, publisher and keywords) are represented using one-hot encoding.An adversarial learning mechanism is then applied to find semantic correlations between different modalities.A novel hybrid similarity loss method based on Graph Affinity Metric and Local Similarity Metric is used to incorporate the user's sentiments (i.e., positive, negative or neutral), which are obtained using VADER.Finally, a fully connected layer with softmax is applied as a classifier.

The multimodal framework in [131,132] makes use of text and images from source-target pairs, in which the target corresponds to information from fake news datasets, while the source corresponds to background information associated with a target data item, extracted from credible websites.BERT and ResNET18 [196] are firstly used to encode the text and images of source-target pairs, respectively.The textual and visual features are concatenated to obtain multimodal feature representations.These are encoded using VisualBERT [197], which is designed to capture the rich semantics found in images and their associated text.A novelty detection module then uses these multimodal representations to determine the credibility of the new news (target) with respect to prior verified news (source), using supervised contrastive learning (SCL) such that target representations attract source representations that provide support, and repel them otherwise.The second module pretrains a neural network to predict image emotion labels using two classes, i.e. joy/love/sadness vs. fear/surprise/anger.All features are then fused and passed to MLP with softmax to make predictions.

Uppada et al. [167] developed a framework for fake news detection that combines visual and textual features.The architecture, which consists of two fine-tuned Xception models, makes use of the Error Level Analysis (ELA) technique to help to identify digitally altered images.One fine-tuned Xception model is trained on an ELA image dataset to detect editing traces in digital images, while the other is trained on a visual sentiment analysis dataset to determine whether images convey positive or negative sentiments.BERT is applied to learn contextual knowledge from image captions.The output of the three branches is combined and passed to the fake image classifier.","[['b172', 'b131', 'b28', 'b166', 'b149', 'b130'], ['b149'], ['b195', 'b196', 'b130', 'b131'], ['b166']]","[['b172', 'b131', 'b28', 'b166', 'b149', 'b130'], ['b149'], ['b195', 'b196', 'b130', 'b131'], ['b166']]",12,"sent1: On platforms like Twitter or Weibo, people often attach images to their textual posts to better express their opinions or emotions.
sent2: Several studies have thus attempted to exploit information from these images to improve the detection of rumors, mainly based on two different frameworks, both of which involve combining features from text and images, but which differ in terms of whether emotion features are extracted from text (Figure 11 (a)) [150,173] or images (Figure 11 (b)) [131,132,167].The
sent3: Title-Text Similarity and Emotion-Aware Fake News Detection method [173] applies BERT with a fully connected layer and ResNet-50 to obtain textual and visual features, respectively.
sent4: The publisher emotion extractor from [29] (described above in Section 4.4.2) is used to obtain a range of emotion-based feature values from textual news content.
sent5: The scaled dot-product attention mechanism is also used to capture the similarity between the title and textual features, based on the observation that authors of fake news may attempt to catch the reader's attention by using titles that are not relevant to the news content.
sent6: All features are subsequently combined and fed into a FC layer with softmax to make predictions.
sent7: The SAME multi-modal embedding model [150] incorporates user sentiment for fake news detection.
sent8: Firstly, VGGNet and CNN are used to represent images, while text is represented using Glove and MLP, and profiles (i.e., source, publisher and keywords) are represented using one-hot encoding.
sent9: An adversarial learning mechanism is then applied to find semantic correlations between different modalities.
sent10: A novel hybrid similarity loss method based on Graph Affinity Metric and Local Similarity Metric is used to incorporate the user's sentiments (i.e., positive, negative or neutral), which are obtained using VADER.Finally, a fully connected layer with softmax is applied as a classifier.
sent11: The multimodal framework in [131,132] makes use of text and images from source-target pairs, in which the target corresponds to information from fake news datasets, while the source corresponds to background information associated with a target data item, extracted from credible websites.
sent12: BERT and ResNET18 [196] are firstly used to encode the text and images of source-target pairs, respectively.
sent13: The textual and visual features are concatenated to obtain multimodal feature representations.
sent14: These are encoded using VisualBERT [197], which is designed to capture the rich semantics found in images and their associated text.
sent15: A novelty detection module then uses these multimodal representations to determine the credibility of the new news (target) with respect to prior verified news (source), using supervised contrastive learning (SCL) such that target representations attract source representations that provide support, and repel them otherwise.
sent16: The second module pretrains a neural network to predict image emotion labels using two classes, i.e. joy/love/sadness vs. fear/surprise/anger.
sent17: All features are then fused and passed to MLP with softmax to make predictions.
sent18: Uppada et al. [167] developed a framework for fake news detection that combines visual and textual features.
sent19: The architecture, which consists of two fine-tuned Xception models, makes use of the Error Level Analysis (ELA) technique to help to identify digitally altered images.
sent20: One fine-tuned Xception model is trained on an ELA image dataset to detect editing traces in digital images, while the other is trained on a visual sentiment analysis dataset to determine whether images convey positive or negative sentiments.
sent21: BERT is applied to learn contextual knowledge from image captions.
sent22: The output of the three branches is combined and passed to the fake image classifier."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s15,Multitask Learning,"Multi-task learning optimizes several learning tasks simultaneously, exploiting shared information to improve the prediction performance of the model for each task.Auxiliary tasks can be added to the main task to boost the performance.Several studies have explored how emotion and sentiment detection can act as auxiliary tasks in a multi-task learning framework to enhance misinformation detection accuracy.

The method developed by Choudhry et al. [30,181,182] (Figure 10 (a)) aims to address the issue of cross-domain robustness in determining the veracity of news articles.Generalizability of the method across different domains is achieved using a domain-adaptive framework, whose aim is to facilitate the extraction of domain-invariant features by aligning the source and target domains in the feature space.The multi-task learning setup trains an emotion classifier as an auxiliary task in parallel to a fake news detector, to try to improve the alignment between the source and target domains, while adversarial training helps to make the model robust to outliers.The emotion classifier assigns emotion labels according to Ekman's or Plutchik's emotions, with the aid of the Unison model [195].An LSTM is used as the feature extractor, which is trained using the accumulated loss from the fake news classifier, emotion classifier and a domain classifier, the latter of which acts as a discriminator in learning domain-invariant features.

Based on the relatedness between the tasks of detecting fake news, novelty, emotion, and sentiment, Kumari et al. [160] (Figure 10 (b)) developed a multi-task learning framework in which the latter three of these are treated as auxiliary tasks.Using premise-hypothesis pairs as input, the model detects whether or not the hypothesis is fake with respect to the premise.Pre-trained and/or fine-tuned models are firstly used to determine whether the hypothesis is novel with respect to the premise, and whether or not the hypothesis and premise differ in terms of binary emotion values (i,e., sadness/joy/trust vs. anger/fear/disgust/surprise) and sentiment (positive or negative).Different Bi-LSTMs that use pretrained GloVe and BERT-based embeddings are employed to obtain two different input textual representations, which are concatenated and used as the input to the three auxiliary tasks and the main task of fake news detection.","[[], ['b29', 'b194', 'b180', 'b181'], ['b159']]","[[], ['b29', 'b194', 'b180', 'b181'], ['b159']]",5,"sent1: Multi-task learning optimizes several learning tasks simultaneously, exploiting shared information to improve the prediction performance of the model for each task.
sent2: Auxiliary tasks can be added to the main task to boost the performance.
sent3: Several studies have explored how emotion and sentiment detection can act as auxiliary tasks in a multi-task learning framework to enhance misinformation detection accuracy.
sent4: The method developed by Choudhry et al. [30,181,182] (Figure 10 (a)) aims to address the issue of cross-domain robustness in determining the veracity of news articles.
sent5: Generalizability of the method across different domains is achieved using a domain-adaptive framework, whose aim is to facilitate the extraction of domain-invariant features by aligning the source and target domains in the feature space.
sent6: The multi-task learning setup trains an emotion classifier as an auxiliary task in parallel to a fake news detector, to try to improve the alignment between the source and target domains, while adversarial training helps to make the model robust to outliers.
sent7: The emotion classifier assigns emotion labels according to Ekman's or Plutchik's emotions, with the aid of the Unison model [195].An LSTM is used as the feature extractor, which is trained using the accumulated loss from the fake news classifier, emotion classifier and a domain classifier, the latter of which acts as a discriminator in learning domain-invariant features.
sent8: Based on the relatedness between the tasks of detecting fake news, novelty, emotion, and sentiment, Kumari et al. [160] (Figure 10 (b)) developed a multi-task learning framework in which the latter three of these are treated as auxiliary tasks.
sent9: Using premise-hypothesis pairs as input, the model detects whether or not the hypothesis is fake with respect to the premise.
sent10: Pre-trained and/or fine-tuned models are firstly used to determine whether the hypothesis is novel with respect to the premise, and whether or not the hypothesis and premise differ in terms of binary emotion values (i,e., sadness/joy/trust vs. anger/fear/disgust/surprise) and sentiment (positive or negative).Different Bi-LSTMs that use pretrained GloVe and BERT-based embeddings are employed to obtain two different input textual representations, which are concatenated and used as the input to the three auxiliary tasks and the main task of fake news detection."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s9,Deep Learning Methods,"DL is a sub-field of machine learning that has made breakthrough progress in many fields, especially in computer vision, NLP, speech recognition, and other AI fields [183].Compared to conventional machine learning methods, DL techniques can handle larger and more complex datasets and can result in improved performance on certain tasks [184].DL algorithms build complex models by stacking multiple neural network layers, which are called deep neural networks.Pre-training is a DL model training strategy, in which models are initially trained on a large-scale data set to learn a common feature representation that is suitable for application in a range of different scenarios.Pre-trained models are subsequently fine-tuned to achieve optimal results when applied to specific tasks.As shown in Table 3, DL approaches have been widely used in both sentiment/emotion analysis and misinformation detection.For example, Iwendi et al. [21] explored the use of RNN, GRU, and LSTM as classifiers to detect fake news relating to COVID-19 based on 39 features (including sentiment, linguistic, and named entities) extracted from news articles and social media posts.Ajao et al. [152] employed various machine learning methods and an LSTM with hierarchical attention networks (HAN) [185] for rumor detection.A Bi-LSTM was used by Hamed et al. [172] to detect misinformation using dual emotions and content features.In [20], the authors adopted CNN and Bi-GRU to extract dual emotion features.To evaluate the effectiveness of their proposed multi-tasking framework for rumor detection, Choudhry et al. [30] employed various DL methods, including LSTM, BERT, CNN, RoBERTa, CapsuleNet [186] and HAN.Various studies have applied GCN and GNN to model the graph-like structure of social media posts [162,178].

Pre-trained models, including BERT, DistilBERT, and RoBERTa, have frequently been used used as the basis for extracting sentiment and emotion features in the context of misinformation detection [22,24,160,162,168,175,177,179].A popular technique has been to use transfer learning to fine-tune these pre-trained models on large emotion detection datasets (e.g., GoEmotions [187] and DailyDialogue [188]) prior to labeling misinformation datasets.Moreover, there exists a small number of pre-trained models for languages other than English, such as AraBERT-Twitter and MARBERT, which were used for rumor detection in Arabic social media [28].

For multimodal data, Resnet18, VGG16 and Xception have been used to extract image features [131,132,166,167].Similarly to text-only datasets, these studies use transfer learning to fine-tune pre-trained image models on visual sentiment datasets, then extract image features from the misinformation dataset, and finally merge them with text features for misinformation detection.","[['b29', 'b184', 'b185', 'b161', 'b151', 'b20', 'b171', 'b183', 'b182', 'b177', 'b19'], ['b27', 'b178', 'b161', 'b186', 'b174', 'b159', 'b21', 'b23', 'b167', 'b187', 'b176'], ['b166', 'b165', 'b130', 'b131']]","[['b29', 'b184', 'b185', 'b161', 'b151', 'b20', 'b171', 'b183', 'b182', 'b177', 'b19'], ['b27', 'b178', 'b161', 'b186', 'b174', 'b159', 'b21', 'b23', 'b167', 'b187', 'b176'], ['b166', 'b165', 'b130', 'b131']]",26,"sent1: DL is a sub-field of machine learning that has made breakthrough progress in many fields, especially in computer vision, NLP, speech recognition, and other AI fields [183].Compared to conventional machine learning methods, DL techniques can handle larger and more complex datasets and can result in improved performance on certain tasks [184].DL algorithms build complex models by stacking multiple neural network layers, which are called deep neural networks.
sent2: Pre-training is a DL model training strategy, in which models are initially trained on a large-scale data set to learn a common feature representation that is suitable for application in a range of different scenarios.
sent3: Pre-trained models are subsequently fine-tuned to achieve optimal results when applied to specific tasks.
sent4: As shown in Table 3, DL approaches have been widely used in both sentiment/emotion analysis and misinformation detection.
sent5: For example, Iwendi et al. [21] explored the use of RNN, GRU, and LSTM as classifiers to detect fake news relating to COVID-19 based on 39 features (including sentiment, linguistic, and named entities) extracted from news articles and social media posts.
sent6: Ajao et al. [152] employed various machine learning methods and an LSTM with hierarchical attention networks (HAN) [185] for rumor detection.
sent7: A Bi-LSTM was used by Hamed et al. [172] to detect misinformation using dual emotions and content features.
sent8: In [20], the authors adopted CNN and Bi-GRU to extract dual emotion features.
sent9: To evaluate the effectiveness of their proposed multi-tasking framework for rumor detection, Choudhry et al. [30] employed various DL methods, including LSTM, BERT, CNN, RoBERTa, CapsuleNet [186] and HAN.Various studies have applied GCN and GNN to model the graph-like structure of social media posts [162,178].Pre-trained models, including BERT, DistilBERT, and RoBERTa, have frequently been used used as the basis for extracting sentiment and emotion features in the context of misinformation detection [22,24,160,162,168,175,177,179].A popular technique has been to use transfer learning to fine-tune these pre-trained models on large emotion detection datasets (e.g., GoEmotions [187] and DailyDialogue [188]) prior to labeling misinformation datasets.
sent10: Moreover, there exists a small number of pre-trained models for languages other than English, such as AraBERT-Twitter and MARBERT, which were used for rumor detection in Arabic social media [28].
sent11: For multimodal data, Resnet18, VGG16 and Xception have been used to extract image features [131,132,166,167].Similarly to text-only datasets, these studies use transfer learning to fine-tune pre-trained image models on visual sentiment datasets, then extract image features from the misinformation dataset, and finally merge them with text features for misinformation detection."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s13,Methods Based on Tree or Graph Structures,"Due to the inherent relationships among posts relating to fake news, such as retweets or likes of source posts from followers on Twitter, social media data may be viewed as tree structures and graph structures through which information propagates.Accordingly, several methods employ tree or graph structures to model the spread of information and capture the relationship between nodes of the tree.The words  and phrases that make up sentences can also be arranged into hierarchical tree-like structures, according to the grammatical and semantic relationships that hold between them.Several misinformation detection methods make use of features based on these relationships, including dependency tree and sentiment tree information.

In [23], an earliest rumor detection approach for social media is described (Figure 7 (a)).It considers only publisher posts without their follow-up social comments, with the aim of catching potentially harmful rumors before they become widespread.The use of a syntax and sentiment enhanced version BERT (SSE-BERT) is inspired by the observations that both the sentiment and syntactic features of rumors are often distinct from non-rumors.Syntactic dependency trees firstly are obtained for each source post using DDParser [192], and are encoded into a dependency sequence by preorder traversal.Sentiment-denoting words in seven different categories are then recognised using an external sentiment lexicon (i.e., ALO [193]).Specific embeddings are then assigned to each token according to the sentiment lexicon.All features are learned by BERT and distilled using elementwise addition.Finally, the vector of [CLS] in BERT is fed into a fully connected network with softmax to detect rumorous publisher posts.

Driven by the scarcity of high-quality annotated training data, [174] developed an unsupervised approach, ptVAE (Figure 7 (b)).Based on the observations that rumorous tweets exhibit different sentiment patterns compared to rumorous tweets, and that they diffuse more rapidly, deeply and broadly, the method aims to detect rumors by identifying collections of tweets whose propagation patterns and sentiment characteristics differ from those of normal (i.e, non-rumorous) collections.The proposed model consists of a Sentiment Pattern Module (SPM), Propagation Feature Module (PFM), and Cross-alignment module.In the SPM (left of Figure 7 (b)), a tree encoder infers the pattern of sentiment labels along the input propagation tree and uses a GRU to encode this pattern into a latent vector z 1 .The original sentiment labels for each node are then reconstructed by decoding z 1 using a node label decoder and a child label distribution decoder which, respectively, predict the label of each node and the label distribution of the node's children.The PFM (right of Figure 7 (b)) creates vectors capturing the speed, and depth & breadth of propagation, and combines them as the input to a VAE, whose encoder and decoder are based on a multilayer perception.The Cross-alignment module then jointly learns the propagation tree of the SPM and the propagation characteristics of the PFM.

Li et al. [88] propose the Multi-EmoBERT model (Figure 7 (c)) to detect multiple co-existing emotions in fake news content on social media platforms.The first part consists of a Word Encoder to obtain representations of words, and a Hashtag Encoder to obtain representations of emotionword hashtags and emojis, which are common features of social media text.The second part is the Sentiment Semantic Composition Encoder, which uses the Stanford CoreNLP toolkit to construct sentiment trees, and employs a selfattention mechanism and phrase node selection to obtain phrase level vectors.The final part is a label correlation layer that uses a parameter to capture correlations between co-existing emotions.A subsequent analysis, revealing that multiple emotions are often conveyed within a single fake news posting, demonstrates the potential value of Multi-EmoBERT in detecting fake news posts.

The method described in [178] combines the use of semantic and sentiment information, along with the structure of information propagation in social network posts to obtain enriched features for rumor detection.BERT is used to separately capture information about publisher tweets and follow-up social comments, while Bi-GRU with Attention is used to encode sentiment information conveyed in followup tweets.Propagation features of tweets are obtained with the aid of a Bi-GCN network.The various features are then spliced and fused to detect rumors.

Figure 8 (a) illustrates the graph attention network-based model (MHN) developed by Zhang et al. [175], aimed at detecting longer news articles that contain misinformation.The approach is based on the finding that patterns of sentiments expressed across sentences in fake news articles are usually very different from patterns in real news articles.The model employs two types of graph structures.Firstly, a sentiment interaction network encodes sentence-level sentiment features using a pre-trained RoBERTa model, and captures changes in sentiment in that occur in the context of the surrounding sentences.A sentiment comparison model calculates comparison vectors between each contextual sentiment representation obtained from the input news document and its corresponding original sentiment embedding; discrepancies between these embeddings could be indicative of fake news.Secondly, a heterogeneous document graph encodes the semantic content of the article, by capturing interactions between sentences, topics, and entities.A comparison between the contextual entity vectors and those obtained from a knowledge graph is aimed at detecting potential information inconsistencies that could denote fake news.The sentiment comparison vector, entity comparison vector and article representations are combined and passed through a Softmax layer to make predictions.Dong et al. [162] designed a Sentiment-Aware Hypergraph Attention Network (SA-HyperGAT) for fake news detection in social media (Figure 8 (b)).The use of hypergraphs is intended to capture higher-order dependency information between words and sentences, compared to general graphs.Separate hypergraphs are constructed for publisher posts and follow-up social comments.In the former hypergraph, each node corresponds to a word in the news text, while in the latter, nodes correspond to user comments.Sentiment labels for each comment, obtained using a fine-tuned RoBERTa model, are used as hyperedges in the graph.Representations of comments are learnt using an LSTM, after which nodeto-edge attention and edge-to-node attention are applied to learn the representation of the hypergraphs.Final feature vectors are obtained by applying mean pooling to both hypergraphs; these vectors are combined and then fed into a softmax classifier to obtain the final prediction.

The graph-based contextual and semantic learning (GCS) method for detecting rumors in tweets [165] (Figure 8 (c)) is based on a novel approach to graph-based representation learning, and the identification of two prevalent categories of words that constitute the building blocks for constructing contextual patterns for rumor detection, i.e., substantial words, which are used to express emotions, sentiments, or suspicions about the event, and bridge words, which connect substantial words.After data pre-processing, publisher and social tweets are combined to allow important relationships to be identified, e.g., social tweets may convey skepticism, correction, verification, etc, towards the publisher tweet.The combined tweets are represented as word co-occurrence graphs, to which clustering coefficient and eigenvector centrality are applied to identify substantial and topical words and bridge words, respectively.These are further enriched with negative emotional patterns and skeptical patterns.Next, a modified TF-IDF formula is used to rank and select the top-k patterns most likely to be indicative of rumor.Semantic vectors are then generated for both tweets and patterns using word embeddings, which are combined and then converted into features using cosine similarity for subsequent use by different conventional machine classification algorithms (i.e., support vector machine, gradient boosting, conditional random field, and logistic regression).","[[], ['b191', 'b192', 'b22'], ['b173'], ['b87'], ['b177'], ['b161', 'b174'], ['b164']]","[[], ['b191', 'b192', 'b22'], ['b173'], ['b87'], ['b177'], ['b161', 'b174'], ['b164']]",9,"sent1: Due to the inherent relationships among posts relating to fake news, such as retweets or likes of source posts from followers on Twitter, social media data may be viewed as tree structures and graph structures through which information propagates.
sent2: Accordingly, several methods employ tree or graph structures to model the spread of information and capture the relationship between nodes of the tree.
sent3: The words  and phrases that make up sentences can also be arranged into hierarchical tree-like structures, according to the grammatical and semantic relationships that hold between them.
sent4: Several misinformation detection methods make use of features based on these relationships, including dependency tree and sentiment tree information.
sent5: In [23], an earliest rumor detection approach for social media is described (Figure 7 (a)).It considers only publisher posts without their follow-up social comments, with the aim of catching potentially harmful rumors before they become widespread.
sent6: The use of a syntax and sentiment enhanced version BERT (SSE-BERT) is inspired by the observations that both the sentiment and syntactic features of rumors are often distinct from non-rumors.
sent7: Syntactic dependency trees firstly are obtained for each source post using DDParser [192], and are encoded into a dependency sequence by preorder traversal.
sent8: Sentiment-denoting words in seven different categories are then recognised using an external sentiment lexicon (i.e., ALO [193]).Specific embeddings are then assigned to each token according to the sentiment lexicon.
sent9: All features are learned by BERT and distilled using elementwise addition.
sent10: Finally, the vector of [CLS] in BERT is fed into a fully connected network with softmax to detect rumorous publisher posts.
sent11: Driven by the scarcity of high-quality annotated training data, [174] developed an unsupervised approach, ptVAE (Figure 7 (b)).Based on the observations that rumorous tweets exhibit different sentiment patterns compared to rumorous tweets, and that they diffuse more rapidly, deeply and broadly, the method aims to detect rumors by identifying collections of tweets whose propagation patterns and sentiment characteristics differ from those of normal (i.e, non-rumorous) collections.
sent12: The proposed model consists of a Sentiment Pattern Module (SPM), Propagation Feature Module (PFM), and Cross-alignment module.
sent13: In the SPM (left of Figure 7 (b)), a tree encoder infers the pattern of sentiment labels along the input propagation tree and uses a GRU to encode this pattern into a latent vector z 1 .The original sentiment labels for each node are then reconstructed by decoding z 1 using a node label decoder and a child label distribution decoder which, respectively, predict the label of each node and the label distribution of the node's children.
sent14: The PFM (right of Figure 7 (b)) creates vectors capturing the speed, and depth & breadth of propagation, and combines them as the input to a VAE, whose encoder and decoder are based on a multilayer perception.
sent15: The Cross-alignment module then jointly learns the propagation tree of the SPM and the propagation characteristics of the PFM.
sent16: Li et al. [88] propose the Multi-EmoBERT model (Figure 7 (c)) to detect multiple co-existing emotions in fake news content on social media platforms.
sent17: The first part consists of a Word Encoder to obtain representations of words, and a Hashtag Encoder to obtain representations of emotionword hashtags and emojis, which are common features of social media text.
sent18: The second part is the Sentiment Semantic Composition Encoder, which uses the Stanford CoreNLP toolkit to construct sentiment trees, and employs a selfattention mechanism and phrase node selection to obtain phrase level vectors.
sent19: The final part is a label correlation layer that uses a parameter to capture correlations between co-existing emotions.
sent20: A subsequent analysis, revealing that multiple emotions are often conveyed within a single fake news posting, demonstrates the potential value of Multi-EmoBERT in detecting fake news posts.
sent21: The method described in [178] combines the use of semantic and sentiment information, along with the structure of information propagation in social network posts to obtain enriched features for rumor detection.
sent22: BERT is used to separately capture information about publisher tweets and follow-up social comments, while Bi-GRU with Attention is used to encode sentiment information conveyed in followup tweets.
sent23: Propagation features of tweets are obtained with the aid of a Bi-GCN network.
sent24: The various features are then spliced and fused to detect rumors.
sent25: Figure 8 (a) illustrates the graph attention network-based model
sent26: (MHN) developed by Zhang et al. [175], aimed at detecting longer news articles that contain misinformation.
sent27: The approach is based on the finding that patterns of sentiments expressed across sentences in fake news articles are usually very different from patterns in real news articles.
sent28: The model employs two types of graph structures.
sent29: Firstly, a sentiment interaction network encodes sentence-level sentiment features using a pre-trained RoBERTa model, and captures changes in sentiment in that occur in the context of the surrounding sentences.
sent30: A sentiment comparison model calculates comparison vectors between each contextual sentiment representation obtained from the input news document and its corresponding original sentiment embedding; discrepancies between these embeddings could be indicative of fake news.
sent31: Secondly, a heterogeneous document graph encodes the semantic content of the article, by capturing interactions between sentences, topics, and entities.
sent32: A comparison between the contextual entity vectors and those obtained from a knowledge graph is aimed at detecting potential information inconsistencies that could denote fake news.
sent33: The sentiment comparison vector, entity comparison vector and article representations are combined and passed through a Softmax layer to make predictions.
sent34: Dong et al. [162] designed a Sentiment-Aware Hypergraph Attention Network (SA-HyperGAT) for fake news detection in social media (Figure 8 (b)).The use of hypergraphs is intended to capture higher-order dependency information between words and sentences, compared to general graphs.
sent35: Separate hypergraphs are constructed for publisher posts and follow-up social comments.
sent36: In the former hypergraph, each node corresponds to a word in the news text, while in the latter, nodes correspond to user comments.
sent37: Sentiment labels for each comment, obtained using a fine-tuned RoBERTa model, are used as hyperedges in the graph.
sent38: Representations of comments are learnt using an LSTM, after which nodeto-edge attention and edge-to-node attention are applied to learn the representation of the hypergraphs.
sent39: Final feature vectors are obtained by applying mean pooling to both hypergraphs; these vectors are combined and then fed into a softmax classifier to obtain the final prediction.
sent40: The graph-based contextual and semantic learning (GCS) method for detecting rumors in tweets [165] (Figure 8 (c)) is based on a novel approach to graph-based representation learning, and the identification of two prevalent categories of words that constitute the building blocks for constructing contextual patterns for rumor detection, i.e., substantial words, which are used to express emotions, sentiments, or suspicions about the event, and bridge words, which connect substantial words.
sent41: After data pre-processing, publisher and social tweets are combined to allow important relationships to be identified, e.g., social tweets may convey skepticism, correction, verification, etc, towards the publisher tweet.
sent42: The combined tweets are represented as word co-occurrence graphs, to which clustering coefficient and eigenvector centrality are applied to identify substantial and topical words and bridge words, respectively.
sent43: These are further enriched with negative emotional patterns and skeptical patterns.
sent44: Next, a modified TF-IDF formula is used to rank and select the top-k patterns most likely to be indicative of rumor.
sent45: Semantic vectors are then generated for both tweets and patterns using word embeddings, which are combined and then converted into features using cosine similarity for subsequent use by different conventional machine classification algorithms (i.e., support vector machine, gradient boosting, conditional random field, and logistic regression)."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s12,Mining of Dual Emotions,"A number of studies has investigated how misinformation detection in social media can be improved by taking into account information about the different emotions expressed   [29], (c) EFN [134] in posts that announce news (i.e., publisher posts) and posts that comment on or react to these source posts (i.e., social posts) Luvembe et al. [20] developed a deep normalized attentionbased mechanism for enriched extraction of dual emotion features (Figure 6 (a)), which combines CNN and Bi-GRU.The CNN layer is used to obtain embeddings for both publisher and social posts, after which a stacked Bi-GRU with attention is utilized to extract and concatenate emotion features from each type of post.Classification of publisher tweets according to whether or not they report misinformation is performed using a random forest model, whose features are guided by a genetic algorithm, which determines the subset of features that can achieve optimal classification performance.

The MDE model [29] (Figure 6 (b)) detects misinformation in social media posts by integrating features from existing Bi-GRU fake news detectors with publisher and social emotion features and the relationship between them.A vector representing emotions in the publisher post emo  , is obtained by concatenating the emotion category, lexiconbased emotion score, emotional intensity, sentiment score, and other auxiliary features (e.g., emoticons and punctuation).A vector is created for each social post, by applying the same method used to obtain the publisher emotion vector.The individual social emotion vectors are subsequently combined, after which they are aggregated in two ways, i.e. using mean pooling emo   (to represent average emotion signals) and max pooling emo   (to capture extreme emotional signals).These two types of aggregation are then concatenated to obtain the overall Social Emotion emo  .The Emotion Gap emo  represents the difference between the publisher and social emotions, and is obtained by concatenating (emo  -emo   ) and (emo  -emo   ).Finally, dual emotion features are obtained by concatenating the publisher emotion (emo  ), the social emotion (emo  ) and the Emotion Gap (emo  ).These features are combined with those from the existing Bi-GRU fake news detector, and fed into a multi-layer perceptron (MLP) layer and a softmax layer to determine whether or not the publisher post represents fake news.

The end-to-end emotion-based fake news detection framework for social media (EFN) proposed by Guo et al. [134] (Figure 6 (c)) consists of a content module, comment module and fake news detection module.The content module (left of the figure) is used to encode publisher posts using Bi-GRUs for word embeddings and emotion embeddings, the latter of which is trained using large-scale Weibo datasets, with emoticons as the emotion labels.A gate recurrent unit (Gate_N) is then applied to combine word embeddings, emotion embeddings, and 19 sentence-based emotion features.Subsequently, all vectors are fed into another Bi-GRU, whose final hidden state is used as the representation of the publisher post.The comment module (right of the figure) represents information about follow-up social posts.The comment module architecture is similar to the content module, except that all comments are concatenated before being fed into the Bi-GRU, and sentence-based emotion features are not used.A different Gate (Gate_C) is used to fuse features.Finally, the output of the third Gate (Gate_M), which combines the content and comment representations, is fed to a fully connected layer with softmax activation to determine whether or not the publisher post constitutes fake news.","[['b133', 'b19', 'b28'], ['b28'], ['b133']]","[['b133', 'b19', 'b28'], ['b28'], ['b133']]",5,"sent1: A number of studies has investigated how misinformation detection in social media can be improved by taking into account information about the different emotions expressed   [29], (c)
sent2: EFN [134] in posts that announce news (i.e., publisher posts) and posts that comment on or react to these source posts (i.e., social posts) Luvembe et al. [20] developed a deep normalized attentionbased mechanism for enriched extraction of dual emotion features (Figure 6 (a)), which combines CNN and Bi-GRU.The CNN layer is used to obtain embeddings for both publisher and social posts, after which a stacked Bi-GRU with attention is utilized to extract and concatenate emotion features from each type of post.Classification of publisher tweets according to whether or not they report misinformation is performed using a random forest model, whose features are guided by a genetic algorithm, which determines the subset of features that can achieve optimal classification performance.
sent3: The MDE model [29] (Figure 6 (b)) detects misinformation in social media posts by integrating features from existing Bi-GRU fake news detectors with publisher and social emotion features and the relationship between them.
sent4: A vector representing emotions in the publisher post emo  , is obtained by concatenating the emotion category, lexiconbased emotion score, emotional intensity, sentiment score, and other auxiliary features (e.g., emoticons and punctuation).A vector is created for each social post, by applying the same method used to obtain the publisher emotion vector.
sent5: The individual social emotion vectors are subsequently combined, after which they are aggregated in two ways, i.e. using mean pooling emo   (to represent average emotion signals) and max pooling emo   (to capture extreme emotional signals).These two types of aggregation are then concatenated to obtain the overall Social Emotion emo  .The
sent6: Emotion Gap emo  represents the difference between the publisher and social emotions, and is obtained by concatenating (emo  -emo   ) and (emo  -emo   ).Finally, dual emotion features are obtained by concatenating the publisher emotion (emo  ), the social emotion (emo  ) and the Emotion Gap
sent7: (emo  ).These features are combined with those from the existing Bi-GRU fake news detector, and fed into a multi-layer perceptron (MLP) layer and a softmax layer to determine whether or not the publisher post represents fake news.
sent8: The end-to-end emotion-based fake news detection framework for social media (EFN) proposed by Guo et al. [134] (Figure 6 (c)) consists of a content module, comment module and fake news detection module.
sent9: The content module (left of the figure) is used to encode publisher posts using Bi-GRUs for word embeddings and emotion embeddings, the latter of which is trained using large-scale Weibo datasets, with emoticons as the emotion labels.
sent10: A gate recurrent unit (Gate_N) is then applied to combine word embeddings, emotion embeddings, and 19 sentence-based emotion features.
sent11: Subsequently, all vectors are fed into another Bi-GRU, whose final hidden state is used as the representation of the publisher post.
sent12: The comment module (right of the figure) represents information about follow-up social posts.
sent13: The comment module architecture is similar to the content module, except that all comments are concatenated before being fed into the Bi-GRU, and sentence-based emotion features are not used.
sent14: A different Gate (Gate_C) is used to fuse features.
sent15: Finally, the output of the third Gate (Gate_M), which combines the content and comment representations, is fed to a fully connected layer with softmax activation to determine whether or not the publisher post constitutes fake news."
264833081,Emotion Detection for Misinformation: A Review,Computer Science,https://www.semanticscholar.org/paper/acbb28ffe548d3f87c701c5f9a965bdca474a9a4,s11,Methods Combining Emotion with Other Text-Based Features,"Various methods have attempted to exploit the wealth of valuable information conveyed in text by combining emotion/sentiment features with other features derived from the textual content of news articles or social media posts.

Ghanem et al. [124] proposed the FakeFlow model (Figure 5 (a)), which aims to model the flow of affective information in fake news articles, based on the hypothesis that the pattern of affective information in fake news differs from that found in genuine news, e.g., emotions of fear are often evoked towards the start of fake new articles.The model consists of two modules, the first encoding topic information, extracted using a CNN, and the second capturing 23 affective features relating to emotion, sentiment, morality, imageability and hyperbola.In the first module, potential relationships between topics and affective information are captured by concatenating their respective vectors (e.g., emotions in a fake article about Islam are likely to vary from those in an article in favor of a politician).A contextaware self-attention mechanism is subsequently applied to weight segments according to their similarity to neighboring segments.In the second module, the flow of the affective information within the articles is modeled by feeding the affective vectors to Bi-GRUs.Finally, a dot product and  Mixture-of-Experts [177], (c) EmoAttentionBERT [163] (d) LSTM (with Fuzzy Sentiment) [164] average operation are applied to distill the output of the two modules into a compact representation, which is fed into a fully connected layer and a softmax layer to determine the factuality of the article.

The multi-domain fake-news detection system described in [177] (Figure 5 (b)) is based on mixture-of-experts model, which involves training multiple neural networks based on TextCNNs (experts), each targeted at a different part (domain) of a dataset.Pre-trained BERT and CLIP [189] text encoders are applied to obtain two different embeddings of news content, which are combined as a fusion embedding.The use of the CLIP text encoder, which is pre-trained on image-text paired datasets, aims to take advantage of the rich semantic representations obtained through state-of-theart multimodal learning.A Collaboration module adaptively determines the weights of each expert model, to enhance or suppress their contribution in the final mixture-of-experts model.The module consists of a fusion vector C  , which combines sentence-level embeddings e  from attention, sentiment embeddings e  obtained by fine-tuning BERT using the Weibo_senti_100k dataset, and domain embeddings e  .The expert networks are accumulated and multiplied via the collaborative influence function C  , which is determined by the Collaboration module, and then used for classification.

The EmoAttention BERT model architecture [163] (Figure 5 (c)) uses both emotion and snippet attention to verify the truth of political claims, supported by evidence from Google news snippets.The content of snippets is encoded using word embeddings, while the NRC Intensity Emotion Lexicon [190] is used to calculate word-level intensities for eight basic emotions.An emotional attention layer assigns a weight to each emotion vector to identify the most relevant emotional signals in a given evidence snippet, while a snippet attention layer weights each evidence snippet with respect to the associated claim.Finally, the vectors from both layers are distilled and fed into a softmax layer to predict the truth of the claim.

Mohamed et al. [164] detects fake news using an LSTM that combines textual embeddings from Word2Vec with fuzzy sentiment features (Figure 5 (d)).Sentiment features are extracted by firstly identifying opinion-denoting words and associated polarity information using the SentiWordNet [191] and WordNet5 , resulting in an initial score.These values are subsequently modified using fuzzy logic functions, according to the presence of different types of linguistic hedges (i.e., words that modify the intensity and meaning of an expressed opinion, such as not, very, and quite), using fuzzy logic functions to obtain the final sentiment score.","[[], ['b123', 'b163', 'b162', 'b176'], ['b188', 'b176'], ['b189', 'b162'], ['b163', 'b190']]","[[], ['b123', 'b163', 'b162', 'b176'], ['b188', 'b176'], ['b189', 'b162'], ['b163', 'b190']]",10,"sent1: Various methods have attempted to exploit the wealth of valuable information conveyed in text by combining emotion/sentiment features with other features derived from the textual content of news articles or social media posts.
sent2: Ghanem et al. [124] proposed the FakeFlow model (Figure 5 (a)), which aims to model the flow of affective information in fake news articles, based on the hypothesis that the pattern of affective information in fake news differs from that found in genuine news, e.g., emotions of fear are often evoked towards the start of fake new articles.
sent3: The model consists of two modules, the first encoding topic information, extracted using a CNN, and the second capturing 23 affective features relating to emotion, sentiment, morality, imageability and hyperbola.
sent4: In the first module, potential relationships between topics and affective information are captured by concatenating their respective vectors (e.g., emotions in a fake article about Islam are likely to vary from those in an article in favor of a politician).A contextaware self-attention mechanism is subsequently applied to weight segments according to their similarity to neighboring segments.
sent5: In the second module, the flow of the affective information within the articles is modeled by feeding the affective vectors to Bi-GRUs.
sent6: Finally, a dot product and  Mixture-of-Experts [177], (c) EmoAttentionBERT [163] (d) LSTM (with Fuzzy Sentiment) [164] average operation are applied to distill the output of the two modules into a compact representation, which is fed into a fully connected layer and a softmax layer to determine the factuality of the article.
sent7: The multi-domain fake-news detection system described in [177] (Figure 5 (b)) is based on mixture-of-experts model, which involves training multiple neural networks based on TextCNNs (experts), each targeted at a different part (domain) of a dataset.
sent8: Pre-trained BERT and CLIP [189] text encoders are applied to obtain two different embeddings of news content, which are combined as a fusion embedding.
sent9: The use of the CLIP text encoder, which is pre-trained on image-text paired datasets, aims to take advantage of the rich semantic representations obtained through state-of-theart multimodal learning.
sent10: A Collaboration module adaptively determines the weights of each expert model, to enhance or suppress their contribution in the final mixture-of-experts model.
sent11: The module consists of a fusion vector C  , which combines sentence-level embeddings e  from attention, sentiment embeddings e  obtained by fine-tuning BERT using the Weibo_senti_100k dataset, and domain embeddings e  .The
sent12: expert networks are accumulated and multiplied via the collaborative influence function C  , which is determined by the Collaboration module, and then used for classification.
sent13: The EmoAttention BERT model architecture [163] (Figure 5 (c)) uses both emotion and snippet attention to verify the truth of political claims, supported by evidence from Google news snippets.
sent14: The content of snippets is encoded using word embeddings, while the NRC Intensity Emotion Lexicon [190] is used to calculate word-level intensities for eight basic emotions.
sent15: An emotional attention layer assigns a weight to each emotion vector to identify the most relevant emotional signals in a given evidence snippet, while a snippet attention layer weights each evidence snippet with respect to the associated claim.
sent16: Finally, the vectors from both layers are distilled and fed into a softmax layer to predict the truth of the claim.
sent17: Mohamed et al. [164] detects fake news using an LSTM that combines textual embeddings from Word2Vec with fuzzy sentiment features (Figure 5 (d)).Sentiment features are extracted by firstly identifying opinion-denoting words and associated polarity information using the SentiWordNet [191] and WordNet5 , resulting in an initial score.
sent18: These values are subsequently modified using fuzzy logic functions, according to the presence of different types of linguistic hedges (i.e., words that modify the intensity and meaning of an expressed opinion, such as not, very, and quite), using fuzzy logic functions to obtain the final sentiment score."
265607982,Overview of PragTag-2023: Low-Resource Multi-Domain Pragmatic Tagging of Peer Reviews,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/a6d2fd7c874c0074135ffc09a81f7c409e5b1ae1,s10,Submissions,"Out of over 20 teams that signed up for the competition, five teams have made it to the final submission.The submitted systems explore a wide range of techniques and architectures for multi-domain pragmatic tagging in low-resource scenarios.We summarize the main ideas behind each submission below and refer to the system papers for details.

CATALPA_EduNLP (Ding et al., 2023) investigated a wide array of approaches.For the full-and low-data setting, this includes supervised sentence labeling via RoBERTa (Liu et al., 2019) augmented with additional features (domain, position, context, word normalization), as well as IOB-style sequence tagging using long-document Transformers and nearest-neighbor-based labeling using SBERT (Reimers and Gurevych, 2019).In the zero-shot setting, the team experimented with labeling test instances based on their similarity to class defini- tions from the shared task description, as well as with prompting via GPT3.5.The participants used the ARR-22 auxiliary data, addressing the gap in label distribution between ARR and the core data via subsampling, and explored data augmentation based on F1000raw auxiliary data.By ensembling best per-domain configurations selected on the validation set, they found that a BERT-based model with additional features outperforms sequence tagging and nearest-neighbor labeling on the full data, while a BERT-based model augmented with additional data performs best in the low-data setting.Prompting GPT3.5 in the zero-shot setting was shown vastly superior to SBERT-based classification based on task definitions -yet, following the PragTag rules, GPT3.5 result was not used for the leaderboard submission.

DeepBlueAI (Luo et al., 2023) focused their approach on increasing the robustness of pre-trained models in the sentence labeling setting.The experiments were conducted using three models -RoBERTa, DeBERTa (He et al., 2023) and XLM-RoBERTa (Conneau et al., 2020).The participants augmented the model via max pooling and attention pooling, introduced adversarial training via fast gradient method, and reported comparative performance of the models trained under different settings via cross-fold validation, showing that the modifications lead to variable performance gains.The authors report that the DeBERTa model consistently outperforms the other two models on the task.To tackle the secret test set in the final phase of the competition, the authors used a voting approach combining a range of models trained in different configurations and selecting the label with the maximum vote, stressing the benefits of fusing different types of models for prediction.

NUS-IDS (Gollapalli et al., 2023) explored multiple approaches to the task for each experimental condition.In the zero-shot no-data condition, the participants proposed two methods: a questionanswering model that selects passages from the peer review based on a set of questions derived from peer reviewing guidelines of NLP conferences, and a prompting-based approach based on the Flan-T5 (Chung et al., 2022) model.For the low-and full-data setting, the participants experimented with fine-tuning pre-trained language models, additionally exploring ensembling and data augmentation techniques by tentatively labeling the auxiliary shared task data.The results indicate that prompting via Flan-T5 outperforms questionanswering based approach in the no-data setting; in low-and full-data data, fine-tuning a T5 model (Raffel et al., 2019) on tentatively labeled auxiliary data followed by fine-tuning on the core task data performs best.

MILAB (Lee et al., 2023) approached the problem of data scarcity and domain shift via data augmentation.In particular, to compensate for the lack of data, the team applied an ensemble of RoBERTa-based classifiers to label auxiliary data from F1000raw and ARR-22.Apart from majority labeling, the authors explored a novel recall labeling technique: the models assign tentative labels to the unlabeled instances in the decreasing order of recall on a validation set, while labeling the residual instances as Other.Additionally, the authors experimented with diversifying the data by applying off-the-shelf synonym generation followed by BERTScore filtering (Zhang et al., 2020).The results indicate that the proposed data augmentation techniques combined with ensembling improve the model performance on the task, especially in the no-data condition.

SuryaKiran (Suri et al., 2023) explored the use of unsupervised pre-training on F1000raw auxiliary data to increase domain robustness of the pragmatic tag classifier.In particular, the participants pre-trained the DeBERTa model on F1000raw using masked language modeling objective (Devlin et al., 2019), and later used an ensemble of five models further fine-tuned on different training data splits to make the test set prediction.Their results demonstrate that pre-training via masked language modeling leads to improved performance only in some cases; the authors attribute this to the vocabulary discrepancies between the domains.The team submitted their system only to the final evaluation.","[[], ['b4', 'b21', 'b16'], ['b9', 'b2', 'b17'], ['b20', 'b1'], ['b15', 'b28'], ['b3', 'b24']]","[[], ['b4', 'b21', 'b16'], ['b9', 'b2', 'b17'], ['b20', 'b1'], ['b15', 'b28'], ['b3', 'b24']]",12,"sent1: Out of over 20 teams that signed up for the competition, five teams have made it to the final submission.
sent2: The submitted systems explore a wide range of techniques and architectures for multi-domain pragmatic tagging in low-resource scenarios.
sent3: We summarize the main ideas behind each submission below and refer to the system papers for details.
sent4: CATALPA_EduNLP (Ding et al., 2023) investigated a wide array of approaches.
sent5: For the full-and low-data setting, this includes supervised sentence labeling via RoBERTa (Liu et al., 2019) augmented with additional features (domain, position, context, word normalization), as well as IOB-style sequence tagging using long-document Transformers and nearest-neighbor-based labeling using SBERT (Reimers and Gurevych, 2019).In the zero-shot setting, the team experimented with labeling test instances based on their similarity to class defini- tions from the shared task description, as well as with prompting via GPT3.5.The participants used the ARR-22 auxiliary data, addressing the gap in label distribution between ARR and the core data via subsampling, and explored data augmentation based on F1000raw auxiliary data.
sent6: By ensembling best per-domain configurations selected on the validation set, they found that a BERT-based model with additional features outperforms sequence tagging and nearest-neighbor labeling on the full data, while a BERT-based model augmented with additional data performs best in the low-data setting.
sent7: Prompting GPT3.5 in the zero-shot setting was shown vastly superior to SBERT-based classification based on task definitions -yet, following the PragTag rules, GPT3.5 result was not used for the leaderboard submission.
sent8: DeepBlueAI (Luo et al., 2023) focused their approach on increasing the robustness of pre-trained models in the sentence labeling setting.
sent9: The experiments were conducted using three models -RoBERTa, DeBERTa (He et al., 2023) and XLM-RoBERTa (Conneau et al., 2020).The participants augmented the model via max pooling and attention pooling, introduced adversarial training via fast gradient method, and reported comparative performance of the models trained under different settings via cross-fold validation, showing that the modifications lead to variable performance gains.
sent10: The authors report that the DeBERTa model consistently outperforms the other two models on the task.
sent11: To tackle the secret test set in the final phase of the competition, the authors used a voting approach combining a range of models trained in different configurations and selecting the label with the maximum vote, stressing the benefits of fusing different types of models for prediction.
sent12: NUS-IDS (Gollapalli et al., 2023) explored multiple approaches to the task for each experimental condition.
sent13: In the zero-shot no-data condition, the participants proposed two methods: a questionanswering model that selects passages from the peer review based on a set of questions derived from peer reviewing guidelines of NLP conferences, and a prompting-based approach based on the Flan-T5 (Chung et al., 2022) model.
sent14: For the low-and full-data setting, the participants experimented with fine-tuning pre-trained language models, additionally exploring ensembling and data augmentation techniques by tentatively labeling the auxiliary shared task data.
sent15: The results indicate that prompting via Flan-T5 outperforms questionanswering based approach in the no-data setting; in low-and full-data data, fine-tuning a T5 model (Raffel et al., 2019) on tentatively labeled auxiliary data followed by fine-tuning on the core task data performs best.
sent16: MILAB (Lee et al., 2023) approached the problem of data scarcity and domain shift via data augmentation.
sent17: In particular, to compensate for the lack of data, the team applied an ensemble of RoBERTa-based classifiers to label auxiliary data from F1000raw and ARR-22.Apart from majority labeling, the authors explored a novel recall labeling technique: the models assign tentative labels to the unlabeled instances in the decreasing order of recall on a validation set, while labeling the residual instances as Other.
sent18: Additionally, the authors experimented with diversifying the data by applying off-the-shelf synonym generation followed by BERTScore filtering (Zhang et al., 2020).The results indicate that the proposed data augmentation techniques combined with ensembling improve the model performance on the task, especially in the no-data condition.
sent19: SuryaKiran (Suri et al., 2023) explored the use of unsupervised pre-training on F1000raw auxiliary data to increase domain robustness of the pragmatic tag classifier.
sent20: In particular, the participants pre-trained the DeBERTa model on F1000raw using masked language modeling objective (Devlin et al., 2019), and later used an ensemble of five models further fine-tuned on different training data splits to make the test set prediction.
sent21: Their results demonstrate that pre-training via masked language modeling leads to improved performance only in some cases; the authors attribute this to the vocabulary discrepancies between the domains.
sent22: The team submitted their system only to the final evaluation."
265607982,Overview of PragTag-2023: Low-Resource Multi-Domain Pragmatic Tagging of Peer Reviews,"Computer Science, Linguistics",https://www.semanticscholar.org/paper/a6d2fd7c874c0074135ffc09a81f7c409e5b1ae1,s1,+ aux data 4,"Figure 1: PragTag-2023 Overview.Given a mixeddomain corpus of peer reviews annotated with pragmatic tags, the participants submit systems trained with varying amounts of training data (1-3) with optional use of unlabeled auxiliary data (4).The systems are evaluated in each of the five domains (Section 3.1), as well as on a previously unseen secret domain (5).

for peer review analysis aims to investigate those issues by analyzing argumentation in peer review reports (e.g.Kang et al., 2018;Cheng et al., 2020;Hua et al., 2019;Kuznetsov et al., 2022;Dycke et al., 2023).The resulting systems have numerous potential applications, incl.facilitating metascientific analysis of reviewing practices, helping authors and program chairs aggregate information from multiple reviews, and supporting junior reviewers in giving thorough, objective and helpful feedback.

Standards and practices of scholarly communication vary across research communities.Yet, to date, NLP for peer review has focused on data from machine learning conferences (Kang et al., 2018;Hua et al., 2019;Cheng et al., 2020;Kennard et al., 2022), and the applications outside of this domain remain under-investigated.This over-focus on one domain can be attributed to data scarcity -while some communities make their reviewing public, peer reviews are generally hard to obtain and legally clear for research use (Dycke et al., The authors address the issue of...This idea reminded me of the work by... Please compare your method to...The discussion is superficial.

The paper is original and sound.2022).In addition, due to the technical nature of peer review texts, they are expensive to annotate.Measuring the effects and mitigating the impact of domain shift and data scarcity are important and under-researched questions in NLP for peer reviews.

The introduction of open multi-domain corpora of peer reviews (Dycke et al., 2023) and domainneutral review analysis tasks (Kuznetsov et al., 2022) makes it possible to investigate these questions empirically.The PragTag-2023 Shared Task1 collaboratively explored multi-domain NLP for peer reviews under data scarcity.As an exemplary task we took pragmatic tagging -a sentence-level argumentation labeling task that classifies peer review statements by their communicative purpose (Section 2).PragTag-2023 has received five diverse submissions that provide new insights into multi-domain low-data pragmatic tagging, and propose a wide spectrum of methods to increase model robustness under four increasingly challenging conditions.This paper describes the shared task setup, summarizes the submissions, and aggregates the main insights from the competition.To support further investigation of multi-domain low-data NLP for peer review, we archive the code and data of the shared task and make them publicly available2 .","[[], ['b14', 'b10', 'b7', 'b12', 'b0'], ['b10', 'b12', None, 'b13', 'b0'], [], ['b14', 'b7']]","[[], ['b14', 'b10', 'b7', 'b12', 'b0'], ['b10', 'b12', None, 'b13', 'b0'], [], ['b14', 'b7']]",12,"sent1: Figure 1: PragTag-2023 Overview.
sent2: Given a mixeddomain corpus of peer reviews annotated with pragmatic tags, the participants submit systems trained with varying amounts of training data (1-3) with optional use of unlabeled auxiliary data (4).The systems are evaluated in each of the five domains (Section 3.1), as well as on a previously unseen secret domain (5).
sent3: for peer review analysis aims to investigate those issues by analyzing argumentation in peer review reports (e.g.Kang et al., 2018;Cheng et al., 2020;Hua et al., 2019;Kuznetsov et al., 2022;Dycke et al., 2023).The resulting systems have numerous potential applications, incl.facilitating metascientific analysis of reviewing practices, helping authors and program chairs aggregate information from multiple reviews, and supporting junior reviewers in giving thorough, objective and helpful feedback.
sent4: Standards and practices of scholarly communication vary across research communities.
sent5: Yet, to date, NLP for peer review has focused on data from machine learning conferences (Kang et al., 2018;Hua et al., 2019;Cheng et al., 2020;Kennard et al., 2022), and the applications outside of this domain remain under-investigated.
sent6: This over-focus on one domain can be attributed to data scarcity -while some communities make their reviewing public, peer reviews are generally hard to obtain and legally clear for research use (Dycke et al., The authors address the issue of...
sent7: This idea reminded me of the work by...
sent8: Please compare your method to...
sent9: The discussion is superficial. The paper is original and sound.2022).In addition, due to the technical nature of peer review texts, they are expensive to annotate.
sent10: Measuring the effects and mitigating the impact of domain shift and data scarcity are important and under-researched questions in NLP for peer reviews.
sent11: The introduction of open multi-domain corpora of peer reviews (Dycke et al., 2023) and domainneutral review analysis tasks (Kuznetsov et al., 2022) makes it possible to investigate these questions empirically.
sent12: The PragTag-2023 Shared Task1 collaboratively explored multi-domain NLP for peer reviews under data scarcity.
sent13: As an exemplary task we took pragmatic tagging -a sentence-level argumentation labeling task that classifies peer review statements by their communicative purpose (Section 2).PragTag-2023 has received five diverse submissions that provide new insights into multi-domain low-data pragmatic tagging, and propose a wide spectrum of methods to increase model robustness under four increasingly challenging conditions.
sent14: This paper describes the shared task setup, summarizes the submissions, and aggregates the main insights from the competition.
sent15: To support further investigation of multi-domain low-data NLP for peer review, we archive the code and data of the shared task and make them publicly available2 ."
265928847,Fake account detection in social media using machine learning methods: literature review,Computer Science,https://www.semanticscholar.org/paper/3ae6eeba3711501965bb004760510f1979f4b52f,s4,Dataset,"Two types of datasets can be used for fake account detection: free datasets and self-made datasets.The majority of researchers chose to make their datasets that comprise fake and real accounts.One of the reasons to build their dataset is because of no available public open datasets for detecting fake accounts [24].Some researchers collected survey data using a questionnaire [21] or even hired a company to make a part of the dataset [23].For fake account classification, the dataset is collected from Facebook, Instagram, and Twitter (as shown in Tables 2 and 3).Other platforms are excluded from this literature review.1,162 accounts Gupta and Kaushal [7] 4,708 accounts Khalil et al. [19] Fake accounts: 13,000 Real accounts: 5,386 Twitter Ersahin et al. [8] Fake accounts: 501 Real accounts: 499 Cresci et al. [18] 13,101 accounts Walt and Eloff [20] 223,796 accounts Akyon and Kalfaoglu [24] Fake accounts: 700 Real accounts: 700 Bharti and Pandey [33] Real accounts: 1,103 Narayan [34] Fake accounts: 1,056 Real accounts: 1,176 Instagram Meshram et al. [14] Fake accounts: 3,231 Real accounts: 6,868 Purba et al. [15] Fake accounts: 32,869 Real accounts: 32,460 Sheikhi [1] Fake accounts: 3,132 Real accounts: 6,868 Durga and Sudhakar [40] Fake accounts: 201 Real accounts: 1,002  [29] The fake project dataset 11,737 accounts Khaled et al. [26] MIB dataset Fake accounts: 3,351 Real accounts: 1,950 Wang et al. [31] CLEF2019 dataset 7,120 accounts Bharti and Pandey [33] The fake project [18] 5.870 accounts Chakraborty et al. [36] MIB dataset Fake accounts: 3,474 Real accounts: 3,351 Kadam and Sharma [38] GitHub 2,820 accounts Instagram Kesharwani et al. [32] Fake, spammer, and genuine Instagram accounts 696 accounts Das et al. [37] Kaggle dataset 576 accounts

Various methods are used in gathering and compiling new datasets.Some of them take advantage of third-party websites [15], web data crawlers, and social media API.After data has been gathered, commonly the fake accounts and the real accounts are separated manually.There are also other methods to simplify the data-gathering process without classifying the accounts one by one.The method used by Khalil et al. [19] Bulletin of Electr Eng & Inf ISSN: 2302-9285  Fake account detection in social media using machine learning methods: … (Nalia Graciella Kerrysa) 3793 involved a university's Twitter account that has a lot of followers and verifies which accounts are real or not.Meanwhile, fake accounts are obtained by buying them from a website with affordable prices.","[['b20', 'b35', 'b14', 'b22', 'b18', 'b30', 'b19', 'b32', 'b6', 'b28', 'b25', 'b37', 'b7', 'b39', 'b13', 'b33', 'b17', 'b23', 'b31', 'b36', 'b0'], ['b14', 'b18']]","[['b20', 'b35', 'b14', 'b22', 'b18', 'b30', 'b19', 'b32', 'b6', 'b28', 'b25', 'b37', 'b7', 'b39', 'b13', 'b33', 'b17', 'b23', 'b31', 'b36', 'b0'], ['b14', 'b18']]",23,"sent1: Two types of datasets can be used for fake account detection: free datasets and self-made datasets.
sent2: The majority of researchers chose to make their datasets that comprise fake and real accounts.
sent3: One of the reasons to build their dataset is because of no available public open datasets for detecting fake accounts [24].Some researchers collected survey data using a questionnaire [21] or even hired a company to make a part of the dataset [23].For fake account classification, the dataset is collected from Facebook, Instagram, and Twitter (as shown in Tables 2 and 3).Other platforms are excluded from this literature review.1,162 accounts Gupta and Kaushal [7] 4,708 accounts Khalil et al. [19] Fake accounts: 13,000 Real accounts: 5,386 Twitter Ersahin et al. [8] Fake accounts: 501 Real accounts: 499 Cresci et al. [18] 13,101 accounts Walt and Eloff [20] 223,796 accounts Akyon and Kalfaoglu [24] Fake accounts: 700 Real accounts: 700 Bharti and Pandey [33]
sent4: Real accounts: 1,103 Narayan [34] Fake accounts: 1,056 Real accounts: 1,176 Instagram Meshram et al. [14]
sent5: Fake accounts: 3,231 Real accounts: 6,868 Purba et al. [15] Fake accounts: 32,869 Real accounts: 32,460 Sheikhi [1] Fake
sent6: accounts: 3,132 Real accounts: 6,868 Durga and Sudhakar [40] Fake accounts: 201 Real accounts: 1,002
sent7: [29] The fake project dataset 11,737 accounts Khaled et al. [26] MIB dataset Fake accounts: 3,351 Real accounts: 1,950 Wang et al. [31] CLEF2019 dataset 7,120 accounts Bharti and Pandey [33]
sent8: The fake project [18] 5.870 accounts Chakraborty et al. [36] MIB dataset Fake accounts: 3,474 Real accounts: 3,351 Kadam and Sharma [38] GitHub 2,820 accounts Instagram Kesharwani et al. [32] Fake, spammer, and genuine Instagram accounts 696 accounts Das et al. [37] Kaggle dataset 576 accountsVarious methods are used in gathering and compiling new datasets.
sent9: Some of them take advantage of third-party websites [15], web data crawlers, and social media API.After data has been gathered, commonly the fake accounts and the real accounts are separated manually.
sent10: There are also other methods to simplify the data-gathering process without classifying the accounts one by one.
sent11: The method used by Khalil et al. [19] Bulletin of Electr Eng & Inf ISSN: 2302-9285 
sent12: Fake account detection in social media using machine learning methods: … (Nalia Graciella Kerrysa) 3793 involved a university's Twitter account that has a lot of followers and verifies which accounts are real or not.
sent13: Meanwhile, fake accounts are obtained by buying them from a website with affordable prices."
265928847,Fake account detection in social media using machine learning methods: literature review,Computer Science,https://www.semanticscholar.org/paper/3ae6eeba3711501965bb004760510f1979f4b52f,s6,Reference,"Features selected Total Gupta and Kaushal [7] Received likes, likes, received comments, comments, tags, tag user, tags from other users, page tags, tags in comments, page tags in the comments section, tags by other users in the comments section, shared posts, wall posts, like wall posts, comments in wall posts, used applications.17 Elyusufi et al. [11] Status, followers, friends, favorites.

4 Reddy [23] Profile ID, name, status, followers, friends, location, account creation date, shares, gender, language.Wang et al. [31] The average of mentions, emojis, stop words, topics, links, retweets, similar posts, post length, forwarded posts, and punctuations.","[['b10', 'b6'], ['b30', 'b22']]","[['b10', 'b6'], ['b30', 'b22']]",4,"sent1: Features selected Total Gupta and Kaushal [7]
sent2: Received likes, likes, received comments, comments, tags, tag user, tags from other users, page tags, tags in comments, page tags in the comments section, tags by other users in the comments section, shared posts, wall posts, like wall posts, comments in wall posts, used applications.17 Elyusufi et al. [11] Status, followers, friends, favorites.
sent3: 4 Reddy [23] Profile ID, name, status, followers, friends, location, account creation date, shares, gender, language.
sent4: Wang et al. [31] The average of mentions, emojis, stop words, topics, links, retweets, similar posts, post length, forwarded posts, and punctuations."
265928847,Fake account detection in social media using machine learning methods: literature review,Computer Science,https://www.semanticscholar.org/paper/3ae6eeba3711501965bb004760510f1979f4b52f,s13,11,"Chakraborty et al. [36] Number of friends, followers, status, favorites, listed count, language count, geo-enabled.7

Durga and Sudhakar [40] Numbers of followers, followings, media, biography count, profile picture, private account, username digit count, username length, biography emoji count.9

Pashwan and Ravi [35] Numbers of followers, friends, favorites, tweets, tweet frequency, location, verified account.7

Shreya et al. [39] User age, gender, account age, link in the description, status, friends count, location, location IP, status.9","[['b35'], ['b39'], ['b34'], ['b38']]","[['b35'], ['b39'], ['b34'], ['b38']]",4,"sent1: Chakraborty et al. [36] Number of friends, followers, status, favorites, listed count, language count, geo-enabled.7Durga and Sudhakar [40] Numbers of followers, followings, media, biography count, profile picture, private account, username digit count, username length, biography emoji count.9Pashwan and Ravi [35] Numbers of followers, friends, favorites, tweets, tweet frequency, location, verified account.7Shreya et al.
sent2: [39] User age, gender, account age, link in the description, status, friends count, location, location IP, status.9"
265928847,Fake account detection in social media using machine learning methods: literature review,Computer Science,https://www.semanticscholar.org/paper/3ae6eeba3711501965bb004760510f1979f4b52f,s14,Machine learning model,"Machine learning is used to perform the detection process of fake accounts on social media.The majority of research studies used more than one algorithm to find the best model.Combining 2 algorithms has been possible to increase the accuracy like a study conducted by Mohammad et al. [27].They combined a convolutional neural network (CNN) and an artificial neural networks (ANN) model.Another was by Khaled et al. [26] which integrated an SVM with an ANN model.[31] 1 Random forest [18], [20], [28], [30], [34], [36] 6 AdaBoost [18], [20], [28], [36] 4 XGBoost [28], [36] 2 SVM

[18]- [21], [26], [29], [30], [35], [38] 9 CNB

[21] 1 CNN

[27] 1 ANN [26], [27], [30], [38] 4 CNN-ANN

[27] 1 Simple logistic regression [18], [19], [28]- [30], [33] 6 SVM-ANN [26] 1 Naïve bayes [8], [29], [33], [34], [38] 5 Decorate

[18] 1 Decision tree [18], [33], [34], [","[['b27', 'b29', 'b17', 'b25', 'b35', 'b26', 'b33', 'b19'], ['b29', 'b28', 'b20', 'b25', 'b37', 'b34'], [], ['b25', 'b29', 'b26', 'b37'], ['b27', 'b29', 'b17', 'b28', 'b25', 'b37', 'b7', 'b18', 'b33', 'b32'], ['b33', None, 'b17', 'b32']]","[['b27', 'b29', 'b17', 'b25', 'b35', 'b26', 'b33', 'b19'], ['b29', 'b28', 'b20', 'b25', 'b37', 'b34'], [], ['b25', 'b29', 'b26', 'b37'], ['b27', 'b29', 'b17', 'b28', 'b25', 'b37', 'b7', 'b18', 'b33', 'b32'], ['b33', None, 'b17', 'b32']]",32,"sent1: Machine learning is used to perform the detection process of fake accounts on social media.
sent2: The majority of research studies used more than one algorithm to find the best model.
sent3: Combining 2 algorithms has been possible to increase the accuracy like a study conducted by Mohammad et al. [27].They combined a convolutional neural network (CNN) and an artificial neural networks (ANN) model.
sent4: Another was by Khaled et al. [26] which integrated an SVM with an ANN model.[31] 1 Random forest [18], [20], [28], [30], [34], [36] 6 AdaBoost [18], [20], [28], [36] 4 XGBoost [28], [36] 2 SVM[18]- [21], [26], [29], [30], [35], [38] 9 CNB[21] 1 CNN[27] 1 ANN [26], [27], [30], [38] 4 CNN-ANN[27] 1 Simple logistic regression [18], [19], [28]- [30], [33] 6 SVM-ANN [26] 1 Naïve bayes [8], [29], [33], [34], [38] 5 Decorate[18] 1 Decision tree [18], [33], [34], ["
