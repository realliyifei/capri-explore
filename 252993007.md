# A Review of Data-Driven Discovery for Dynamic Systems

CorpusID: 252993007
 
tags: #Computer_Science, #Mathematics

URL: [https://www.semanticscholar.org/paper/c0fadbb4f48da06d0a8be210e3dd34340906f45a](https://www.semanticscholar.org/paper/c0fadbb4f48da06d0a8be210e3dd34340906f45a)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | True |
| By Annotator      | (Not Annotated) |

---

A Review of Data-Driven Discovery for Dynamic Systems


Joshua S North 
Earth and Environmental Sciences
Lawrence Berkeley National Laboratory
1 Cyclotron RoadBerkeleyCA

Christopher K Wikle 
Department of Statistics
University of Missouri
146 Middlebush HallColumbiaMO

Erin M Schliep 
Department of Statistics
North Carolina State University
2311 Stinson DriveRaleighNC

A Review of Data-Driven Discovery for Dynamic Systems
Differential EquationsDynamic Equation DiscoveryProbabilistic Dynamic Equation Discovery
Many real-world scientific processes are governed by complex nonlinear dynamic systems that can be represented by differential equations. Recently, there has been increased interest in learning, or discovering, the forms of the equations driving these complex nonlinear dynamic system using data-driven approaches. In this paper we review the current literature on data-driven discovery for dynamic systems. We provide a categorization to the different approaches for data-driven discovery and a unified mathematical framework to show the relationship between the approaches. Importantly, we discuss the role of statistics in the data-driven discovery field, describe a possible approach by which the problem can be cast in a statistical framework, and provide avenues for future work.

1 Introduction the underlying laws driving a system -to reinforce their assumptions, uncover extra information about the system, or to produce a more realistic mathematical representation for the system. Historically, scientists have relied on their ability to represent physical systems using mathematical equations in the form of DEs. Dating back to at least the inference of equations describing the motion of orbital bodies around the sun based on the positions of celestial bodies (Legendre, 1806;Gauss, 1809), DEs have been used to model the evolution of complex processes (e.g., the use of susceptible, infected, recovered models for epidemics), and have become ubiquitous across virtually every area of science and engineering. Here, we review some of the methods used to discover the governing equations driving complex, potentially nonlinear, processes, often referred to as data-driven discovery.

Consider the general DE describing the evolution of a continuous process {u(s,t) : s ∈ D s ,t ∈ D t }, u t (J) (s,t) = M u(s,t), u x (s,t), u y (s,t), ..., u t (1) (s,t), ..., u t (J−1) (s,t), ω(s,t) ,

where the vector u(s,t) ∈ R N denotes the state of the system at location s and time t, u t ( j) (s,t) is the jth order temporal derivative of u(s,t), J denotes the highest order of the temporal derivative, M(·) represents the (potentially nonlinear) evolution function, and ω(s,t) represents any covariates that might be included in the system. We will denote partial derivatives by a subscript; that is ∂ u ∂ x = u x and ∂ u ∂t = u t , for example. Here, N is the number of components in the system (e.g., u(s,t) = [u(s,t, 1), u(s,t, 2), ..., u(s,t, N)] , sometimes called the system state), s ∈ {s 1 , ..., s S } = D s is a discrete location in the domain with |D s | = S, and t ∈ {1, ..., T } = D t is the realization of the system at discrete times where |D t | = T . Equation (1) is composed of partial derivatives of the system with D s ∈ R 2 and s = (x, y) (although this can be simplified to R 1 with s = x or generalized to higher dimensions) and is often referred to as a partial differential equation (PDE). Removing the spatial component from (1) results in a temporal ordinary differential equation (ODE), u t (J) (t) = M u(t), u t (1) (t), ..., u t (J−1) (t), ω(t) ,

where M is composed solely of derivatives of the components in time (i.e., no partial derivatives). This review will focus on methods to discover the evolution function M for both PDEs

(1) and ODEs (2).

The goal of data-driven discovery is to learn the governing equation(s) in (1) and (2) specifically the (non)linear function M -having only observed noisy realizations of the true process u (i.e., true derivatives are unknown). Broadly, we group the approaches used for datadriven discovery into three categories -classical sparse methods, classical symbolic methods, and deep modeling methods using either symbolic or sparse regression techniques -but recognize other categorization is possible. The first approach uses sparse regression where a library of potential solutions are proposed and the correct solution set is obtained by regularization based techniques, resulting in a sparse solution. The second uses symbolic regression where the solution is learned, or generated, through the estimation procedure. The third uses deep models to facilitate the discovery process of the previous two approaches (e.g., symbolic regression using deep models). As this is an active area of research, we refer the reader to the special issue Epureanu and Ghadami (2022) for emerging areas of research and applications.

While less common than the deterministic counterparts, methods to quantify uncertainty in the discovered equations have been proposed. However, these methods generally do not account for uncertainty in the observed data, missing a vital piece of the statistical puzzle.

We draw parallels between traditional statistical models and data-driven discovery, discussing how statistical models can be formulated for data-driven discovery and highlighting possible improvments to the methods.

The remainder of the paper is organized as follows. In Section 2 we review sparse regression methods for data-driven discovery, which are sub-categorized into deterministic and probabilistic approaches. In Section 3 we review symbolic methods for data-driven discovery. In Section 4 we review deep modeling approaches for data-driven discovery, which are sub-divided into methods approximating and discovering the underlying dynamics. In Section 5 we show how the problem can be formulated in a statistical paradigm and in Section 6 we review a possible method of data-driven discovery using a fully probablistic approach. Section 7 concludes the paper.


# Sparse Regression

Sparse regression approaches for dynamic discovery of ODEs and PDEs are fundamentally the same. We formulate the general approach using (1), noting that the approach for (2) is equivalent but with only one spatial location (i.e., S = 1). First, consider rewriting (1) as a linear (in parameters) system
u t (J) (s,t) = f (u(s,t), ...) M,
where M is a D × N sparse matrix of coefficients and f(·) is a vector-valued nonlinear transformation function of length D termed the feature library. The input of the arguments for f(·) are general and contain terms that potentially relates to the system (e.g., advection term, polynomial terms, interactions). Sparse identification seeks to identify relevant terms of M, thereby

identifying the components of f that drive the system and discovering the governing dynamics.

Denote the matrix of all data (all components at all time points) for the jth derivative of the system as
U t ( j) =          u t ( j) (s 1 , 1, 1) u t ( j) (s 1 , 1, 2) · · · u t ( j) (s 1 , 1, N) u t ( j) (s 1 , 2, 1) u t ( j) (s 1 , 2, 2) · · · u t ( j) (s 1 , 2, N) . . . . . . . . . u t ( j) (s S , T, 1) u t ( j) (s S , T, 2) · · · u t ( j) (s S , T, N)          .
The response matrix is U t (J) of size (ST ) × N and we generically denote the feature library as
F = 1, U t (0) , ..., U t (J) , U x , U y , U xx , ..., Ω .
where Ω are the associated covariates indexed in space and time and F is a (ST ) × D matrix.

The library may also contain interactions of the components, partial derivatives, and covariates.

We can write the linear system
U t (J) = FM,(3)
whereby identifying the terms of M that are non-zero, the DE is identified.

The derivatives of the system are rarely observed (i.e., only U t (0) (t) is measured). To obtain derivatives in space and time, numerical techniques are used to approximate the derivatives. There are multiple methods to approximate derivatives numerically, and the choice of approximation has the potential to impact the discovered equation (de Silva et al., 2020). Originally, a finite difference approach was suggested, but this approach is sensitive to noise (Chartrand, 2011). When measurement noise is present, data are either smoothed a priori and then derivatives are computed, or derivatives are computed using either total variation regularization (Chartrand, 2011) or polynomial interpolation (Knowles and Renka, 2012).

Due to both the numerical approximation of the derivative and the potential for noise in the observed data, (3) does not hold exactly. Instead,
U t (J) = FM + ,(4)
where i.i.d.

∼ N(0, σ 2 I N ) and σ 2 is the variance associated with the model approximation and the numerical differentiation. To induce sparsity, and thereby identify the relevant terms governing the system, solutions to (4) of the form
M = argmin M U t (J) − F M 2 2 + Pen θ ( M),(5)
are sought, where Pen θ ( M) generically denotes some penalty term based on parameters θ (i.e.,

Pen θ ( M) = λ M 1 where θ = λ for the LASSO penalty).


## Deterministic Approaches

The majority of deterministic approaches are composed of three steps -denoising and differentiation, construction of a feature library, and sparse regression. Assuming data have been properly differentiated and a library has been proposed, the deterministic approach seeks solutions of the form (5). The original sparse regression approach to data-driven discovery, Sparse

Identification of Nonlinear Dynamics (SINDy; Brunton et al., 2016), uses sequential threshold least-squares (STLS; Algorithm 1) to discover the governing terms for ODEs. While the original paper does not discuss the algorithm in terms of a penalty term, STLS has been shown to be equivalent to the 0 penalty, Pen θ ( M) = M 0 , which removes values of M less than some pre-specified threshold κ. That is, at each iteration of the minimization procedure, values of M < κ are set to zero and the remaining values of M are re-estimated.

In the original implementation, the algorithm was only iterated 10 times, but a stopping criteria (e.g., change in loss or identified parameters) could be used. In this manner, a sparse solution set is obtained.

In the literature, SINDy is illustrated on a variety of simulated ODE problems with varying amounts of noise. The examples used generally contain a lot of observations (on the order of hundreds of thousands), and it is unclear the impact of noise if a smaller number of observations is considered. In contrast to the symbolic approaches discussed in Section 3, SINDy can be fit rather quickly. However, a drawback of the approach is the sensitivity to the thresholding parameter and the dependence on the method approximating the derivative.

To extend SINDy to PDEs, Rudy et al. (2017) propose Sequential Threshold Ridge Regression (STRidge, Algorithm 2), a variant to STLS. Due to the correlation present in F for data pertaining to PDEs, STLS is insufficient at finding a sparse solution set. Instead, STRidge uses the same iterative technique as STLS, where values of M < κ are set to zero at each iteration, but with the addition of the penalty term Pen θ ( M) = λ M 2 2 . Cross-validation is then used to find the optimal values for κ and λ . The effectiveness of STRidge is shown on multiple simulated data sets with varying noise. Again, in comparison to the symbolic counterparts, the algorithm is quick, but still dependent on the method used to approximate the derivative.

STRidge can be adapted to allow for parametric PDEs by grouping terms either spatially or temporally (Rudy et al., 2019a). To incorporate parametric PDEs in 3, the coefficients now vary in space or time (i.e., M(s) or M(t)) and F is constructed as a block diagonal matrix of the appropriate form (e.g., either in space or time). Similar to the group LASSO (Meier et al., 2008), coefficients are assigned to a collection g ∈ G by grouping the same location in space over the entire time domain (e.g., g ≡ s and G ≡ D S ) or the same time point over the whole spatial domain (e.g., g ≡ t and G ≡ D T ). Within the STRidge algorithm all coefficients with the same group index are set to zero if M(g) 2 < κ. In this manner, the same dynamics are identified across space and time and only the coefficient estimate is allowed to vary in space or time. Champion et al. (2020) propose a robust unifying algorithm (Algorithm 3) for the SINDy framework based on sparse relaxed regularized regression (SR3; Zhang and Lin, 2018). SR3

introduces an auxiliary variable matrix W within the penalization term, resulting in Pen
θ ( M) = λ R(W) + 1 2ν M − W , where R(·)
is another penalization term (e.g., 1 ). The addition of the auxiliary variables provides a more favorable geometric surface to optimize (Zhang and Lin, 2018). SR3 is shown to be able to handle outliers (a potential issue when numerically differentiating noisy data), accommodate parametric formulations, and allow for physical constraints in the library.

While not discussed in detail here, there are other applications and approaches of DE discovery using sparse methods within the literature. Applying SINDy to stochastic differential equations (Boninsegna et al., 2018) and systems where the dynamics evolve on a different coordinate system (Champion et al., 2019) further increase the SINDy applicability. Instead of using finite differences or total variation regularization, Schaeffer (2017) use spectral methods to compute spatial derivatives and the Douglas-Rachford algorithm (Combettes and Pesquet, 2011) to find a sparse solution. Further consideration of highly corrupt signals (Tran and Ward, 2017), convergence properties of the SINDy algorithm , and the choice of denoising and differentiation methods (Lagergren et al., 2020) have also received treatment within the literature. For ease of use, SINDy and some related variants are available in the python package PySINDy (de Silva et al., 2020).


## Addressing Uncertainty

Bayesian and bootstrapping approaches have been proposed to quantify uncertainty in the parameters for the sparse regression formulation of data-driven discovery. These approaches seek to quantify the variability in the discovered equation and parameters of (4).


### Bayesian Approach

A penalized likelihood estimator of the form (5) can analogously be cast as the posterior mode in a Bayesian framework under the prior p(M|θ ) where Pen( M) θ = log p(M|θ ). That is, (4) can be formulated in the Bayesian framework where priors are put on M and σ 2 . Instead of an optimization procedure, the Bayesian approach aims to sample from the joint posterior
distribution p(M, σ 2 |F, U t ( j) ) ∝ p(U t ( j) |F, M, σ 2 )p(M|θ )p(σ 2 |θ )p(θ ),
where p(U t ( j) |F, M, σ 2 ) is the data likelihood (4), and p(M|θ ) and p(σ 2 |θ ) are prior distributions for M and σ 2 , respectively, and p(θ ) is the prior distribution for the hyperparameters (or penalization parameters in (5)). To enforce a sparse solution set in a Bayesian framework, a regularization prior is placed on the parameter of interest, in this case M. Further discussion comparing the sparse regression approach to a Bayesian formulation of the problem is explored by Niven et al. (2020).

Using the Bayesian framework in an algorithmic setting, Zhang and Lin (2018) propose to the classic hare-lynx population data set (Elton and Nicholson, 1942).

However, these two approaches are limited in that the uncertainty being quantified is the uncertainty in the numerical approximation of the system (i.e., the numerical differentiation and de-noising). That is, because the approximated derivative is, in fact, a single realization of the true derivative (which is unknown), the uncertainty estimates recovered by this approach are biased toward this single approximation of the derivative. A more complete treatment of the problem would be to consider the derivative as a random process and account for uncertainty in the random process. Yang et al. (2020) propose the use of Bayesian differentiable programming as a method by which to discover the dynamics and account for measurement uncertainty when estimating parameters. Generally speaking, Bayesian differentiable programming uses a numerical solver (e.g., Runge-Kutta) to predict the state at a new time, and the loss between the predicted data and observed data is used to estimate parameters. More precisely, letting M θ (u(t)) be the output of a numerical solver at time t, Bayesian differentiable programming aims to minimize
∑ u(t + ∆t) − M θ (u(t)) 2 ,
where ∆t does not need to be uniformly spaced. The parameters are estimated using Hamiltonian Monte Carlo and differentiable programming is used to compute gradients within the Hamiltonian Monte Carlo algorithm. By directly relating the observed data to the dynamics, measurement uncertainty is accounted for in the estimation procedure, providing a more thorough statistical treatment to the data-driven discovery problem. The approach is illustrated on multiple simulated ODE systems with varying amounts of measurement noise.

Bhouri and Perdikaris (2022) extend the idea of Bayesian differential programming for data-driven discovery by using Gaussian process priors on the state variables to model temporal correlations and use NuralODEs (Chen et al., 2018) to perform numerical integration. Additionally, they use the "Finnish Horseshoe prior" to impose variable shrinkage on the learned library, similar to Hirsh et al. (2021). The model parameters are estimated using Hamiltonian Monte Carlo and the No-U-Turn-Sampler (NUTS; Hoffman and Gelman, 2014) algorithm to automatically calibrate model parameters. Similar to Yang et al. (2020), the approach is illustrated on multiple simulated ODE systems in addition to the human motion capture data. Fasel et al. (2021) propose two methods of bootstrapping (3) -either sampling rows of the data (i.e., space-time sampling) or sampling library terms (i.e., columns of F). The first approach samples rows of the data with replacement and uses STRidge to estimate the parameters in the model q times. In the second approach, the columns of F are sampled without replacement to create q data sets, and again STRidge is used to estimate parameters. For both methods, the q models are then averaged and coefficients with an inclusion probability below some prespecified value are set to zero. Uncertainty is quantified by the inclusion probability and the distribution of values obtained from the q different estimates. However, as with Hirsh et al.


### Bootstrap Approach

(2021), the uncertainty associated with the observed data is not considered and the numerical approximation to the derivative is treated as an observation of the derivative, limiting uncer-tainty quantification. The method is illustrated on multiple simulated ODE and PDE systems with varying noise and applied to the classic hare-lynx population data set.


# Symbolic Regression

Symbolic regression is a type of regression that searches over mathematical expressions (e.g., +, −, ×) to find the optimal model for a given data set . This approach differs from classical regression where the model structure is fixed and a set of parameters are estimated. One of the main challenges underlying symbolic regression is that there are an infinite number of combinations of expressions that can be used to fit any particular data set.

Genetic programming is used to efficiently search over the possible model structures (Willis, 1997;Koza et al., 1993;Koza, 1994) and regression techniques are used to determine coefficient values given the model structure. Genetic programming follows Darwin's theory of evolution, selecting the "fittest" solution that is the product of generations of evolution (i.e., iterating through an algorithm). Here, we give a brief overview of genetic programming and its roll in symbolic regression and subsequently data-driven discovery of dynamics. For a more detailed overview of genetic programming, see Minnebo and Stijven (Chapter 4, 2011) and Garg and Tai (2012). or mutate themselves. Crossover is where two individuals swap sub-trees (i.e., a decision node is randomly selected from each tree and exchanged) to produce two new individuals, which is equivalent to parents producing offspring with shared genetics. Mutation is where an individuals decision node is randomly changed (e.g., plus to multiplication or plus to a variable), which is akin to a genetic mutation.

The general algorithm for genetic programming proposes an initial population, assesses the fitness of each individual, and then generates the next population based on the fittest individuals of the current population (Algorithm 4). Taking the fitness function to be based on regression where the goal is to minimize mean squared error (Schmidt and Lipson, 2009), results in symbolic regression. This basic genetic programming/symbolic regression method has generated multiple extensions (Icke and Bongard, 2013;Chen et al., 2017;Amir Haeri et al., 2017;Jin et al., 2019) and incurred extensive discussion (Korns, 2014;Nicolau and Agapitos, 2018;Ahvanooey et al., 2019).

Within the context of data-driven discovery, symbolic regression attempts to find the evolution function M(·) in (1) or (2). The difficulty is relating a proposed choice for M(·) that is generated within the genetic algorithm to derivatives of the observed data. Specifically, because derivatives of the system are unknown, either the fitness function needs to account for the derivative or the derivatives must be obtained in order to use a traditional fitness function. Bongard and Lipson (2007) were the first to apply symbolic regression to data-driven dis-covery of dynamic systems, focusing on the discovery of ODEs. In order to use symbolic regression to discover dynamic models with potentially nonlinear interactions of multiple variables, the authors introduced partitioning, automated probing, and snipping within a symbolic regression algorithm. Partitioning considers each variable in a system separately, even though they may be coupled, substantially reducing the search space of possible equations. With partitioning, a candidate equation for a single variable is integrated with the others assumed fixed.

Automated probing is where initial conditions used for temporal integration of the dynamic equation of the system are found. Last, snipping is the process of simplifying and restructuring models by replacing sub-expressions (sub-trees) in the generated population with a constant.

Using these three components, each variable in the system is integrated forward in time to produce a "test" based on the initial condition and compared to the observed data. The fitness of each potential solution is computed based on the average absolute difference between the observed data and the test. The approach is illustrated on simulated data and on two real-world examples -the classic hare-lynx system and data they collect from a pendulum. Although effective, their method is sensitive to noise in the data and has the same demanding computational requirements as other symbolic regression algorithms.

Schmidt and Lipson (2009) adopt a different approach to data-driven discovery with symbolic regression. They search over a function space constrained by a loss function dependent on partial derivatives computed from the symbolic functions and from the data. Specifically, given two variables observed over time, x(t) and y(t) (i.e., u(t) = [x(t), y(t)] ), the numerical estimate of the partial derivatives between the pair is approximated as ∆x ∆y ≈ dx dt / dy dt , where dx dt and dy dt are estimated using local polynomial fits (Thompson and Wallace, 1998). From a potential solution function (i.e., generated in the genetic algorithm), the partial derivatives can be computed using symbolic differentiation to get δ x δ y (i.e., from the symbolic function). To determine how well the potential function expresses the data, the mean log error between the approximated and symbolic partial derivatives,
− 1 N N ∑ i=1 log 1 + abs ∆x ∆y − δ x δ y ,
is used as the fitness function. While not discussed here, the approach can be extended to systems with more than two variables by looking at pairs of the variables in the system (see supplementary material of Schmidt and Lipson, 2009, for details). In this manner, they assign a fitness to each proposed individual based on how well the derivative of the system relates to the derivative of the data, resulting in data-driven discovery using symbolic regression. However, noise can be impactful because the derivatives from the observed data are approximated numerically. To accommodate measurement uncertainty, Schmidt and Lipson (2009) use Loess smoothing (Cleveland and Devlin, 1988) prior to fitting to remove the high frequency noise.

Their approach is illustrated using simulated data and data collected by motion tracked cameras, showing an ability to recover the equations on complex, real-world problems. However, similar to Bongard and Lipson (2007), the method is also computationally cumbersome with some examples reportedly taking days to converge.

Motivated by symbolic and sparse regression, Maslyaev et al. (2019) embed sparse regression within the coefficient estimation step in a symbolic regression algorithm to discover the governing equations of PDEs. In their approach, derivatives of the data are computed a priori using finite difference (in the same manner as sparse regression discussed in Section 2) and used as the response in symbolic regression. Within the symbolic regression algorithm, after a population has been proposed, sparse regression using an 1 penalty is employed, the fitness of each individual in the population is assessed, and mutation, crossover, and replication are performed in the usual manner. Because derivatives are computed before the estimation procedure, they are able to be incorporated into the function set. This allows for the discovered equations to contain spatial derivatives. The approach is tested on multiple simulated PDEs with varying amounts of measurement noise. However, the robustness to measurement noise is dependent on the numerical method used to approximate the derivative, and it is unclear how this impacts model results. Additionally, while specifics are not given, the approach is computationally cumbersome, owing in part to the symbolic regression.


# Deep Models

Deep modeling has been considered for data-driven dynamic discovery in two different waysapproximating dynamics and learning dynamics. Approximating dynamics using deep models provides a computationally cheap method to generate data from complex systems while still preserving physical aspects of the system (i.e., emulation). While this review is concerned with the discovery of the governing equations and refers to "data-driven discovery" as the discovery of the functional form of the governing system, deep models approximating the dynamics are an important part of the literature and we devote a section to them. Deep models coinciding with our definition of data-driven discovery have also been developed. There are multiple approaches by which dynamics can be approximated and subsequently learned, which we also discuss.


## Approximating Dynamics with Deep Models

One method of approximating dynamics considers a so-called physics-informed neural network where MSE u is the mean squared error of the neural network approximating u(s,t) and MSE g = 1 N g ∑ N g i=1 g(s i ,t i ) 2 is the mean squared error associated with the structure imposed by g(·). In this manner, the neural network obeys the physical constraints imposed by g(·).

Neural networks have also been used to approximate the evolution operator M using a residual network (ResNet). Reframing the problem according to the Euler approximation U(t + ∆t) ≈ U(t) + ∆tM(U(t)), the goal is to find a suitable approximation for M(), there-by approximating the dynamics. In contrast to PINN, physics are not incorporated into the NN and the structure of the NN is dependent completely on the data. Applying the problem to ODEs, Qin et al. (2019) show how a recurrent ResNet with uniform time steps (i.e., uniform ∆t) and a recursive ResNet with adaptive time steps can be used to approximate dynamics. This approach is further extended to PDEs (Wu and Xiu, 2020), where the evolution operator is first approximated by basis functions and coefficients, and a ResNet is fit to the basis coefficients.

While not described in detail here, there are other approaches to approximating DE using deep models. Physics-informed candidate functions can be used with numerical integration in an objective function to restrict the temporal evolution of a NN (Sun et al., 2019). NN have also been used to approximate parametric PDEs (Khoo et al., 2021), represent molecular dynamics (Mardt et al., 2018), learn turbulent dynamical systems (Qi and Harlim, 2022), and approxi- 


## Discovering Dynamics with Deep Models

Deep modeling using neural networks (NNs) has become increasingly popular in recent years due to NNs being a universal approximator (Hornik et al., 1989). Additionally, computing derivatives of NNs is possible through automatic differentiation (e.g., using PyTorch; Paszke et al., 2017). Assuming a surface can be approximated using a NN, derivatives of the surface in space or time or both are obtainable. This approach, where derivatives are computed using NN, is used in many of the deep model approaches to data-driven discovery.


### Deep Models with Sparse Regression

A common issue with data-driven discovery in the "classical" sparse regression approach is the sensitivity to noise when approximating derivatives numerically. To address this issue, Both et al. (2021) propose using a NN to approximate the system, and then perform sparse regression within the NN. Let U be the output of a NN and construct F in (3) using U and derivatives computed from U via automatic differentiation. The NN is trained using the loss
function L = 1 ST ∑ |U − U| 2 + 1 ST ∑ |FM − U t (J) | 2 + λ ∑ |M|.
After training the NN and estimating parameters, most terms of M are still nonzero (but very close to zero), and a thresholding is performed to obtain the final sparse representation.

Through this formulation of the problem whereby derivatives are obtained from a NN, Both et al. (2021) show their ability to recover highly corrupt signals from traditional PDE systems and apply their approach to a real-world electrophoresis experiment.


### Deep Models with Symbolic Regression

Using symbolic regression with a neural network for data-driven discovery has gained popularity in recent years. In a series of papers, Xu et al. (2019Xu et al. ( , 2020Xu et al. ( , 2021) construct a deep-learning genetic algorithm for the discovery of parametric PDEs (DLGA-PDE) with sparse and noisy data. DLGA-PDE first trains a NN that is used to compute derivatives and generate meta-data (global and local data), thereby producing a complete de-noised reconstruction of the surface (i.e., noisy sparse data are handled through the NN). Using the local metadata produced by the NN, a genetic algorithm learns the general form of the PDE and identifies which parameters vary spatially or temporally. At this step, the coefficients may be incorrect or missrepresent the system because the global structure of the data is not accounted for. To correct the coefficient estimates, a second NN is trained using the discovered structure of the PDE and the global metadata. Last, a genetic algorithm is used to discover the general form of the varying coefficients.

One method of implementing symbolic regression within a deep model is to allow the activation functions to be composed of the function set instead of classic activation functions (e.g., sigmoid or ReLU; Martius and Lampert, 2016;Sahoo et al., 2018;Kim et al., 2021). Motivated by this idea, Long et al. (2019) propose a symbolic regression NN, SymNet. Similar to a typical NN, the th layer of SymNet is
f = W [f 0 , f −1 ] + b ,
where f 0 is the function set that contains partial derivatives (e.g., f 0 = [u, u x , u y , ...]). In this manner, each subsequent layer adds a dimension to the activation function based on the previous layer, allowing the construction of complex functions. Following Long et al. (2017), spatial derivatives are computed using finite-difference via convolution operators. To model the time dependence of PDEs, they employ the forward Euler approximation, termed a δt-block, as
U(t + δt) ≈ U(t) + δt · SymNet k m (u, u x , u y , ...),
where δt is the temporal discritization, and SymNet k m (u, u x , u y , ...) has k hidden layers (i.e., = 0, ..., k) and m variables (i.e., number of arguments u, u x , u y , ...). In order to facilitate longterm predictions, they train multiple δt-blocks as a group so the system has long-term accuracy.

Distinct from the previous two approaches, Atkinson et al. (2019) incorporate differential operators into the function set of a genetic algorithm. They train the NN on the observed data and supply the NN to a genetic algorithm where the function set contains typical operators (e.g., addition, multiplication) and differential operators. The differential operators are computed from the NN using PyTorch (Paszke et al., 2017), enabling the inclusion of derivatives in the search space of the genetic algorithm.


# Physical Statistical Models

To account for observational uncertainty and missing data when modeling complex non-linear systems, dynamic equations (DE) parameterized by ordinary and partial differential equations have been incorporated into Bayesian hierarchical models (BHM). While there are various methods by which to model DE in a probabilistic framework, here we focus on physical statistical models (PSM; Berliner, 1996;Royle et al., 1999;Wikle et al., 2001) due to the similarities with data-driven discovery that will soon become apparent. Broadly, PSM are a class of BHMs where scientific knowledge about some process is known and incorporated into the model structure.

PSMs are generally composed of three modeling stages -data, process, and parameter models -where dynamics are modeled in the process model and the observed data are modeled conditioned on the latent dynamics. That is, the observed data are considered to be a noisy realization of the "true" latent dynamic process. This formulation results in the data being described conditionally given the process model, simplifying the dependence structure in the data model and enabling complex structure to be captured in the process stage. The evolution of the latent dynamic process is then parameterized by a DE, incorporating physical dynamics into the modeling framework.

Consider the R(t) × 1 observed data vectors V(t) ≡ [v(r 1 ,t), ..., v(r R(t) ,t)] and {v(r,t) : r ∈ D s ,t ∈ D t } where r ∈ {r 1 , ..., r R(t) } ⊂ D s is a discrete location in the spatial domain D s , and t ∈ {1, ..., T } ⊂ D t is the realization of the system at discrete times in some temporal window D t .

Assume we are interested in the latent "true" dynamic process {u(s,t) : s ∈ D s ,t ∈ D t } where s 1 ,t), ..., u(s S ,t)] is a length S vector. It is common that the observation locations do not coincide with the process (e.g., due to missing data or mismatch in resolution). In the case of missing observations, the observed data are mapped to the latent process using an incidence matrix H(t), which is a matrix of zeros except for a single one in each row corresponding the the observation associated with a process location (see Chapter 7 of Cressie and Wikle, 2011, for examples of H(t)). The general data model for time t is
U(t) ≡ [u(V(t) = H(t)U(t) + η(t),
where H(t) ∈ R L(t)×N and uncertainty in the observations of the process are captured by η(t)
indep. ∼ N L(t) (0, Σ V (t)) where Σ V (t) is the covariance matrix.
The dynamic process is characterized through the specification of the evolution of U(t) over time. For example, the process model, which specifies this evolution under a first-order Markov assumption, is given as
U(t) = M(U(t − 1), θ) + (t),(6)
where M(·) is some (non)linear function relating a previous space-time location (or multiple locations) to the next, θ are parameters associated with M, and (t)
i.i.d.
∼ N(0, Σ U ) is a mean zero Gaussian process with variance/covariance matrix Σ U . While not discussed here, the error term (t) can be considered multiplicative (see Chapter 7 of Cressie and Wikle, 2011, for more detail).

Physical dynamics are encoded through the parameterization of M. Here, we consider physical dynamic parameterizations (i.e., ODEs and PDEs), but a general autoregressive structure for M (i.e., not parameterized with differential equations) can also be considered. Consider the general PDE
U t (t) = M(U(t), θ),
analogous to the motivating PDE (1), which can be approximated using finite differences
U(t) = U(t − 1) + ∆tM(U(t − 1), θ),
where ∆t is the difference in time between time t and t − 1 and θ are parameters associated with the PDE. Because the finite difference approximation can be written as a linear system, we can write
U(t) = MU(t − 1),(7)
where M is a sparse matrix derived from the finite difference scheme. Replacing (6) with (7), the process model parameterized by a linear finite difference equation is
U(t) = MU(t − 1) + (t),
where (t) may now account for approximation error due to the finite difference approximation.

As a clarifying example, assume a spatio-temporal process U(x,t) in one-dimensional space 0 ≤ x ≤ L and time t. Assume the process is approximated by the diffusion equation t) where b is a diffusion constant and the boundary conditions Y (0,t) = U 0 and U(L,t) = U L and initial condition {U(0,t) : 0 ≤ x ≤ L} are known. Using numerical analysis, the time derivative can be approximated using the forward difference
U t (x,t) = bU xx (x,U t (x,t) ≈ U(x,t + ∆t) −U(x,t)
∆t , and the spatial derivative can be approximated by the central difference
U xx (x,t) ≈ U(x + ∆x,t) − 2U(x,t) +U(x − ∆x,t) ∆x 2 .
Using the finite difference approximation, we can reformulate the diffusion equation as
U(x,t + ∆t) ≈ U(x,t) + b∆t ∆x 2 (U(x + ∆x,t) − 2U(x,t) +U(x − ∆x,t)) .
Assuming three internal spatial locations, x 1 , x 2 , x 3 and boundary locations x 0 , x L , let U(t) =

[U(x 1 ,t),U(x 2 ,t),U(x 3 ,t)] and U b (t) = [U(x 0 ,t),U(x L ,t)] . Then,
U(t + ∆t) ≈       1 − 2b∆t ∆x 2 b∆t ∆x 2 0 b∆t ∆x 2 1 − 2b∆t ∆x 2 b∆t ∆x 2 0 b∆t ∆x 2 1 − 2b∆t ∆x 2       U(t) +       b∆t ∆x 2 0 0 0 0 b∆t ∆x 2       U b (t),
which can be written more compactly as
U(t + ∆t) ≈ MU(t) + M b U b (t).
Thus, the PDE dynamics have been "encoded" into the structure of the transition operator, M. In most PSM implementations, the (banded) structure of M is retained, but the specific elements are estimated from the data, rather than given by the finite difference representation. This adds flexibility and explicitly assumes that the PDE is not an exact representation of the data. Note that other PDE representations, such as finite element, or spectral, can be used to motivate such models.

This simple example can be made more complex by considering a parametric diffusion equation (i.e., resulting in M(θ) instead of M) or by placing priors on the boundary conditions and or the initial condition (see Cressie and Wikle, 2011, for details). Additionally, there are certain numerical conditions that need to be satisfied in order to guarantee numerical stability from the approximation, which can vary based on the system and approximation scheme considered (e.g., see CFL condition in Higham et al., 2016). For a more complete overview of PSMs and possible parameterizations, see Berliner (2003), Cressie andWikle (2011), Kuhnert (2017), and references within.

PSMs have been used to study a variety of real-world systems. PSMs parameterized using shallow-water equations (Wikle, 2003) and the Rayleigh friction equation (Milliff et al., 2011) have been used to study ocean surface winds. Using a parametric diffusion equation (Wikle, 2003) and parametric reaction-diffusion equation (Hooten and Wikle, 2008), PSMs have modeled the spread of invasive avian species. PSMs can be grouped into a larger category of models called general quadratic nonlinear model (GQN; Wikle and Hooten, 2010;Wikle and Holan, 2011;Gladish and Wikle, 2014), which accommodate multiple classes of scientific-based parameterization such as PDEs and integro-difference equations.


## General Quadratic Nonlinear Models

General quadratic nonlinear models provide a nice generalization to the PSM framework and, as discussed in the subsequent section, provide an interesting link between data-driven discovery methods and PSMs. The general GQN model is
u(s i ,t) = S ∑ j=1 a i j u(s j ,t − 1) + S ∑ k=1 S ∑ l=1 b i,kl u(s k ,t − 1)g(u(s l ,t − 1); θ) + ε(s i ,t),(8)
for i = 1, ..., S, where a i j are linear evolution parameters, b i,kl are nonlinear evolution parameters, g() is some transformation function of u(t − 1) dependent on parameters θ, and ε(s i ,t) is an error process. The motivation here is that many real-world mechanistic processes have been described by PDEs that have quadratic (nonlinear) interactions, often where the interaction of system components consists of the multiplication of one component times a transformation of another (see Wikle and Hooten, 2010, for details).

Equation (8) can be condensed in matrix form as
U(t) = AU(t − 1) + (I S ⊗ g(U(t − 1); θ) )BU(t − 1) + (t),(9)
where A and B are matrices constructed from a i j and b i,kl , respectively, and I S is a size S identity matrix (see Wikle and Hooten, 2010, for specific details). From (9), we see that letting 
M(U(t − 1), θ) = AU(t − 1) + (I S ⊗ g(V(t − 1); θ) )BU(t − 1

## Relation to Data-Driven Discovery

While unexplored in the literature, there is a strong connection between PSMs (particularly, the more general GQNs) and data-driven discovery. Formulating a BHM where the latent process evolves according to the generic PDE (1), the two-stage data-process model for location s and
time t is v(s,t) = H(s,t)u(s,t) + (s,t) u t (J) (s,t) = M(u(s,t), u x (s,t), ...) + (s,t),
where (s,t) ∼ N(0, Σ V (s,t)) is the measurement error process with Σ V (s,t) a variance/covariance matrix, (s,t) ∼ N(0, Σ U (s,t)) the process model error process with Σ U (s,t) a variance/covariance matrix. However, as discussed in Section 5, PSMs rely on M to be parameterized according to known dynamics. Instead, borrowing the notion of a feature library from the sparse regression approach to data-driven discovery, linearizing the process model results in a matrix of coefficients M and a feature library f(·). Given the feature library, the goal is to find the correct values of M (as in sparse regression). In the case of GQN, we rarely need the whole set of quadratic interactions, so the "discovery" is selecting which quadratic components are needed to describe the data.

As an example, consider two approaches that can be used to incorporate dynamic discovery into PSMs -employing a finite difference scheme or using (4) 

where M represents the coefficients associated with the finite difference and the discovered equation. Directly incorporating (4) in the process model results in
v(s,t) = H(s,t)u(s,t) + (s,t) u t (J) (s,t) = Mf(u(s,t), u x (s,t), ...) + (s,t),(11)
where now the temporal derivative is directly related to a library of potential functions and M represents the coefficients associated only with the discovered equation.

The benefit of formulating the problem using (10) is that a Kalman filter or ensemble Kalman filter can be used to estimate parameters (see Stroud et al. 2018, Katzfuss et al. 2020, and Pulido et al. 2018 for examples of the Kalman filter with dynamic systems in statistics).

Additionally, as mentioned previously, the GQN framework naturally provides a construction of an over-parameterized library of potential dynamical interactions into the library. However, interpreting parameters can be difficult and incorporating spatial derivatives into the library is not as straightforward as with traditional PSMs. In contrast, (11) has a very clear interpretation of parameters but requires a method to relate the previous state to the current state (e.g., numerical differentiation scheme). Additionally, model estimation will rely on Metropolis-Hastings

Monte-Carlo as the Markov assumption required for Kalman filter and EnKF methods is violated. For both approaches, parameter shrinkage or variable selection or both will need to be employed on M to produce a sparse solution set. The field of Bayesian variable selection is quite large and there are a variety of priors that can be used (see George et al., 1993;Park and Casella, 2008;Carvalho et al., 2010;Li and Lin, 2010, for possible choices)

Assuming model estimation is possible, formulating the problem using either (10) or (11) provides significant contributions to the data-driven discovery. In contrast to the sparse regression approaches with uncertainty quantification discussed in Section 2.2.1, (10) and (11) treat the latent process u(s,t) as a random process and do not disregard the measurement noise when estimating the system. That is, instead of computing derivatives and de-noising prior to model estimation, uncertainty in the derivatives as a product of measurement noise is accounted for.

This makes estimation more challenging as the derivatives are no longer assumed known a priori. Additionally, missing data can be handled through the incidence matrix H. By formulating the problem within a BHM, known methods accounting for missing data can be used, providing more real-world applicability than the deterministic counterparts.


# Bayesian Dynamic Discovery

In a sequence of papers, North et al. (2022a,b) propose a fully probabilistic Bayesian hierarchical approach to data-driven discovery of dynamic equations. Similar to the sparse regression approached discussed in Section 2, the Bayesian approach uses a library of potential functions to identify the governing dynamics. However, in contrast to the methods discussed in Sections 2, 3, and 4, the dynamic system is modeled as a random process and assumed latent. Specifically, North et al. (2022a,b) use the approach detailed by (11), where the process model in the BHM,
u t (J) (s,t) = Mf(u(s,t), u x (s,t), ...) + (s,t),(12)
directly relates the time derivative of the dynamic system to a library of potential functions.

In its most general form, (12) has three dimensions, space (S), time (T ), and the number of components (N), and can be represented as a tensor -a higher-order representation of a matrix (see Kolda, 2006, for a details). Let U = {u(s,t, n) : s ∈ D s ,t = 1, ..., T, n = 1, ..., N}

where U ∈ R S×T ×N is the tensor of the dynamic process. Similarly, let F ∈ R S×T ×D be the tensor of the function f(·) evaluated at each location in space-time and η ∈ R S×T ×N the spacetime-component uncertainty tensor. The process model can represented compactly using tensor notation as
U t (J) = F × 3 M + η.(13)
While not explicitly stated, F is still a function of the state process U and its derivatives.

The process U can be defined using a finite collection of spatial, temporal, and component basis functions. Let
U ≈ P ∑ p=1 Q ∑ q=1 R ∑ r=1 a(p, q, r)ψ(p) • φ(q) • θ(r) = A × 1 Ψ × 2 Φ × 3 Θ := [[A; Ψ, Φ, Θ]],
where A ∈ R P×Q×R , Ψ ∈ R S×P , Φ ∈ R T ×Q , and Θ ∈ R N×R . Here, Ψ, Φ, and Θ are matrices of spatial, temporal, and component basis functions, respectively, and A is a tensor of basis coefficients (traditionally called the core tensor). Defining Ψ and Φ to be matrices of basis functions differentiable up to at least the highest order considered in (1), derivatives of U can be obtained analytically by computing derivatives of the basis functions. For example
∂ 3 ∂ x∂ y∂t U = U xyt = A × 1 Ψ xy × 2 Φ t × 3 Θ = [[A; Ψ xy , Φ t , Θ]].
Defining the compact tensor representation of the dynamic process (13) using the basis decomposition, we can write
[[A; Ψ, Φ t (J) , Θ]] = F × 3 M + η,(14)
where η may now include truncation error. While not explicitly stated, the arguments of F are now Ψ, Φ, Θ, A, and appropriate derivatives of Ψ and Φ.

Taking the mode-3 matricization of (14) -the flattening of a tensor to a matrix (see Kolda, 2006, for detail on this operation) -yields ΘA(φ t (J) (t) ⊗ ψ(s)) = Mf(A, ψ(s), ψ x (s), ψ y (s), ψ xy (s), ..., φ t (0) (t), ..., φ t (J) (t), ω(s,t)) + η(s,t),

where A is a R × PQ matrix of basis coefficients, ψ(s) is a length-P vector of spatial basis functions, φ(t) is a length-Q vector of temporal basis functions, and Θ is a N × R matrix of component basis functions (see North et al., 2022b, for a detailed explanation). This form is convenient because only A (or A) needs to be estimated in order to fully define the dynamic process and its derivatives as opposed to requiring all spatial, temporal, or spatio-temporal derivatives of the process,

Equation (1) can be extended by rewriting the left-hand side (LHS) to accommodate spatiotemporal derivatives of the process (e.g., ∇ 2 u t (s,t) = u xxt (s,t)+ u yyt (s,t)), which is common in fluid dynamics (see Higham et al., 2016, for examples). Specifically, consider the more general PDE g(u t (J) (s,t)) = M u(s,t), u x (s,t), u y (s,t), ψ xy (s), ..., u t (1) (s,t), ..., u t (J−1) (s,t), ω(s,t) , (15) where g(·) is some linear differential operator. The basis formulation of (15) is ΘA(φ t (J) (t) ⊗ g(ψ(s))) = Mf(A, ψ(s), ψ x (s), ψ y (s), ψ xy (s), ..., φ t (0) (t), ..., φ t (J) (t), ω(s,t)) + η(s,t), ΘA(φ t (J) (t) ⊗ g(ψ(s))) = Mf(A, ψ(s), ψ x (s), ψ y (s), ψ xy (s), ..., φ t (0) (t), ..., φ t (J) (t), ω(s,t)) + η(s,t),

where (s,t)

indep.

∼ N L(s,t) (0, Σ V (s,t)) and η(s,t)

i.i.d.

∼ N N (0, Σ U ).

Model parameters are estimated by sampling from their full-conditional distributions using Markov chain Monte Carlo, requiring the specification of prior distributions. Here, we provide a brief summary of the model priors, but for complete model specification, we refer the reader to North et al. (2022a) and North et al. (2022b). Standard diffuse priors can be assigned to Σ V (s,t) and Σ U . In order to induce sparsity in M, the spike-and-slab prior (Mitchell and Beauchamp, 1988;George et al., 1993) is used, where a latent indicator variable γ = [γ 1 , ..., γ D ]

denotes if an element of M is included in the discovered equation or not. A constant issue in discovering equations for PDE systems is multicollinearity in the library. See North et al.

(2022b) for a subsampling approach proposed to mitigate the impacts of multicollinearity. Last, the elastic net prior (Li and Lin, 2010) is assigned to A to help regularize the basis coefficients.

Because A is embedded in the nonlinear function f(·), estimation can be problematic (see North et al., 2022b, for more detail). In order to provide a conjugate updating scheme and reduce computation time, A is sampled using an adapted version of stochastic gradient descent (SGD) with a constant learning rate (SGDCL; Mandt et al., 2016).

While the Bayesian dynamic discovery model proposed in this section relies on more model assumptions and parameters to estimate compared to the methods discussed in Section 2, the benefit is a fully probabilistic discovery of the dynamic system. The latent variable γ provides an inclusion probability for each element of M, enabling a researcher to identify a model based on their own desired confidence (i.e., a model identified where each component is included with at least 50% probability). Additionally, uncertainty quantification can be obtained for the dynamic system u(s,t) and all of its subsequent derivatives given the full posterior distribution of A


# Discussion

While relatively young, the field of data-driven discovery is expanding quickly. Areas currently under-studied include methods that properly account for uncertainty quantification and missing data and applications to real-world data sets (see Table 1). BHMs can address these issues, however they rely on the same assumptions as the sparse regression approach -the library is pre-specified. Relaxing the pre-specified library assumption while retaining the benefits of the statistical approach promises to be a major improvement in the data-driven discovery realm. To this aim, one promising approach is the recent extension in symbolic regression to the Bayesian framework (Jin et al., 2019). The incorporation of Bayesian symbolic regression into a BHM could provide the next step to a truly user-free, unbiased, method at data-driven discovery.

Recent advances in deep modeling such as embedding NNs in the BHM (Zammit-Mangion the model output. Additionally, methods combining the model output with in situ and remotely sensed measurements, where the spatial and temporal domains may be different (e.g., change of support problem), could provide an extension to not only data-driven discovery but also for change-of-support related methods.

A final direction for future work we discuss are methods that can accommodate mixed data types. For example, a predator-prey system that takes into account the vegetation coverage of the prey's spatial domain. Here, vegetation coverage, a positive continuous variable, is dependent on the number of prey, a positive integer valued variable, which in turn is dependent on the number of predators, another positive integer valued variable. In a system such as this, the vegetation coverage, which is a positive continuous variable, is dependent on the number of prey, a positive integer valued variable, which in turn is dependent on the number of predators, another positive integer valued variable. Methods that are able to discover the dynamics of a system with various observed data type provides a much wider range for real-world applications.


using the priors p(m d |α d ) = ∏ D d=1 N(0, α −1 d ), p(σ 2 ) = IG(a s , b s ), and p(α −1 d ) = IG(a a , b a ), where N is the normal (Gaussian) distribution and IG is the inverse Gamma distribution. They estimate the parameters using a threshold sparse Bayesian regression algorithm, which maximizes the marginal likelihood instead of sampling from the full conditional distributions as in Markov chain Monte Carlo. Their algorithm uses a hard thresholding parameter, similar to the deterministic sparse regression approaches, where at each iteration, values of the posterior M(i, j) < κ are set to zero. Their procedure assigns what they term "error bars" to their parameter estimates based on the ratio of the estimate for the posterior variance to the estimate for the posterior mean squared. Zhang and Lin (2018) consider many of the same simulated ODEs and PDEs used to illustrate the deterministic approaches and provide error bars to the parameter estimates for these systems with varying amounts of measurement noise. As an interesting application, Zanna and Bolton (2020) use this framework to discovery unknown equations in ocean mesoscale closures. Hirsh et al. (2021) explore the use of two common Bayesian selection priors on system discovery and uncertainty quantification -the continuous spike and slab (i.e., stochastic search variable selection (SSVS); Mitchell and Beauchamp, 1988; George et al., 1993; George and McCulloch, 1997), and the regularized horseshoe (Carvalho et al., 2010; Piironen and Vehtari, 2017) -calling the approach uncertainty quantification SINDy (UQ-SINDy). Their choice of priors are distinct in that SSVS is a mixture of two continuous mean zero Gaussian distributions and the horseshoe is part of the global-local shrinkage prior family. For the SSVS prior, variables that are not to be included in the model are sampled from a mean zero Gaussian distribution with a small variance, rendering their effect on inference negligible, and variables that are to be included are sampled from a mean zero Gaussian distribution with a larger variance. The posterior inclusion probability for a variable is the number of times it was sampled from the Gaussian with a large variance over the total number of samples. In contrast, the horseshoe prior has a hyper-prior performing global shrinkage on all variables in conjunction with individual hyper-priors performing individual shrinkage. To determine the probability a variable is included under the regularized horseshoe, the ratio of each element of the estimate of M with no prior and with the horseshoe prior is computed, providing pseudo-inclusion probabilities (i.e., not necessarily bounded by 0 and 1). Using both of these priors, Hirsh et al. (2021) provide inclusion probabilities for multiple simulated ODE systems with varying amounts of noise and

## Figure 1 :
1Genetic programming relies on a predefined function set of mathematical expressions. For symbolic regression, the function set typically consists of basic mathematical expressions such as addition, multiplication, and trigonometric terms (seeNicolau and Agapitos, 2018, for details on function set choice). Possible model solutions are constructed using a combination of functions from the function set and encoded in a tree structure(Figure 1). Within the tree, the mathematical expressions are the decision nodes and input data passed into the mathematical expression are the terminal nodes. To make the searchable space smaller, the maximum node size of the tree can be specified. A population of potential solutions is composed of individual potential solutions. The ability of an individual to properly represent data is determined based on the fitness function, which is analogous to an objective or loss function in statistics. Individuals can then reproduce to create a copy of themselves, crossover with another individual, Symbolic representation of f (X,Y ) = X 10 +Y * 1.2 cos(X).

## (
PINN;Raissi et al., 2017a,b;Raissi, 2018;Raissi et al., 2019Raissi et al., , 2020. PINNs are applicable to both continuous and discrete time models, yet we discuss only the continuous version here.Define g(s,t) = u t (J) (s,t) + M (u(s,t), u x (s,t), ...) ,where the form of M is assumed known. Approximating u(s,t) with a neural network results in the PINN g(s,t), where the derivatives associated with the PINN are computed using automatic differentiation. The neural network is trained using the loss function MSE = MSE u + MSE g


mate ODEs with time-varying measurement data. Using deep models to learn the time-stepping scheme of DE,Rudy et al. (2019b) andLiu et al. (2022) independently show how the approximation of the dynamics are improved when dynamic time-stepping is accounted for in the estimation proceedure. Wikle and Zammit-Mangion (2022) review methods using deep learning for spatial processes and for approximating spatio-temporal DEs.


) recovers the PSM model. The GQN framework is very flexible, due in part to the overparameterization of the model from all possible quadratic interactions. To constrain the parameter space, thereby learning which dynamic interactions are important, either physics-informed priors or strong shrinkage priors are used. For examples on what these constraints may be and their underlying physical motivation, see Wikle and Hooten (2010).


for the process model -each of which have their own pros and cons. The finite difference approach results in the same model as in Section 5, v(s,t) = H(s,t)u(s,t) + (s,t) u(s,t) = Mf(u(s,t − 1), u x (s,t − 1), ...) + (s,t),

## ∼
N N (0, Σ U ) in space and time (see North et al., 2022b, for a details). This results in the general BHM for location s and time t v(s,t) = H(s,t)ΘA(φ t (0) (t) ⊗ ψ(s)) + (s,t)
et al., 2021) could also be explored. To this aim, symbolic regression using a NN can be combined with a BHM, providing an alternate method of joining the approaches.Real-world data come from a variety of sources such as gridded model output (e.g., reanalysis models), in situ observations, and remotely sensed (e.g., satellite) measurements. While gridded model output is convenient because it is generally complete and spatially and temporally continuous, the discovered dynamics are biased due to the nature of how the data product is constructed. Conversely, in situ and remotely sensed measurements, which are a more direct and unbiased observation of the true dynamic process, are more difficult to use because of they can be missing data and the observations are generally inconsistent in space and time. Discovery methods that can either use in situ measurements, remotely sensed measurements, or both would be beneficial in that the discovered dynamics would be of the "true" process and not of
Input: K, κ, λ ,tolerance; /* prox is the proximal gradient */ err = W k − W k−1 /ν; end Algorithm 4: General Genetic Algorithm Input: Stopping criteriaξ , function set, fitness function -f (), summary statistic -T () Result: Best individual Initialize: P = Randomly generate the initial population based on the defined functional set,; /* Evaluate fitness of current individuals */ P = Generate new population based on reproduction, crossover, and mutation where individuals are chosen based on fitness level (i.e., higher fitness equals higher probability of being chosen) ; ∆ N = f (P) ; /* Evaluate fitness of new individuals */ end
A survey of genetic programming and its applications. M T Ahvanooey, Q Li, M Wu, Wang , S , KSII Transactions on Internet and Information Systems. 134Ahvanooey, M. T., Li, Q., Wu, M., and Wang, S. (2019). A survey of genetic programming and its applications. KSII Transactions on Internet and Information Systems, 13(4):1765-1794.

Statistical genetic programming for symbolic regression. M Amir Haeri, M M Ebadzadeh, G Folino, Applied Soft Computing Journal. 60Amir Haeri, M., Ebadzadeh, M. M., and Folino, G. (2017). Statistical genetic programming for symbolic regression. Applied Soft Computing Journal, 60:447-469.

S Atkinson, W Subber, L Wang, G Khan, P Hawi, R Ghanem, arXiv:1910.05117Data-driven discovery of free-form governing differential equations. arXiv preprintAtkinson, S., Subber, W., Wang, L., Khan, G., Hawi, P., and Ghanem, R. (2019). Data-driven discovery of free-form governing differential equations. arXiv preprint arXiv:1910.05117, pages 1-7.

Hierarchical Bayesian time series models. L M Berliner, Maximum Entropy and Bayesian Methods. Netherlands, DordrechtSpringerBerliner, L. M. (1996). Hierarchical Bayesian time series models. In Maximum Entropy and Bayesian Methods, pages 15-22. Springer Netherlands, Dordrecht.

Physical-statistical modeling in geophysics. L M Berliner, Journal of Geophysical Research: Atmospheres. D24108Berliner, L. M. (2003). Physical-statistical modeling in geophysics. Journal of Geophysical Research: Atmospheres, 108(D24).

Gaussian processes meet NeuralODEs: a Bayesian framework for learning the dynamics of partially observed systems from scarce and noisy data. M A Bhouri, P Perdikaris, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 380Bhouri, M. A. and Perdikaris, P. (2022). Gaussian processes meet NeuralODEs: a Bayesian framework for learning the dynamics of partially observed systems from scarce and noisy data. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engi- neering Sciences, 380(2229).

Automated reverse engineering of nonlinear dynamical systems. J Bongard, H Lipson, Proceedings of the National Academy of Sciences. 10424Bongard, J. and Lipson, H. (2007). Automated reverse engineering of nonlinear dynamical systems. Proceedings of the National Academy of Sciences, 104(24):9943-9948.

Sparse learning of stochastic dynamical equations. L Boninsegna, F Nüske, C Clementi, The Journal of Chemical Physics. 14824241723Boninsegna, L., Nüske, F., and Clementi, C. (2018). Sparse learning of stochastic dynamical equations. The Journal of Chemical Physics, 148(24):241723.

DeepMoD: Deep learning for model discovery in noisy data. G.-J Both, S Choudhury, P Sens, R Kusters, Journal of Computational Physics. 4281109985Both, G.-J., Choudhury, S., Sens, P., and Kusters, R. (2021). DeepMoD: Deep learning for model discovery in noisy data. Journal of Computational Physics, 428(1):109985.

Discovering governing equations from data by sparse identification of nonlinear dynamical systems. S L Brunton, J L Proctor, J N Kutz, Proceedings of the National Academy of Sciences. 11315Brunton, S. L., Proctor, J. L., and Kutz, J. N. (2016). Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the National Academy of Sciences, 113(15):3932-3937.

The horseshoe estimator for sparse signals. C M Carvalho, N G Polson, J G Scott, Biometrika. 972Carvalho, C. M., Polson, N. G., and Scott, J. G. (2010). The horseshoe estimator for sparse signals. Biometrika, 97(2):465-480.

Data-driven discovery of coordinates and governing equations. K Champion, B Lusch, J N Kutz, S L Brunton, Proceedings of the National Academy of Sciences. the National Academy of Sciences116Champion, K., Lusch, B., Kutz, J. N., and Brunton, S. L. (2019). Data-driven discovery of coordinates and governing equations. Proceedings of the National Academy of Sciences, 116(45):22445-22451.

A unified sparse optimization framework to learn parsimonious physics-informed models from data. K Champion, P Zheng, A Y Aravkin, S L Brunton, J N Kutz, IEEE Access. 8Champion, K., Zheng, P., Aravkin, A. Y., Brunton, S. L., and Kutz, J. N. (2020). A unified sparse optimization framework to learn parsimonious physics-informed models from data. IEEE Access, 8:169259-169271.

Numerical differentiation of noisy, nonsmooth data. R Chartrand, ISRN Applied Mathematics. Chartrand, R. (2011). Numerical differentiation of noisy, nonsmooth data. ISRN Applied Math- ematics, 2011:1-11.

Feature selection to improve generalization of genetic programming for high-dimensional symbolic regression. Q Chen, M Zhang, B Xue, IEEE Transactions on Evolutionary Computation. 215Chen, Q., Zhang, M., and Xue, B. (2017). Feature selection to improve generalization of ge- netic programming for high-dimensional symbolic regression. IEEE Transactions on Evolu- tionary Computation, 21(5):792-806.

Neural ordinary differential equations. R T Q Chen, Y Rubanova, J Bettencourt, D K Duvenaud, H Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, R Garnett, Advances in Neural Information Processing Systems. Bengio, S.,Curran Associates, Inc31Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. (2018). Neural ordinary dif- ferential equations. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.

Locally weighted regression: An approach to regression analysis by local fitting. W S Cleveland, S J Devlin, Journal of the American Statistical Association. 83403Cleveland, W. S. and Devlin, S. J. (1988). Locally weighted regression: An approach to regres- sion analysis by local fitting. Journal of the American Statistical Association, 83(403):596- 610.

Proximal splitting methods in signal processing. P L Combettes, J.-C Pesquet, Fixed-Point Algorithms for Inverse Problems in Science and Engineering. New YorkSpringerCombettes, P. L. and Pesquet, J.-C. (2011). Proximal splitting methods in signal processing. In Fixed-Point Algorithms for Inverse Problems in Science and Engineering, pages 185-212. Springer New York.

Statistics for spatio-temporal data. N A C Cressie, C K Wikle, John Wiley & SonsCressie, N. A. C. and Wikle, C. K. (2011). Statistics for spatio-temporal data. John Wiley & Sons.

PySINDy: A Python package for the sparse identification of nonlinear dynamical systems from data. B De Silva, K Champion, M Quade, J.-C Loiseau, J Kutz, S Brunton, Journal of Open Source Software. 5492104de Silva, B., Champion, K., Quade, M., Loiseau, J.-C., Kutz, J., and Brunton, S. (2020). PySINDy: A Python package for the sparse identification of nonlinear dynamical systems from data. Journal of Open Source Software, 5(49):2104.

The ten-year cycle in numbers of the lynx in Canada. C Elton, M Nicholson, Journal of Animal Ecology. 112Elton, C. and Nicholson, M. (1942). The ten-year cycle in numbers of the lynx in Canada. Journal of Animal Ecology, 11(2):215-244.

Data-driven prediction in dynamical systems. B I Epureanu, A Ghadami, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 380Special EditionEpureanu, B. I. and Ghadami, A. (2022). Data-driven prediction in dynamical systems. Philo- sophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sci- ences, 380(2229). Special Edition.

Ensemble-SINDy: Robust sparse model discovery in the low-data, high-noise limit, with active learning and control. U Fasel, J N Kutz, B W Brunton, S L Brunton, arXiv:2111.10992arXiv preprintFasel, U., Kutz, J. N., Brunton, B. W., and Brunton, S. L. (2021). Ensemble-SINDy: Robust sparse model discovery in the low-data, high-noise limit, with active learning and control. arXiv preprint arXiv:2111.10992, pages 1-18.

Review of genetic programming in modeling of machining processes. A Garg, K Tai, Proceedings of 2012 International Conference on Modelling, Identification and Control, ICMIC 2012. 2012 International Conference on Modelling, Identification and Control, ICMIC 2012Garg, A. and Tai, K. (2012). Review of genetic programming in modeling of machining pro- cesses. Proceedings of 2012 International Conference on Modelling, Identification and Con- trol, ICMIC 2012, pages 653-658.

Theoria motus corporum coelestium in sectionibus conicis solem ambientium. C F Gauss, Gauss, C. F. (1809). Theoria motus corporum coelestium in sectionibus conicis solem ambien- tium.

Approaches for Bayesian variable selection. E I George, R E Mcculloch, Statistica Sinica. 7George, E. I. and McCulloch, R. E. (1997). Approaches for Bayesian variable selection. Sta- tistica Sinica, 7(2):339-373.

Variable selection via Gibbs sampling. E I George, R E Mcculloch, E I George, R E Mcculloch, Journal of the American Statistical Association. 88423George, E. I., Mcculloch, R. E., George, E. I., and Mcculloch, R. E. (1993). Variable selection via Gibbs sampling. Journal of the American Statistical Association, 88(423):881-889.

Physically motivated scale interaction parameterization in reduced rank quadratic nonlinear dynamic spatio-temporal models. D Gladish, C Wikle, Environmetrics. 254Gladish, D. and Wikle, C. (2014). Physically motivated scale interaction parameterization in reduced rank quadratic nonlinear dynamic spatio-temporal models. Environmetrics, 25(4):230-244.

The Princeton companion to applied mathematics. N J Higham, M R Dennis, P Glendinning, P A Martin, F Santosa, J Tanner, Princeton University PressHigham, N. J., Dennis, M. R., Glendinning, P., Martin, P. A., Santosa, F., and Tanner, J. (2016). The Princeton companion to applied mathematics. Princeton University Press.

Sparsifying priors for Bayesian uncertainty quantification in model discovery. S M Hirsh, D A Barajas-Solano, J N Kutz, arXiv:2107.02107arXiv preprintHirsh, S. M., Barajas-Solano, D. A., and Kutz, J. N. (2021). Sparsifying priors for Bayesian uncertainty quantification in model discovery. arXiv preprint arXiv:2107.02107, pages 1-22.

The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. M D Hoffman, A Gelman, Journal of Machine Learning Research. 15Hoffman, M. D. and Gelman, A. (2014). The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15:1593- 1623.

A hierarchical Bayesian non-linear spatio-temporal model for the spread of invasive species with application to the Eurasian Collared-Dove. M B Hooten, C K Wikle, Environmental and Ecological Statistics. 151Hooten, M. B. and Wikle, C. K. (2008). A hierarchical Bayesian non-linear spatio-temporal model for the spread of invasive species with application to the Eurasian Collared-Dove. Environmental and Ecological Statistics, 15(1):59-70.

Multilayer feedforward networks are universal approximators. K Hornik, M Stinchcombe, H White, Neural Networks. 25Hornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359-366.

Improving genetic programming based symbolic regression using deterministic machine learning. I Icke, J C Bongard, IEEE Congress on Evolutionary Computation. IEEEIcke, I. and Bongard, J. C. (2013). Improving genetic programming based symbolic regression using deterministic machine learning. In 2013 IEEE Congress on Evolutionary Computation, pages 1763-1770. IEEE.

. Y Jin, W Fu, J Kang, J Guo, J Guo, arXiv:1910.08892Bayesian symbolic regression. arXiv preprintJin, Y., Fu, W., Kang, J., Guo, J., and Guo, J. (2019). Bayesian symbolic regression. arXiv preprint arXiv:1910.08892.

Ensemble Kalman methods for highdimensional hierarchical dynamic space-time models. M Katzfuss, J R Stroud, C K Wikle, Journal of the American Statistical Association. 115530Katzfuss, M., Stroud, J. R., and Wikle, C. K. (2020). Ensemble Kalman methods for high- dimensional hierarchical dynamic space-time models. Journal of the American Statistical Association, 115(530):866-885.

Solving parametric PDE problems with artificial neural networks. Y Khoo, J Lu, Ying , L , European Journal of Applied Mathematics. 323Khoo, Y., Lu, J., and Ying, L. (2021). Solving parametric PDE problems with artificial neural networks. European Journal of Applied Mathematics, 32(3):421-435.

Integration of neural network-based symbolic regression in deep learning for scientific discovery. S Kim, P Y Lu, S Mukherjee, M Gilbert, L Jing, V Ceperic, M Soljacic, IEEE Transactions on Neural Networks and Learning Systems. 329Kim, S., Lu, P. Y., Mukherjee, S., Gilbert, M., Jing, L., Ceperic, V., and Soljacic, M. (2021). Integration of neural network-based symbolic regression in deep learning for scientific dis- covery. IEEE Transactions on Neural Networks and Learning Systems, 32(9):4166-4177.

Methods for numerical differentiation of noisy data. I Knowles, R J Renka, Electronic Journal of Differential Equations Conference. 21Knowles, I. and Renka, R. J. (2012). Methods for numerical differentiation of noisy data. Electronic Journal of Differential Equations Conference, 21(2012):235-246.

Multilinear operators for higher-order decompositions. T Kolda, Sandia National Laboratories (SNL). Albuquerque, NM, and Livermore, CA; United StatesTechnical reportKolda, T. (2006). Multilinear operators for higher-order decompositions. Technical report, Sandia National Laboratories (SNL), Albuquerque, NM, and Livermore, CA (United States).

Extreme accuracy in symbolic regression. M F Korns, Genetic Programming Theory and Practice XI. New YorkSpringerKorns, M. F. (2014). Extreme accuracy in symbolic regression. In Genetic Programming Theory and Practice XI, pages 1-30. Springer New York.

Genetic programming as a means for programming computers by natural selection. J Koza, Statistics and Computing. 42Koza, J. (1994). Genetic programming as a means for programming computers by natural selection. Statistics and Computing, 4(2):1-26.

Performance improvement of machine learning via automatic discovery of facilitating functions as applied to a problem of symbolic system identification. J Koza, M Keane, Rice , J , IEEE International Conference on Neural Networks. IEEEKoza, J., Keane, M., and Rice, J. (1993). Performance improvement of machine learning via automatic discovery of facilitating functions as applied to a problem of symbolic system identification. In IEEE International Conference on Neural Networks, pages 191-198. IEEE.

Physical-statistical modeling. P M Kuhnert, Wiley StatsRef: Statistics Reference Online. WileyKuhnert, P. M. (2017). Physical-statistical modeling. In Wiley StatsRef: Statistics Reference Online, pages 1-5. Wiley.

Learning partial differential equations for biological transport models from noisy spatiotemporal data. J H Lagergren, J T Nardini, G Michael Lavigne, E M Rutter, K B Flores, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 47620190800Lagergren, J. H., Nardini, J. T., Michael Lavigne, G., Rutter, E. M., and Flores, K. B. (2020). Learning partial differential equations for biological transport models from noisy spatio- temporal data. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 476(2234):20190800.

Nouvelles méthodes pour la détermination des orbites des cometes. A M. ; F Legendre, Didot, Legendre, A. M. (1806). Nouvelles méthodes pour la détermination des orbites des cometes. F. Didot.

The Bayesian elastic net. Q Li, N Lin, Bayesian Analysis. 51Li, Q. and Lin, N. (2010). The Bayesian elastic net. Bayesian Analysis, 5(1):151-170.

Hierarchical deep learning of multiscale differential equation time-steppers. Y Liu, J N Kutz, S L Brunton, Philosophical Transactions of the Royal Society A: Mathematical. 380Physical and Engineering SciencesLiu, Y., Kutz, J. N., and Brunton, S. L. (2022). Hierarchical deep learning of multiscale dif- ferential equation time-steppers. Philosophical Transactions of the Royal Society A: Mathe- matical, Physical and Engineering Sciences, 380(2229).

PDE-Net 2.0: Learning PDEs from data with a numericsymbolic hybrid deep network. Z Long, Y Lu, Dong , B , Journal of Computational Physics. 399108925Long, Z., Lu, Y., and Dong, B. (2019). PDE-Net 2.0: Learning PDEs from data with a numeric- symbolic hybrid deep network. Journal of Computational Physics, 399:108925.

PDE-Net: Learning PDEs from data. Z Long, Y Lu, X Ma, Dong , B , 35th International Conference on Machine Learning, ICML 2018. 7Long, Z., Lu, Y., Ma, X., and Dong, B. (2017). PDE-Net: Learning PDEs from data. 35th International Conference on Machine Learning, ICML 2018, 7:5067-5078.

A variational analysis of stochastic gradient algorithms. S Mandt, M Hoffman, D Blei, Proceedings of The 33rd International Conference on Machine Learning. The 33rd International Conference on Machine Learning48Mandt, S., Hoffman, M., and Blei, D. (2016). A variational analysis of stochastic gradient al- gorithms. Proceedings of The 33rd International Conference on Machine Learning, 48:354- 363.

VAMPnets for deep learning of molecular kinetics. A Mardt, L Pasquali, H Wu, F Noé, Nature Communications. 915Mardt, A., Pasquali, L., Wu, H., and Noé, F. (2018). VAMPnets for deep learning of molecular kinetics. Nature Communications, 9(1):5.

Extrapolation and learning equations. G Martius, C H Lampert, 5th International Conference on Learning Representations, ICLR 2017 -Workshop Track Proceedings. 1Martius, G. and Lampert, C. H. (2016). Extrapolation and learning equations. 5th International Conference on Learning Representations, ICLR 2017 -Workshop Track Proceedings, 1:1- 13.

Data-driven partial derivative equations discovery with evolutionary approach. M Maslyaev, A Hvatov, A Kalyuzhnaya, Computational Science -ICCS 2019. Springer International PublishingMaslyaev, M., Hvatov, A., and Kalyuzhnaya, A. (2019). Data-driven partial derivative equa- tions discovery with evolutionary approach. In Computational Science -ICCS 2019, pages 635-641. Springer International Publishing.

The group lasso for logistic regression. L Meier, S Van De Geer, P Bühlmann, Journal of the Royal Statistical Society: Series B (Statistical Methodology). 701Meier, L., Van De Geer, S., and Bühlmann, P. (2008). The group lasso for logistic regression. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(1):53-71.

Ocean ensemble forecasting. Part I: Ensemble Mediterranean winds from a Bayesian hierarchical model. R F Milliff, A Bonazzi, C K Wikle, N Pinardi, L M Berliner, Quarterly Journal of the Royal Meteorological Society. 137657Milliff, R. F., Bonazzi, A., Wikle, C. K., Pinardi, N., and Berliner, L. M. (2011). Ocean ensemble forecasting. Part I: Ensemble Mediterranean winds from a Bayesian hierarchical model. Quarterly Journal of the Royal Meteorological Society, 137(657):858-878.

Empowering knowledge computing with variable selection -On variable importance and variable selection in regression random forests and symbolic regression. W Minnebo, S Stijven, BelgiumAntwerp UniversityPhD thesisMinnebo, W. and Stijven, S. (2011). Empowering knowledge computing with variable selection -On variable importance and variable selection in regression random forests and symbolic regression. PhD thesis, Antwerp University, Belgium.

Bayesian variable selection in linear regression. T J Mitchell, J J Beauchamp, Journal of the American Statistical Association. 834041023Mitchell, T. J. and Beauchamp, J. J. (1988). Bayesian variable selection in linear regression. Journal of the American Statistical Association, 83(404):1023.

On the effect of function set to the generalisation of symbolic regression models. M Nicolau, A Agapitos, Proceedings of the Genetic and Evolutionary Computation Conference Companion. the Genetic and Evolutionary Computation Conference CompanionNew York, NY, USAACMNicolau, M. and Agapitos, A. (2018). On the effect of function set to the generalisation of symbolic regression models. In Proceedings of the Genetic and Evolutionary Computation Conference Companion, pages 272-273, New York, NY, USA. ACM.

Bayesian identification of dynamical systems. R Niven, A Mohammad-Djafari, L Cordier, M Abel, M Quade, Proceedings. 33133Niven, R., Mohammad-Djafari, A., Cordier, L., Abel, M., and Quade, M. (2020). Bayesian identification of dynamical systems. Proceedings, 33(1):33.

A Bayesian approach for data-driven dynamic equation discovery. J S North, C K Wikle, E M Schliep, Journal of Agricultural, Biological, and Environmental Statistics. 11North, J. S., Wikle, C. K., and Schliep, E. M. (2022a). A Bayesian approach for data-driven dy- namic equation discovery. Journal of Agricultural, Biological, and Environmental Statistics, 1(1):1-28.

A Bayesian Approach for Spatio-Temporal Data-Driven Dynamic Equation Discovery. J S North, C K Wikle, E M Schliep, arXiv:2209.02750arXiv preprintNorth, J. S., Wikle, C. K., and Schliep, E. M. (2022b). A Bayesian Approach for Spatio- Temporal Data-Driven Dynamic Equation Discovery. arXiv preprint arXiv:2209.02750, pages 1-42.

The Bayesian lasso. T Park, G Casella, Journal of the American Statistical Association. 103482Park, T. and Casella, G. (2008). The Bayesian lasso. Journal of the American Statistical Association, 103(482):681-686.

Automatic differentiation in PyTorch Adam. A Paszke, S Gross, S Chintala, G Chanan, E Yang, Z Devito, Z Lin, A Desmaison, L Antiga, A Lerer, 31st Conference on Neural Information Processing Systems (NIPS 2017). Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. (2017). Automatic differentiation in PyTorch Adam. In 31st Conference on Neural Information Processing Systems (NIPS 2017), pages 1-4.

Sparsity information and regularization in the horseshoe and other shrinkage priors. J Piironen, A Vehtari, Electronic Journal of Statistics. 112Piironen, J. and Vehtari, A. (2017). Sparsity information and regularization in the horseshoe and other shrinkage priors. Electronic Journal of Statistics, 11(2):5018-5051.

Stochastic parameterization identification using ensemble Kalman filtering combined with maximum likelihood methods. M Pulido, P Tandeo, M Bocquet, A Carrassi, M Lucini, Tellus A: Dynamic Meteorology and Oceanography. 701442099Pulido, M., Tandeo, P., Bocquet, M., Carrassi, A., and Lucini, M. (2018). Stochastic parameter- ization identification using ensemble Kalman filtering combined with maximum likelihood methods. Tellus A: Dynamic Meteorology and Oceanography, 70(1):1442099.

Machine learning-based statistical closure models for turbulent dynamical systems. D Qi, J Harlim, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 380Qi, D. and Harlim, J. (2022). Machine learning-based statistical closure models for turbu- lent dynamical systems. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 380(2229).

Data driven governing equations approximation using deep neural networks. T Qin, K Wu, D Xiu, Journal of Computational Physics. 395Qin, T., Wu, K., and Xiu, D. (2019). Data driven governing equations approximation using deep neural networks. Journal of Computational Physics, 395:620-635.

Deep hidden physics models: Deep learning of nonlinear partial differential equations. M Raissi, Journal of Machine Learning Research. 19Raissi, M. (2018). Deep hidden physics models: Deep learning of nonlinear partial differential equations. Journal of Machine Learning Research, 19:1-24.

Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, P Perdikaris, G Karniadakis, Journal of Computational Physics. 378Raissi, M., Perdikaris, P., and Karniadakis, G. (2019). Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686-707.

Physics informed deep learning (Part I): Data-driven solutions of nonlinear partial differential equations. M Raissi, P Perdikaris, G E Karniadakis, arXiv:1711.10561arXiv preprintRaissi, M., Perdikaris, P., and Karniadakis, G. E. (2017a). Physics informed deep learning (Part I): Data-driven solutions of nonlinear partial differential equations. arXiv preprint arXiv:1711.10561, Part I:1-22.

M Raissi, P Perdikaris, G E Karniadakis, arXiv:1711.10566Data-driven discovery of nonlinear partial differential equations. IIarXiv preprintPartRaissi, M., Perdikaris, P., and Karniadakis, G. E. (2017b). Physics informed deep learning (Part II): Data-driven discovery of nonlinear partial differential equations. arXiv preprint arXiv:1711.10566, Part II:1-19.

Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. M Raissi, A Yazdani, G E Karniadakis, Science. 3676481Raissi, M., Yazdani, A., and Karniadakis, G. E. (2020). Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations. Science, 367(6481):1026-1030.

A Hierarchical spatial model for constructing wind fields from scatterometer data in the Labrador Sea. J A Royle, L M Berliner, C K Wikle, R Milliff, Case Studies in Bayesian Statistics. New York, NYSpringerRoyle, J. A., Berliner, L. M., Wikle, C. K., and Milliff, R. (1999). A Hierarchical spatial model for constructing wind fields from scatterometer data in the Labrador Sea. In Case Studies in Bayesian Statistics., pages 367-382. Springer, New York, NY.

Data-driven identification of parametric partial differential equations. S Rudy, A Alla, S L Brunton, J N Kutz, SIAM Journal on Applied Dynamical Systems. 182Rudy, S., Alla, A., Brunton, S. L., and Kutz, J. N. (2019a). Data-driven identification of parametric partial differential equations. SIAM Journal on Applied Dynamical Systems, 18(2):643-660.

Data-driven discovery of partial differential equations. S H Rudy, S L Brunton, J L Proctor, J N Kutz, Science Advances. 341602614Rudy, S. H., Brunton, S. L., Proctor, J. L., and Kutz, J. N. (2017). Data-driven discovery of partial differential equations. Science Advances, 3(4):e1602614.

Deep learning of dynamics and signal-noise decomposition with time-stepping constraints. S H Rudy, J Nathan Kutz, S L Brunton, Journal of Computational Physics. 396Rudy, S. H., Nathan Kutz, J., and Brunton, S. L. (2019b). Deep learning of dynamics and signal-noise decomposition with time-stepping constraints. Journal of Computational Physics, 396:483-506.

Learning equations for extrapolation and control. S S Sahoo, C H Lampert, G Martius, 35th International Conference on Machine Learning. 10Sahoo, S. S., Lampert, C. H., and Martius, G. (2018). Learning equations for extrapolation and control. 35th International Conference on Machine Learning, ICML 2018, 10:7053-7061.

Learning partial differential equations via data discovery and sparse optimization. H Schaeffer, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 47320160446Schaeffer, H. (2017). Learning partial differential equations via data discovery and sparse optimization. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 473(2197):20160446.

Distilling free-form natural laws from experimental data. M Schmidt, H Lipson, Science. 3245923Schmidt, M. and Lipson, H. (2009). Distilling free-form natural laws from experimental data. Science, 324(5923):81-85.

A Bayesian adaptive ensemble Kalman filter for sequential state and parameter estimation. J R Stroud, M Katzfuss, C K Wikle, Monthly Weather Review. 1461Stroud, J. R., Katzfuss, M., and Wikle, C. K. (2018). A Bayesian adaptive ensemble Kalman filter for sequential state and parameter estimation. Monthly Weather Review, 146(1):373- 386.

NeuPDE: Neural network based ordinary and partial differential equations for modeling time-dependent data. Y Sun, L Zhang, H Schaeffer, arXiv:1908.03190107arXiv preprintSun, Y., Zhang, L., and Schaeffer, H. (2019). NeuPDE: Neural network based ordi- nary and partial differential equations for modeling time-dependent data. arXiv preprint arXiv:1908.03190, 107(2016):352-372.

The Arctic oscillation signature in the wintertime geopotential height and temperature fields. D W J Thompson, J M Wallace, Geophysical Research Letters. 259Thompson, D. W. J. and Wallace, J. M. (1998). The Arctic oscillation signature in the winter- time geopotential height and temperature fields. Geophysical Research Letters, 25(9):1297- 1300.

Exact recovery of chaotic systems from highly corrupted data. G Tran, R Ward, Multiscale Modeling & Simulation. 153Tran, G. and Ward, R. (2017). Exact recovery of chaotic systems from highly corrupted data. Multiscale Modeling & Simulation, 15(3):1108-1129.

Symbolic regression in materials science. Y Wang, N Wagner, J M Rondinelli, MRS Communications. 93Wang, Y., Wagner, N., and Rondinelli, J. M. (2019). Symbolic regression in materials science. MRS Communications, 9(3):793-805.

Hierarchical Bayesian models for predicting the spread of ecological processes. C K Wikle, Ecology. 846Wikle, C. K. (2003). Hierarchical Bayesian models for predicting the spread of ecological processes. Ecology, 84(6):1382-1394.

Polynomial nonlinear spatio-temporal integro-difference equation models. C K Wikle, S H Holan, Journal of Time Series Analysis. 324Wikle, C. K. and Holan, S. H. (2011). Polynomial nonlinear spatio-temporal integro-difference equation models. Journal of Time Series Analysis, 32(4):339-350.

A general science-based framework for dynamical spatio-temporal models. C K Wikle, M B Hooten, TEST. 193Wikle, C. K. and Hooten, M. B. (2010). A general science-based framework for dynamical spatio-temporal models. TEST, 19(3):417-451.

Spatiotemporal hierarchical Bayesian modeling: Tropical ocean surface winds. C K Wikle, R F Milliff, D Nychka, L M Berliner, Journal of the American Statistical Association. 96454Wikle, C. K., Milliff, R. F., Nychka, D., and Berliner, L. M. (2001). Spatiotemporal hierarchi- cal Bayesian modeling: Tropical ocean surface winds. Journal of the American Statistical Association, 96(454):382-397.

C K Wikle, A Zammit-Mangion, arXiv:2206.02218Statistical Deep Learning for Spatial and Spatio-Temporal Data. arXiv preprintWikle, C. K. and Zammit-Mangion, A. (2022). Statistical Deep Learning for Spatial and Spatio- Temporal Data. arXiv preprint arXiv:2206.02218.

Genetic programming: an introduction and survey of applications. M.-J Willis, Second International Conference on Genetic Algorithms in Engineering Systems. IETWillis, M.-J. (1997). Genetic programming: an introduction and survey of applications. In Second International Conference on Genetic Algorithms in Engineering Systems, pages 314- 319. IET.

Numerical aspects for approximating governing equations using data. K Wu, D Xiu, Journal of Computational Physics. 384Wu, K. and Xiu, D. (2019). Numerical aspects for approximating governing equations using data. Journal of Computational Physics, 384:200-221.

Data-driven deep learning of partial differential equations in modal space. K Wu, D Xiu, Journal of Computational Physics. 408109307Wu, K. and Xiu, D. (2020). Data-driven deep learning of partial differential equations in modal space. Journal of Computational Physics, 408:109307.

DL-PDE: Deep-learning based data-driven discovery of partial differential equations from discrete and noisy data. H Xu, H Chang, D Zhang, Communications in Computational Physics. 293Xu, H., Chang, H., and Zhang, D. (2019). DL-PDE: Deep-learning based data-driven dis- covery of partial differential equations from discrete and noisy data. Communications in Computational Physics, 29(3):698-728.

DLGA-PDE: Discovery of PDEs with incomplete candidate library via combination of deep learning and genetic algorithm. H Xu, H Chang, D Zhang, Journal of Computational Physics. 418109584Xu, H., Chang, H., and Zhang, D. (2020). DLGA-PDE: Discovery of PDEs with incomplete candidate library via combination of deep learning and genetic algorithm. Journal of Com- putational Physics, 418:109584.

Deep-learning of parametric partial differential equations from sparse and noisy data. H Xu, D Zhang, J Zeng, Physics of Fluids. 33337132Xu, H., Zhang, D., and Zeng, J. (2021). Deep-learning of parametric partial differential equa- tions from sparse and noisy data. Physics of Fluids, 33(3):037132.

Bayesian differential programming for robust systems identification under uncertainty. Y Yang, M Aziz Bhouri, P Perdikaris, Proceedings of the Royal Society A: Mathematical. 47620200290Physical and Engineering SciencesYang, Y., Aziz Bhouri, M., and Perdikaris, P. (2020). Bayesian differential programming for robust systems identification under uncertainty. Proceedings of the Royal Society A: Mathe- matical, Physical and Engineering Sciences, 476(2243):20200290.

Deep compositional spatial models. A Zammit-Mangion, T L J Ng, Q Vu, M Filippone, Journal of the American Statistical Association. 00Zammit-Mangion, A., Ng, T. L. J., Vu, Q., and Filippone, M. (2021). Deep compositional spatial models. Journal of the American Statistical Association, 0(0):1-22.

Data-driven equation discovery of ocean mesoscale closures. L Zanna, T Bolton, Geophysical Research Letters. 1747Zanna, L. and Bolton, T. (2020). Data-driven equation discovery of ocean mesoscale closures. Geophysical Research Letters, 47(17).

On the convergence of the SINDy algorithm. L Zhang, H Schaeffer, Multiscale Modeling & Simulation. 173Zhang, L. and Schaeffer, H. (2019). On the convergence of the SINDy algorithm. Multiscale Modeling & Simulation, 17(3):948-972.

Robust data-driven discovery of governing physical laws with error bars. S Zhang, G Lin, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 47420180305Zhang, S. and Lin, G. (2018). Robust data-driven discovery of governing physical laws with error bars. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 474(2217):20180305.

A Algorithms Algorithm 1: Sequential Threshold Least-Squares: SINDy Input: K, κ Data: U t (J) , F Result: M Initialize: M = (F F + λ I) −1 F U t (J) for k = 1 to K do γ = |M| < κ. A Algorithms Algorithm 1: Sequential Threshold Least-Squares: SINDy Input: K, κ Data: U t (J) , F Result: M Initialize: M = (F F + λ I) −1 F U t (J) for k = 1 to K do γ = |M| < κ ;

. /* Matrix, 0/* Matrix identifying small coefficients */ M(γ) = 0 ;

N do i := γ(n) == 0. . . /* Threshold M */ For N = 1, /* Threshold M */ for n = 1, ..., N do i := γ(n) == 0 ;