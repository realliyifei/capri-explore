# Learning Mean Field Games: A Survey

CorpusID: 249062809
 
tags: #Mathematics, #Computer_Science

URL: [https://www.semanticscholar.org/paper/cfd71424c57a32a5a0f721982ec3526f1d034884](https://www.semanticscholar.org/paper/cfd71424c57a32a5a0f721982ec3526f1d034884)
 
| Is Survey?        | Result          |
| ----------------- | --------------- |
| By Classifier     | False |
| By Annotator      | (Not Annotated) |

---

Learning Mean Field Games: A Survey


Mathieu Laurière 
Sarah Perrin 
Matthieu Geist 
Olivier Pietquin 
Learning Mean Field Games: A Survey

Non-cooperative and cooperative games with a very large number of players have many applications but remain generally intractable when the number of players increases. Introduced byLasry and Lions (2007)andHuang et al. (2006), Mean Field Games (MFGs) rely on a mean-field approximation to allow the number of players to grow to infinity. Traditional methods for solving these games generally rely on solving partial or stochastic differential equations with a full knowledge of the model. Recently, Reinforcement Learning (RL) has appeared promising to solve complex problems. By combining MFGs and RL, we hope to solve games at a very large scale both in terms of population size and environment complexity. In this survey, we review the quickly growing recent literature on RL methods to learn Nash equilibria in MFGs. We first identify the most common settings (static, stationary, and evolutive). We then present a general framework for classical iterative methods (based on best-response computation or policy evaluation) to solve MFGs in an exact way. Building on these algorithms and the connection with Markov Decision Processes, we explain how RL can be used to learn MFG solutions in a model-free way. Last, we present numerical illustrations on a benchmark problem, and conclude with some perspectives.

# Introduction

Since their introduction by Lasry and Lions (Lasry and Lions, 2007) and Caines, Huang and Malhamé (Huang et al., 2006), mean field games (MFGs for short) have gained momentum as a powerful paradigm to study large populations of strategic agents. The main idea, borrowed from statistical physics, is to use the mean field distribution corresponding to the limiting mean field situation with an infinite number of players. All the individual interactions can then be replaced by the interaction between a representative player and the mean field distribution, which considerably simplifies the model and the analysis. This approximation relies on the assumption that the population is homogeneous and that the interactions are symmetric in the sense that each player interacts only with the empirical distribution of the other players. The solution to the MFG provides an -Nash equilibrium for the corresponding N -player game, with going to 0 as N goes to infinity. Furthermore, under suitable assumptions, N -player Nash equilibria or social optima converge to the corresponding mean field equilibrium or social optimum. Such results build on the idea of propagation of chaos (Sznitman, 1991) but are more subtle since the players are not simple particles but rational agents making optimal decisions and reacting to other players' decisions (Lacker, 2017;Cecchin and Pelino, 2019;Lacker, 2020;Cardaliaguet et al., 2019).


## Mean field games

General intuition. We start this survey by defining at a high level what a Mean Field Game (MFG) is. Intuitively, a Mean Field Game is a game with an infinite number of identical players. All players have a similar behavior, i.e. they are symmetric: we do not need to retain the identity of a player as part of its state. Furthermore, as we have an infinite number of players, we can replace the atomic players by their distributions over the state (and sometimes action) space. The population's distribution enables to focus only on the interaction between a so-called representative player, which is sampled from the population's distribution, and the population's distribution itself. Our ultimate goal is to compute a Nash equilibrium, which corresponds to the situation where no player has an interest in deviating from its current behavior, provided that the other players do not deviate either. Looking for a Nash equilibrium makes the assumption that the players are all perfectly rational, i.e their goal is to maximize their own reward (or minimize their cost).

Most of the literature focuses on two types of problems: Nash equilibria or social optimum. These two settings are typically referred to respectively as mean field game and mean field control (or control of McKean-Vlasov dynamics), (Bensoussan et al., 2013;Carmona and Delarue, 2018a). In both cases, the solutions are typically characterized through optimality conditions taking the form of a coupled system of forward-backward equations. The forward equation describes the evolution of the population distribution while the backward equation represents the evolution of the value function (i.e. the utility of its behaviour) for a representative player. In the continuous time and continuous space setting, the equations can be partial differential equations (PDEs) (Lasry and Lions, 2007) or stochastic differential equations (SDEs) of McKean-Vlasov type (Carmona and Delarue, 2013) depending on whether one relies on the analytical approach or the probabilistic approach. We refer to e.g. Bensoussan et al. (2013); Carmona and Delarue (2018a,b);  for more details. In this survey, we will focus on the discrete time case, which is closer to the framework of Markov Decision Processes (Bertsekas and Shreve, 1996;Puterman, 2014).


## Example.

As a typical example, we can consider crowd motion. Each player is an agent represented by her position and is able to control her velocity so as to reach a target destination while minimizing the effort made to move. Typically, passing through a crowded region, i.e. a region with a high density of players, requires extra efforts or reduces the velocity, which creates some congestion. If we assume that the number of agents is extremely large and that these agents are homogeneous and have symmetric interactions, then we can approximate the empirical distribution of positions by the mean field distribution corresponding to the limiting regime with an infinite population. This allows to simplify tremendously the computation of a Nash equilibrium because we only need to compute the optimal policy of the representative player.

Remark 1 (On MFGs and non-atomic anonymous games). Games modeling infinite populations of agents have also been studied in the framework of non-atomic anonymous games, which have founds applications particularly in economics, see e.g. Aumann (1964); Schmeidler (1973); Aumann and Shapley (2015). In such games, there is typically a continuum of players, indexed by, say, real numbers in I = [0, 1] and the population is represented by a non-atomic measure on I. Each player perceives the other players through some aggregate quantity. Although this is very similar to the MFG framework, the key difference is that the MFG approach completely avoids representing the continuum of players. The main idea is to exploit the homogeneity of the population and the symmetry of interactions to simplify the characterization of an equilibrium: it is sufficient to understand the behavior of a single representative player facing a distribution representing the aggregate information available to this player. The analysis is greatly simplified, particularly when it comes to stochastic games. Defining rigorously a continuum of random variables with nice measurability properties is not trivial, as noticed for instance by Duffie and Sun (2012); Sun (2006) who used the concept of rich Fubini extension to develop an exact law of large numbers. The MFG framework allows to carry out the mathematical analysis of Nash equilibria without requiring such sophistication.

Some applications. MFGs have found various applications such as population dynamics (Guéant et al., 2011;Achdou et al., 2017a;Cardaliaguet et al., 2016), crowd motion modeling (Achdou and Lasry, 2019;Burger et al., 2013;Djehiche et al., 2017a;Aurell and Djehiche, 2019;Achdou and Laurière, 2016;Chevalier et al., 2015), flocking (Nourian et al., 2010(Nourian et al., , 2011Grover et al., 2018;Perrin et al., 2021b), opinion dynamics and consensus formation (Stella et al., 2013;Bauso et al., 2016a;Parise et al., 2015), autonomous vehicles Shiri et al., 2019), epidemics control (Laguzet and Turinici, 2015;Hubert and Turinici, 2018;Elie et al., 2020a;Lee et al., 2021b;Aurell et al., 2022;Doncel et al., 2022), macro-economic models (Achdou et al., 2017b;Elie et al., 2019b;Achdou et al., 2017bAchdou et al., , 2014Chan and Sircar, 2015;Gomes et al., 2014a;Djehiche et al., 2017b), finance (Lachapelle et al., 2016;Cardaliaguet and Lehalle, 2018;Lasry and Lions, 2007;Lachapelle et al., 2016;Gomes and Saúde, 2020;Carmona, 2020), energy production and management (Alasseur et al., 2020;Couillet et al., 2012;Elie et al., 2019a;Bagagiolo and Bauso, 2014;Kizilkale et al., 2019;Li et al., 2016;Guéant et al., 2011;Chan and Sircar, 2017;Graber and Bensoussan, 2018), security and communication (Mériaux et al., 2012;Samarakoon et al., 2015;Hamidouche et al., 2016;Yang et al., 2017;Kolokoltsov and Bensoussan, 2016;Kolokoltsov and Malafeyev, 2018), traffic modeling (Bauso et al., 2016b;Salhab et al., 2018;Huang et al., 2019;Tanaka et al., 2020;Cabannes et al., 2021) or engineering (Djehiche et al., 2017b).


## Numerical methods.

Most existing numerical methods for MFGs and MFC problems are based on the aforementioned optimality conditions phrased in terms of PDEs or SDEs. Such approaches typically rely suitable discretizations, e.g. by finite differences (Achdou and Capuzzo-Dolcetta, 2010;Achdou et al., 2012;Briceño Arias et al., 2018;Briceño Arias et al., 2019), semi-Lagrangian schemes Silva, 2014, 2015), or probabilistic approaches Angiuli et al., 2019). We refer to e.g. Achdou and Laurière (2020); Lauriere (2021) for recent surveys on these methods. Although these methods are very well understood and very successful in small dimension, they cannot tackle MFGs with high dimensional states or controls due to the curse of dimensionality (Bellman, 1957). To address this limitation, stochastic methods based on approximations by neural networks have recently been introduced by Carmona andLaurière (2021, 2019); Fouque and Zhang (2020); Germain et al. (2022) using optimality conditions for general mean field games, by Ruthotto et al. (2020) for MFGs which can be written as a control problem, and by Cao et al. (2020); Lin et al. (2020) for variational MFGs in connection with generative adversarial networks (GANs) (Goodfellow et al., 2014). We refer to Hu and Laurière (2022) for a recent survey on machine learning methods for control and games, with applications to MFGs and MFC problems. However, these methods still try to solve the problems in an exact way by relying on exact computations of gradients by exploiting the full knowledge of the model. The learning methods we focus on in this survey aim at solving MFGs and MFC in a model-free fashion to foster the scalability of numerical methods for these problems.


## Learning

Two notions of learning.

The second question we need to answer before diving more into the survey is what learning means in our context. There are mainly two interpretations of learning. The first one comes from game theory and economics and, according to Fudenberg et al. (1998), refers to "The theory of learning in games [. . . ] examines how, which, and what kind of equilibrium might arise as a consequence of along-run non equilibrium process of learning, adaptation, and/or imitation." From this point of view, the main focus is on how the players iteratively adjust their behavior until convergence to an equilibrium. The second interpretation of learning is mainly used in Machine Learning and in Reinforcement Learning. As Mitchell et al. (1997) puts it, "a computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E." In this concept, the concept of learning is very related to the idea of improving one's performance by using data or samples. In this survey, we are interested in delineating these two notions of learning while explaining how they can be combined.

Learning in games. More specifically, we will focus on learning in MFGs. This research direction finds its roots in the literature on learning in games, which goes back to the works of Shannon (1959) and Samuel (1959). Since then, a lot of progress has been made but remains mostly limited to games with a small number of players. Many of the recent breakthrough results have been obtained using a combination of reinforcement learning (Sutton and Barto, 2018) and deep neural networks (Goodfellow et al., 2016), see e.g. Go (Silver et al., , 2017(Silver et al., , 2018, Chess (Campbell et al., 2002), Checkers (Schaeffer et al., 2007;Samuel, 1959), Hex (Anthony et al., 2017), Starcraft II (Vinyals et al., 2019), poker games (Brown andSandholm, 2017, 2019;Moravčík et al., 2017;Bowling et al., 2015) or Stratego (McAleer et al., 2020).

Learning in mean field games. The goal of this survey is to review the quickly growing literature at the intersection of learning and MFGs. We hope that combining mean field approximations, which allow to tackle large population games, and reinforcement leanring, which allows to handle highly complex environments, we will be able to solve multi-agent systems at a very large scale, both in terms of population size and model complexity.


## Outline of the survey

In the rest of this section, we introduce a few useful notation. In Section 2, we present several settings of MFG and MFC problems that have appeared in the literature. We stress the similarities and the differences, in terms of definitions and in terms of solutions. In Section 3, we present algorithms to compute MFG and MFC solutions. We start by recalling some classical methods to solve MDPs and then describe mainly two classes of algorithms to find Nash equilibria in MFGs. These algorithms are based on iteratively updating the mean field and the policy, so we refer to them as iterative methods. Building on these methods and the connection between MDPs and RL, we explain in Section 4 how RL and deep RL methods can be adapted to solve MFGs and MFC problems. Section 5 discusses metrics that can be used to assess the numerical convergence of algorithms and illustrate some of the methods on a representative MFG example. Finally, we conclude in Section 6 with a short summary and some perspectives.


## Useful notation

Here we introduce some general notation that we use throughout the text:

• X and A denote a finite state set and a finite action set respectively.

• |E| denotes the cardinal of a set E.

• 2 E denotes the set of subsets of a set E.

• ∆ E is the set of probability distributions on a set E; when E is finite (which is generally the case for us), it is also identified with the corresponding simplex in the Euclidean space of dimension |E|, and we view probability distributions as (normalized) vectors in R |E| .

• argmax is understood as the set of all maximizers.

• P, E denote respectively probability and expectation.

• Unless otherwise specified, bold symbols denote time-dependent objects, which can be viewed as functions of time or as sequences indexed by time steps.

• We will use subscripts with n ∈ {1, . . . , N T } for time indices and superscript with k ∈ {1, . . . , K} for iteration indices in algorithms.

• Given a probability distribution p on a set X and a function ϕ :
X → R, E x∼p [ϕ(x)] = E[ϕ(x) | x ∼ p] = x∈X p(x)ϕ(x).

# Definition of the problems

In this section, we present several settings of MFGs which depend on how time is involved (or not) in the problem. These settings correspond to different applications and different notions of Nash equilibrium. Here, we focus on four settings, that can be summarized as follows. We start with games in which the agents take a single decision. There is no notion of time intrinsic to the game so we call them static MFGs. We then turn to games in which there is a dynamical aspect. In such games, each agent has a state that evolves along time, and she can act on this evolution. At the level of the population, in some situations, we can expect the distribution of states to converge to a stationary regime, in which the population is stable at a macroscopic level, even though each agent's state is possibly changing. We refer to this setting as a stationary MFG. In other cases, one wants to understand not only the stationary regime, but how the population evolves, starting from an initial configuration. This is relevant for applications in which the agents' behaviors change along time, for instance because there is a finite horizon at which the game stops. We call such games evolutive MFGs. This setting comes at the expense of having policies and mean-field terms that depend on time and are thus harder to compute. To mitigate this complexity while not falling completely into the stationary regime, an intermediate model has been introduced. The idea is to try to keep the best of the stationary and evolutive settings by considering a proxy for the whole evolution of the distribution. We call this setting discounted stationary MFGs. In the rest of this section, we define each setting as well as the corresponding notion of Nash equilibrium, along with relevant concepts.


## Static MFG

Let A be a finite action space. The behavior of one player, called a strategy, is an element of ∆ A , that is a distribution over the action set. In this setting, the behavior of the population is also an element of ∆ A . We denote a generic element of A by a, and we denote a generic individual behavior and a generic population behavior by π and ξ respectively. Besides the action space A, the model is completely defined by a reward function r : A × ∆ A → R. Given a population behavior ξ ∈ ∆ A , the reward of a player using π ∈ ∆ A is defined as the expected reward when sampling an action according to π: J static (π; ξ) = E a∼π [r(a, ξ)] .

The reward function r : A × ∆ A → R can typically encode crowd aversion or attraction towards a population action of interest.

Example 1. One of the first examples in the MFG literature is the problem of choosing a starting time for a meeting, introduced and solved explicitly by Guéant et al. (2011). In this problem, the players choose at what time they want to arrive to the meeting room so that they are neither too late nor too early. The global outcome is the time at which the meeting actually starts, which is not known in advance and depends on the everyone's arrival time. Despite its name, there is no dynamic aspect in the original formulation of the example. Another popular example is the problem in which each agent chooses a location on a beach, see e.g. Perrin et al. (2020). They want to put their towel close to an ice cream stall but not in a very crowded area. The global outcome is the distribution of towels on the beach. To be specific, a simple model can be as follows: A = [0, 1], which represents possible positions on the beach, a stall ∈ A is the position of the stall, and the reward is r(a, ξ) = −|a − a stall | − ln(ξ(a)), where the first term penalizes the distance to the stall and the second term penalizes choosing a location a at which the density ξ(a) of people is high.

A central concept is the notion of best response. Let us define the (set-valued) best response map by:
BR static : ∆ A → 2 ∆ A , ξ → BR static (ξ) := argmax π∈∆ A J static (π; ξ).
Definition 1 (Static MF Nash Equilibrium).π ∈ ∆ A is a static mean field Nash equilibrium (static MFNE) if it satisfies the following condition:π ∈ BR static (π).

The above definition has the advantage to clearly show that the equilibrium is a fixed point of BR static . Another point of view, which will be useful in the dynamic settings presented in the sequel, consists in saying that the equilibrium is given by a pair of a representative agent's behavior and the population's behavior. Here, it means that the equilibrium is a pair (π,ξ) ∈ ∆ A × ∆ A such that:

1.π is optimal for the representative agent facingξ, i.e.,π ∈ BR static (ξ), 2.ξ corresponds to the population behavior when all every agent usesπ, i.e.,ξ =π.

The second point represents the fact that all the agents are "rational in the same way" and hence, at equilibrium, adopt the same behavior. This viewpoint is unnecessarily complicated in this setting asπ alone is enough to define the MFNE, but will be useful in dynamic settings.

Remark 2. Consistently with the literature on normal-form games (Fudenberg and Tirole, 1991), each player chooses a distribution over actions without seeing the strategy chosen by other players and the resulting distribution at the population level. Each agent thus tries to anticipate, in a rational way, the distribution generated by other players' actions.

A Nash equilibrium corresponds to a situation in which no selfish player has any incentive to deviate unilaterally. However, it is not necessarily a situation that is optimal from the point of view of the whole population. The notion of social optimum is discussed below in Section 2.7.

Remark 3. Although we provided an intuitive explanation for ξ in terms of a continuum of players, we want to stress that in the definition of an MFG equilibrium or social optimum, we actually never need to consider a continuum of players. As already pointed out in Remark 1, this shortcut is one of the main advantages of the MFG paradigm compared with non-atomic anonymous games.


## Notations for the dynamic setting

In contrast with the static case, in the dynamic setting, each agent has a state which evolves in time. The agent can influence the evolution of their own state using actions. The population's state is the distribution of the agents' states, the joint distribution of their states and actions. This is what constitutes the mean field, with which every agent interacts through its dynamics and its rewards.

As far as the population distribution is concerned, we will consider mainly two types of settings: one in which the population distribution is fixed, and one in which it can also evolve. Typically, the former is conceptually simpler and easier to compute but the latter is more realistic since many real world applications involve a population evolving in time. In each cases, several variants can be considered. For the sake of brevity, we shall focus only on the main ideas.

We will consider discrete time models, with time typically denoted by n ∈ N. If a time horizon is imposed, we will typically use the notation N T . Let X be a finite state space. A stationary policy is an element of Π := (∆ A ) X . In this setting, we assume that the interactions occur through an aggregate variable which represents the behavior of the population. A mean field state is an element of ∆ X , which is the set of probability distributions on the state space. It represents the state of the population at a given time. We denote generic elements of X , A, Π, and ∆ X respectively by x, a, π, and µ.

Depending on the setting, we might consider policies that depend on time or on an initial distribution. More details will be provided below, as we introduce several setups.

Remark 4. To alleviate the presentation, we choose to focus on finite state and action spaces and discrete time. In some cases, continuous space or continuous time models might be more relevant. They are typically analyzed using partial differential equations or stochastic differential equations. Suitable discretizations can lead to (possibly nontrivial) approximations of these models with discrete ones, as presented in this survey, see e.g. Hadikhanloo and Silva (2019) for more details on the convergence of a finite MFG to a continuous one. We do not discuss in detail the continuous settings here and we refer the interested reader to the literature, e.g., Huang et al. (2006); Lasry and Lions (2007); Bensoussan et al. (2013); Carmona and Delarue (2018a,b).


## Background on MDPs

We recall a few important concepts pertaining to optimal control in discrete time for a single agent. We will only review the main ideas and we refer to e.g. Bertsekas and Shreve (1996); Puterman (2014) for more details. The notion of Markov decision processes will play a key role in the description of dynamic MFGs.


### Stationary MDP

A stationary Markov decision process (MDP) is a tuple (X , A, p, r, γ) where X is a state space, A is an action space, p : X × A → ∆ X is a transition kernel, r : X × A → R is a reward function and γ ∈ (0, 1) is a discount factor. Using action a when the current state is x leads to a new state distributed according to p(·|x, a) ∈ ∆ X and produces a reward r(x, a). The reward could be stochastic but to simplify the presentation, we consider that r is a deterministic function of the state and the action. A stationary policy π : X → ∆ A , x → π(·|x) provides a distribution over actions for each state. The goal of the MDP is to find a policy π * which maximizes the total return defined as the expected (discounted) sum of future rewards: J(π) = E n≥0 γ n r(x n , a n ) , subject to:

x 0 ∼ m 0 , a n ∼ π(·|x n ), x n+1 ∼ p(·|x n , a n ), n ≥ 0,

where m 0 is an initial distribution whose choice does not influence the set of optimal policies. Assuming the model is fully known to the agent, the problem can be solved using for instance dynamic programming. The state-action value function associated to a stationary policy π is defined as:
Q π (x, a) = E   n≥0
γ n r(x n , a n ) x 0 = x, a 0 = a, a n ∼ π(·|x n ), x n+1 ∼ p(·|x n , a n )   .

By dynamic programming, it satisfies the following fixed point equation with unknown Q : X × A → R:
Q = B π Q,
where B π denotes the Bellman operator associated to π:
(B π Q)(x, a) = r(x, a) + γE x ∼p(·|x,a),a ∼π(·|x ) [Q(x , a )].
(1)

We recall that the expectation is to be understood as:
E x ∼p(·|x,a),a ∼π(·|x ) [Q(x , a )] = x p(x |x, a) a π(a |x )Q(x , a ).
The optimal state-action value function is defined as:
Q * (x, a) = sup π Q π (x, a).
It satisfies the fixed point equation:
Q = B * Q,(2)
where B * denotes the optimal Bellman operator:
(B * Q)(x, a) = r(x, a) + γE x ∼p(·|x,a) [max a Q(x , a )], with E x ∼p(·|x,a) [max a Q(x , a )] = x p(x |x, a) max a Q(x , a ).
It is also convenient to introduce the (state only) value function associated to a policy V π : x → E a∼π(·|x) [Q π (x, a)] and the (state only) optimal value function V * :
x → E a∼π * (·|x) [Q * (x, a)],
where π * is an optimal policy. These value functions can also be characterized as fixed points of two Bellman operators. Note that these objects are all independent of time, as we search for a stationary solution.


### Finite Horizon MDP

One can also consider problems set with a finite time horizon. A finite-horizon Markov decision process (MDP) is a tuple (X , A, p, r, N T ) where X is a state space, A is an action space, N T is a time horizon, p : {0, . . . , N T − 1} × X × A → P(X ) is a transition kernel, and r : {0, . . . , N T } × X × A → R is a reward function. At time n, using action a when the current state is x leads to a new state distributed according to p n (·|x, a) ∈ ∆ X and produces a reward r n (x, a) ∈ R. A policy π : {0, . . . , N T − 1} × X → P(A), (n, x) → π n (·|x) provides a distribution over actions for each state at time n. The goal of the MDP is to find a policy π * which maximizes the total return defined as the expected (discounted) sum of future rewards:
J(π) = E N T n=0
r n (x n , a n ) , subject to:

x 0 ∼ m 0 , a n ∼ π n (·|x n ), x n+1 ∼ p n (·|x n , a n ), n = 0, . . . , N T , where m 0 is an initial distribution whose choice does not influence the set of optimal policies. Here again, assuming the model is known to the agent, the problem can be solved using for instance dynamic programming. The state-action value function associated to a stationary policy π is defined as:
             Q π N T (x, a) = r N T (x, a) Q π n (x, a) = E   n ≥n
r n (x n , a n ) x n = x, a n = a, a n ∼ π n (·|x n ), x n +1 ∼ p n (·|x n , a n )   , n = N T − 1, . . . , 0.

The optimal state-action value function is defined as:
Q * (x, a) = sup π Q π (x, a).
Here again, we can introduce the (state only) value function associated to a policy: V π n (x) = E a∼πn(·|x) [Q π n (x, a)], and the optimal value function:
V * n (x) = E a∼π * n (·|x) [Q * n (x, a)]
, π * is an optimal policy. Formally, the finite-horizon MDP can be restated as a stationary MDP by incorporating the time n in the state. However, it can be simpler to directly tackle this MDP using techniques that are specific to the finite-horizon setting. In particular we stress that, in contrast with the stationary setting presented above, the value functions are here characterized by equations which are not fixed point equations but backward equations. They can be solved by backward induction, as we will discuss in the sequel (see Section 3). For more details on finite-horizon MDP we refer to e.g. (Puterman, 2014).


## Stationary setting

Here we consider an infinite horizon model, meaning that there is no terminal time. We assume that the players interact through a stationary distribution, which represents a steady state of the population. The model is defined by a tuple (X , A, p, r, γ) consisting of:

• a state space X and an action space A,

• a one-step transition probability kernel p : X × A × ∆ X → ∆ X ,

• a one-step reward function r : X × A × ∆ X → R,

• and a discount factor γ ∈ [0, 1].

The main difference with standard MDPs as recalled in Section 2.3 is the presence of a third input for p and r, which is an element of the mean field state space ∆ X . It plays the role of the population's state, which influences the dynamics and the rewards.

Assume the state of the population is given by µ ∈ ∆ X and consider a representative agent using policy π ∈ Π. The total, discounted reward of this player is given by:
J statio (π; µ) = E ∞ n=0 γ n r(x n , a n , µ) ,(3)
where the state of the agent evolves according to:

x 0 ∼ µ,

x n+1 ∼ p(·|x n , a n , µ), a n ∼ π(·|x n ), n ≥ 0.

Remark 5 (State-action distribution). An extension of the above model is to consider that the agents interact through the state-action distribution. In this case, assume the state of the population is given by ξ ∈ ∆ X ×A and consider a representative agent using policy π ∈ Π. The total, discounted reward of a representative player is given by:
J statio (π; ξ) = E ∞ n=0
γ n r(x n , a n , ξ) ,

where the state of the agent evolves according to:
x 0 ∼ µ = ξ 1 ,
x n+1 ∼ p(·|x n , a n , ξ), a n ∼ π(·|x n ), n ≥ 0.

with µ = ξ 1 ∈ ∆ X denoting the first marginal of ξ. This setting is considered for instance by Guo et al. (2019Guo et al. ( , 2020a. MFG with interactions through state-action distributions have first been studied by Gomes et al. (2014b); Gomes and Voskanyan (2016) and are sometimes referred to as extended MFGs or MFG of controls, see Cardaliaguet and Lehalle (2018); Kobeissi (2022). Let us stress that a state-action distribution is not always a product distribution, meaning that for some ξ ∈ ∆ X ×A there is no µ ∈ ∆ X and ν ∈ ∆ A such that ξ = µ ⊗ ν. In fact, in general the actions of a player are given by a function of the player's states, and hence the joint distribution cannot be written as a product. To simplify the presentation, we restrict our attention to interactions through state-only distributions but most of the ideas can be extended to state-action distributions. The interested reader is referred to Carmona and Delarue (2018a, Section 4.6) and the references therein.

This stationary MFG setting has been studied for instance by Subramanian and Mahajan (2019) with applications to malware spread and investments in product quality, by Guo et al. (2019Guo et al. ( , 2020a with applications to auctions and by Angiuli et al. (2022b) in the context of linear-quadratic MFGs.

Example 2 (Repeated auction game). As an example, Guo et al. (2019) consider a repeated game in which the players bid in an auction game. At a given time, a player's state and action are respectively her budget and her bid for the next auction.

The evolution of the population is given by a transition matrix defined by: for allμ ∈ ∆ X , π ∈ Π, µ ∈ ∆ X and x ∈ X , (P μ,π µ)(y) = x µ(x) a π(a|x)p(y|x, a,μ).

In words, P μ,π µ is the next state distribution for a representative agent starting with state distribution µ and using policy π while the population has state distributionμ. Given a population state, the goal for a representative agent, is to find the best reaction, i.e., a policy that maximizes their total reward. We define the (set-valued) best response map by:
BR statio,γ : ∆ X → 2 Π , µ → BR statio,γ (µ) := argmax π∈Π J statio (π; µ) ⊆ Π,
and the (set-valued) population behavior map by:
M statio : Π → 2 ∆ X , π → M statio (π) := {µ ∈ ∆ X | µ = P µ,π µ},(5)
which is the stationary distribution obtained when using π (that we assume to be unique).

Remark 6. Note that solving the equation is not trivial since µ is involved in the transition matrix P µ,π . We come back to this point in Section 3.3.1.

Definition 2 (Stationary MF Nash Equilibrium). A pair (π,μ) ∈ Π×∆ X is a stationary mean field Nash equilibrium (stationary MFNE) if it satisfies the following two conditions:

•π ∈ BR statio,γ (μ);

•μ ∈ M statio (π).

Alternatively, an equilibrium can be defined as a fixed point:π is a stationary MFNE policy if it is a fixed point of BR statio,γ • M statio , andμ is a stationary MFNE distribution if it is the stationary distribution of a stationary MFNE policy.

In this setting, the state-action value function associated to a stationary policy π for a given distribution µ is defined as:
Q π,µ (x, a) = E   n≥0 γ n r(x n , a n , µ) x 0 = x, a 0 = a, x n+1 ∼ p(·|x n , a n , µ), a n ∼ π(·|x n )   .
The problem then reduces to a standard stationary MDP, parameterized by µ. By dynamic programming, Q π,µ satisfies the fixed point equation:
Q = B π,µ Q,
where B π,µ denotes the Bellman operator associated to π and µ:
(B π,µ Q)(x, a) = r(x, a, µ) + γE x ∼p(·|x,a,µ),a ∼π(·|x ) [Q(x , a )],(6)
where
E x ∼p(·|x,a,µ),a ∼π(·|x ) [Q(x , a )] = x p(x |x, a, µ) a π(a |x )Q(x , a ).
The optimal state-action value function is defined as:
Q * ,µ (x, a) = sup π Q π,µ (x, a).
It satisfies the fixed point equation:
Q = B * ,µ Q,
where B * ,µ denotes the optimal Bellman operator associated to µ:
(B * ,µ Q)(x, a) = r(x, a, µ) + γE x ∼p(·|x,a,µ) [max a Q(x , a )], with E x ∼p(·|x,a,µ) [max a Q(x , a )] = x p(x |x, a, µ) max a Q(x , a ).
Note that the functions Q π,µ and Q * ,µ , and the operators B π,µ and B * ,µ are all independent of time, as we search for stationary equilibria.


## Evolutive setting

We next turn our attention to a model in which not only the agents' state can evolve, but the population's distribution too. In this case, the mean field is not stationary. At each time step, the transition and the reward of every agent depends on the current distribution instead of the stationary one. The model is defined by a tuple (X , A, m 0 , N T , p, r) consisting of:

• a state space X and an action space A,
• an initial distribution m 0 ∈ ∆ X , • a time horizon N T ∈ N ∪ {+∞},
• a sequence of one-step transition probability kernels p n : X × A × ∆ X → ∆ X , n ≥ 0,

• a sequence of one-step reward functions r n : X × A × ∆ X → R, n ≥ 0.

In this context, a population behavior is a mean field flow, generally denoted by µ, which is an element of ∆ N T X . A policy is an element of Π N T , generally denoted by π. We use bold letter to stress that these are sequences, which can also be viewed as functions of time.

The total reward is:
J evol (π; µ) = E N T n=0 r n (x n , a n , µ n ) ,
subject to the following evolution of the agent's state:
x 0 ∼ m 0 ,
x n+1 ∼ p n (·|x n , a n , µ n ), a n ∼ π n (·|x n ), n ≥ 0.

This evolution is analogous to (4) except that the stationary mean-field state is replaced by the current mean-field state at time n. We assume that the transition p and the reward r are such that the total reward is well defined.

Remark 7 (Finite and infinite horizon discounted settings). Our notation covers two very common settings:

• Finite horizon: N T < +∞.

• Infinite horizon: N T = +∞. In this case, it is common to assume that p is independent of time, and that r is of the form r n (x, a, µ) = γ nr (x, a, µ) wherer is independent of time and bounded.

Note that even in the infinite horizon setting and even if p and r are stationary (constant with respect to the time parameter n), in general the optimal policy still depends on time. This is in contrast with the stationary setting (section 2.4) and is due to the fact that the population distribution starts from m 0 and evolves. The player needs to take that into account in its decisions. To be specific, even if r n (x, a, µ) =r(x, a, µ) is independent of time, the reward associated to a fixed state-action pair (x, a) isr(x, a, µ n ) at time n andr(x, a, µ n ) at time n . Unless the mean-field state is stationary, in general the two reward values will be different.

Example 3 (Crowd motion). This setting is probably the most commonly studied one in the MFG literature. As a typical example, we can think of a model for crowd motion in the spirit of e.g. Achdou and Lasry (2019): the agents start from an initial position and want to reach a point of interest while avoiding crowded areas. Because the population distribution changes as the agents move, looking for a stationary solution is not satisfactory if we want to compute the evolution of the whole population. This is because a stationary solution would only give the optimal policy (from the Nash equilibrium perspective) against the stationary distribution, and would not be able to recover the full evolution of the agents. In contrast, a time-dependent policy in the evolutive setup is able to adjust the agents' behavior step by step.

Remark 8. Discrete time finite state mean field games have been introduced by Gomes et al. (2010). In the model analyzed therein, the players can directly control their transition probabilities. Note that the model we consider here is a bit more general since the transition probabilities are functions of the actions, but they are not necessarily chosen freely by the players.

We define the (set-valued) best response map by:
BR evol,m0,N T (µ) := argmax π∈Π N T J evol (π; µ) ⊆ Π N T .
Let us define the population behavior map by:
M evol,m0,N T : Π N T → ∆ N T X , M evol,m0
,N T (π) := mean field flow when using π and starting from m 0 ,

where this flow is defined by: µ 0 = m 0 , µ n+1 = P n,µ n ,πn µ n , n ≥ 0.

When the context is clear, we will simply write µ m0,π .

Definition 3 (Evolutive MFG Nash Equilibrium). A pair (π,μ) ∈ Π N T × ∆ N T X is an evolutive mean field Nash equilibrium (evolutive MFNE) if it satisfies the following two conditions:
• Best response:π ∈ BR evol,m0,N T (μ); • Mean field flow:μ = M evol,m0,N T (π).
Alternatively, an evolutive MFNE can be defined as a fixed point:π is an evolutive MFNE policy if it is a fixed point of BR evol,m0,N T • M evol,m0,N T , andμ is an evolutive MFNE flow if it is the mean field flow generated by an evolutive MFNE policy.

The state-action value function associated to a policy π and the optimal state-action value function are defined analogously to standard MDP but parameterized by µ. We denote them respectively by Q π,µ and Q * ,µ .

For the sake of completeness, let us provide more details in the finite horizon setting. Assume N T < +∞. The state-action value function associated to a policy π for a given distribution µ is defined as:
           Q π,µ N T (x, a) = r N T (x, a, µ N T ) Q π,µ n (x, a) = E N T n =n
r n (x n , a n , µ n ) x n = x, a n = a, x n +1 ∼ p n (·|x n , a n , µ n ), a n ∼ π n (·|x n ) ,
n = N T − 1, . . . , 0.
The optimal state-action value function is defined as:
Q * ,µ (x, a) = sup π Q π,µ (x, a).

## Discounted stationary setting

We now discuss a setting that is somehow between the stationary and the evolutive ones. Note that in the stationary setting, we focus on the stationary distribution of the population while in the evolutive setting, we care about the entire sequence starting from m 0 . In the first case, we can restrict our attention to stationary policies, whereas this is not possible in the second case, as highlighted in Remark 7. An intermediate approach consists in replacing the distribution µ m0,π n at time n by an aggregate which keeps some memory of m 0 , instead of the stationary distribution. The model is defined by a tuple (X , A, m 0 , p, r, γ) consisting of:

• a state space X and an action space A,

• an initial distribution m 0 ∈ ∆ X , • a one-step transition probability kernel p :
X × A × ∆ X → ∆ X ,
• a one-step reward function r : X × A × ∆ X → R"

• a discount factor γ ∈ [0, 1).

We define the discounted distribution as:
M statio,γ,m0 (π) := µ m0,π γ := (1 − γ) n≥0 γ n µ m0,π n ∈ ∆ X ,
where µ m0,π follows the dynamics (8) but with π n = π for all n ≥ 0 after starting from m 0 at time 0, and with the mean-field term replaced by µ m0,π γ , i.e.,
µ m0,π 0 = m 0 , µ n+1 = P µ m 0 ,π γ ,π µ m0,π n , n ≥ 0.
This formulation allows us to work with a single distribution, which plays the role of a summary of what happens along the mean field flow. In contrast with the stationary MFG setting, here the initial distribution m 0 still influences the mean field term, namely, µ m0,π γ . However, we can restrict our attention to stationary policies just as in the stationary MFG setting.

Example 4 (Exploration). In (Perrin et al., 2020;Geist et al., 2021), this setting has been used for an MFG in which the agents explore the spatial domain. From the point of view of the population, it amounts to maximizing the entropy of the distribution. The discounted stationary distribution can be used as a proxy to evaluate with a single distribution how well the population explore the state space.

Remark 9. The discounted distribution can be interpreted as the stationary distribution of an agent who starts with distribution m 0 , uses policy π but has a probability to stop at any time step. To be specific, let τ be a random variable with geometric distribution on {0, 1, 2 . . . , } with parameter (1 − γ). Let us denote by µ γ,m0,π n is the distribution of x n , where:
     x 0 ∼ m 0 , x n+1 ∼ p(·|x n , a n , µ m0,π γ ), a n ∼ π(·|x n ), 0 ≤ n ≤ τ x n+1 = x n , τ ≤ n
Then we have: for every x ∈ X ,
P(x n = x) = k≤n P(τ = k)P(x k = x|τ = k) + P(τ > n)P(x n = x|τ > n) = (1 − γ) k≤n γ k µ γ,m0,π k (x) + P(τ > n)P(x n = x|τ > n).
When n → +∞, P(τ > n) → 0, so we obtain that:
µ m0,π γ (x) = lim n→+∞ P(x n = x) = (1 − γ) k γ k µ γ,m0,π k (x).
Definition 4 (Discounted stationary MFG Nash Equilibrium). A pair (π,μ) ∈ Π × ∆ X is a discounted stationary mean field Nash equilibrium (discounted stationary MFNE) if it satisfies the following two conditions:
•π ∈ BR statio,γ (μ); •μ = M statio,γ,m0 (π).
Alternatively,μ is a discounted stationary mean field Nash equilibrium distribution if it is a fixed point of: M statio,γ,m0 • BR statio,γ .


## Social optimum and Mean Field Control

The notions of MFNE discussed above correspond to non-cooperative games, in which each player maximizes her own reward while trying to anticipate the behavior of other selfish agents. A different question consists in considering that the agents are cooperative and try to maximizer a social welfare criterion by choosing together a suitable policy. This situation can also be interpreted as an optimization problem from the point of view of a social planner, who tries to figure out which behavior is optimal from the society standpoint.

Static setting. The social welfare function is defined as the reward obtained on average by the agents:
π → J social static (π) := J static (π; π).
A strategy π * is a static mean field social optimum (static MFSO) if it maximizes the social welfare function J social static .

Stationary case. The total, discounted social welfare associated to a policy π is:
J social statio (π) = J statio (π; µ π ) = E ∞ n=0
γ n r(x n , a n , µ π ) subject to:
x 0 ∼ µ π ,
x n+1 ∼ p(·|x n , a n , µ π ), a n ∼ π(·|x n ), n ≥ 0,

where µ π is the stationary distribution induced by π. Here we see that perturbing π changes µ π , which is reflected in the third argument of the transition function and the reward function. An optimal stationary policy is a π ∈ Π maximizing J social statio . This setting has been considered by Subramanian and Mahajan (2019) or by Angiuli et al. (2022b).

Evolutive case. The total social welfare is:
J social evol (π) = J evol (π; µ m0,π ) = E N T −1 n=0 r n (x n , a n , µ m0,π n ) ,
subject to the following evolution of the agent's state:
x 0 ∼ m 0 ,
x n+1 ∼ p n (·|x n , a n , µ m0,π n ), a n ∼ π n (·|x n ), n ≥ 0.

An optimal policy is a π maximizing J social evol .

Discounted stationary case. The total social welfare is:
J social d−statio (π) = J evol (π; µ m0,π γ ) = E N T −1 n=0 r n (x n , a n , µ m0,π γ ) ,
subject to:
x 0 ∼ m 0 ,
x n+1 ∼ p n (·|x n , a n , µ m0,π γ ), a n ∼ π n (·|x n ), n ≥ 0,

where µ m0,π γ = (1 − γ) n≥0 γ n µ m0,π n ∈ ∆ X is the discounted distribution introduced in Section 2.6.

Price of anarchy. The average reward obtained by a representative player can only be higher in an MFSO than in an MFNE, by the very definition of a social optimum. The discrepancy between the two situations is quantified by the following notion the price of anarchy (PoA). In the static setting, it is defined as:
PoA static = sup π J social static (π) infπ ∈N Estatic J social static (π)
.

In the denominator, N E static denotes the set of static MFNE. The PoA can be defined analogously in the other settings. The term "Price of Anarchy" has been coined by Koutsoupias and Papadimitriou (1999). Since then, this notion has been widely studied in game theory and can be viewed as a way to measure the inefficiency of Nash equilibria (Roughgarden and Tardos, 2007). In the context of MFGs, it has been studied e.g. by Lacker and Ramanan (2019) in a static setting, and by Carmona et al. (2019a) in a dynamic setting.


## Extensions

We conclude this section by mentioning a few extensions. For the sake of readability, we use the basic settings described above in the sequel. However, several variants have been considered in the literature.


## Multiple populations.

Mean field theory allows us to approximate a homogeneous population of individuals by the limiting probability distribution. In multi-population MFGs, there is a finite number of sub-populations, each of them representing a homogeneous group of agents. The transition function and the reward function are the same for all the agents of one sub-population, but may be different from one group to the other. In this way, the MFG paradigm can still be used. We refer for instance to Huang et al. (2006); Feleqi (2013); Cirant (2015); Bensoussan et al. (2018) for an analytical approach and to Carmona and Delarue (2018a, Section 7.1.1) for a probabilistic formulation. In the context of reinforcement learning, multi-population MFGs have been studied e.g. by Subramanian et al. (2020a).

Population-dependent policies. In all the previous settings, the policies are independent of the population distribution. This aspect is classical in the MFG and MFC literature because, if a player anticipates correctly the policy used by the rest of the population, they can anticipate the whole population behavior without uncertainty. As a consequence, the distribution needs not be an input to the agent's policy. However, this aspect might be counter-intuitive from a learning perspective, because it means that the agents react optimally only to the equilibrium population behavior but they cannot adjust their behavior if the distribution deviates from this equilibrium.

Population-dependent policies are tightly connected with population-dependent value functions, and the so-called Master equation in MFGs. This equation has been introduced by P.-L. Lions in the continuous setting (continuous time, state and action) (Lions, 2012). There, it is a partial differential equation (PDE) which corresponds to the limit of systems of Hamilton-Jacobi-Bellman PDEs characterizing Nash equilibria in symmetric N -player games. For more details in the continuous setting, we refer the interested reader to Bensoussan et al. (2015); Cardaliaguet et al. (2019). In the discrete time and space setting, population-dependent value functions and policies have been studied by Mishra et al. (2020) and by Perrin et al. (2021a), where a deep RL method to learn such policies is developed.


## Common noise.

Besides idiosyncratic noise affecting the evolution of each agent independently, it is possible to consider macroscopic shocks affecting the whole population. This is referred to as common noise in the MFG literature. Because the whole population's evolution is stochastic, using policies functions of the player's state only is in general suboptimal. This is because even if the player knows the policy used by all the other players, she cannot predict with certainty the evolution of the distribution. In this case, it is more efficient to use population-dependent policies. We refer to Carmona et al. (2016) and to Cardaliaguet et al. (2019) for respectively a probabilistic and an analytical treatment of MFGs with common noise.


# Iterative methods

We now turn our attention to the question of computing mean field Nash equilibria in the settings presented above. The goal is to compute a pair consisting of a policy and a mean field which form a fixed point. A simple strategy is, starting with some initial pair, to update alternatively the policy and the mean field until convergence to an equilibrium. In this section, we assume that the model is completely known. We call the algorithms presented here iterative methods for the sake of convenience and to distinguish them from the RL algorithms discussed later on. As we will discuss in the sequel, these methods rely on fixed point-type iterations. In contrast with the MDP setting, the underlying operator for these iterations is not always contractive, which triggers the introduction of variants to help ensuring convergence.


## Overview of the methods

As explained above, the main idea is to alternate an update of the population distribution and an update of the representative agent's policy, which can be represented as:
· · · → µ policy update −−−−−−−−−−−→ π +1 mean field update −−−−−−−−−−−−−−→ µ +1 → . . .(9)
At a high level, we expect the scheme described in (9) to converge towards a fixed point (μ,π) which is a Nash equilibrium.

The mean field update is done using the population distribution or the sequence of distributions induced by the current policy. Notice that, since the dynamics is known, it is straightforward to compute the mean field induced by a given policy. The converse is more challenging: except in some special cases, given a mean field, it is hard to find which policy generated it as many policies can generate the same mean field. Thus, computing not only the mean field but also an equilibrium policy is a crucial point.

The policy update can typically be done in two different ways. In the first family of methods, the policy is updated by computing a best response against the mean field. In the second family, the policy is updated based on the evaluation of the previous policy. We call these two families best-response based and policy-evaluation based respectively. In fact, this distinction stems from an analogous distinction between two families of methods to solve standard MDPs, respectively value iteration and policy iteration.

In the rest of this section, we first recall value iteration and policy iteration methods in standard MDPs. We then explain how these methods are adapted in the MFG setting. For the sake of simplicity, we focus on two settings: the stationary setting and the finite horizon evolutive setting. We stress the main similarities and differences between the methods to solve these types of MFGs. The methods in these two settings can be adapted to tackle the static setting and the γ-discounted setting, which are thus omitted for the sake of brevity.


## Solving standard MDPs

We recall here two fundamental families of methods to compute optimal policies: value iteration and policy iteration. For more details on these methods, we refer to e.g. (Sutton and Barto, 2018;Bertsekas and Shreve, 1996;Bertsekas, 2012;Puterman, 2014;Meyn, 2022).


### Value iteration

One way to obtain an optimal policy is to first compute the optimal value function by using the optimal Bellman operator, and then consider a greedy policy with respect to this optimal value function. Since we are motivated by applications to RL algorithms, we focus on the state-action value function.

Stationary MDP. In a stationary MDP, the value iteration method can be expressed as follows: Q 0 is given, and for k = 0, . . . , K − 1,
Q k+1 = B * Q k .(10)
At the end we use the following policy as an approximation of the optimal policy:
π K ∈ GQ K ,
where G denotes the greedy policy operator defined by:
GQ = π : ∀x ∈ X , a∈A π(a|x)Q(x, a) = argmax a Q(x, a) .(11)
Thanks to the fact that the Bellman operator is a γ-contraction, when K → +∞, π K → π * under suitable conditions on the MDP. Equivalently, the above iterations can also be written as follows, by introducing the greedy policy at every iteration: Q 0 is given, and for k = 0, . . . , K − 1,
π k = GQ k , Q k+1 = B π k Q k ,
where the Bellman operator B π k associated to the current policy π k is defined in (1).

Finite horizon MDP. In a finite horizon MDP, the optimal value function Q * can be computed by dynamic programming since it satisfies the optimal Bellman equation:
         Q * N T (x, a) = r N T (x, a), (x, a) ∈ X × A, Q * n (x, a) = r n (x, a) + γE max a∈A Q * n+1 (x n+1 , a) x n+1 ∼ p n (·|x, a) , (x, a) ∈ X × A, n = N T − 1, . . . , 0.(12)
Computing Q * using the above equation is called backward induction. Once it has been computed, an optimal policy can be found by taking the greedy policy at each step, i.e.:
π * = GQ * ,
where G is the finite-horizon greedy policy operator defined as:
(GQ) n = GQ n , n = 0, . . . , N T − 1.(13)
Remark 10. Notice that the Bellman equation (12) is a backward equation and not a fixed-point equation, contrary to Eq.

(2) characterizing the optimal value function in the stationary case. Since the horizon is finite, the optimal value function is computed with a finite number of steps, which is an important difference with the stationary MDP setting.


### Policy iteration

The optimal policy can also be computed by successive improvements of a policy. Starting from an initial policy, at each iteration, we evaluate the performance of this policy by computing the associated value function, and then we take a greedy step to improve the policy. These two steps are called policy evaluation and policy improvement, and the overall algorithm is called policy iteration.

Stationary MDP. In a stationary MDP, the method consists in applying the Bellman operator B π k associated to the current policy π k (see (1)) and then applying the greedy policy operator defined in (11). Thus, this method takes the following form: π 0 is given, and for k = 0, . . . , K − 1:
Q k+1 = Q π k , π k+1 ∈ GQ k+1 .
At the end, we return π K and use it as an approximation of π * . As K → +∞, we have π K → π * under suitable assumptions on the MDP. At iteration k, the value function Q π k can be computed by applying repeatedly the Bellman operator B π k until convergence to its fixed point, or until an approximation of Q π k is obtained with a finite number of iterations: with Q k,0 given, we repeat for m = 0, . . . , M − 1,
Q k,m+1 = B π k Q k,m ,(14)
and we use Q k,M as an approximation of Q π k .

Finite horizon MDP. In a finite horizon, we can define the following method by analogy with the stationary case: π 0 is given, and for k = 0, . . . , K − 1:
Q k+1 = Q π k , π k+1 ∈ GQ k+1 .
where G is the finite-horizon greedy policy operator defined in (13). At the end, we return π K and use it as an approximation of π * . At each iteration, the state-action value function associated to the current policy can be computed by backward induction. Indeed, for a given policy π, Q π satisfies the following Bellman equation, which holds by dynamic programming:
       Q π N T (x, a) = 0, (x, a) ∈ X × A, Q π n (x, a) = r n (x, a) + γE Q π n+1 (x n+1 , a n+1 ) x n+1 ∼ p n (·|x, a), a n+1 ∼ π n (·|x) , (x, a) ∈ X × A, n = N T − 1, . . . , 0.

## Solving MFGs

As explained at the beginning of this section (see Eq. (9)), the main idea underlying the methods we present below is to alternate an update of the population distribution and an update the representative agent's policy.

Inspired by the above methods for standard MDPs, we can distinguish two families of methods for MFGs, depending on whether the policy update consists in computing an optimal policy against µ or simply improving the current policy. We call these two family of methods best-response based and policy-evaluation based.


### Best response-based methods

Since an MFG equilibrium can be defined as the fixed point of a mapping, a basic strategy consists in repeatedly applying this mapping. Under suitable conditions, this method converges and the limit is automatically a fixed point.

Stationary MFG. In the stationary MFG setting (see Section 2.4), we recall that a Nash equilibrium consists of a stationary distributionμ ∈ ∆ X and a stationary policyπ ∈ Π. The policyπ is characterized as an optimal policy for a representative player facing the population distributionμ. This problem can be phrased in the framework of MDPs.

If the stationary mean field is µ, then the MDP that a representative player needs to solve is:

(X , A, p(·, ·, µ), r(·, ·, µ), γ),

where the transition and the reward functions are given by:

p(·, ·, µ) : X × A → P(X ), r(·, ·, µ) : X × A → R.

The optimal policy for this MDP is the best response against µ, which is denoted by BR statio,γ (µ). It can be obtained for example by applying the policy iteration or the value iteration algorithms as recalled in Section 3.2. Conversely, given a policy π, the induced mean field is the stationary distribution (assuming it is unique for simplicity) induced by π and denoted by M statio (π), see Eq. (5). This is summarized as follows: µ 0 is given, and for = 0, . . . , L − 1,
π +1 = BR statio,γ (µ ) µ +1 = M statio (π +1 ).(15)
At the end, we use (π L , µ L ) as a proxy for the MFG equilibrium. Under suitable conditions, it is close to (π,μ) when L is large enough. We come back to the question of convergence in Section 3.4 below.

In the above iterative method, we update the mean field term by using the operator M statio , which can be approximated by applying a large number of times the transition matrix defined in (8). In other words, in practice, µ +1 is often defined by first computing: µ n+1 = P n,µ n ,π +1 µ n , n = 0, . . . , M − 1, with µ 0 a given initial distribution. For instance we can take µ 0 = µ from the previous iteration. As M → +∞, we expect µ M → M statio (π +1 ), so we use µ M as an approximation of µ +1 .

In fact, taking M relatively small can have some advantages. In some sense, it amounts to slowing down the updates of the mean-field term. This can bring more stability to the iterative method, particularly when the policy π +1 is computed approximately (e.g., in a reinforcement learning setup). We will come back to this idea of damping the update of the mean field in Section 3.4 below, but let us immediately emphasize that a variant of the above iterative method consists in doing only one application of the transition matrix at each iteration . This can be summarized as: µ 0 is given, and for = 0, . . . , L − 1, π +1 = BR statio,γ (µ )
µ +1 = P n,µ ,π +1 µ .
This method has been used for instance by Guo et al. (2019); Anahtarci et al. (2020). It is also in line with the idea of using a two-timescale approach for mean field Nash equilibria (Subramanian and Mahajan, 2019;Mguni et al., 2018;Angiuli et al., 2022b;Xie et al., 2021). A similar method has been analyzed in (Anahtarcı et al., 2019(Anahtarcı et al., , 2020 for average cost MFGs (in the latter work, it is referred to as value iteration algorithm for MFGs).

Finite-horizon MFG. In the evolutive MFG setting with a finite horizon N T (see Section 2.5), an equilibrium is a sequence of distributionsμ = (μ n ) n=0,...,N T and a sequence of policiesπ = (π n ) n=0,...,N T , indexed by the time steps in the game. Given a sequence of distributionsμ = (μ n ) n=0,...,N T , a representative player needs to solve the following finite-horizon MDP (see Section 2.3.2):
(X , A, p µ , r µ , N T ),
where:
p µ : {0, . . . , N T − 1} × X × A → P(X ), p µ : (n, x, a) → p n (·|x, a, µ n )
and r µ : {0, . . . , N T } × X × A → R, r µ : (n, x, a) → r n (x, a, µ n ).

The optimal policy for this MDP is the best response against µ, which is denoted by BR evol,m0,N T (µ) . It can be obtained as a greedy policy for the optimal value function Q * ,µ , which can be computed by backward induction as described in Section 3.2.1. Alternatively, the optimal policy can be computed by policy iteration as described in Section 3.2.2. Conversely, given a policy π = (π n ) n=0,...,N T , the induced mean-field is the sequence of distributions generated by starting from m 0 (remember that m 0 is fixed in the definition of the MFG, see Section 2.5) and using π n at time step n, n = 0, . . . , N T − 1. The resulting mean-field sequence is denoted by M evol,m0,N T (π), see Eq. (7). This is summarized below, using the notation introduced in Section 2.5: µ 0 is given, and for = 0, . . . , L − 1,
π +1 = BR evol,m0,N T (µ ) µ +1 = M evol,m0,N T (π +1 ).
At the end, we use (π L , µ L ) as a proxy for the MFG equilibrium. Under suitable conditions on the MFG, this pair is close to (π,μ) when L is large enough. For the sake of completeness and future reference, we provide here the Bellman equations satisfied by Q * ,µ and Q π,µ , which can be derived by dynamic programming: a) x n+1 ∼ p n (·|x n , a n , µ n ) ,
         Q * ,µ N T (x, a) = r N T (x, a, µ N T ) Q * ,µ n (x, a) = r n (x, a, µ n ) + E max a∈A Q * ,µ n+1 (x n+1 ,n = N T − 1, . . . , 0,(16)
and Perrin et al. (2020Perrin et al. ( , 2021a used backward induction to compute the optimal value function for finite-horizon MFG (embedded in fictitious play iterations, see Section 3.4), which served as a baseline to assess the performance of RL-based methods (see next section). Cui and Koeppl (2021a) solved finite-horizon MFG using fixed point iterations combined with RL methods and entropy regularization (we come back to this point in Section 3.4 below). Mishra et al. (2020) also solved MFGs based on a best-response computation, but by computing a best response backward in time in the spirit of dynamic programming, which requires solving for all possible distributions since the equilibrium mean field sequence is not known a priori. The aforementioned two-timescale approach originally studied in the stationary setting has been extended by Angiuli et al. (2021) to solve finite-horizon MFGs.
       Q π,µ N T (x, a) = r N T (x, a, µ N T ) Q π,µ n (x, a) = r n (x, a, µ n ) + E Q π,µ n+1 (x n+1 , a n+1 ) x n+1 ∼ p n (·|x n , a n , µ n ), a n+1 ∼ π n+1 (·|x n+1 ) , n = N T − 1, . . . , 0.(17)
Remark 11. In the stationary regime, we can view iterations as time steps. Taking a large number of iterations amounts to looking at the long time behavior. However, in the finite-horizon MFG setting, the index of iterations does not coincide with the index of time in the game. At each iteration , the policy and the distributions are updated for all time steps, n = 0, . . . , N T .


### Policy evaluation-based methods

Instead of computing a full-fledged best response for the policy update at each iteration of (9), we can simply do one step of policy improvement. Intuitively, evaluating the current policy should be computationally faster than computing an optimal policy (except when the state space is small or when we have an explicit formula for the optimizer of the value function). To improve the policy, we can first evaluate the current policy given the latest mean field, and then take a greedy policy.

Stationary MFG. In a stationary MFG, we can proceed as follows: we first compute the state-action value function associated to the current policy against the current population distribution (policy evaluation step). We then define the new policy as a greedy policy for the newly computed value function (policy improvement step). Last, we deduce the stationary population distribution induced by this policy (mean field update step). Concretely, the method is: µ 0 and π 0 are given, and for = 0, . . . , L − 1,
       Q +1 = Q π ,µ π +1 ∈ GQ +1 µ +1 = M statio (π +1 ).(18)
This method is referred to as the Policy Iteration (PI) algorithm for MFGs and was introduced by Cacace, Simone et al. (2021) for continuous time, continuous space MFGs. It is not to be confused with the method that consists in using standard policy iteration to compute a best response against a given distribution (i.e., replacing BR statio,γ (µ ) in (15) by the result of a policy iteration method). In practice, the evaluation step can be done by applying a finite number of times the Bellman operator B π ,µ as defined in Eq. (6). Thanks to the contraction property of this operator, we obtain an approximation of Q π ,µ . Furthermore, as discussed above, M statio (π +1 ) can be approximated by applying a large but finite number of times the transition matrix.

Finite-horizon MFG. In a finite-horizon MFG, the same strategy can be applied, except that we need to take into account the evolutive aspect of the game. Each of the step is done for all the time steps. The method can be summarized as follows: µ 0 and π 0 are given, and for = 0, . . . , L − 1,
       Q +1 = Q π ,µ π +1 ∈ GQ +1 µ +1 = M evol,m0,N T (π +1 ).(19)
In this setting, Q π ,µ can be computed by backward induction, thanks to the dynamic programming equation (17). Similarly, M evol,m0,N T (π +1 ) can be computed by following N T transitions, see (8).

Cacace, Simone et al. (2021), mentioned above, also studied policy iteration in the finite-horizon setting and proved convergence under suitable conditions. Still in the finite-horizon setting, the convergence results were extended to other settings by Camilli and Tang (2022); Laurière et al. (2021). Using a purely greedy policy often leads to instabilities, particularly in the finite state case; see e.g. (Cui and Koeppl, 2021a) and the next section for more details. For this reason, variants with regularized policies have been introduced, such as the online mirror descent, as we explain below.


## Convergence and variants

Convergence of fixed point iterations. Intuitively, the scheme described in (9) indeed converges towards a fixed point if the mapping (µ , π ) → (µ +1 , π +1 ) is a strict contraction on a suitably defined space. In a stationary setting, this property can be ensured by assuming that the reward function and the transition function are smooth enough. Typically, this amounts to assuming that they are Lipschitz continuous with small enough Lipschitz constants. In a finite horizon setting, this condition can sometimes be replaced by an assumption on the smallness of the time horizon. One advantage of having a contraction is that it provides a constructive way to get the equilibrium through Banach-Picard iterations. This technique is commonly used in the literature on MFGs, both to show existence and to derive algorithms. See e.g. Huang et al. (2006) in the context of existence and uniqueness of the equilibrium or Carlini and Silva (2014) in the context of numerical methods. It is in general difficult to formulate sufficient conditions on the model (i.e., the reward and the transition) to ensure the strict contraction property because the mapping involves the policy update step, for which there is in general no explicit formula. In the linear-quadratic case, several sufficient conditions are formulated by Hu (2021, Proposition 3.1). Furthermore, regularizing the policy can help to alleviate some of the conditions ensuring the contraction property, see e.g. Guo et al. (2019); Anahtarci et al. (2020). Using regularization of the policy, Guo et al. (2020a) have proved convergence and analyzed the complexity of value-based and policy-based algorithms.

However, conditions guaranteeing the strict contraction property are generally very restrictive and fails to hold for many games. For example, Cui and Koeppl (2021a, Theorem 2) show that non-contractivity is the rule rather than the exception. Without contractivity, Banach-Picard iterations typically lead to oscillations, see e.g. Chassagneux et al. (2019, Figure 3) in the context of a method based on the probabilistic interpretation of MFGs, or Lauriere (2021, Figure 4) in the context of linear-quadratic MFGs.

To address this issue, several variants of the pure Banach-Picard fixed point iterations have been proposed in the literature, relying on a few key principles.

Before describing these principles, let us mention that besides the aforementioned class of assumptions to ensure contractivity which are somehow quantitative assumptions since they boil down to smallness of some coefficients, an alternative class of hypotheses are in some sense qualitative assumptions which pertain to the structure of the game. For example, potential structure and MFG satisfying Lasry-Lions monotonicity (Lasry and Lions, 2007) can be used to prove convergence of best-response based and policy evaluation based algorithms, see respectively (Cardaliaguet and Hadikhanloo, 2017;Perrin et al., 2020;Geist et al., 2021) and (Hadikhanloo, 2017;Perolat et al., 2021). In particular, the Lasry-Lions monotonicity condition, which basically refers to the fact that players tend to avoid crowded regions, has been interpreted in terms of exploitability (see Section 5.1.2). These convergence results do not rely on smallness conditions on the coefficients. However, even for MFGs with such nice structure, pure fixed point iterations rarely converge and smoothing the iterations if typically required to ensure convergence.


## Smoothing the mean field updates.

First, a simple modification consists in using damping to slow down the updates of the mean field term. Even if the mapping µ → π → µ +1 is not contractive, we can hope that the following mapping is contractive, at least for small enough values of α ∈ (0, 1):
µ → π →μ +1 := (1 − α)μ + αµ +1 .(20)
Here µ +1 is the mean field associated to policy π whileμ is an average over past mean field terms. See e.g. Lauriere (2021, Section 2) for an example in which damping with a constant coefficient helps ensuring numerical convergence. We also refer to Tembine et al. (2012) for more algorithms developed along these lines and presented in the context of static games. We can also let α change with the iteration index, i.e., take a different α for = 1, 2, . . . . One of the most popular versions consist in taking α = 1/( + 1) and is called Fictitious Play. It was first introduced in two-player games by Brown (1951); Robinson (1951) and extended to MFG by Cardaliaguet and Hadikhanloo (2017); Hadikhanloo (2018); Hadikhanloo and Silva (2019). In the context of stationary MFGs for example, (15) is replaced by: µ 0 is given, and for = 0, . . . , L − 1,
         π +1 = BR statio,γ (μ ) µ +1 = M statio (π +1 ) µ +1 = + 1μ + 1 + 1 µ +1 .(21)
Under suitable assumptions,μ converges to a stationary MFG equilibrium distribution. It is important to note that in general the last iterate π of the policy does not generateμ and hence does not converge towards an equilibrium policy. If one cares about the equilibrium policy, it is thus required to learn a policy generatingμ . In some cases, convergence of the last iterate towards an equilibrium holds, see e.g. Cardaliaguet and Hadikhanloo (2017). For finite-state MFGs, a rate of convergence has been obtained by Perrin et al. (2020) for continuous-time FP under monotonicity condition and by Geist et al. (2021); Bonnans et al. (2021) respectively for discrete-time FP in some potential MFGs. In linear-quadratic MFGs, a rate of convergence has been obtained by Delarue and Vasileiadis (2021), who also studied the impact of common noise.

Slowing down the updates of the mean field term is also in line with the idea of using a two-timescale approach for mean field Nash equilibria (Subramanian and Mahajan, 2019;Mguni et al., 2018;Angiuli et al., 2022b;Xie et al., 2021). Here, the distribution and the policy (or the value function) are both updated at every iteration but the distribution is updated at a slower rate than the policy. Intuitively, this implies that the representative agent has enough time to compute an approximate best response before the distribution changes too much.

Smoothing the policy updates.

Another way to bring more stability to the iterative method is to regularize the policy update. For instance, the greedy policy operator defined in (11) is very sensitive to perturbations of the stateaction value function. Small changes in this value function might lead to significant changes in the induced greedy policy. To mitigate this problem, it is common to replace the argmax by a softmax, meaning that we can define:
π (k+1) (·|x) = softmax τ Q(x, ·),
where τ > 0 is an inverse temperature parameter and softmax : R |A| → ∆ A is defined by: for q = (q 1 , . . . , q |A| ),
softmax τ (q) = e τ qi |A| j=1 e τ qj i=1,...,|A| .
It transforms a vector of Q-values into a discrete probability distribution on the action space in which the actions with larger value have a higher probability. Using a softmax instead of the argmax generally yields smoother and more stable learning curves, see e.g. Guo et al. (2019); Anahtarci et al. (2020).

In fact, finite-state finite-action MFGs typically admit only randomized policy equilibria and no pure equilibria. This is also the reason why we generally allow for randomized policies in finite-player games (Nash, 1950(Nash, , 1951. Hence, iterative methods with pure greedy policies cannot be expected to converge to Nash equilibria in general, and using mixed policies is unavoidable.

Regularized policies can be obtained e.g. by directly changing the way the policy is obtained from the value function Perolat et al., 2021) or by adding a penalty in the reward function, which changes the value function and hence the policy, see e.g. Anahtarci et al. (2019); Guo et al. (2020b); Cui and Koeppl (2021a); Firoozi and Jaimungal (2022); Laurière et al. (2022). However, it should be noted that regularizing the policies also has drawbacks: if π is forced to be smooth, this constraint might prevent the iterative method from converging towards the Nash equilibrium since π can only be smooth version of the equilibrium policy.

One way to circumvent this limitation and to allow the regularized policy to concentrate on optimal actions is to let the underlying Q-function take larger and larger values. This can be achieved by considering a cumulative Q-function, which leads to the Online Mirror Descent (OMD) algorithm (Hadikhanloo, 2017;Perolat et al., 2021):
           Q +1 = Q π ,µ Q +1 =Q + αQ +1 π +1 = softmax τQ +1 µ +1 = M statio (π +1 ).(22)
where α > 0 is a parameter which determines the cumulative factor. This algorithm can be viewed as a modification of the policy evaluation method described in (18) with a cumulative Q-function and a regularized greedy policy. Instead of the softmax, we can more generally take the gradient of the convex conjugate of a strongly convex regularizer, see Perolat et al. (2021) for more details.


## Iterative methods for Mean Field Control

We recall that the MFC problem introduced in Section 2.7 corresponds to the maximization of a social reward. In the evolutive case, it can be reformulated as an MDP by considering the population distribution as the state. Indeed, we can rewrite: , subject to the following evolution of the mean field state:
µ m0,π 0 = m 0 , µ m0,π n+1 = P n,µ m 0 ,π n ,πn µ m0,π n , n ≥ 0.
This is an MDP with:

• state space ∆ X ,

• action space Π,

• probability transition function:p n (·|µ, π) = (P µ,π n ) µ,

• reward function:r n (µ, π) = x∈X a∈A r n (x, a, µ)µ(x)π(a|x).

We will refer to this MDP as the mean field MDP (MFMDP). An action, taken by the central planner or collectively by the population, is an element of A = (∆ A ) X . A one-step policy at the level of the population is a function from X to ∆ A . Note that, even if X and A are finite, the state space X and the action space A of the MFMDP are continuous and hence rigorously defining and analyzing this MDP requires a careful formulation. We refer to the work of e.g. Gast et al. (2012); Gu et al. (2019Gu et al. ( , 2021a; Motte and Pham (2019); Carmona et al. (2019c); Bäuerle (2021) for more details on MFMDP.

Let us stress that this MFMDP is not to be confused with the MDP arising in MFGs, which is the MDP for a single representative player when the mean field term is given. In the latter case, the state is simply the agent's state and not the population state.

With this reformulation, the evolutive MFC problem can be analyzed and solved using methods developed from MDP. However, notice that the policies are, in general, functions of both the representative agent's state and the mean field state. The main challenges thus pertain to the numerical implementation of these methods, since we need to represent efficiently the distribution and the policy. We will come back to this question in Section 4.3.

Remark 12. Note that, in the present model, the evolution of µ m0,π is in fact completely deterministic once m 0 and π are given. Noise affecting the distribution and making its evolution stochastic is referred to as common noise. We refer to Motte and Pham (2019); Carmona et al. (2019c) for more details. Furthermore, since an action is an element of Π, a policy is a functionπ : ∆ X µ →π(µ) ∈ ∆ Π . Sampling fromπ(µ) amounts to sample an element π to be used by the whole population. Carmona et al. (2019c) referred to this as common randomness.


# Reinforcement learning algorithms

The iterative methods presented in the previous section are described with exact updates, meaning that we assume that the model is fully known and that there are no numerical approximations in the computation of the rewards or the transitions. In this context, the only approximations that we have to cope with are in situations where an infinite number of iterations would be needed but we can only afford a finite number of iterations (e.g., to compute a stationary distribution or a stationary value function).

However, in many situations, these methods cannot be implemented as such. A typical scenario is when the model is not completely known from the agent that is trying to learning an optimal behavior. Another instance is when the model is known, but the state space or the action space are too big for us to compute the solution on the whole domain. In such cases, exact dynamic programming cannot be used. Instead (model-free) reinforcement learning (RL) methods have been developed. Here we will focus on methods relying on approximate dynamic programming (ADP). The question of exploring efficiently the state-action domain plays a crucial role.

RL ideas have first been developed for finite and small state and action spaces, in which case the algorithms are called tabular methods since the value function can be described by a table (i.e., a matrix). However, many of the recent breakthrough applications of RL have been obtained thanks to a combination of RL methods with neural network approximations and deep learning techniques, which leads to deep reinforcement learning (DRL) methods. The flexibility and the generalization capabilities of deep neural networks allow us to efficiently learn solution to highly complex problems. In the context of games, some striking examples that were successfully tackled are ALE (Atari Learning Environment) (Mnih et al., 2013;Bellemare et al., 2013), Go , poker (Brown and Sandholm, 2017;Moravčík et al., 2017) or Starcraft (Vinyals et al., 2019).

In the context of MFGs, we will build on the iterative methods presented in Section 3. These methods boil down to alternating mean-field updates and policy updates, and the policy updates stem from standard MDP techniques. As a consequence, standard RL techniques can readily be injected at this level to learn policies or value functions.

In this section, starting from exact dynamic programming, we discuss some key ideas underlying ADP and RL methods. We then move on to neural network approximations and DRL. Finally we explain how these ideas can be adapted to the MFG setting.
Environment Agent Reward r n+1 State x n+1
Action a n Reward r n State x n Figure 1: Reinforcement learning environment: classical single-agent setup. Here, at iteration n, the current state of the MDP is x n , the action taken by the agent is a n , the new state is x n+1 ∼ p(·|x n , a n ) and the reward is r n = r(x n , a n ).

The new state x n+1 is observed by the agent and is also used for the next step of the environment's evolution.


## Background on reinforcement learning

Environment. Traditional RL aims at solving a stationary MDP, see Section 2.3.1. In the typical setting, the agent who is trying to find an optimal policy for the MDP interacts with an environment through experiments that can be summarized as follows:

1. The agent observes the current state x of the MDP (which is referred to as the state of the environment but could be for instance her own state or the state of the world).

2. The agent takes an action a, which is going to influence the state of the MDP through the transition kernel p and produces a reward through the function r.

3. The agent observes the new state x ∼ p(·|x, a) as well as the reward r(x, a) resulting from her action.

The agents can repeat such experiments. We provide in Fig. 1 a schematic representation of this setting. It is often assumed that the agent can reboot the environment from time to time. To avoid remaining stuck in local maxima, it is common to assume that the new state is picked randomly, which is referred to an exploring start. We stress that the agent does not observe directly the functions p and r that are used to compute the new state and the reward. The agent only observes the outputs of these functions. In some cases, recovering the functions p and r from such observations is feasible, leading to the concept of model-based RL. However, for complex environments (i.e., complex p and r), recovering the functions would require such a large number of observations that we generally prefer to directly aim for an optimal policy, which leads to the concept of model-free RL. The agent needs to interact multiple times to figure out the most suitable actions for a given state of the world. For more details, we refer the interested reader to e.g. Sutton and Barto (2018); Bertsekas (2012); Szepesvári (2010); Meyn (2022).

Approximate dynamic programming. Some of the most popular RL methods are based on approximations of the exact dynamic programming equations satisfied by the value functions. Focusing on a stationary MDP, let us recall that an optimal policy can be computed for instance by value iteration or policy iteration (see Section 3.2), which require computing the state-action value functions Q * or Q π respectively. These two functions satisfy fixed-point equations whose solutions can be approximated by repeatedly applying the corresponding Bellman operators B * and B π , see (10) and (14). This amounts to repeating:
Q π (x, a) ← r(x, a) + γE x ∼p(·|x,a),a ∼π(·|x ) [Q(x , a )], ∀(x, a) ∈ X × A Q * (x, a) ← r(x, a) + γE x ∼p(·|x,a) [max a Q * (x , a )], ∀(x, a) ∈ X × A.
The arrow is used to denote that the value of Q π (x, a) is replaced by the value in the right hand side.

In the context of RL, we assume that the agent does not know r or p, so she cannot perform the above updates. However, these updates can be performed approximately provided we assume that the agent can query the environment and ask, for any pair (x, a), the value of r(x, a) and a sample x ∼ p(·|x, a) (picked independently at each realization). Then to update Q π (x, a) and Q * (x, a), the agent can query multiple times the pair (x, a) and replace the expectations by empirical averages:
Q π (x, a) ← r(x, a) + γ 1 I I i=1Q (x i , a i ), x i ∼ p(·|x, a), a i ∼ π(·|x i ), i = 1, . . . , I, ∀(x, a) ∈ X × Ã Q * (x, a) ← r(x, a) + γ 1 I I i=1 max a Q * (x i , a ), x i ∼ p(·|x, a), i = 1, . . . , I, ∀(x, a) ∈ X × A,
where the Monte Carlo samples x i and a i are independent. However, it is generally too computationally expensive to update every pair (x, a) using a batch of I samples. Furthermore, in many scenarios the agent does not have the freedom to query any state x. Instead, she is usually bound to observe the state of the environment, which is updated iteration after iteration in a sequential manner by following the dynamics of the state. She can influence the evolution of the state, but she cannot pick any new state that she wants at every iteration. In such scenarios, the agent can only perform updates by using the available information at each iteration.

To be specific, let us assume that the agent has a policyπ that she uses to generate a trajectory by interacting with the environment:

x 0 ∼ m 0 , a n ∼π(·|x n ), x n+1 ∼ p(·|x n , a n ), n ≥ 0.

The fixed-point equation satisfied by Q * says thatQ * (x, a) is well estimated if:
Q * (x, a) = r(x, a) + γE x ∼p(·|x,a),a ∼π(·|x ) max a Q * (x , a ) .(23)
So it is natural to use the discrepancy between the right hand side and the left hand side to improve the estimateQ * of Q * . Since we are bound to follow the trajectory, we cannot get the expectation over x and, instead, we perform sampled-based updates using one sample at each step and a learning α > 0:

Q * (x n , a n ) ←Q * (x n , a n ) + α r(x n , a n ) + γ max a Q * (x n+1 , a ) −Q * (x n , a n ) .

This leads to the celebrated Q-learning algorithm introduced by Watkins (1989) and whose convergence under suitable conditions has been proved by Watkins and Dayan (1992). Estimating correctly the whole function Q * can be ensured if every pair (x, a) is visited infinitely often, which can be guaranteed under some assumptions on the dynamics of the state and by takingπ for instance as an -greedy policy (according to which in every state, every action has some probability to be selected). To estimateQ π , a similar strategy can be used.


## Deep reinforcement learning.

When state and action spaces are finite, a state-action value function is simply a matrix, which can be stored in memory and processed easily when the spaces are small enough. Thanks to this, we can update the value function point by point (one point being a state-action pair in the case of Q-functions). However, when the spaces are very large or even continuous, it becomes impossible to evaluate precisely every pair (x, a). Furthermore, it is also impossible to visit all pairs during training, impliying that the question of generalization (i.e., performance on unvisited pairs) cannot be avoided. Motivated by both efficiency and generalization capabilities, we can approximate the state-action value functions by parameterized non-linear functions such as neural networks. For example, let us approximate Q * by a neural network Q θ with a given architecture and parameters θ. Going back to (23), we note that now, not only we do not know the expected value, but also we cannot update the function only at a specific pair without changing its value at other pairs. Instead, we use the discrepancy between the left hand side and an estimation of the right hand side to define a loss function that can be used to train the neural network Q θ . Since this neural network appears in both sides of the equation, to make the learning process more stable, we introduce an auxiliary neural network Q θtarget called target network and we use it to replace Q θ in the right hand side. The parameters θ target are fixed when we update θ, and are updated from time to time but less frequently than θ. To be specific, we define the loss:
L(θ; θ target ) = E x,a Q θ (x, a) − r(x, a) − E x ∼p(·|x,a) max a Q θtarget (x , a ) 2 ,(24)
where the expectation is over state-action pairs. We do not specify here the distribution of this pair, but in practice it typically comes from played trajectories stored in a replay buffer. In practice, this loss is minimized using stochastic gradient descent on mini-batches sampled from this replay buffer. This leads to the DQN algorithm introduced by Mnih et al. (2013). The above approach uses the fact that we can easily compute the maximal value of Q θtarget (x , ·) over the action space. This is typically possible only if the action space is finite and not too large. Otherwise, we can use another neural network to encode a policy and train this neural network so that it approximates an optimal policy, using a so called policy loss. Then, in loss (24), the term max a Q θtarget (x , a ) is replaced by the expectation of the target Q-value according to the learnt policy. The resulting agent is called an actor-critc (the actor is the policy, the critic is the state-action value function). Since our goal is not to present an exhaustive list of methods, we simply mention below three popular approaches, to give an idea of the variety of algorithms:

• If we consider only deterministic policies, we can replace the policy by a parameterized function ϕ ω : X → A with parameters ω. In this case, the corresponding policy loss optimises for
max ω E x [Q θtarget (x , ϕ ω (x ))].
Its gradient can be obtained using the chain rule. This leads to an algorithm that is reminiscent of the the Deep Deterministic Policy Gradient (DDPG) of Lillicrap et al. (2016).

• Another approach is to look for a general stochastic policy π, in which case we have the interpretation :
E a ∼π(·|x ) [Q θtarget (x , a )] = A Q θtarget (x , a )π(a |x )da .
Taking the gradient and using the log trick yields the state-wise gradient:
E a ∼π(·|x ) Q θtarget (x , a )∇ log π(a |x ) .
The resulting algorithm is reminiscent of Reinforce (Williams, 1992). The related empirical gradient requires sampling from the learnt policy and is usually of high variance. An alternative approach consists in using the reparameterization trick. For example, we can restrict our attention to Gaussian policies
π ω (·|x) = Φ N (mω(x),σω(x)I) (·),
where Φ N (mω(x),σω(x)I) denotes the density function of the normal distribution N (m ω (x), σ ω (x)I), with m ω and σ ω being two parameterized functions with parameters ω. Here I denotes the identity matrix on A. This leads to the following approximation:
E a ∼π(·|x ) [Q θtarget (x , a )] ≈ E a ∼πω(·|x ) [Q θtarget (x , a )] = E ∼N (0,I) [Q θtarget (x , m ω (x) + σ ω (x) )].
Here again, we can replace the expectation by an average over a finite number of realizations of . In this way, we obtain an algorithm which is similar to TD3 (Fujimoto et al., 2018).

Since the goal of this section is simply to describe some of the key ideas behind DRL, we do not go further into a detailed presentation of the variety of existing methods. We refer the interested reader to e.g. Arulkumaran et al. 


## Reinforcement learning for MFGs

We now turn our attention to RL methods for MFGs. The iterative methods presented in Section 3 allow us to solve an MFG by iteratively updating the population distribution and the policy, and the policy update can be done using standard MDP techniques. As a consequence, RL techniques to solve MDPs can directly be adapted to solve MFGs in a model-free fashion.


### Environments with mean field interactions

To study RL methods for MFGs, the first question is the definition of the environment. In MFGs, the transitions and the rewards depend on the population distribution, which should thus be part of environment. Since we are going to focus on how a representative agent learns an equilibrium policy, we also include the state of this representative agent in the state of the environment.

The next question is: What is the information available to the agent who is learning? In other words, we should decide what the output of one query to the environment is. Remember that for a given population distribution (or sequence of distributions in the evolutive setting), the agent tries to solve an MDP parameterized by this distribution but the policy is not a function of the population distribution (see e.g. Section 4.1 in the stationary MFG case). From this point of view, to learn an optimal policy, she does not need to observe the rest of the population: it is sufficient to observe the result of the reward function and some samples of transitions.

Remark 13. The fact that the representative agent does not need to observe the rest of the population in order to learn an optimal policy is specific to the mean-field setting with an infinite number of players. In a finite-player game, the equilibrium policy of each player generally depends on the configuration of the rest of the population, even when the interactions are symmetric or when a mean-field approximation is used, see e.g. Yang et al. (2018b); Yang and Wang (2020); Zhang et al. (2021) in the context of MARL. Focusing on population-independent policies (which are sometimes referred to as decentralized policies) is one of the main advantage of the MFG approach compared with a finite-player game framework.

We summarize the environment in Fig. 2, which is very similar to the classical RL setup described in Fig. 1 except that the distribution is involved in the environment.

Remark 14 (On the implementation of the environment). In some cases, the environment is truly based on the mean field state corresponding to the regime with an infinite number of agents. This can be the case for example when the state space and the action space are finite and small, and the evolution of the distribution or the stationary distribution can be computed exactly using the transition matrix. However, in general, the environment relies on some approximate version of the population distribution (e.g., using an empirical distribution with a finite number of agents, or using some function approximations). Compared with the ideal environment with the true mean-field distribution, this adds an extra layer of approximation which can be neglected if one is purely interested in the performance of RL algorithms. We come back to this point in Section 4.3 below, in the context of MFC.


### Reinforcement learning for MFGs

We focus on two settings: stationary and finite horizon. The ideas developed in these cases can be adapted to tackle static and infinite horizon MFGs.


## Stationary MFG setting.

In a nutshell, in the stationary MFG setting, when using one of the iterative methods presented in Section 3, at each iteration the representative agent faces a stationary MDP parameterized with a fixed distribution µ. We can thus use off-the-shelf RL methods.

To be more specific, we assume that a representative agent is encoded by a stationary policy π ∈ Π, either explicitly or implicitly (through a Q-function) and can interact with the environment in the following way: at each step, the agent Environment Agent Reward r n+1


## State

x n+1

Distribution µ n Action a n Reward r n State x n Figure 2: Environment for MFGs: Here, the current state of the MDP is the representative agent's state x n and the population distribution µ n , the action taken by the agent is a n , the new state is x n+1 ∼ p(·|x n , a n , µ n ) and the reward is r n = r(x n , a n , µ n ). The new state x n+1 is observed by the agent and is also used for the next step of the environment's evolution along with µ n .

observes its current state x, chooses action a ∼ π(·|x), and the environment returns a realization of x ∼ p(·|x, a, µ) and r(x, a, µ). Note that the agent does not need to observe directly the mean field flow µ, which is stored in the environment and simply enters as a parameter of the transition and reward functions p and r. In this stationary setting, in Fig. 2, µ n is constant equal to µ for all n. Based on such samples, the representative agent can implement any of the RL methods (e.g. the ones discussed in Section 4.1) for standard MDPs.

Notice that, at each new step of the iterative method described in (9), after the mean field update the environment needs to be updated with the new population distribution. That is to say, when the mean field state is µ ( ) , the agent uses the environment of Fig. 2 with µ n = µ ( ) for every n to learn a best-response or evaluate a policy. Here n is the index of the RL method iteration. Then, the new mean field µ ( +1) is computed. For the next iteration, the MDP is updated so the agent interacts with the environment of Fig. 2 but now with µ n = µ ( ) for every n.

For stationary MFG equilibria, Guo et al. (2019) introduced a best-response based iterative method with tabular Qlearning to compute the best response at each iteration. Moreover, they proved convergence using bounds on classical Q-learning, combined with a strict contraction argument. Guo et al. (2020a) generalized this idea notably using a policy gradient approach in lieu of Q-learning. A similar algorithm but combined with fitted Q-learning instead of tabular Q-learning was analyzed and proved to converge by Anahtarci et al. (2019). Furthermore, Anahtarci et al. (2020Anahtarci et al. ( , 2021 showed that some of the conditions to obtain convergence in the tabular Q-learning case can be relaxed if the MDP is regularized. Subramanian and Mahajan (2019); Angiuli et al. (2022b) used a two-timescale approach combined with model-free RL to compute stationary equilibria. The convergence has been proved under suitable conditions on the underlying ODEs by using stochastic approximation techniques (Borkar, 2009).

In the γ-discounted setting, Elie et al. (2020b) analysed the propagation of error in fictitious play (i.e., how errors made in the computation of the best response propagate through the algorithm) and implemented this scheme with an actor-critic DRL method (namely, DDPG (Lillicrap et al., 2016)) to compute the best response. Fictitious play and DRL combined with neural network approximation of the population distribution allowed Perrin et al. (2021b) to solve a flocking model with continuous and high-dimensional space. Still in the γ-discounted setting, Perrin et al. (2020) provided a convergence rate for continuous-time fictitious play under monotonicity assumption, while Geist et al. (2021) established a rate of convergence for discrete-time fictitious play in MFGs with a potential structure.

Finite horizon MFG setting. To learn finite horizon MFG solutions in a model-free way, we assume that a representative agent is encoded by a time-dependent policy π = (π n ) n=0,...,N T −1 and can interact with the environment to realize episodes. Each episode is done in the following way: the environment picks x 0 ∼ m 0 and reveals it to the agent; then for n = 0, . . . , N T , the agent observes x n , chooses action a n ∼ π n (·|x n ), and the environment returns a realization of x n+1 ∼ p n (·|x n , a n , µ n ) as well as the value of r n (x n , a n , µ n ). Note that the agent does not need to observe directly the mean field flow (µ n ) n=0,...,N T , which simply enters as a parameter of the transition and reward functions p n and r n .

Based on such episodes, the agent can for example estimate a policy π by approximately computing the stateaction value function Q π,µ , or compute a best response by first approximating the optimal value function Q * ,µ . The value functions can be estimated by backward induction as in (17) and (16), replacing the expectation by empirical averages over Monte Carlo samples. Perrin et al. (2020) solved finite-horizon MFG by a fictitious play method in which the best responses are computing using tabular Q-learning. Mishra et al. (2020) proposed a combination of RL and backward induction to solve finite-horizon MFGs by approximating the policy starting from the terminal time. Cui and Koeppl (2021a) applied best-response based and policy-evaluation based methods combined with DRL techniques and studied numerically the impact of entropy regularization on the convergence of these methods. Although DRL methods offer many promises in terms of scalability, it is in general hard to average or sum non-linear function approximators such as neural networks. Laurière et al. (2022) proposed best-response based and policy-evaluation based methods (namely fictitious play and OMD) with DRL techniques in such ways that average or sum of neural networks can be approximated efficiently. This leads to scalable model-free methods for finite-horizon MFGs.


### Some remarks about the distribution

Observing the mean field. In the above presentation, we assume that the agent does not observe the distribution, or at least does not exploit this information to learn the equilibrium policy. Although this is the most common approach in the RL and MFGs literature, the question of learning population-dependent policies arises quite naturally since one could expect that agents learn how to react to the current distribution they observe. This is usual in MARL, see e.g. Yang et al. (2018b) who consider Q-functions depending on the actions of all the other players. In MFGs, we can expect that by learning a population-dependent policy, the agent will be able to generalize, i.e., to behave (approximately) optimally even for population configurations that have not been encountered during training.

Such policies take as input a distribution, which is a high-dimensional object. As a consequence, they are much more challenging to approximate than population-independent policies. Mishra et al. (2020) considered a populationdependent value function and proposed an approach based on solving a fixed point at each time step for every possible distribution. Implementing this approach (at least in its current form) seems feasible only for very small state space. Perrin et al. (2021a), introduced the concept of master policies, which are population-dependent policies allowing to recover an equilibrium policy for any observed population distribution. They can be approximately computed by a combination of Fictitious play, DRL, and a suitable randomization of the initial distribution.

The concept of a value function depending on the population distribution is connected to the so-called Master equation in MFGs. Introduced by Lions (2012) in continuous MFGs (continuous time, continuous state and action spaces), this partial differential equation (PDE) corresponds to the limit of systems of Hamilton-Jacobi-Bellman PDEs characterizing Nash equilibria in symmetric N -player games. We refer the interested reader to e.g. Bensoussan et al. (2015); Cardaliaguet et al. (2019) for more details on this topic.

Distribution estimation. When the state space is finite but very large, storing the population distribution in a tabular way for every state and computing the evolution of this distribution in an exact way is prohibitive in terms of memory and computational time. Representing and updating the distribution is even more challenging in the continuous space setting, even if it is just for the purpose of implementing the RL environment. In this case, one needs to rely on approximations. As already mentioned above, a possible method consists in using an empirical distribution, whose evolution can be implemented by Monte Carlo samples of an interacting agent system. This amounts to using a finite population of agents to simulate the environment. For example, in linear-quadratic MFGs the interactions are only through the mean, which can be estimated even using a single agent, see Angiuli et al. (2022b) in the stationary setting and (Angiuli et al., 2021;uz Zaman et al., 2020;Miehling et al., 2022) in the finite-horizon setting. However, it should be noted that even if a finite number of agents is used in the environment, this approach does not directly reduce the problem to a MARL problem because the goal is still to learn the equilibrium policy for the MFG instead of the finite-agent equilibrium policy. Another approach consists in representing efficiently the distribution using function approximation. This raises the questions of the choice of parameterization and of the training method for the parameters. This approach can be implemented in a model-free way using Monte Carlo samples, which is particularly suitable for spaces that are too large to be explored in an exhaustive fashion. For example, Perrin et al. (2021b) used a kind of deep neural networks, namely normalizing flows (Rezende and Mohamed, 2015;Papamakarios et al., 2021), to represent the distribution of agents in a flocking model.


## Reinforcement learning for Mean Field Control and MFMDP

As discussed in Section 3.5, the problem of maximizing the social reward can be interpreted in the MDP framework through a mean-field MDP in which the state incorporates the whole mean field state. Adapting in a straightforward way the RL framework represented in Fig. 1, we can consider the environment described in Fig. 3 where the state is µ ∈ X = ∆ X instead of x, and the reward and the transition are given respectively byr andp. An action, taken by the central planner or collectively by the population, is an element of A = (∆ A ) X .

The problem can be interpreted as a situation in which all the agents in the population cooperate to learn a socially optimal behavior. Alternatively, we can adopt the point of view of a central planner trying to find an optimal policy that leads to a social optimum if it is followed by all the agents. In both cases, we assume here that the agent who is learning observes the whole population distribution (which is sometimes referred to as the centralized setting). The value function and the policy can thus depend on the state of the mean field, which is consistent with the dynamic programming equations presented in Section 3.5.

From here, standard RL techniques can be adapted to solve an MFMDP. In their implementation, the main challenge is the representation of the population distribution. A few noticeable cases are the following:

• In continuous space linear-quadratic case, the interaction is only through the mean so we do not need to give the full distribution as an input to the policy but only its first moment. In this case, policy gradient for the parameters of a suitable representation of the policy can be implemented and shown to converge, (Carmona et al., 2019b;Gu et al., 2020Gu et al., , 2021b. This approach can also be extended to more complex settings such as mean-field type games .

• When the state space X is finite, the mean field state can be represented as an element of the simplex, identified as a subset of R |X | : {µ ∈ [0, 1] |X | :
|X | i=1 µ i = 1}
. We can then use two different approaches.

-First, this simplex can be discretized and replaced by a finite setX ⊂ X . We can then approximate the MFMDP by an MDP with this finite state space. The action space is in principle ∆X , which is continuous, but if we are also willing to discretize this space, then we obtain a finite state space, finite action space MDP for which tabular RL methods can be used. For example tabular Q-learning can be shown to converge under suitable conditions (Carmona et al., 2019c;Gu et al., 2020). -Alternatively, the original MFMDP can be tackled without space discretization by using RL techniques for continuous space MDPs. For example, Carmona et al. (2019c) used deep RL to learn optimal policies as functions of the population distribution viewed as an element of {µ ∈ [0, 1] |X | :
|X | i=1 µ i = 1}.
It can be argued that the environment described in Fig. 3 is not very realistic because in general, we cannot assume that an agent observes the mean field distribution. Indeed, this distribution corresponds to the regime with an infinite number of agents while in practice, the number of agents is always finite. One can thus replace the "ideal" McKean-Vlasov environment by a more realistic finite-population environment. The former can be viewed as an approximation of the latter. The quality of the approximation gets better as the number of agents N in the environment increases. These two types of environments are discussed e.g. by Carmona et al. (2019b), in which the finite-population environment analysis benefits from the analysis of the McKean-Vlasov environment. The connection between RL for MFC and finite-agent problems has also been analyzed by Wang et al. (2020); Chen et al. (2021a); Li et al. (2021). Lastly, as in the MFG setting, for some MFC problems, it has been shows that observing the state of a single agent is sufficient to approximate the mean field distribution and learn the optimal behavior, see Angiuli et al. (2022bAngiuli et al. ( , 2021.


# Numerical experiments

We now present numerical experiments to illustrate some of the techniques introduced in the previous sections. We first discuss metrics that can be used to assess convergence. We then present an MFG model in which the agents are encouraged to explore the spatial domain. Last, we present numerical results obtained using iterative methods.


## Metrics

Here we discuss ways to measure convergence of the iterative methods discussed in Section 3. First, since many methods are based on fixed point iterations, we can measure distances between mean field terms or policies. Second, we can also measure convergence in terms of the exploitability of the current policy.


### Wasserstein distance

Let us recall that the iterative methods described previously are based on the scheme described in (9). The pair (µ , π ) computed at iteration is expected to converge to a fixed point. We can thus use the distance between µ and µ +1 , and the distance between π and π +1 to see whether the method has converged. Since both the mean field and the policy are distributions (respectively on the state space and the action space), we can use for instance the Wasserstein distance.

Let us focus on the mean field and look at the macroscopic behavior, at the scale of the whole population. We can proceed similarly with the policy. For simplicity, let us assume the state space X is a finite set endowed with a distance denoted by d. The Wasserstein distance W (or earth mover's distance) measures the minimum cost of turning one distribution into another and is defined as follows: for µ, µ ∈ ∆ X ,
W(µ, µ ) = inf ν∈Γ(µ,µ ) (x,x )∈X ×X d(x, x )ν(x, x ),
where Γ(µ, µ ) is the set of probability distributions on X × X with marginals µ and µ . In a finite-horizon setting, the mean field term is a sequence of distributions, so we average the distances over the N T + 1 time steps to get the following distance between mean field flows: for µ, µ ∈ ∆ N T X ,
W T (µ, µ ) = 1 N T + 1 N T n=0 W(µ n , µ n ).
This distance can be used in two ways to assess convergence in the context of the iterative scheme (9). First, in some cases the Nash equilibrium distribution (or an approximation of the equilibrium)μ is known, so we can use W(µ ,μ) to assess convergence. The MFG solution is typically unknown but in a few cases it admits an analytical solution, which can be convenient to check if a new numerical method works properly. Second, we can always measure the distance between two successive iterates, namely, W(µ , µ +1 ). Although there is in general no guarantee that this distance should decrease monotonically, it goes to zero if the method converges to a fixed point. Similar ideas can be used for policies.

If W(µ , µ +1 ) or W(π , π +1 ) provide some information about the scale of the changes occurring between two iterations, these quantities do not directly say how close the pair (µ , π ) is to a Nash equilibrium. One way to tackle this question is to measure the exploitability.


### Exploitability

Instead of focusing directly on the quantities that are updated in the iterative procedure, namely the mean field and the policy, another way to assess the convergence of learning algorithms is to consider the reward function. Indeed, the definition of an approximate Nash equilibrium can be formalized by measuring to what extent a representative player can improve their reward by deviating from the policy used by the rest of the population.

In the stationary setting for instance (similar ideas can be used in the other settings), the exploitability of a policy π is defined as (Perrin et al., 2020) 
E(π) = sup π J statio (π ; µ π ) − J statio (π; µ π ),
where µ π = M statio (π) is the stationary mean field distribution induced by π as defined in (5), and J statio is defined in (3). This notion is inspired by analogous concepts introduced in the context of computational game theory (Zinkevich et al., 2007;Lanctot et al., 2009).

Using this notion, we can rephrase the definition of mean field Nash equilibrium (see Definition 2) as:π is a stationary MFNE policy if: E(π) = 0.

Furthermore, E is always non-negative, and for > 0, E(π) ≤ corresponds to saying that the policy π is an -stationary MFNE, meaning that a representative player can improve her reward by at most by unilaterally deviating from the policy π used by the rest of the population. As such, the exploitability offers a different perspective than the Wasserstein distance discussed above to assess the convergence of learning methods. In the context of iterations described by (9), the quantity E(π ) measures convergence from the point of view of the potential reward improvement by deviating from π . In practice, the exploitability of a policy π can be computed only if we can compute sup π J statio (π ; µ π ). For many problems, there is not explicit formula for this value and if the environment is complex, exact methods cannot be used. However an approximation can be computed by learning an approximate best response to µ π , for example using RL. If this step is too computationally expensive, then we can replace the supremum by a maximum over a finite set, say Π. For example, we can take the set of policies computed in previous iterations. We then obtain a notion of approximate exploitability (Perrin et al., 2021b,a): E(π) = max π ∈ Π J statio (π ; µ π ) − J statio (π; µ π ).

We conclude this section by mentioning that in the case of MFC, the convergence can be assessed through the social cost, see Section 2.7.


## Experiments

In this section, we present a canonical example and we compare how the algorithms perform. The game and the algorithms are implemented in OpenSpiel (Lanctot et al., 2019) and are publicly available, along with more examples and algorithms. OpenSpiel is a framework for many games besides MFGs, and it contains many reinforcement learning algorithms besides exact algorithms. See https://github.com/deepmind/open_spiel/.

Canonical example: Exploration via entropy maximization. We consider the following model, where the state space is a two dimensional grid world as in Figure 4. At each time step, the representative agent stay still or move by one step in the four directions provided there are no obstacles:

x n+1 = x n + a n + n with a n ∈ {(−1, ), (0, −1), (0, 0), (0, 1), (1, 0)} and such that x n+1 belongs to the admissible states. Here n+1 is a perturbation which pushes the agents to a neighbor state with a small probability. In particular, in this simple model, the transitions probabilities do not depend on the mean field state. we define the reward as:

r(x, a, µ) = r(x, a, µ(x)) = − log(µ(x))

The goal for the representative agent is thus to avoid the crowd because the reward decreases as the density increases. Overall, we expect the population to spread as much as possible. In fact, at the macroscopic level of the population, the average one-step reward if the population distribution is µ is:
E x∼µ [− log(µ(x)] = − x µ(x) log(µ(x)),
which is the entropy of µ. So maximizing the average reward amounts to maximizing the entropy. This model has been introduced and studied by Geist et al. (2021), in which it is shown that it relates to an MFC problem.

For the numerical tests below, we assume that the initial distribution µ 0 is concentrated in the top-left corner as in Figure 4 (left). Since the reward is maximal when the distribution is uniform over the domain, one can wonder whether a uniform policy provides an approximately optimal solution. However, this is not the case, see Figure 4 (right), where we see that the distribution diffuses but remains mostly concentrated near the starting point. So learning a policy which induces a uniform distribution is not trivial. 


## Numerical experiments.

For the sake of illustration, we now focus on the evolutive setting and compare the algorithms on the canonical example. We focus here on the exact iterative methods as described in Section 3, i.e. without RL approximations. In Figure 5, we use the notion of exploitability to check the performance of the following algorithms, which are based on the iterations described in (9):

• Fixed point: π +1 is a best response against µ and µ +1 is the mean field sequence induced by π +1 . We see that this method does not converge and the population concentrates on only a few states.

• Fictitious play: As described in (21), we update the mean field by averaging over past iterations. This method converges since the exploitability goes towards 0. Furthermore, the final distribution is close to uniform.

• Online Mirror Descent: As described in (22), the policy is updated by first computing a cumulative Qfunction and then taking a softmax. This method converges even faster than Fictitious play, possibly because the size of each update in Fictitious play decreases with the iteration index whereas this is not the case for OMD. Accordingly, for the same number of iterations, the distribution is even closer to being uniform than with Fictitious play.

• Damped Fixed Point: As described in (20), the mean field term is updated by taking the average of the previous mean field and the mean field induced by the most recent policy, with constant weights for the average. We see that the exploitability decreases but does not seem to converge, and the induced terminal distribution is not very close to uniform.

• Softmax fixed point: This method is like the fixed point iterations except that the policy is a softmax of the Q-function instead of being an argmax as in the pure best response case. We see that the method does not converge, even though the exploitability is a bit lower than in the pure fixed point method case. The induced terminal distribution is concentrated in one of the four rooms.

• Softmax fictitious play: This method is like the fictitious play iterations except that the policy is a softmax of the Q-function instead of being an argmax as in the pure fictitious play case. In this method, the exploitability goes down more quickly than the pure fictitious play case, as quickly as in the OMD case. However, it does not go towards 0. Instead, it remains roughly constant after a number of iterations. This is probably due to the regularization of the policy which prevents convergence towards the true Nash equilibrium policy. The induced terminal distribution is quite close to uniform but we see that the agents tend to avoid the walls, which is probably due to the extra regularization of the policy.

• Boltzmann policy iteration: This method corresponds to the policy iteration method described in (19) except that the policy is a softmax of the Q-function instead of being an argmax as in the pure fictitious play case. This method does not converge as the exploitability increases to a very high level. The induced terminal distribution is concentrated in one small part of the domain.

Based on these observations, we see that a few methods are failing to converge even when using exact updates. As discussed in Section 3.4, this is probably due to the lack of contraction property for the operator on which the iterations rely.

For other methods, it is worth investigating how they perform when combined with RL as described in Section 4. The model of exploration with four rooms considered above as well as a few other examples have been treated using DRL in Laurière et al. (2022), where it is observed that a DRL versions of pure fixed point iterations still fail to converge, while a DRL version of OMD still tends to perform better than a DRL version of fictitious play. This tends to show that testing methods with exact methods before implementing DRL versions can be helpful to compare the performance of learning methods. 


# Conclusion and perspectives

In this work, we have surveyed some of the main recent developments related to the question of learning MFGs and MFC solutions. We first clarified the definitions of several classical settings that have appeared in the literature. As far as we know, it is the first time that these settings are summarized and discussed in comparison with each other. Second, we proposed an overview of iterative methods to learn MFG and MFC solutions by updating the mean field and the policy. Starting from simple fixed-point iterations, we explained how these procedures can be enhanced by incorporating various smoothing methods. Along the way, we clarified the link with the framework of MDPs. Third, building on this connection with MDPs, we presented RL and deep RL methods for MFGs and MFC. Finally, we provided some numerical results on a simple benchmark problem.

Several aspects were not discussed in detail here, such as:

• games with multiple groups or sub-populations (Subramanian et al., 2018(Subramanian et al., , 2020aPerolat et al., 2021;Carmona et al., 2020;Cui and Koeppl, 2021b;Angiuli et al., 2022a;Mondal et al., 2022),

• static models and bandit problems (Gummadi et al., 2013;Iyer et al., 2014;Maghsudi and Hossain, 2017;Wang and Jia, 2021),

• games with strategic complementarities (Adlakha and Johari, 2013;Lee et al., 2020Lee et al., , 2021a),

• problems with more complex structures such as partially observable problems (Subramanian et al., 2020b), models with leader-follower or major-minor structures (Ghosh and Aggarwal, 2020), and so on,

• other types of equilibria (Muller et al., 2021;Wang et al., 2022),

• other forms of RL techniques such as model-based RL (Pasztor et al., 2021) or inverse reinforcement learning (Yang et al., 2018a;Chen et al., 2021bChen et al., , 2022.

Apart from the points covered in this survey, many directions remain to be investigated. For example, the question of approximation of the distribution has received relatively less interest than the question of approximating the policy and the value function. Efficiently representing and learning the distribution is important for mean field problems, particularly for large and complex environments for which exact tabular representations are not suitable.

Furthermore, the literature is quickly expanding in various directions and the numerical results are not always easy to compare. We hope that this survey will contribute to harmonizing the research on this topic although many aspects remain to be unified. Along these lines, having common benchmark problems and a common framework seem important. Recently, MFGs have been incorporated to the OpenSpiel library (Lanctot et al., 2019). 1 Last but not least, one of the main motivations to use RL methods for MFGs is to be able to compute Nash equilibria at a large scale. We thus hope that the methods presented here and their extensions will find concrete applications in the near future.


(2017); François-Lavet et al. (2018).

## Figure 3 :
3Environment for MFC and MFMDP.

## Figure 4 :
4Reading order: (a) the considered environment initial state in yellow, walls in white); (b) the log-density of a uniform policy (to illustrate entropy maximization).

## Figure 5 :# Introduction

Since their introduction by Lasry and Lions (Lasry and Lions, 2007) and Caines, Huang and Malhamé (Huang et al., 2006), mean field games (MFGs for short) have gained momentum as a powerful paradigm to study large populations of strategic agents. The main idea, borrowed from statistical physics, is to use the mean field distribution corresponding to the limiting mean field situation with an infinite number of players. All the individual interactions can then be replaced by the interaction between a representative player and the mean field distribution, which considerably simplifies the model and the analysis. This approximation relies on the assumption that the population is homogeneous and that the interactions are symmetric in the sense that each player interacts only with the empirical distribution of the other players. The solution to the MFG provides an -Nash equilibrium for the corresponding N -player game, with going to 0 as N goes to infinity. Furthermore, under suitable assumptions, N -player Nash equilibria or social optima converge to the corresponding mean field equilibrium or social optimum. Such results build on the idea of propagation of chaos (Sznitman, 1991) but are more subtle since the players are not simple particles but rational agents making optimal decisions and reacting to other players' decisions (Lacker, 2017;Cecchin and Pelino, 2019;Lacker, 2020;Cardaliaguet et al., 2019).


## Mean field games

General intuition. We start this survey by defining at a high level what a Mean Field Game (MFG) is. Intuitively, a Mean Field Game is a game with an infinite number of identical players. All players have a similar behavior, i.e. they are symmetric: we do not need to retain the identity of a player as part of its state. Furthermore, as we have an infinite number of players, we can replace the atomic players by their distributions over the state (and sometimes action) space. The population's distribution enables to focus only on the interaction between a so-called representative player, which is sampled from the population's distribution, and the population's distribution itself. Our ultimate goal is to compute a Nash equilibrium, which corresponds to the situation where no player has an interest in deviating from its current behavior, provided that the other players do not deviate either. Looking for a Nash equilibrium makes the assumption that the players are all perfectly rational, i.e their goal is to maximize their own reward (or minimize their cost).

Most of the literature focuses on two types of problems: Nash equilibria or social optimum. These two settings are typically referred to respectively as mean field game and mean field control (or control of McKean-Vlasov dynamics), (Bensoussan et al., 2013;Carmona and Delarue, 2018a). In both cases, the solutions are typically characterized through optimality conditions taking the form of a coupled system of forward-backward equations. The forward equation describes the evolution of the population distribution while the backward equation represents the evolution of the value function (i.e. the utility of its behaviour) for a representative player. In the continuous time and continuous space setting, the equations can be partial differential equations (PDEs) (Lasry and Lions, 2007) or stochastic differential equations (SDEs) of McKean-Vlasov type (Carmona and Delarue, 2013) depending on whether one relies on the analytical approach or the probabilistic approach. We refer to e.g. Bensoussan et al. (2013); Carmona and Delarue (2018a,b);  for more details. In this survey, we will focus on the discrete time case, which is closer to the framework of Markov Decision Processes (Bertsekas and Shreve, 1996;Puterman, 2014).


## Example.

As a typical example, we can consider crowd motion. Each player is an agent represented by her position and is able to control her velocity so as to reach a target destination while minimizing the effort made to move. Typically, passing through a crowded region, i.e. a region with a high density of players, requires extra efforts or reduces the velocity, which creates some congestion. If we assume that the number of agents is extremely large and that these agents are homogeneous and have symmetric interactions, then we can approximate the empirical distribution of positions by the mean field distribution corresponding to the limiting regime with an infinite population. This allows to simplify tremendously the computation of a Nash equilibrium because we only need to compute the optimal policy of the representative player.

Remark 1 (On MFGs and non-atomic anonymous games). Games modeling infinite populations of agents have also been studied in the framework of non-atomic anonymous games, which have founds applications particularly in economics, see e.g. Aumann (1964); Schmeidler (1973); Aumann and Shapley (2015). In such games, there is typically a continuum of players, indexed by, say, real numbers in I = [0, 1] and the population is represented by a non-atomic measure on I. Each player perceives the other players through some aggregate quantity. Although this is very similar to the MFG framework, the key difference is that the MFG approach completely avoids representing the continuum of players. The main idea is to exploit the homogeneity of the population and the symmetry of interactions to simplify the characterization of an equilibrium: it is sufficient to understand the behavior of a single representative player facing a distribution representing the aggregate information available to this player. The analysis is greatly simplified, particularly when it comes to stochastic games. Defining rigorously a continuum of random variables with nice measurability properties is not trivial, as noticed for instance by Duffie and Sun (2012); Sun (2006) who used the concept of rich Fubini extension to develop an exact law of large numbers. The MFG framework allows to carry out the mathematical analysis of Nash equilibria without requiring such sophistication.

Some applications. MFGs have found various applications such as population dynamics (Guéant et al., 2011;Achdou et al., 2017a;Cardaliaguet et al., 2016), crowd motion modeling (Achdou and Lasry, 2019;Burger et al., 2013;Djehiche et al., 2017a;Aurell and Djehiche, 2019;Achdou and Laurière, 2016;Chevalier et al., 2015), flocking (Nourian et al., 2010(Nourian et al., , 2011Grover et al., 2018;Perrin et al., 2021b), opinion dynamics and consensus formation (Stella et al., 2013;Bauso et al., 2016a;Parise et al., 2015), autonomous vehicles Shiri et al., 2019), epidemics control (Laguzet and Turinici, 2015;Hubert and Turinici, 2018;Elie et al., 2020a;Lee et al., 2021b;Aurell et al., 2022;Doncel et al., 2022), macro-economic models (Achdou et al., 2017b;Elie et al., 2019b;Achdou et al., 2017bAchdou et al., , 2014Chan and Sircar, 2015;Gomes et al., 2014a;Djehiche et al., 2017b), finance (Lachapelle et al., 2016;Cardaliaguet and Lehalle, 2018;Lasry and Lions, 2007;Lachapelle et al., 2016;Gomes and Saúde, 2020;Carmona, 2020), energy production and management (Alasseur et al., 2020;Couillet et al., 2012;Elie et al., 2019a;Bagagiolo and Bauso, 2014;Kizilkale et al., 2019;Li et al., 2016;Guéant et al., 2011;Chan and Sircar, 2017;Graber and Bensoussan, 2018), security and communication (Mériaux et al., 2012;Samarakoon et al., 2015;Hamidouche et al., 2016;Yang et al., 2017;Kolokoltsov and Bensoussan, 2016;Kolokoltsov and Malafeyev, 2018), traffic modeling (Bauso et al., 2016b;Salhab et al., 2018;Huang et al., 2019;Tanaka et al., 2020;Cabannes et al., 2021) or engineering (Djehiche et al., 2017b).


## Numerical methods.

Most existing numerical methods for MFGs and MFC problems are based on the aforementioned optimality conditions phrased in terms of PDEs or SDEs. Such approaches typically rely suitable discretizations, e.g. by finite differences (Achdou and Capuzzo-Dolcetta, 2010;Achdou et al., 2012;Briceño Arias et al., 2018;Briceño Arias et al., 2019), semi-Lagrangian schemes Silva, 2014, 2015), or probabilistic approaches Angiuli et al., 2019). We refer to e.g. Achdou and Laurière (2020); Lauriere (2021) for recent surveys on these methods. Although these methods are very well understood and very successful in small dimension, they cannot tackle MFGs with high dimensional states or controls due to the curse of dimensionality (Bellman, 1957). To address this limitation, stochastic methods based on approximations by neural networks have recently been introduced by Carmona andLaurière (2021, 2019); Fouque and Zhang (2020); Germain et al. (2022) using optimality conditions for general mean field games, by Ruthotto et al. (2020) for MFGs which can be written as a control problem, and by Cao et al. (2020); Lin et al. (2020) for variational MFGs in connection with generative adversarial networks (GANs) (Goodfellow et al., 2014). We refer to Hu and Laurière (2022) for a recent survey on machine learning methods for control and games, with applications to MFGs and MFC problems. However, these methods still try to solve the problems in an exact way by relying on exact computations of gradients by exploiting the full knowledge of the model. The learning methods we focus on in this survey aim at solving MFGs and MFC in a model-free fashion to foster the scalability of numerical methods for these problems.


## Learning

Two notions of learning.

The second question we need to answer before diving more into the survey is what learning means in our context. There are mainly two interpretations of learning. The first one comes from game theory and economics and, according to Fudenberg et al. (1998), refers to "The theory of learning in games [. . . ] examines how, which, and what kind of equilibrium might arise as a consequence of along-run non equilibrium process of learning, adaptation, and/or imitation." From this point of view, the main focus is on how the players iteratively adjust their behavior until convergence to an equilibrium. The second interpretation of learning is mainly used in Machine Learning and in Reinforcement Learning. As Mitchell et al. (1997) puts it, "a computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E." In this concept, the concept of learning is very related to the idea of improving one's performance by using data or samples. In this survey, we are interested in delineating these two notions of learning while explaining how they can be combined.

Learning in games. More specifically, we will focus on learning in MFGs. This research direction finds its roots in the literature on learning in games, which goes back to the works of Shannon (1959) and Samuel (1959). Since then, a lot of progress has been made but remains mostly limited to games with a small number of players. Many of the recent breakthrough results have been obtained using a combination of reinforcement learning (Sutton and Barto, 2018) and deep neural networks (Goodfellow et al., 2016), see e.g. Go (Silver et al., , 2017(Silver et al., , 2018, Chess (Campbell et al., 2002), Checkers (Schaeffer et al., 2007;Samuel, 1959), Hex (Anthony et al., 2017), Starcraft II (Vinyals et al., 2019), poker games (Brown andSandholm, 2017, 2019;Moravčík et al., 2017;Bowling et al., 2015) or Stratego (McAleer et al., 2020).

Learning in mean field games. The goal of this survey is to review the quickly growing literature at the intersection of learning and MFGs. We hope that combining mean field approximations, which allow to tackle large population games, and reinforcement leanring, which allows to handle highly complex environments, we will be able to solve multi-agent systems at a very large scale, both in terms of population size and model complexity.


## Outline of the survey

In the rest of this section, we introduce a few useful notation. In Section 2, we present several settings of MFG and MFC problems that have appeared in the literature. We stress the similarities and the differences, in terms of definitions and in terms of solutions. In Section 3, we present algorithms to compute MFG and MFC solutions. We start by recalling some classical methods to solve MDPs and then describe mainly two classes of algorithms to find Nash equilibria in MFGs. These algorithms are based on iteratively updating the mean field and the policy, so we refer to them as iterative methods. Building on these methods and the connection between MDPs and RL, we explain in Section 4 how RL and deep RL methods can be adapted to solve MFGs and MFC problems. Section 5 discusses metrics that can be used to assess the numerical convergence of algorithms and illustrate some of the methods on a representative MFG example. Finally, we conclude in Section 6 with a short summary and some perspectives.


## Useful notation

Here we introduce some general notation that we use throughout the text:

• X and A denote a finite state set and a finite action set respectively.

• |E| denotes the cardinal of a set E.

• 2 E denotes the set of subsets of a set E.

• ∆ E is the set of probability distributions on a set E; when E is finite (which is generally the case for us), it is also identified with the corresponding simplex in the Euclidean space of dimension |E|, and we view probability distributions as (normalized) vectors in R |E| .

• argmax is understood as the set of all maximizers.

• P, E denote respectively probability and expectation.

• Unless otherwise specified, bold symbols denote time-dependent objects, which can be viewed as functions of time or as sequences indexed by time steps.

• We will use subscripts with n ∈ {1, . . . , N T } for time indices and superscript with k ∈ {1, . . . , K} for iteration indices in algorithms.

• Given a probability distribution p on a set X and a function ϕ :
X → R, E x∼p [ϕ(x)] = E[ϕ(x) | x ∼ p] = x∈X p(x)ϕ(x).

# Definition of the problems

In this section, we present several settings of MFGs which depend on how time is involved (or not) in the problem. These settings correspond to different applications and different notions of Nash equilibrium. Here, we focus on four settings, that can be summarized as follows. We start with games in which the agents take a single decision. There is no notion of time intrinsic to the game so we call them static MFGs. We then turn to games in which there is a dynamical aspect. In such games, each agent has a state that evolves along time, and she can act on this evolution. At the level of the population, in some situations, we can expect the distribution of states to converge to a stationary regime, in which the population is stable at a macroscopic level, even though each agent's state is possibly changing. We refer to this setting as a stationary MFG. In other cases, one wants to understand not only the stationary regime, but how the population evolves, starting from an initial configuration. This is relevant for applications in which the agents' behaviors change along time, for instance because there is a finite horizon at which the game stops. We call such games evolutive MFGs. This setting comes at the expense of having policies and mean-field terms that depend on time and are thus harder to compute. To mitigate this complexity while not falling completely into the stationary regime, an intermediate model has been introduced. The idea is to try to keep the best of the stationary and evolutive settings by considering a proxy for the whole evolution of the distribution. We call this setting discounted stationary MFGs. In the rest of this section, we define each setting as well as the corresponding notion of Nash equilibrium, along with relevant concepts.


## Static MFG

Let A be a finite action space. The behavior of one player, called a strategy, is an element of ∆ A , that is a distribution over the action set. In this setting, the behavior of the population is also an element of ∆ A . We denote a generic element of A by a, and we denote a generic individual behavior and a generic population behavior by π and ξ respectively. Besides the action space A, the model is completely defined by a reward function r : A × ∆ A → R. Given a population behavior ξ ∈ ∆ A , the reward of a player using π ∈ ∆ A is defined as the expected reward when sampling an action according to π: J static (π; ξ) = E a∼π [r(a, ξ)] .

The reward function r : A × ∆ A → R can typically encode crowd aversion or attraction towards a population action of interest.

Example 1. One of the first examples in the MFG literature is the problem of choosing a starting time for a meeting, introduced and solved explicitly by Guéant et al. (2011). In this problem, the players choose at what time they want to arrive to the meeting room so that they are neither too late nor too early. The global outcome is the time at which the meeting actually starts, which is not known in advance and depends on the everyone's arrival time. Despite its name, there is no dynamic aspect in the original formulation of the example. Another popular example is the problem in which each agent chooses a location on a beach, see e.g. Perrin et al. (2020). They want to put their towel close to an ice cream stall but not in a very crowded area. The global outcome is the distribution of towels on the beach. To be specific, a simple model can be as follows: A = [0, 1], which represents possible positions on the beach, a stall ∈ A is the position of the stall, and the reward is r(a, ξ) = −|a − a stall | − ln(ξ(a)), where the first term penalizes the distance to the stall and the second term penalizes choosing a location a at which the density ξ(a) of people is high.

A central concept is the notion of best response. Let us define the (set-valued) best response map by:
BR static : ∆ A → 2 ∆ A , ξ → BR static (ξ) := argmax π∈∆ A J static (π; ξ).
Definition 1 (Static MF Nash Equilibrium).π ∈ ∆ A is a static mean field Nash equilibrium (static MFNE) if it satisfies the following condition:π ∈ BR static (π).

The above definition has the advantage to clearly show that the equilibrium is a fixed point of BR static . Another point of view, which will be useful in the dynamic settings presented in the sequel, consists in saying that the equilibrium is given by a pair of a representative agent's behavior and the population's behavior. Here, it means that the equilibrium is a pair (π,ξ) ∈ ∆ A × ∆ A such that:

1.π is optimal for the representative agent facingξ, i.e.,π ∈ BR static (ξ), 2.ξ corresponds to the population behavior when all every agent usesπ, i.e.,ξ =π.

The second point represents the fact that all the agents are "rational in the same way" and hence, at equilibrium, adopt the same behavior. This viewpoint is unnecessarily complicated in this setting asπ alone is enough to define the MFNE, but will be useful in dynamic settings.

Remark 2. Consistently with the literature on normal-form games (Fudenberg and Tirole, 1991), each player chooses a distribution over actions without seeing the strategy chosen by other players and the resulting distribution at the population level. Each agent thus tries to anticipate, in a rational way, the distribution generated by other players' actions.

A Nash equilibrium corresponds to a situation in which no selfish player has any incentive to deviate unilaterally. However, it is not necessarily a situation that is optimal from the point of view of the whole population. The notion of social optimum is discussed below in Section 2.7.

Remark 3. Although we provided an intuitive explanation for ξ in terms of a continuum of players, we want to stress that in the definition of an MFG equilibrium or social optimum, we actually never need to consider a continuum of players. As already pointed out in Remark 1, this shortcut is one of the main advantages of the MFG paradigm compared with non-atomic anonymous games.


## Notations for the dynamic setting

In contrast with the static case, in the dynamic setting, each agent has a state which evolves in time. The agent can influence the evolution of their own state using actions. The population's state is the distribution of the agents' states, the joint distribution of their states and actions. This is what constitutes the mean field, with which every agent interacts through its dynamics and its rewards.

As far as the population distribution is concerned, we will consider mainly two types of settings: one in which the population distribution is fixed, and one in which it can also evolve. Typically, the former is conceptually simpler and easier to compute but the latter is more realistic since many real world applications involve a population evolving in time. In each cases, several variants can be considered. For the sake of brevity, we shall focus only on the main ideas.

We will consider discrete time models, with time typically denoted by n ∈ N. If a time horizon is imposed, we will typically use the notation N T . Let X be a finite state space. A stationary policy is an element of Π := (∆ A ) X . In this setting, we assume that the interactions occur through an aggregate variable which represents the behavior of the population. A mean field state is an element of ∆ X , which is the set of probability distributions on the state space. It represents the state of the population at a given time. We denote generic elements of X , A, Π, and ∆ X respectively by x, a, π, and µ.

Depending on the setting, we might consider policies that depend on time or on an initial distribution. More details will be provided below, as we introduce several setups.

Remark 4. To alleviate the presentation, we choose to focus on finite state and action spaces and discrete time. In some cases, continuous space or continuous time models might be more relevant. They are typically analyzed using partial differential equations or stochastic differential equations. Suitable discretizations can lead to (possibly nontrivial) approximations of these models with discrete ones, as presented in this survey, see e.g. Hadikhanloo and Silva (2019) for more details on the convergence of a finite MFG to a continuous one. We do not discuss in detail the continuous settings here and we refer the interested reader to the literature, e.g., Huang et al. (2006); Lasry and Lions (2007); Bensoussan et al. (2013); Carmona and Delarue (2018a,b).


## Background on MDPs

We recall a few important concepts pertaining to optimal control in discrete time for a single agent. We will only review the main ideas and we refer to e.g. Bertsekas and Shreve (1996); Puterman (2014) for more details. The notion of Markov decision processes will play a key role in the description of dynamic MFGs.


### Stationary MDP

A stationary Markov decision process (MDP) is a tuple (X , A, p, r, γ) where X is a state space, A is an action space, p : X × A → ∆ X is a transition kernel, r : X × A → R is a reward function and γ ∈ (0, 1) is a discount factor. Using action a when the current state is x leads to a new state distributed according to p(·|x, a) ∈ ∆ X and produces a reward r(x, a). The reward could be stochastic but to simplify the presentation, we consider that r is a deterministic function of the state and the action. A stationary policy π : X → ∆ A , x → π(·|x) provides a distribution over actions for each state. The goal of the MDP is to find a policy π * which maximizes the total return defined as the expected (discounted) sum of future rewards: J(π) = E n≥0 γ n r(x n , a n ) , subject to:

x 0 ∼ m 0 , a n ∼ π(·|x n ), x n+1 ∼ p(·|x n , a n ), n ≥ 0,

where m 0 is an initial distribution whose choice does not influence the set of optimal policies. Assuming the model is fully known to the agent, the problem can be solved using for instance dynamic programming. The state-action value function associated to a stationary policy π is defined as:
Q π (x, a) = E   n≥0
γ n r(x n , a n ) x 0 = x, a 0 = a, a n ∼ π(·|x n ), x n+1 ∼ p(·|x n , a n )   .

By dynamic programming, it satisfies the following fixed point equation with unknown Q : X × A → R:
Q = B π Q,
where B π denotes the Bellman operator associated to π:
(B π Q)(x, a) = r(x, a) + γE x ∼p(·|x,a),a ∼π(·|x ) [Q(x , a )].
(1)

We recall that the expectation is to be understood as:
E x ∼p(·|x,a),a ∼π(·|x ) [Q(x , a )] = x p(x |x, a) a π(a |x )Q(x , a ).
The optimal state-action value function is defined as:
Q * (x, a) = sup π Q π (x, a).
It satisfies the fixed point equation:
Q = B * Q,(2)
where B * denotes the optimal Bellman operator:
(B * Q)(x, a) = r(x, a) + γE x ∼p(·|x,a) [max a Q(x , a )], with E x ∼p(·|x,a) [max a Q(x , a )] = x p(x |x, a) max a Q(x , a ).
It is also convenient to introduce the (state only) value function associated to a policy V π : x → E a∼π(·|x) [Q π (x, a)] and the (state only) optimal value function V * :
x → E a∼π * (·|x) [Q * (x, a)],
where π * is an optimal policy. These value functions can also be characterized as fixed points of two Bellman operators. Note that these objects are all independent of time, as we search for a stationary solution.


### Finite Horizon MDP

One can also consider problems set with a finite time horizon. A finite-horizon Markov decision process (MDP) is a tuple (X , A, p, r, N T ) where X is a state space, A is an action space, N T is a time horizon, p : {0, . . . , N T − 1} × X × A → P(X ) is a transition kernel, and r : {0, . . . , N T } × X × A → R is a reward function. At time n, using action a when the current state is x leads to a new state distributed according to p n (·|x, a) ∈ ∆ X and produces a reward r n (x, a) ∈ R. A policy π : {0, . . . , N T − 1} × X → P(A), (n, x) → π n (·|x) provides a distribution over actions for each state at time n. The goal of the MDP is to find a policy π * which maximizes the total return defined as the expected (discounted) sum of future rewards:
J(π) = E N T n=0
r n (x n , a n ) , subject to:

x 0 ∼ m 0 , a n ∼ π n (·|x n ), x n+1 ∼ p n (·|x n , a n ), n = 0, . . . , N T , where m 0 is an initial distribution whose choice does not influence the set of optimal policies. Here again, assuming the model is known to the agent, the problem can be solved using for instance dynamic programming. The state-action value function associated to a stationary policy π is defined as:
             Q π N T (x, a) = r N T (x, a) Q π n (x, a) = E   n ≥n
r n (x n , a n ) x n = x, a n = a, a n ∼ π n (·|x n ), x n +1 ∼ p n (·|x n , a n )   , n = N T − 1, . . . , 0.

The optimal state-action value function is defined as:
Q * (x, a) = sup π Q π (x, a).
Here again, we can introduce the (state only) value function associated to a policy: V π n (x) = E a∼πn(·|x) [Q π n (x, a)], and the optimal value function:
V * n (x) = E a∼π * n (·|x) [Q * n (x, a)]
, π * is an optimal policy. Formally, the finite-horizon MDP can be restated as a stationary MDP by incorporating the time n in the state. However, it can be simpler to directly tackle this MDP using techniques that are specific to the finite-horizon setting. In particular we stress that, in contrast with the stationary setting presented above, the value functions are here characterized by equations which are not fixed point equations but backward equations. They can be solved by backward induction, as we will discuss in the sequel (see Section 3). For more details on finite-horizon MDP we refer to e.g. (Puterman, 2014).


## Stationary setting

Here we consider an infinite horizon model, meaning that there is no terminal time. We assume that the players interact through a stationary distribution, which represents a steady state of the population. The model is defined by a tuple (X , A, p, r, γ) consisting of:

• a state space X and an action space A,

• a one-step transition probability kernel p : X × A × ∆ X → ∆ X ,

• a one-step reward function r : X × A × ∆ X → R,

• and a discount factor γ ∈ [0, 1].

The main difference with standard MDPs as recalled in Section 2.3 is the presence of a third input for p and r, which is an element of the mean field state space ∆ X . It plays the role of the population's state, which influences the dynamics and the rewards.

Assume the state of the population is given by µ ∈ ∆ X and consider a representative agent using policy π ∈ Π. The total, discounted reward of this player is given by:
J statio (π; µ) = E ∞ n=0 γ n r(x n , a n , µ) ,(3)
where the state of the agent evolves according to:

x 0 ∼ µ,

x n+1 ∼ p(·|x n , a n , µ), a n ∼ π(·|x n ), n ≥ 0.

Remark 5 (State-action distribution). An extension of the above model is to consider that the agents interact through the state-action distribution. In this case, assume the state of the population is given by ξ ∈ ∆ X ×A and consider a representative agent using policy π ∈ Π. The total, discounted reward of a representative player is given by:
J statio (π; ξ) = E ∞ n=0
γ n r(x n , a n , ξ) ,

where the state of the agent evolves according to:
x 0 ∼ µ = ξ 1 ,
x n+1 ∼ p(·|x n , a n , ξ), a n ∼ π(·|x n ), n ≥ 0.

with µ = ξ 1 ∈ ∆ X denoting the first marginal of ξ. This setting is considered for instance by Guo et al. (2019Guo et al. ( , 2020a. MFG with interactions through state-action distributions have first been studied by Gomes et al. (2014b); Gomes and Voskanyan (2016) and are sometimes referred to as extended MFGs or MFG of controls, see Cardaliaguet and Lehalle (2018); Kobeissi (2022). Let us stress that a state-action distribution is not always a product distribution, meaning that for some ξ ∈ ∆ X ×A there is no µ ∈ ∆ X and ν ∈ ∆ A such that ξ = µ ⊗ ν. In fact, in general the actions of a player are given by a function of the player's states, and hence the joint distribution cannot be written as a product. To simplify the presentation, we restrict our attention to interactions through state-only distributions but most of the ideas can be extended to state-action distributions. The interested reader is referred to Carmona and Delarue (2018a, Section 4.6) and the references therein.

This stationary MFG setting has been studied for instance by Subramanian and Mahajan (2019) with applications to malware spread and investments in product quality, by Guo et al. (2019Guo et al. ( , 2020a with applications to auctions and by Angiuli et al. (2022b) in the context of linear-quadratic MFGs.

Example 2 (Repeated auction game). As an example, Guo et al. (2019) consider a repeated game in which the players bid in an auction game. At a given time, a player's state and action are respectively her budget and her bid for the next auction.

The evolution of the population is given by a transition matrix defined by: for allμ ∈ ∆ X , π ∈ Π, µ ∈ ∆ X and x ∈ X , (P μ,π µ)(y) = x µ(x) a π(a|x)p(y|x, a,μ).

In words, P μ,π µ is the next state distribution for a representative agent starting with state distribution µ and using policy π while the population has state distributionμ. Given a population state, the goal for a representative agent, is to find the best reaction, i.e., a policy that maximizes their total reward. We define the (set-valued) best response map by:
BR statio,γ : ∆ X → 2 Π , µ → BR statio,γ (µ) := argmax π∈Π J statio (π; µ) ⊆ Π,
and the (set-valued) population behavior map by:
M statio : Π → 2 ∆ X , π → M statio (π) := {µ ∈ ∆ X | µ = P µ,π µ},(5)
which is the stationary distribution obtained when using π (that we assume to be unique).

Remark 6. Note that solving the equation is not trivial since µ is involved in the transition matrix P µ,π . We come back to this point in Section 3.3.1.

Definition 2 (Stationary MF Nash Equilibrium). A pair (π,μ) ∈ Π×∆ X is a stationary mean field Nash equilibrium (stationary MFNE) if it satisfies the following two conditions:

•π ∈ BR statio,γ (μ);

•μ ∈ M statio (π).

Alternatively, an equilibrium can be defined as a fixed point:π is a stationary MFNE policy if it is a fixed point of BR statio,γ • M statio , andμ is a stationary MFNE distribution if it is the stationary distribution of a stationary MFNE policy.

In this setting, the state-action value function associated to a stationary policy π for a given distribution µ is defined as:
Q π,µ (x, a) = E   n≥0 γ n r(x n , a n , µ) x 0 = x, a 0 = a, x n+1 ∼ p(·|x n , a n , µ), a n ∼ π(·|x n )   .
The problem then reduces to a standard stationary MDP, parameterized by µ. By dynamic programming, Q π,µ satisfies the fixed point equation:
Q = B π,µ Q,
where B π,µ denotes the Bellman operator associated to π and µ:
(B π,µ Q)(x, a) = r(x, a, µ) + γE x ∼p(·|x,a,µ),a ∼π(·|x ) [Q(x , a )],(6)
where
E x ∼p(·|x,a,µ),a ∼π(·|x ) [Q(x , a )] = x p(x |x, a, µ) a π(a |x )Q(x , a ).
The optimal state-action value function is defined as:
Q * ,µ (x, a) = sup π Q π,µ (x, a).
It satisfies the fixed point equation:
Q = B * ,µ Q,
where B * ,µ denotes the optimal Bellman operator associated to µ:
(B * ,µ Q)(x, a) = r(x, a, µ) + γE x ∼p(·|x,a,µ) [max a Q(x , a )], with E x ∼p(·|x,a,µ) [max a Q(x , a )] = x p(x |x, a, µ) max a Q(x , a ).
Note that the functions Q π,µ and Q * ,µ , and the operators B π,µ and B * ,µ are all independent of time, as we search for stationary equilibria.


## Evolutive setting

We next turn our attention to a model in which not only the agents' state can evolve, but the population's distribution too. In this case, the mean field is not stationary. At each time step, the transition and the reward of every agent depends on the current distribution instead of the stationary one. The model is defined by a tuple (X , A, m 0 , N T , p, r) consisting of:

• a state space X and an action space A,
• an initial distribution m 0 ∈ ∆ X , • a time horizon N T ∈ N ∪ {+∞},
• a sequence of one-step transition probability kernels p n : X × A × ∆ X → ∆ X , n ≥ 0,

• a sequence of one-step reward functions r n : X × A × ∆ X → R, n ≥ 0.

In this context, a population behavior is a mean field flow, generally denoted by µ, which is an element of ∆ N T X . A policy is an element of Π N T , generally denoted by π. We use bold letter to stress that these are sequences, which can also be viewed as functions of time.

The total reward is:
J evol (π; µ) = E N T n=0 r n (x n , a n , µ n ) ,
subject to the following evolution of the agent's state:
x 0 ∼ m 0 ,
x n+1 ∼ p n (·|x n , a n , µ n ), a n ∼ π n (·|x n ), n ≥ 0.

This evolution is analogous to (4) except that the stationary mean-field state is replaced by the current mean-field state at time n. We assume that the transition p and the reward r are such that the total reward is well defined.

Remark 7 (Finite and infinite horizon discounted settings). Our notation covers two very common settings:

• Finite horizon: N T < +∞.

• Infinite horizon: N T = +∞. In this case, it is common to assume that p is independent of time, and that r is of the form r n (x, a, µ) = γ nr (x, a, µ) wherer is independent of time and bounded.

Note that even in the infinite horizon setting and even if p and r are stationary (constant with respect to the time parameter n), in general the optimal policy still depends on time. This is in contrast with the stationary setting (section 2.4) and is due to the fact that the population distribution starts from m 0 and evolves. The player needs to take that into account in its decisions. To be specific, even if r n (x, a, µ) =r(x, a, µ) is independent of time, the reward associated to a fixed state-action pair (x, a) isr(x, a, µ n ) at time n andr(x, a, µ n ) at time n . Unless the mean-field state is stationary, in general the two reward values will be different.

Example 3 (Crowd motion). This setting is probably the most commonly studied one in the MFG literature. As a typical example, we can think of a model for crowd motion in the spirit of e.g. Achdou and Lasry (2019): the agents start from an initial position and want to reach a point of interest while avoiding crowded areas. Because the population distribution changes as the agents move, looking for a stationary solution is not satisfactory if we want to compute the evolution of the whole population. This is because a stationary solution would only give the optimal policy (from the Nash equilibrium perspective) against the stationary distribution, and would not be able to recover the full evolution of the agents. In contrast, a time-dependent policy in the evolutive setup is able to adjust the agents' behavior step by step.

Remark 8. Discrete time finite state mean field games have been introduced by Gomes et al. (2010). In the model analyzed therein, the players can directly control their transition probabilities. Note that the model we consider here is a bit more general since the transition probabilities are functions of the actions, but they are not necessarily chosen freely by the players.

We define the (set-valued) best response map by:
BR evol,m0,N T (µ) := argmax π∈Π N T J evol (π; µ) ⊆ Π N T .
Let us define the population behavior map by:
M evol,m0,N T : Π N T → ∆ N T X , M evol,m0
,N T (π) := mean field flow when using π and starting from m 0 ,

where this flow is defined by: µ 0 = m 0 , µ n+1 = P n,µ n ,πn µ n , n ≥ 0.

When the context is clear, we will simply write µ m0,π .

Definition 3 (Evolutive MFG Nash Equilibrium). A pair (π,μ) ∈ Π N T × ∆ N T X is an evolutive mean field Nash equilibrium (evolutive MFNE) if it satisfies the following two conditions:
• Best response:π ∈ BR evol,m0,N T (μ); • Mean field flow:μ = M evol,m0,N T (π).
Alternatively, an evolutive MFNE can be defined as a fixed point:π is an evolutive MFNE policy if it is a fixed point of BR evol,m0,N T • M evol,m0,N T , andμ is an evolutive MFNE flow if it is the mean field flow generated by an evolutive MFNE policy.

The state-action value function associated to a policy π and the optimal state-action value function are defined analogously to standard MDP but parameterized by µ. We denote them respectively by Q π,µ and Q * ,µ .

For the sake of completeness, let us provide more details in the finite horizon setting. Assume N T < +∞. The state-action value function associated to a policy π for a given distribution µ is defined as:
           Q π,µ N T (x, a) = r N T (x, a, µ N T ) Q π,µ n (x, a) = E N T n =n
r n (x n , a n , µ n ) x n = x, a n = a, x n +1 ∼ p n (·|x n , a n , µ n ), a n ∼ π n (·|x n ) ,
n = N T − 1, . . . , 0.
The optimal state-action value function is defined as:
Q * ,µ (x, a) = sup π Q π,µ (x, a).

## Discounted stationary setting

We now discuss a setting that is somehow between the stationary and the evolutive ones. Note that in the stationary setting, we focus on the stationary distribution of the population while in the evolutive setting, we care about the entire sequence starting from m 0 . In the first case, we can restrict our attention to stationary policies, whereas this is not possible in the second case, as highlighted in Remark 7. An intermediate approach consists in replacing the distribution µ m0,π n at time n by an aggregate which keeps some memory of m 0 , instead of the stationary distribution. The model is defined by a tuple (X , A, m 0 , p, r, γ) consisting of:

• a state space X and an action space A,

• an initial distribution m 0 ∈ ∆ X , • a one-step transition probability kernel p :
X × A × ∆ X → ∆ X ,
• a one-step reward function r : X × A × ∆ X → R"

• a discount factor γ ∈ [0, 1).

We define the discounted distribution as:
M statio,γ,m0 (π) := µ m0,π γ := (1 − γ) n≥0 γ n µ m0,π n ∈ ∆ X ,
where µ m0,π follows the dynamics (8) but with π n = π for all n ≥ 0 after starting from m 0 at time 0, and with the mean-field term replaced by µ m0,π γ , i.e.,
µ m0,π 0 = m 0 , µ n+1 = P µ m 0 ,π γ ,π µ m0,π n , n ≥ 0.
This formulation allows us to work with a single distribution, which plays the role of a summary of what happens along the mean field flow. In contrast with the stationary MFG setting, here the initial distribution m 0 still influences the mean field term, namely, µ m0,π γ . However, we can restrict our attention to stationary policies just as in the stationary MFG setting.

Example 4 (Exploration). In (Perrin et al., 2020;Geist et al., 2021), this setting has been used for an MFG in which the agents explore the spatial domain. From the point of view of the population, it amounts to maximizing the entropy of the distribution. The discounted stationary distribution can be used as a proxy to evaluate with a single distribution how well the population explore the state space.

Remark 9. The discounted distribution can be interpreted as the stationary distribution of an agent who starts with distribution m 0 , uses policy π but has a probability to stop at any time step. To be specific, let τ be a random variable with geometric distribution on {0, 1, 2 . . . , } with parameter (1 − γ). Let us denote by µ γ,m0,π n is the distribution of x n , where:
     x 0 ∼ m 0 , x n+1 ∼ p(·|x n , a n , µ m0,π γ ), a n ∼ π(·|x n ), 0 ≤ n ≤ τ x n+1 = x n , τ ≤ n
Then we have: for every x ∈ X ,
P(x n = x) = k≤n P(τ = k)P(x k = x|τ = k) + P(τ > n)P(x n = x|τ > n) = (1 − γ) k≤n γ k µ γ,m0,π k (x) + P(τ > n)P(x n = x|τ > n).
When n → +∞, P(τ > n) → 0, so we obtain that:
µ m0,π γ (x) = lim n→+∞ P(x n = x) = (1 − γ) k γ k µ γ,m0,π k (x).
Definition 4 (Discounted stationary MFG Nash Equilibrium). A pair (π,μ) ∈ Π × ∆ X is a discounted stationary mean field Nash equilibrium (discounted stationary MFNE) if it satisfies the following two conditions:
•π ∈ BR statio,γ (μ); •μ = M statio,γ,m0 (π).
Alternatively,μ is a discounted stationary mean field Nash equilibrium distribution if it is a fixed point of: M statio,γ,m0 • BR statio,γ .


## Social optimum and Mean Field Control

The notions of MFNE discussed above correspond to non-cooperative games, in which each player maximizes her own reward while trying to anticipate the behavior of other selfish agents. A different question consists in considering that the agents are cooperative and try to maximizer a social welfare criterion by choosing together a suitable policy. This situation can also be interpreted as an optimization problem from the point of view of a social planner, who tries to figure out which behavior is optimal from the society standpoint.

Static setting. The social welfare function is defined as the reward obtained on average by the agents:
π → J social static (π) := J static (π; π).
A strategy π * is a static mean field social optimum (static MFSO) if it maximizes the social welfare function J social static .

Stationary case. The total, discounted social welfare associated to a policy π is:
J social statio (π) = J statio (π; µ π ) = E ∞ n=0
γ n r(x n , a n , µ π ) subject to:
x 0 ∼ µ π ,
x n+1 ∼ p(·|x n , a n , µ π ), a n ∼ π(·|x n ), n ≥ 0,

where µ π is the stationary distribution induced by π. Here we see that perturbing π changes µ π , which is reflected in the third argument of the transition function and the reward function. An optimal stationary policy is a π ∈ Π maximizing J social statio . This setting has been considered by Subramanian and Mahajan (2019) or by Angiuli et al. (2022b).

Evolutive case. The total social welfare is:
J social evol (π) = J evol (π; µ m0,π ) = E N T −1 n=0 r n (x n , a n , µ m0,π n ) ,
subject to the following evolution of the agent's state:
x 0 ∼ m 0 ,
x n+1 ∼ p n (·|x n , a n , µ m0,π n ), a n ∼ π n (·|x n ), n ≥ 0.

An optimal policy is a π maximizing J social evol .

Discounted stationary case. The total social welfare is:
J social d−statio (π) = J evol (π; µ m0,π γ ) = E N T −1 n=0 r n (x n , a n , µ m0,π γ ) ,
subject to:
x 0 ∼ m 0 ,
x n+1 ∼ p n (·|x n , a n , µ m0,π γ ), a n ∼ π n (·|x n ), n ≥ 0,

where µ m0,π γ = (1 − γ) n≥0 γ n µ m0,π n ∈ ∆ X is the discounted distribution introduced in Section 2.6.

Price of anarchy. The average reward obtained by a representative player can only be higher in an MFSO than in an MFNE, by the very definition of a social optimum. The discrepancy between the two situations is quantified by the following notion the price of anarchy (PoA). In the static setting, it is defined as:
PoA static = sup π J social static (π) infπ ∈N Estatic J social static (π)
.

In the denominator, N E static denotes the set of static MFNE. The PoA can be defined analogously in the other settings. The term "Price of Anarchy" has been coined by Koutsoupias and Papadimitriou (1999). Since then, this notion has been widely studied in game theory and can be viewed as a way to measure the inefficiency of Nash equilibria (Roughgarden and Tardos, 2007). In the context of MFGs, it has been studied e.g. by Lacker and Ramanan (2019) in a static setting, and by Carmona et al. (2019a) in a dynamic setting.


## Extensions

We conclude this section by mentioning a few extensions. For the sake of readability, we use the basic settings described above in the sequel. However, several variants have been considered in the literature.


## Multiple populations.

Mean field theory allows us to approximate a homogeneous population of individuals by the limiting probability distribution. In multi-population MFGs, there is a finite number of sub-populations, each of them representing a homogeneous group of agents. The transition function and the reward function are the same for all the agents of one sub-population, but may be different from one group to the other. In this way, the MFG paradigm can still be used. We refer for instance to Huang et al. (2006); Feleqi (2013); Cirant (2015); Bensoussan et al. (2018) for an analytical approach and to Carmona and Delarue (2018a, Section 7.1.1) for a probabilistic formulation. In the context of reinforcement learning, multi-population MFGs have been studied e.g. by Subramanian et al. (2020a).

Population-dependent policies. In all the previous settings, the policies are independent of the population distribution. This aspect is classical in the MFG and MFC literature because, if a player anticipates correctly the policy used by the rest of the population, they can anticipate the whole population behavior without uncertainty. As a consequence, the distribution needs not be an input to the agent's policy. However, this aspect might be counter-intuitive from a learning perspective, because it means that the agents react optimally only to the equilibrium population behavior but they cannot adjust their behavior if the distribution deviates from this equilibrium.

Population-dependent policies are tightly connected with population-dependent value functions, and the so-called Master equation in MFGs. This equation has been introduced by P.-L. Lions in the continuous setting (continuous time, state and action) (Lions, 2012). There, it is a partial differential equation (PDE) which corresponds to the limit of systems of Hamilton-Jacobi-Bellman PDEs characterizing Nash equilibria in symmetric N -player games. For more details in the continuous setting, we refer the interested reader to Bensoussan et al. (2015); Cardaliaguet et al. (2019). In the discrete time and space setting, population-dependent value functions and policies have been studied by Mishra et al. (2020) and by Perrin et al. (2021a), where a deep RL method to learn such policies is developed.


## Common noise.

Besides idiosyncratic noise affecting the evolution of each agent independently, it is possible to consider macroscopic shocks affecting the whole population. This is referred to as common noise in the MFG literature. Because the whole population's evolution is stochastic, using policies functions of the player's state only is in general suboptimal. This is because even if the player knows the policy used by all the other players, she cannot predict with certainty the evolution of the distribution. In this case, it is more efficient to use population-dependent policies. We refer to Carmona et al. (2016) and to Cardaliaguet et al. (2019) for respectively a probabilistic and an analytical treatment of MFGs with common noise.


# Iterative methods

We now turn our attention to the question of computing mean field Nash equilibria in the settings presented above. The goal is to compute a pair consisting of a policy and a mean field which form a fixed point. A simple strategy is, starting with some initial pair, to update alternatively the policy and the mean field until convergence to an equilibrium. In this section, we assume that the model is completely known. We call the algorithms presented here iterative methods for the sake of convenience and to distinguish them from the RL algorithms discussed later on. As we will discuss in the sequel, these methods rely on fixed point-type iterations. In contrast with the MDP setting, the underlying operator for these iterations is not always contractive, which triggers the introduction of variants to help ensuring convergence.


## Overview of the methods

As explained above, the main idea is to alternate an update of the population distribution and an update of the representative agent's policy, which can be represented as:
· · · → µ policy update −−−−−−−−−−−→ π +1 mean field update −−−−−−−−−−−−−−→ µ +1 → . . .(9)
At a high level, we expect the scheme described in (9) to converge towards a fixed point (μ,π) which is a Nash equilibrium.

The mean field update is done using the population distribution or the sequence of distributions induced by the current policy. Notice that, since the dynamics is known, it is straightforward to compute the mean field induced by a given policy. The converse is more challenging: except in some special cases, given a mean field, it is hard to find which policy generated it as many policies can generate the same mean field. Thus, computing not only the mean field but also an equilibrium policy is a crucial point.

The policy update can typically be done in two different ways. In the first family of methods, the policy is updated by computing a best response against the mean field. In the second family, the policy is updated based on the evaluation of the previous policy. We call these two families best-response based and policy-evaluation based respectively. In fact, this distinction stems from an analogous distinction between two families of methods to solve standard MDPs, respectively value iteration and policy iteration.

In the rest of this section, we first recall value iteration and policy iteration methods in standard MDPs. We then explain how these methods are adapted in the MFG setting. For the sake of simplicity, we focus on two settings: the stationary setting and the finite horizon evolutive setting. We stress the main similarities and differences between the methods to solve these types of MFGs. The methods in these two settings can be adapted to tackle the static setting and the γ-discounted setting, which are thus omitted for the sake of brevity.


## Solving standard MDPs

We recall here two fundamental families of methods to compute optimal policies: value iteration and policy iteration. For more details on these methods, we refer to e.g. (Sutton and Barto, 2018;Bertsekas and Shreve, 1996;Bertsekas, 2012;Puterman, 2014;Meyn, 2022).


### Value iteration

One way to obtain an optimal policy is to first compute the optimal value function by using the optimal Bellman operator, and then consider a greedy policy with respect to this optimal value function. Since we are motivated by applications to RL algorithms, we focus on the state-action value function.

Stationary MDP. In a stationary MDP, the value iteration method can be expressed as follows: Q 0 is given, and for k = 0, . . . , K − 1,
Q k+1 = B * Q k .(10)
At the end we use the following policy as an approximation of the optimal policy:
π K ∈ GQ K ,
where G denotes the greedy policy operator defined by:
GQ = π : ∀x ∈ X , a∈A π(a|x)Q(x, a) = argmax a Q(x, a) .(11)
Thanks to the fact that the Bellman operator is a γ-contraction, when K → +∞, π K → π * under suitable conditions on the MDP. Equivalently, the above iterations can also be written as follows, by introducing the greedy policy at every iteration: Q 0 is given, and for k = 0, . . . , K − 1,
π k = GQ k , Q k+1 = B π k Q k ,
where the Bellman operator B π k associated to the current policy π k is defined in (1).

Finite horizon MDP. In a finite horizon MDP, the optimal value function Q * can be computed by dynamic programming since it satisfies the optimal Bellman equation:
         Q * N T (x, a) = r N T (x, a), (x, a) ∈ X × A, Q * n (x, a) = r n (x, a) + γE max a∈A Q * n+1 (x n+1 , a) x n+1 ∼ p n (·|x, a) , (x, a) ∈ X × A, n = N T − 1, . . . , 0.(12)
Computing Q * using the above equation is called backward induction. Once it has been computed, an optimal policy can be found by taking the greedy policy at each step, i.e.:
π * = GQ * ,
where G is the finite-horizon greedy policy operator defined as:
(GQ) n = GQ n , n = 0, . . . , N T − 1.(13)
Remark 10. Notice that the Bellman equation (12) is a backward equation and not a fixed-point equation, contrary to Eq.

(2) characterizing the optimal value function in the stationary case. Since the horizon is finite, the optimal value function is computed with a finite number of steps, which is an important difference with the stationary MDP setting.


### Policy iteration

The optimal policy can also be computed by successive improvements of a policy. Starting from an initial policy, at each iteration, we evaluate the performance of this policy by computing the associated value function, and then we take a greedy step to improve the policy. These two steps are called policy evaluation and policy improvement, and the overall algorithm is called policy iteration.

Stationary MDP. In a stationary MDP, the method consists in applying the Bellman operator B π k associated to the current policy π k (see (1)) and then applying the greedy policy operator defined in (11). Thus, this method takes the following form: π 0 is given, and for k = 0, . . . , K − 1:
Q k+1 = Q π k , π k+1 ∈ GQ k+1 .
At the end, we return π K and use it as an approximation of π * . As K → +∞, we have π K → π * under suitable assumptions on the MDP. At iteration k, the value function Q π k can be computed by applying repeatedly the Bellman operator B π k until convergence to its fixed point, or until an approximation of Q π k is obtained with a finite number of iterations: with Q k,0 given, we repeat for m = 0, . . . , M − 1,
Q k,m+1 = B π k Q k,m ,(14)
and we use Q k,M as an approximation of Q π k .

Finite horizon MDP. In a finite horizon, we can define the following method by analogy with the stationary case: π 0 is given, and for k = 0, . . . , K − 1:
Q k+1 = Q π k , π k+1 ∈ GQ k+1 .
where G is the finite-horizon greedy policy operator defined in (13). At the end, we return π K and use it as an approximation of π * . At each iteration, the state-action value function associated to the current policy can be computed by backward induction. Indeed, for a given policy π, Q π satisfies the following Bellman equation, which holds by dynamic programming:
       Q π N T (x, a) = 0, (x, a) ∈ X × A, Q π n (x, a) = r n (x, a) + γE Q π n+1 (x n+1 , a n+1 ) x n+1 ∼ p n (·|x, a), a n+1 ∼ π n (·|x) , (x, a) ∈ X × A, n = N T − 1, . . . , 0.

## Solving MFGs

As explained at the beginning of this section (see Eq. (9)), the main idea underlying the methods we present below is to alternate an update of the population distribution and an update the representative agent's policy.

Inspired by the above methods for standard MDPs, we can distinguish two families of methods for MFGs, depending on whether the policy update consists in computing an optimal policy against µ or simply improving the current policy. We call these two family of methods best-response based and policy-evaluation based.


### Best response-based methods

Since an MFG equilibrium can be defined as the fixed point of a mapping, a basic strategy consists in repeatedly applying this mapping. Under suitable conditions, this method converges and the limit is automatically a fixed point.

Stationary MFG. In the stationary MFG setting (see Section 2.4), we recall that a Nash equilibrium consists of a stationary distributionμ ∈ ∆ X and a stationary policyπ ∈ Π. The policyπ is characterized as an optimal policy for a representative player facing the population distributionμ. This problem can be phrased in the framework of MDPs.

If the stationary mean field is µ, then the MDP that a representative player needs to solve is:

(X , A, p(·, ·, µ), r(·, ·, µ), γ),

where the transition and the reward functions are given by:

p(·, ·, µ) : X × A → P(X ), r(·, ·, µ) : X × A → R.

The optimal policy for this MDP is the best response against µ, which is denoted by BR statio,γ (µ). It can be obtained for example by applying the policy iteration or the value iteration algorithms as recalled in Section 3.2. Conversely, given a policy π, the induced mean field is the stationary distribution (assuming it is unique for simplicity) induced by π and denoted by M statio (π), see Eq. (5). This is summarized as follows: µ 0 is given, and for = 0, . . . , L − 1,
π +1 = BR statio,γ (µ ) µ +1 = M statio (π +1 ).(15)
At the end, we use (π L , µ L ) as a proxy for the MFG equilibrium. Under suitable conditions, it is close to (π,μ) when L is large enough. We come back to the question of convergence in Section 3.4 below.

In the above iterative method, we update the mean field term by using the operator M statio , which can be approximated by applying a large number of times the transition matrix defined in (8). In other words, in practice, µ +1 is often defined by first computing: µ n+1 = P n,µ n ,π +1 µ n , n = 0, . . . , M − 1, with µ 0 a given initial distribution. For instance we can take µ 0 = µ from the previous iteration. As M → +∞, we expect µ M → M statio (π +1 ), so we use µ M as an approximation of µ +1 .

In fact, taking M relatively small can have some advantages. In some sense, it amounts to slowing down the updates of the mean-field term. This can bring more stability to the iterative method, particularly when the policy π +1 is computed approximately (e.g., in a reinforcement learning setup). We will come back to this idea of damping the update of the mean field in Section 3.4 below, but let us immediately emphasize that a variant of the above iterative method consists in doing only one application of the transition matrix at each iteration . This can be summarized as: µ 0 is given, and for = 0, . . . , L − 1, π +1 = BR statio,γ (µ )
µ +1 = P n,µ ,π +1 µ .
This method has been used for instance by Guo et al. (2019); Anahtarci et al. (2020). It is also in line with the idea of using a two-timescale approach for mean field Nash equilibria (Subramanian and Mahajan, 2019;Mguni et al., 2018;Angiuli et al., 2022b;Xie et al., 2021). A similar method has been analyzed in (Anahtarcı et al., 2019(Anahtarcı et al., , 2020 for average cost MFGs (in the latter work, it is referred to as value iteration algorithm for MFGs).

Finite-horizon MFG. In the evolutive MFG setting with a finite horizon N T (see Section 2.5), an equilibrium is a sequence of distributionsμ = (μ n ) n=0,...,N T and a sequence of policiesπ = (π n ) n=0,...,N T , indexed by the time steps in the game. Given a sequence of distributionsμ = (μ n ) n=0,...,N T , a representative player needs to solve the following finite-horizon MDP (see Section 2.3.2):
(X , A, p µ , r µ , N T ),
where:
p µ : {0, . . . , N T − 1} × X × A → P(X ), p µ : (n, x, a) → p n (·|x, a, µ n )
and r µ : {0, . . . , N T } × X × A → R, r µ : (n, x, a) → r n (x, a, µ n ).

The optimal policy for this MDP is the best response against µ, which is denoted by BR evol,m0,N T (µ) . It can be obtained as a greedy policy for the optimal value function Q * ,µ , which can be computed by backward induction as described in Section 3.2.1. Alternatively, the optimal policy can be computed by policy iteration as described in Section 3.2.2. Conversely, given a policy π = (π n ) n=0,...,N T , the induced mean-field is the sequence of distributions generated by starting from m 0 (remember that m 0 is fixed in the definition of the MFG, see Section 2.5) and using π n at time step n, n = 0, . . . , N T − 1. The resulting mean-field sequence is denoted by M evol,m0,N T (π), see Eq. (7). This is summarized below, using the notation introduced in Section 2.5: µ 0 is given, and for = 0, . . . , L − 1,
π +1 = BR evol,m0,N T (µ ) µ +1 = M evol,m0,N T (π +1 ).
At the end, we use (π L , µ L ) as a proxy for the MFG equilibrium. Under suitable conditions on the MFG, this pair is close to (π,μ) when L is large enough. For the sake of completeness and future reference, we provide here the Bellman equations satisfied by Q * ,µ and Q π,µ , which can be derived by dynamic programming: a) x n+1 ∼ p n (·|x n , a n , µ n ) ,
         Q * ,µ N T (x, a) = r N T (x, a, µ N T ) Q * ,µ n (x, a) = r n (x, a, µ n ) + E max a∈A Q * ,µ n+1 (x n+1 ,n = N T − 1, . . . , 0,(16)
and Perrin et al. (2020Perrin et al. ( , 2021a used backward induction to compute the optimal value function for finite-horizon MFG (embedded in fictitious play iterations, see Section 3.4), which served as a baseline to assess the performance of RL-based methods (see next section). Cui and Koeppl (2021a) solved finite-horizon MFG using fixed point iterations combined with RL methods and entropy regularization (we come back to this point in Section 3.4 below). Mishra et al. (2020) also solved MFGs based on a best-response computation, but by computing a best response backward in time in the spirit of dynamic programming, which requires solving for all possible distributions since the equilibrium mean field sequence is not known a priori. The aforementioned two-timescale approach originally studied in the stationary setting has been extended by Angiuli et al. (2021) to solve finite-horizon MFGs.
       Q π,µ N T (x, a) = r N T (x, a, µ N T ) Q π,µ n (x, a) = r n (x, a, µ n ) + E Q π,µ n+1 (x n+1 , a n+1 ) x n+1 ∼ p n (·|x n , a n , µ n ), a n+1 ∼ π n+1 (·|x n+1 ) , n = N T − 1, . . . , 0.(17)
Remark 11. In the stationary regime, we can view iterations as time steps. Taking a large number of iterations amounts to looking at the long time behavior. However, in the finite-horizon MFG setting, the index of iterations does not coincide with the index of time in the game. At each iteration , the policy and the distributions are updated for all time steps, n = 0, . . . , N T .


### Policy evaluation-based methods

Instead of computing a full-fledged best response for the policy update at each iteration of (9), we can simply do one step of policy improvement. Intuitively, evaluating the current policy should be computationally faster than computing an optimal policy (except when the state space is small or when we have an explicit formula for the optimizer of the value function). To improve the policy, we can first evaluate the current policy given the latest mean field, and then take a greedy policy.

Stationary MFG. In a stationary MFG, we can proceed as follows: we first compute the state-action value function associated to the current policy against the current population distribution (policy evaluation step). We then define the new policy as a greedy policy for the newly computed value function (policy improvement step). Last, we deduce the stationary population distribution induced by this policy (mean field update step). Concretely, the method is: µ 0 and π 0 are given, and for = 0, . . . , L − 1,
       Q +1 = Q π ,µ π +1 ∈ GQ +1 µ +1 = M statio (π +1 ).(18)
This method is referred to as the Policy Iteration (PI) algorithm for MFGs and was introduced by Cacace, Simone et al. (2021) for continuous time, continuous space MFGs. It is not to be confused with the method that consists in using standard policy iteration to compute a best response against a given distribution (i.e., replacing BR statio,γ (µ ) in (15) by the result of a policy iteration method). In practice, the evaluation step can be done by applying a finite number of times the Bellman operator B π ,µ as defined in Eq. (6). Thanks to the contraction property of this operator, we obtain an approximation of Q π ,µ . Furthermore, as discussed above, M statio (π +1 ) can be approximated by applying a large but finite number of times the transition matrix.

Finite-horizon MFG. In a finite-horizon MFG, the same strategy can be applied, except that we need to take into account the evolutive aspect of the game. Each of the step is done for all the time steps. The method can be summarized as follows: µ 0 and π 0 are given, and for = 0, . . . , L − 1,
       Q +1 = Q π ,µ π +1 ∈ GQ +1 µ +1 = M evol,m0,N T (π +1 ).(19)
In this setting, Q π ,µ can be computed by backward induction, thanks to the dynamic programming equation (17). Similarly, M evol,m0,N T (π +1 ) can be computed by following N T transitions, see (8).

Cacace, Simone et al. (2021), mentioned above, also studied policy iteration in the finite-horizon setting and proved convergence under suitable conditions. Still in the finite-horizon setting, the convergence results were extended to other settings by Camilli and Tang (2022); Laurière et al. (2021). Using a purely greedy policy often leads to instabilities, particularly in the finite state case; see e.g. (Cui and Koeppl, 2021a) and the next section for more details. For this reason, variants with regularized policies have been introduced, such as the online mirror descent, as we explain below.


## Convergence and variants

Convergence of fixed point iterations. Intuitively, the scheme described in (9) indeed converges towards a fixed point if the mapping (µ , π ) → (µ +1 , π +1 ) is a strict contraction on a suitably defined space. In a stationary setting, this property can be ensured by assuming that the reward function and the transition function are smooth enough. Typically, this amounts to assuming that they are Lipschitz continuous with small enough Lipschitz constants. In a finite horizon setting, this condition can sometimes be replaced by an assumption on the smallness of the time horizon. One advantage of having a contraction is that it provides a constructive way to get the equilibrium through Banach-Picard iterations. This technique is commonly used in the literature on MFGs, both to show existence and to derive algorithms. See e.g. Huang et al. (2006) in the context of existence and uniqueness of the equilibrium or Carlini and Silva (2014) in the context of numerical methods. It is in general difficult to formulate sufficient conditions on the model (i.e., the reward and the transition) to ensure the strict contraction property because the mapping involves the policy update step, for which there is in general no explicit formula. In the linear-quadratic case, several sufficient conditions are formulated by Hu (2021, Proposition 3.1). Furthermore, regularizing the policy can help to alleviate some of the conditions ensuring the contraction property, see e.g. Guo et al. (2019); Anahtarci et al. (2020). Using regularization of the policy, Guo et al. (2020a) have proved convergence and analyzed the complexity of value-based and policy-based algorithms.

However, conditions guaranteeing the strict contraction property are generally very restrictive and fails to hold for many games. For example, Cui and Koeppl (2021a, Theorem 2) show that non-contractivity is the rule rather than the exception. Without contractivity, Banach-Picard iterations typically lead to oscillations, see e.g. Chassagneux et al. (2019, Figure 3) in the context of a method based on the probabilistic interpretation of MFGs, or Lauriere (2021, Figure 4) in the context of linear-quadratic MFGs.

To address this issue, several variants of the pure Banach-Picard fixed point iterations have been proposed in the literature, relying on a few key principles.

Before describing these principles, let us mention that besides the aforementioned class of assumptions to ensure contractivity which are somehow quantitative assumptions since they boil down to smallness of some coefficients, an alternative class of hypotheses are in some sense qualitative assumptions which pertain to the structure of the game. For example, potential structure and MFG satisfying Lasry-Lions monotonicity (Lasry and Lions, 2007) can be used to prove convergence of best-response based and policy evaluation based algorithms, see respectively (Cardaliaguet and Hadikhanloo, 2017;Perrin et al., 2020;Geist et al., 2021) and (Hadikhanloo, 2017;Perolat et al., 2021). In particular, the Lasry-Lions monotonicity condition, which basically refers to the fact that players tend to avoid crowded regions, has been interpreted in terms of exploitability (see Section 5.1.2). These convergence results do not rely on smallness conditions on the coefficients. However, even for MFGs with such nice structure, pure fixed point iterations rarely converge and smoothing the iterations if typically required to ensure convergence.


## Smoothing the mean field updates.

First, a simple modification consists in using damping to slow down the updates of the mean field term. Even if the mapping µ → π → µ +1 is not contractive, we can hope that the following mapping is contractive, at least for small enough values of α ∈ (0, 1):
µ → π →μ +1 := (1 − α)μ + αµ +1 .(20)
Here µ +1 is the mean field associated to policy π whileμ is an average over past mean field terms. See e.g. Lauriere (2021, Section 2) for an example in which damping with a constant coefficient helps ensuring numerical convergence. We also refer to Tembine et al. (2012) for more algorithms developed along these lines and presented in the context of static games. We can also let α change with the iteration index, i.e., take a different α for = 1, 2, . . . . One of the most popular versions consist in taking α = 1/( + 1) and is called Fictitious Play. It was first introduced in two-player games by Brown (1951); Robinson (1951) and extended to MFG by Cardaliaguet and Hadikhanloo (2017); Hadikhanloo (2018); Hadikhanloo and Silva (2019). In the context of stationary MFGs for example, (15) is replaced by: µ 0 is given, and for = 0, . . . , L − 1,
         π +1 = BR statio,γ (μ ) µ +1 = M statio (π +1 ) µ +1 = + 1μ + 1 + 1 µ +1 .(21)
Under suitable assumptions,μ converges to a stationary MFG equilibrium distribution. It is important to note that in general the last iterate π of the policy does not generateμ and hence does not converge towards an equilibrium policy. If one cares about the equilibrium policy, it is thus required to learn a policy generatingμ . In some cases, convergence of the last iterate towards an equilibrium holds, see e.g. Cardaliaguet and Hadikhanloo (2017). For finite-state MFGs, a rate of convergence has been obtained by Perrin et al. (2020) for continuous-time FP under monotonicity condition and by Geist et al. (2021); Bonnans et al. (2021) respectively for discrete-time FP in some potential MFGs. In linear-quadratic MFGs, a rate of convergence has been obtained by Delarue and Vasileiadis (2021), who also studied the impact of common noise.

Slowing down the updates of the mean field term is also in line with the idea of using a two-timescale approach for mean field Nash equilibria (Subramanian and Mahajan, 2019;Mguni et al., 2018;Angiuli et al., 2022b;Xie et al., 2021). Here, the distribution and the policy (or the value function) are both updated at every iteration but the distribution is updated at a slower rate than the policy. Intuitively, this implies that the representative agent has enough time to compute an approximate best response before the distribution changes too much.

Smoothing the policy updates.

Another way to bring more stability to the iterative method is to regularize the policy update. For instance, the greedy policy operator defined in (11) is very sensitive to perturbations of the stateaction value function. Small changes in this value function might lead to significant changes in the induced greedy policy. To mitigate this problem, it is common to replace the argmax by a softmax, meaning that we can define:
π (k+1) (·|x) = softmax τ Q(x, ·),
where τ > 0 is an inverse temperature parameter and softmax : R |A| → ∆ A is defined by: for q = (q 1 , . . . , q |A| ),
softmax τ (q) = e τ qi |A| j=1 e τ qj i=1,...,|A| .
It transforms a vector of Q-values into a discrete probability distribution on the action space in which the actions with larger value have a higher probability. Using a softmax instead of the argmax generally yields smoother and more stable learning curves, see e.g. Guo et al. (2019); Anahtarci et al. (2020).

In fact, finite-state finite-action MFGs typically admit only randomized policy equilibria and no pure equilibria. This is also the reason why we generally allow for randomized policies in finite-player games (Nash, 1950(Nash, , 1951. Hence, iterative methods with pure greedy policies cannot be expected to converge to Nash equilibria in general, and using mixed policies is unavoidable.

Regularized policies can be obtained e.g. by directly changing the way the policy is obtained from the value function Perolat et al., 2021) or by adding a penalty in the reward function, which changes the value function and hence the policy, see e.g. Anahtarci et al. (2019); Guo et al. (2020b); Cui and Koeppl (2021a); Firoozi and Jaimungal (2022); Laurière et al. (2022). However, it should be noted that regularizing the policies also has drawbacks: if π is forced to be smooth, this constraint might prevent the iterative method from converging towards the Nash equilibrium since π can only be smooth version of the equilibrium policy.

One way to circumvent this limitation and to allow the regularized policy to concentrate on optimal actions is to let the underlying Q-function take larger and larger values. This can be achieved by considering a cumulative Q-function, which leads to the Online Mirror Descent (OMD) algorithm (Hadikhanloo, 2017;Perolat et al., 2021):
           Q +1 = Q π ,µ Q +1 =Q + αQ +1 π +1 = softmax τQ +1 µ +1 = M statio (π +1 ).(22)
where α > 0 is a parameter which determines the cumulative factor. This algorithm can be viewed as a modification of the policy evaluation method described in (18) with a cumulative Q-function and a regularized greedy policy. Instead of the softmax, we can more generally take the gradient of the convex conjugate of a strongly convex regularizer, see Perolat et al. (2021) for more details.


## Iterative methods for Mean Field Control

We recall that the MFC problem introduced in Section 2.7 corresponds to the maximization of a social reward. In the evolutive case, it can be reformulated as an MDP by considering the population distribution as the state. Indeed, we can rewrite: , subject to the following evolution of the mean field state:
µ m0,π 0 = m 0 , µ m0,π n+1 = P n,µ m 0 ,π n ,πn µ m0,π n , n ≥ 0.
This is an MDP with:

• state space ∆ X ,

• action space Π,

• probability transition function:p n (·|µ, π) = (P µ,π n ) µ,

• reward function:r n (µ, π) = x∈X a∈A r n (x, a, µ)µ(x)π(a|x).

We will refer to this MDP as the mean field MDP (MFMDP). An action, taken by the central planner or collectively by the population, is an element of A = (∆ A ) X . A one-step policy at the level of the population is a function from X to ∆ A . Note that, even if X and A are finite, the state space X and the action space A of the MFMDP are continuous and hence rigorously defining and analyzing this MDP requires a careful formulation. We refer to the work of e.g. Gast et al. (2012); Gu et al. (2019Gu et al. ( , 2021a; Motte and Pham (2019); Carmona et al. (2019c); Bäuerle (2021) for more details on MFMDP.

Let us stress that this MFMDP is not to be confused with the MDP arising in MFGs, which is the MDP for a single representative player when the mean field term is given. In the latter case, the state is simply the agent's state and not the population state.

With this reformulation, the evolutive MFC problem can be analyzed and solved using methods developed from MDP. However, notice that the policies are, in general, functions of both the representative agent's state and the mean field state. The main challenges thus pertain to the numerical implementation of these methods, since we need to represent efficiently the distribution and the policy. We will come back to this question in Section 4.3.

Remark 12. Note that, in the present model, the evolution of µ m0,π is in fact completely deterministic once m 0 and π are given. Noise affecting the distribution and making its evolution stochastic is referred to as common noise. We refer to Motte and Pham (2019); Carmona et al. (2019c) for more details. Furthermore, since an action is an element of Π, a policy is a functionπ : ∆ X µ →π(µ) ∈ ∆ Π . Sampling fromπ(µ) amounts to sample an element π to be used by the whole population. Carmona et al. (2019c) referred to this as common randomness.


# Reinforcement learning algorithms

The iterative methods presented in the previous section are described with exact updates, meaning that we assume that the model is fully known and that there are no numerical approximations in the computation of the rewards or the transitions. In this context, the only approximations that we have to cope with are in situations where an infinite number of iterations would be needed but we can only afford a finite number of iterations (e.g., to compute a stationary distribution or a stationary value function).

However, in many situations, these methods cannot be implemented as such. A typical scenario is when the model is not completely known from the agent that is trying to learning an optimal behavior. Another instance is when the model is known, but the state space or the action space are too big for us to compute the solution on the whole domain. In such cases, exact dynamic programming cannot be used. Instead (model-free) reinforcement learning (RL) methods have been developed. Here we will focus on methods relying on approximate dynamic programming (ADP). The question of exploring efficiently the state-action domain plays a crucial role.

RL ideas have first been developed for finite and small state and action spaces, in which case the algorithms are called tabular methods since the value function can be described by a table (i.e., a matrix). However, many of the recent breakthrough applications of RL have been obtained thanks to a combination of RL methods with neural network approximations and deep learning techniques, which leads to deep reinforcement learning (DRL) methods. The flexibility and the generalization capabilities of deep neural networks allow us to efficiently learn solution to highly complex problems. In the context of games, some striking examples that were successfully tackled are ALE (Atari Learning Environment) (Mnih et al., 2013;Bellemare et al., 2013), Go , poker (Brown and Sandholm, 2017;Moravčík et al., 2017) or Starcraft (Vinyals et al., 2019).

In the context of MFGs, we will build on the iterative methods presented in Section 3. These methods boil down to alternating mean-field updates and policy updates, and the policy updates stem from standard MDP techniques. As a consequence, standard RL techniques can readily be injected at this level to learn policies or value functions.

In this section, starting from exact dynamic programming, we discuss some key ideas underlying ADP and RL methods. We then move on to neural network approximations and DRL. Finally we explain how these ideas can be adapted to the MFG setting.
Environment Agent Reward r n+1 State x n+1
Action a n Reward r n State x n Figure 1: Reinforcement learning environment: classical single-agent setup. Here, at iteration n, the current state of the MDP is x n , the action taken by the agent is a n , the new state is x n+1 ∼ p(·|x n , a n ) and the reward is r n = r(x n , a n ).

The new state x n+1 is observed by the agent and is also used for the next step of the environment's evolution.


## Background on reinforcement learning

Environment. Traditional RL aims at solving a stationary MDP, see Section 2.3.1. In the typical setting, the agent who is trying to find an optimal policy for the MDP interacts with an environment through experiments that can be summarized as follows:

1. The agent observes the current state x of the MDP (which is referred to as the state of the environment but could be for instance her own state or the state of the world).

2. The agent takes an action a, which is going to influence the state of the MDP through the transition kernel p and produces a reward through the function r.

3. The agent observes the new state x ∼ p(·|x, a) as well as the reward r(x, a) resulting from her action.

The agents can repeat such experiments. We provide in Fig. 1 a schematic representation of this setting. It is often assumed that the agent can reboot the environment from time to time. To avoid remaining stuck in local maxima, it is common to assume that the new state is picked randomly, which is referred to an exploring start. We stress that the agent does not observe directly the functions p and r that are used to compute the new state and the reward. The agent only observes the outputs of these functions. In some cases, recovering the functions p and r from such observations is feasible, leading to the concept of model-based RL. However, for complex environments (i.e., complex p and r), recovering the functions would require such a large number of observations that we generally prefer to directly aim for an optimal policy, which leads to the concept of model-free RL. The agent needs to interact multiple times to figure out the most suitable actions for a given state of the world. For more details, we refer the interested reader to e.g. Sutton and Barto (2018); Bertsekas (2012); Szepesvári (2010); Meyn (2022).

Approximate dynamic programming. Some of the most popular RL methods are based on approximations of the exact dynamic programming equations satisfied by the value functions. Focusing on a stationary MDP, let us recall that an optimal policy can be computed for instance by value iteration or policy iteration (see Section 3.2), which require computing the state-action value functions Q * or Q π respectively. These two functions satisfy fixed-point equations whose solutions can be approximated by repeatedly applying the corresponding Bellman operators B * and B π , see (10) and (14). This amounts to repeating:
Q π (x, a) ← r(x, a) + γE x ∼p(·|x,a),a ∼π(·|x ) [Q(x , a )], ∀(x, a) ∈ X × A Q * (x, a) ← r(x, a) + γE x ∼p(·|x,a) [max a Q * (x , a )], ∀(x, a) ∈ X × A.
The arrow is used to denote that the value of Q π (x, a) is replaced by the value in the right hand side.

In the context of RL, we assume that the agent does not know r or p, so she cannot perform the above updates. However, these updates can be performed approximately provided we assume that the agent can query the environment and ask, for any pair (x, a), the value of r(x, a) and a sample x ∼ p(·|x, a) (picked independently at each realization). Then to update Q π (x, a) and Q * (x, a), the agent can query multiple times the pair (x, a) and replace the expectations by empirical averages:
Q π (x, a) ← r(x, a) + γ 1 I I i=1Q (x i , a i ), x i ∼ p(·|x, a), a i ∼ π(·|x i ), i = 1, . . . , I, ∀(x, a) ∈ X × Ã Q * (x, a) ← r(x, a) + γ 1 I I i=1 max a Q * (x i , a ), x i ∼ p(·|x, a), i = 1, . . . , I, ∀(x, a) ∈ X × A,
where the Monte Carlo samples x i and a i are independent. However, it is generally too computationally expensive to update every pair (x, a) using a batch of I samples. Furthermore, in many scenarios the agent does not have the freedom to query any state x. Instead, she is usually bound to observe the state of the environment, which is updated iteration after iteration in a sequential manner by following the dynamics of the state. She can influence the evolution of the state, but she cannot pick any new state that she wants at every iteration. In such scenarios, the agent can only perform updates by using the available information at each iteration.

To be specific, let us assume that the agent has a policyπ that she uses to generate a trajectory by interacting with the environment:

x 0 ∼ m 0 , a n ∼π(·|x n ), x n+1 ∼ p(·|x n , a n ), n ≥ 0.

The fixed-point equation satisfied by Q * says thatQ * (x, a) is well estimated if:
Q * (x, a) = r(x, a) + γE x ∼p(·|x,a),a ∼π(·|x ) max a Q * (x , a ) .(23)
So it is natural to use the discrepancy between the right hand side and the left hand side to improve the estimateQ * of Q * . Since we are bound to follow the trajectory, we cannot get the expectation over x and, instead, we perform sampled-based updates using one sample at each step and a learning α > 0:

Q * (x n , a n ) ←Q * (x n , a n ) + α r(x n , a n ) + γ max a Q * (x n+1 , a ) −Q * (x n , a n ) .

This leads to the celebrated Q-learning algorithm introduced by Watkins (1989) and whose convergence under suitable conditions has been proved by Watkins and Dayan (1992). Estimating correctly the whole function Q * can be ensured if every pair (x, a) is visited infinitely often, which can be guaranteed under some assumptions on the dynamics of the state and by takingπ for instance as an -greedy policy (according to which in every state, every action has some probability to be selected). To estimateQ π , a similar strategy can be used.


## Deep reinforcement learning.

When state and action spaces are finite, a state-action value function is simply a matrix, which can be stored in memory and processed easily when the spaces are small enough. Thanks to this, we can update the value function point by point (one point being a state-action pair in the case of Q-functions). However, when the spaces are very large or even continuous, it becomes impossible to evaluate precisely every pair (x, a). Furthermore, it is also impossible to visit all pairs during training, impliying that the question of generalization (i.e., performance on unvisited pairs) cannot be avoided. Motivated by both efficiency and generalization capabilities, we can approximate the state-action value functions by parameterized non-linear functions such as neural networks. For example, let us approximate Q * by a neural network Q θ with a given architecture and parameters θ. Going back to (23), we note that now, not only we do not know the expected value, but also we cannot update the function only at a specific pair without changing its value at other pairs. Instead, we use the discrepancy between the left hand side and an estimation of the right hand side to define a loss function that can be used to train the neural network Q θ . Since this neural network appears in both sides of the equation, to make the learning process more stable, we introduce an auxiliary neural network Q θtarget called target network and we use it to replace Q θ in the right hand side. The parameters θ target are fixed when we update θ, and are updated from time to time but less frequently than θ. To be specific, we define the loss:
L(θ; θ target ) = E x,a Q θ (x, a) − r(x, a) − E x ∼p(·|x,a) max a Q θtarget (x , a ) 2 ,(24)
where the expectation is over state-action pairs. We do not specify here the distribution of this pair, but in practice it typically comes from played trajectories stored in a replay buffer. In practice, this loss is minimized using stochastic gradient descent on mini-batches sampled from this replay buffer. This leads to the DQN algorithm introduced by Mnih et al. (2013). The above approach uses the fact that we can easily compute the maximal value of Q θtarget (x , ·) over the action space. This is typically possible only if the action space is finite and not too large. Otherwise, we can use another neural network to encode a policy and train this neural network so that it approximates an optimal policy, using a so called policy loss. Then, in loss (24), the term max a Q θtarget (x , a ) is replaced by the expectation of the target Q-value according to the learnt policy. The resulting agent is called an actor-critc (the actor is the policy, the critic is the state-action value function). Since our goal is not to present an exhaustive list of methods, we simply mention below three popular approaches, to give an idea of the variety of algorithms:

• If we consider only deterministic policies, we can replace the policy by a parameterized function ϕ ω : X → A with parameters ω. In this case, the corresponding policy loss optimises for
max ω E x [Q θtarget (x , ϕ ω (x ))].
Its gradient can be obtained using the chain rule. This leads to an algorithm that is reminiscent of the the Deep Deterministic Policy Gradient (DDPG) of Lillicrap et al. (2016).

• Another approach is to look for a general stochastic policy π, in which case we have the interpretation :
E a ∼π(·|x ) [Q θtarget (x , a )] = A Q θtarget (x , a )π(a |x )da .
Taking the gradient and using the log trick yields the state-wise gradient:
E a ∼π(·|x ) Q θtarget (x , a )∇ log π(a |x ) .
The resulting algorithm is reminiscent of Reinforce (Williams, 1992). The related empirical gradient requires sampling from the learnt policy and is usually of high variance. An alternative approach consists in using the reparameterization trick. For example, we can restrict our attention to Gaussian policies
π ω (·|x) = Φ N (mω(x),σω(x)I) (·),
where Φ N (mω(x),σω(x)I) denotes the density function of the normal distribution N (m ω (x), σ ω (x)I), with m ω and σ ω being two parameterized functions with parameters ω. Here I denotes the identity matrix on A. This leads to the following approximation:
E a ∼π(·|x ) [Q θtarget (x , a )] ≈ E a ∼πω(·|x ) [Q θtarget (x , a )] = E ∼N (0,I) [Q θtarget (x , m ω (x) + σ ω (x) )].
Here again, we can replace the expectation by an average over a finite number of realizations of . In this way, we obtain an algorithm which is similar to TD3 (Fujimoto et al., 2018).

Since the goal of this section is simply to describe some of the key ideas behind DRL, we do not go further into a detailed presentation of the variety of existing methods. We refer the interested reader to e.g. Arulkumaran et al. 


## Reinforcement learning for MFGs

We now turn our attention to RL methods for MFGs. The iterative methods presented in Section 3 allow us to solve an MFG by iteratively updating the population distribution and the policy, and the policy update can be done using standard MDP techniques. As a consequence, RL techniques to solve MDPs can directly be adapted to solve MFGs in a model-free fashion.


### Environments with mean field interactions

To study RL methods for MFGs, the first question is the definition of the environment. In MFGs, the transitions and the rewards depend on the population distribution, which should thus be part of environment. Since we are going to focus on how a representative agent learns an equilibrium policy, we also include the state of this representative agent in the state of the environment.

The next question is: What is the information available to the agent who is learning? In other words, we should decide what the output of one query to the environment is. Remember that for a given population distribution (or sequence of distributions in the evolutive setting), the agent tries to solve an MDP parameterized by this distribution but the policy is not a function of the population distribution (see e.g. Section 4.1 in the stationary MFG case). From this point of view, to learn an optimal policy, she does not need to observe the rest of the population: it is sufficient to observe the result of the reward function and some samples of transitions.

Remark 13. The fact that the representative agent does not need to observe the rest of the population in order to learn an optimal policy is specific to the mean-field setting with an infinite number of players. In a finite-player game, the equilibrium policy of each player generally depends on the configuration of the rest of the population, even when the interactions are symmetric or when a mean-field approximation is used, see e.g. Yang et al. (2018b); Yang and Wang (2020); Zhang et al. (2021) in the context of MARL. Focusing on population-independent policies (which are sometimes referred to as decentralized policies) is one of the main advantage of the MFG approach compared with a finite-player game framework.

We summarize the environment in Fig. 2, which is very similar to the classical RL setup described in Fig. 1 except that the distribution is involved in the environment.

Remark 14 (On the implementation of the environment). In some cases, the environment is truly based on the mean field state corresponding to the regime with an infinite number of agents. This can be the case for example when the state space and the action space are finite and small, and the evolution of the distribution or the stationary distribution can be computed exactly using the transition matrix. However, in general, the environment relies on some approximate version of the population distribution (e.g., using an empirical distribution with a finite number of agents, or using some function approximations). Compared with the ideal environment with the true mean-field distribution, this adds an extra layer of approximation which can be neglected if one is purely interested in the performance of RL algorithms. We come back to this point in Section 4.3 below, in the context of MFC.


### Reinforcement learning for MFGs

We focus on two settings: stationary and finite horizon. The ideas developed in these cases can be adapted to tackle static and infinite horizon MFGs.


## Stationary MFG setting.

In a nutshell, in the stationary MFG setting, when using one of the iterative methods presented in Section 3, at each iteration the representative agent faces a stationary MDP parameterized with a fixed distribution µ. We can thus use off-the-shelf RL methods.

To be more specific, we assume that a representative agent is encoded by a stationary policy π ∈ Π, either explicitly or implicitly (through a Q-function) and can interact with the environment in the following way: at each step, the agent Environment Agent Reward r n+1


## State

x n+1

Distribution µ n Action a n Reward r n State x n Figure 2: Environment for MFGs: Here, the current state of the MDP is the representative agent's state x n and the population distribution µ n , the action taken by the agent is a n , the new state is x n+1 ∼ p(·|x n , a n , µ n ) and the reward is r n = r(x n , a n , µ n ). The new state x n+1 is observed by the agent and is also used for the next step of the environment's evolution along with µ n .

observes its current state x, chooses action a ∼ π(·|x), and the environment returns a realization of x ∼ p(·|x, a, µ) and r(x, a, µ). Note that the agent does not need to observe directly the mean field flow µ, which is stored in the environment and simply enters as a parameter of the transition and reward functions p and r. In this stationary setting, in Fig. 2, µ n is constant equal to µ for all n. Based on such samples, the representative agent can implement any of the RL methods (e.g. the ones discussed in Section 4.1) for standard MDPs.

Notice that, at each new step of the iterative method described in (9), after the mean field update the environment needs to be updated with the new population distribution. That is to say, when the mean field state is µ ( ) , the agent uses the environment of Fig. 2 with µ n = µ ( ) for every n to learn a best-response or evaluate a policy. Here n is the index of the RL method iteration. Then, the new mean field µ ( +1) is computed. For the next iteration, the MDP is updated so the agent interacts with the environment of Fig. 2 but now with µ n = µ ( ) for every n.

For stationary MFG equilibria, Guo et al. (2019) introduced a best-response based iterative method with tabular Qlearning to compute the best response at each iteration. Moreover, they proved convergence using bounds on classical Q-learning, combined with a strict contraction argument. Guo et al. (2020a) generalized this idea notably using a policy gradient approach in lieu of Q-learning. A similar algorithm but combined with fitted Q-learning instead of tabular Q-learning was analyzed and proved to converge by Anahtarci et al. (2019). Furthermore, Anahtarci et al. (2020Anahtarci et al. ( , 2021 showed that some of the conditions to obtain convergence in the tabular Q-learning case can be relaxed if the MDP is regularized. Subramanian and Mahajan (2019); Angiuli et al. (2022b) used a two-timescale approach combined with model-free RL to compute stationary equilibria. The convergence has been proved under suitable conditions on the underlying ODEs by using stochastic approximation techniques (Borkar, 2009).

In the γ-discounted setting, Elie et al. (2020b) analysed the propagation of error in fictitious play (i.e., how errors made in the computation of the best response propagate through the algorithm) and implemented this scheme with an actor-critic DRL method (namely, DDPG (Lillicrap et al., 2016)) to compute the best response. Fictitious play and DRL combined with neural network approximation of the population distribution allowed Perrin et al. (2021b) to solve a flocking model with continuous and high-dimensional space. Still in the γ-discounted setting, Perrin et al. (2020) provided a convergence rate for continuous-time fictitious play under monotonicity assumption, while Geist et al. (2021) established a rate of convergence for discrete-time fictitious play in MFGs with a potential structure.

Finite horizon MFG setting. To learn finite horizon MFG solutions in a model-free way, we assume that a representative agent is encoded by a time-dependent policy π = (π n ) n=0,...,N T −1 and can interact with the environment to realize episodes. Each episode is done in the following way: the environment picks x 0 ∼ m 0 and reveals it to the agent; then for n = 0, . . . , N T , the agent observes x n , chooses action a n ∼ π n (·|x n ), and the environment returns a realization of x n+1 ∼ p n (·|x n , a n , µ n ) as well as the value of r n (x n , a n , µ n ). Note that the agent does not need to observe directly the mean field flow (µ n ) n=0,...,N T , which simply enters as a parameter of the transition and reward functions p n and r n .

Based on such episodes, the agent can for example estimate a policy π by approximately computing the stateaction value function Q π,µ , or compute a best response by first approximating the optimal value function Q * ,µ . The value functions can be estimated by backward induction as in (17) and (16), replacing the expectation by empirical averages over Monte Carlo samples. Perrin et al. (2020) solved finite-horizon MFG by a fictitious play method in which the best responses are computing using tabular Q-learning. Mishra et al. (2020) proposed a combination of RL and backward induction to solve finite-horizon MFGs by approximating the policy starting from the terminal time. Cui and Koeppl (2021a) applied best-response based and policy-evaluation based methods combined with DRL techniques and studied numerically the impact of entropy regularization on the convergence of these methods. Although DRL methods offer many promises in terms of scalability, it is in general hard to average or sum non-linear function approximators such as neural networks. Laurière et al. (2022) proposed best-response based and policy-evaluation based methods (namely fictitious play and OMD) with DRL techniques in such ways that average or sum of neural networks can be approximated efficiently. This leads to scalable model-free methods for finite-horizon MFGs.


### Some remarks about the distribution

Observing the mean field. In the above presentation, we assume that the agent does not observe the distribution, or at least does not exploit this information to learn the equilibrium policy. Although this is the most common approach in the RL and MFGs literature, the question of learning population-dependent policies arises quite naturally since one could expect that agents learn how to react to the current distribution they observe. This is usual in MARL, see e.g. Yang et al. (2018b) who consider Q-functions depending on the actions of all the other players. In MFGs, we can expect that by learning a population-dependent policy, the agent will be able to generalize, i.e., to behave (approximately) optimally even for population configurations that have not been encountered during training.

Such policies take as input a distribution, which is a high-dimensional object. As a consequence, they are much more challenging to approximate than population-independent policies. Mishra et al. (2020) considered a populationdependent value function and proposed an approach based on solving a fixed point at each time step for every possible distribution. Implementing this approach (at least in its current form) seems feasible only for very small state space. Perrin et al. (2021a), introduced the concept of master policies, which are population-dependent policies allowing to recover an equilibrium policy for any observed population distribution. They can be approximately computed by a combination of Fictitious play, DRL, and a suitable randomization of the initial distribution.

The concept of a value function depending on the population distribution is connected to the so-called Master equation in MFGs. Introduced by Lions (2012) in continuous MFGs (continuous time, continuous state and action spaces), this partial differential equation (PDE) corresponds to the limit of systems of Hamilton-Jacobi-Bellman PDEs characterizing Nash equilibria in symmetric N -player games. We refer the interested reader to e.g. Bensoussan et al. (2015); Cardaliaguet et al. (2019) for more details on this topic.

Distribution estimation. When the state space is finite but very large, storing the population distribution in a tabular way for every state and computing the evolution of this distribution in an exact way is prohibitive in terms of memory and computational time. Representing and updating the distribution is even more challenging in the continuous space setting, even if it is just for the purpose of implementing the RL environment. In this case, one needs to rely on approximations. As already mentioned above, a possible method consists in using an empirical distribution, whose evolution can be implemented by Monte Carlo samples of an interacting agent system. This amounts to using a finite population of agents to simulate the environment. For example, in linear-quadratic MFGs the interactions are only through the mean, which can be estimated even using a single agent, see Angiuli et al. (2022b) in the stationary setting and (Angiuli et al., 2021;uz Zaman et al., 2020;Miehling et al., 2022) in the finite-horizon setting. However, it should be noted that even if a finite number of agents is used in the environment, this approach does not directly reduce the problem to a MARL problem because the goal is still to learn the equilibrium policy for the MFG instead of the finite-agent equilibrium policy. Another approach consists in representing efficiently the distribution using function approximation. This raises the questions of the choice of parameterization and of the training method for the parameters. This approach can be implemented in a model-free way using Monte Carlo samples, which is particularly suitable for spaces that are too large to be explored in an exhaustive fashion. For example, Perrin et al. (2021b) used a kind of deep neural networks, namely normalizing flows (Rezende and Mohamed, 2015;Papamakarios et al., 2021), to represent the distribution of agents in a flocking model.


## Reinforcement learning for Mean Field Control and MFMDP

As discussed in Section 3.5, the problem of maximizing the social reward can be interpreted in the MDP framework through a mean-field MDP in which the state incorporates the whole mean field state. Adapting in a straightforward way the RL framework represented in Fig. 1, we can consider the environment described in Fig. 3 where the state is µ ∈ X = ∆ X instead of x, and the reward and the transition are given respectively byr andp. An action, taken by the central planner or collectively by the population, is an element of A = (∆ A ) X .

The problem can be interpreted as a situation in which all the agents in the population cooperate to learn a socially optimal behavior. Alternatively, we can adopt the point of view of a central planner trying to find an optimal policy that leads to a social optimum if it is followed by all the agents. In both cases, we assume here that the agent who is learning observes the whole population distribution (which is sometimes referred to as the centralized setting). The value function and the policy can thus depend on the state of the mean field, which is consistent with the dynamic programming equations presented in Section 3.5.

From here, standard RL techniques can be adapted to solve an MFMDP. In their implementation, the main challenge is the representation of the population distribution. A few noticeable cases are the following:

• In continuous space linear-quadratic case, the interaction is only through the mean so we do not need to give the full distribution as an input to the policy but only its first moment. In this case, policy gradient for the parameters of a suitable representation of the policy can be implemented and shown to converge, (Carmona et al., 2019b;Gu et al., 2020Gu et al., , 2021b. This approach can also be extended to more complex settings such as mean-field type games .

• When the state space X is finite, the mean field state can be represented as an element of the simplex, identified as a subset of R |X | : {µ ∈ [0, 1] |X | :
|X | i=1 µ i = 1}
. We can then use two different approaches.

-First, this simplex can be discretized and replaced by a finite setX ⊂ X . We can then approximate the MFMDP by an MDP with this finite state space. The action space is in principle ∆X , which is continuous, but if we are also willing to discretize this space, then we obtain a finite state space, finite action space MDP for which tabular RL methods can be used. For example tabular Q-learning can be shown to converge under suitable conditions (Carmona et al., 2019c;Gu et al., 2020). -Alternatively, the original MFMDP can be tackled without space discretization by using RL techniques for continuous space MDPs. For example, Carmona et al. (2019c) used deep RL to learn optimal policies as functions of the population distribution viewed as an element of {µ ∈ [0, 1] |X | :
|X | i=1 µ i = 1}.
It can be argued that the environment described in Fig. 3 is not very realistic because in general, we cannot assume that an agent observes the mean field distribution. Indeed, this distribution corresponds to the regime with an infinite number of agents while in practice, the number of agents is always finite. One can thus replace the "ideal" McKean-Vlasov environment by a more realistic finite-population environment. The former can be viewed as an approximation of the latter. The quality of the approximation gets better as the number of agents N in the environment increases. These two types of environments are discussed e.g. by Carmona et al. (2019b), in which the finite-population environment analysis benefits from the analysis of the McKean-Vlasov environment. The connection between RL for MFC and finite-agent problems has also been analyzed by Wang et al. (2020); Chen et al. (2021a); Li et al. (2021). Lastly, as in the MFG setting, for some MFC problems, it has been shows that observing the state of a single agent is sufficient to approximate the mean field distribution and learn the optimal behavior, see Angiuli et al. (2022bAngiuli et al. ( , 2021.


# Numerical experiments

We now present numerical experiments to illustrate some of the techniques introduced in the previous sections. We first discuss metrics that can be used to assess convergence. We then present an MFG model in which the agents are encouraged to explore the spatial domain. Last, we present numerical results obtained using iterative methods.


## Metrics

Here we discuss ways to measure convergence of the iterative methods discussed in Section 3. First, since many methods are based on fixed point iterations, we can measure distances between mean field terms or policies. Second, we can also measure convergence in terms of the exploitability of the current policy.


### Wasserstein distance

Let us recall that the iterative methods described previously are based on the scheme described in (9). The pair (µ , π ) computed at iteration is expected to converge to a fixed point. We can thus use the distance between µ and µ +1 , and the distance between π and π +1 to see whether the method has converged. Since both the mean field and the policy are distributions (respectively on the state space and the action space), we can use for instance the Wasserstein distance.

Let us focus on the mean field and look at the macroscopic behavior, at the scale of the whole population. We can proceed similarly with the policy. For simplicity, let us assume the state space X is a finite set endowed with a distance denoted by d. The Wasserstein distance W (or earth mover's distance) measures the minimum cost of turning one distribution into another and is defined as follows: for µ, µ ∈ ∆ X ,
W(µ, µ ) = inf ν∈Γ(µ,µ ) (x,x )∈X ×X d(x, x )ν(x, x ),
where Γ(µ, µ ) is the set of probability distributions on X × X with marginals µ and µ . In a finite-horizon setting, the mean field term is a sequence of distributions, so we average the distances over the N T + 1 time steps to get the following distance between mean field flows: for µ, µ ∈ ∆ N T X ,
W T (µ, µ ) = 1 N T + 1 N T n=0 W(µ n , µ n ).
This distance can be used in two ways to assess convergence in the context of the iterative scheme (9). First, in some cases the Nash equilibrium distribution (or an approximation of the equilibrium)μ is known, so we can use W(µ ,μ) to assess convergence. The MFG solution is typically unknown but in a few cases it admits an analytical solution, which can be convenient to check if a new numerical method works properly. Second, we can always measure the distance between two successive iterates, namely, W(µ , µ +1 ). Although there is in general no guarantee that this distance should decrease monotonically, it goes to zero if the method converges to a fixed point. Similar ideas can be used for policies.

If W(µ , µ +1 ) or W(π , π +1 ) provide some information about the scale of the changes occurring between two iterations, these quantities do not directly say how close the pair (µ , π ) is to a Nash equilibrium. One way to tackle this question is to measure the exploitability.


### Exploitability

Instead of focusing directly on the quantities that are updated in the iterative procedure, namely the mean field and the policy, another way to assess the convergence of learning algorithms is to consider the reward function. Indeed, the definition of an approximate Nash equilibrium can be formalized by measuring to what extent a representative player can improve their reward by deviating from the policy used by the rest of the population.

In the stationary setting for instance (similar ideas can be used in the other settings), the exploitability of a policy π is defined as (Perrin et al., 2020) 
E(π) = sup π J statio (π ; µ π ) − J statio (π; µ π ),
where µ π = M statio (π) is the stationary mean field distribution induced by π as defined in (5), and J statio is defined in (3). This notion is inspired by analogous concepts introduced in the context of computational game theory (Zinkevich et al., 2007;Lanctot et al., 2009).

Using this notion, we can rephrase the definition of mean field Nash equilibrium (see Definition 2) as:π is a stationary MFNE policy if: E(π) = 0.

Furthermore, E is always non-negative, and for > 0, E(π) ≤ corresponds to saying that the policy π is an -stationary MFNE, meaning that a representative player can improve her reward by at most by unilaterally deviating from the policy π used by the rest of the population. As such, the exploitability offers a different perspective than the Wasserstein distance discussed above to assess the convergence of learning methods. In the context of iterations described by (9), the quantity E(π ) measures convergence from the point of view of the potential reward improvement by deviating from π . In practice, the exploitability of a policy π can be computed only if we can compute sup π J statio (π ; µ π ). For many problems, there is not explicit formula for this value and if the environment is complex, exact methods cannot be used. However an approximation can be computed by learning an approximate best response to µ π , for example using RL. If this step is too computationally expensive, then we can replace the supremum by a maximum over a finite set, say Π. For example, we can take the set of policies computed in previous iterations. We then obtain a notion of approximate exploitability (Perrin et al., 2021b,a): E(π) = max π ∈ Π J statio (π ; µ π ) − J statio (π; µ π ).

We conclude this section by mentioning that in the case of MFC, the convergence can be assessed through the social cost, see Section 2.7.


## Experiments

In this section, we present a canonical example and we compare how the algorithms perform. The game and the algorithms are implemented in OpenSpiel (Lanctot et al., 2019) and are publicly available, along with more examples and algorithms. OpenSpiel is a framework for many games besides MFGs, and it contains many reinforcement learning algorithms besides exact algorithms. See https://github.com/deepmind/open_spiel/.

Canonical example: Exploration via entropy maximization. We consider the following model, where the state space is a two dimensional grid world as in Figure 4. At each time step, the representative agent stay still or move by one step in the four directions provided there are no obstacles:

x n+1 = x n + a n + n with a n ∈ {(−1, ), (0, −1), (0, 0), (0, 1), (1, 0)} and such that x n+1 belongs to the admissible states. Here n+1 is a perturbation which pushes the agents to a neighbor state with a small probability. In particular, in this simple model, the transitions probabilities do not depend on the mean field state. we define the reward as:

r(x, a, µ) = r(x, a, µ(x)) = − log(µ(x))

The goal for the representative agent is thus to avoid the crowd because the reward decreases as the density increases. Overall, we expect the population to spread as much as possible. In fact, at the macroscopic level of the population, the average one-step reward if the population distribution is µ is:
E x∼µ [− log(µ(x)] = − x µ(x) log(µ(x)),
which is the entropy of µ. So maximizing the average reward amounts to maximizing the entropy. This model has been introduced and studied by Geist et al. (2021), in which it is shown that it relates to an MFC problem.

For the numerical tests below, we assume that the initial distribution µ 0 is concentrated in the top-left corner as in Figure 4 (left). Since the reward is maximal when the distribution is uniform over the domain, one can wonder whether a uniform policy provides an approximately optimal solution. However, this is not the case, see Figure 4 (right), where we see that the distribution diffuses but remains mostly concentrated near the starting point. So learning a policy which induces a uniform distribution is not trivial. 


## Numerical experiments.

For the sake of illustration, we now focus on the evolutive setting and compare the algorithms on the canonical example. We focus here on the exact iterative methods as described in Section 3, i.e. without RL approximations. In Figure 5, we use the notion of exploitability to check the performance of the following algorithms, which are based on the iterations described in (9):

• Fixed point: π +1 is a best response against µ and µ +1 is the mean field sequence induced by π +1 . We see that this method does not converge and the population concentrates on only a few states.

• Fictitious play: As described in (21), we update the mean field by averaging over past iterations. This method converges since the exploitability goes towards 0. Furthermore, the final distribution is close to uniform.

• Online Mirror Descent: As described in (22), the policy is updated by first computing a cumulative Qfunction and then taking a softmax. This method converges even faster than Fictitious play, possibly because the size of each update in Fictitious play decreases with the iteration index whereas this is not the case for OMD. Accordingly, for the same number of iterations, the distribution is even closer to being uniform than with Fictitious play.

• Damped Fixed Point: As described in (20), the mean field term is updated by taking the average of the previous mean field and the mean field induced by the most recent policy, with constant weights for the average. We see that the exploitability decreases but does not seem to converge, and the induced terminal distribution is not very close to uniform.

• Softmax fixed point: This method is like the fixed point iterations except that the policy is a softmax of the Q-function instead of being an argmax as in the pure best response case. We see that the method does not converge, even though the exploitability is a bit lower than in the pure fixed point method case. The induced terminal distribution is concentrated in one of the four rooms.

• Softmax fictitious play: This method is like the fictitious play iterations except that the policy is a softmax of the Q-function instead of being an argmax as in the pure fictitious play case. In this method, the exploitability goes down more quickly than the pure fictitious play case, as quickly as in the OMD case. However, it does not go towards 0. Instead, it remains roughly constant after a number of iterations. This is probably due to the regularization of the policy which prevents convergence towards the true Nash equilibrium policy. The induced terminal distribution is quite close to uniform but we see that the agents tend to avoid the walls, which is probably due to the extra regularization of the policy.

• Boltzmann policy iteration: This method corresponds to the policy iteration method described in (19) except that the policy is a softmax of the Q-function instead of being an argmax as in the pure fictitious play case. This method does not converge as the exploitability increases to a very high level. The induced terminal distribution is concentrated in one small part of the domain.

Based on these observations, we see that a few methods are failing to converge even when using exact updates. As discussed in Section 3.4, this is probably due to the lack of contraction property for the operator on which the iterations rely.

For other methods, it is worth investigating how they perform when combined with RL as described in Section 4. The model of exploration with four rooms considered above as well as a few other examples have been treated using DRL in Laurière et al. (2022), where it is observed that a DRL versions of pure fixed point iterations still fail to converge, while a DRL version of OMD still tends to perform better than a DRL version of fictitious play. This tends to show that testing methods with exact methods before implementing DRL versions can be helpful to compare the performance of learning methods. 


# Conclusion and perspectives

In this work, we have surveyed some of the main recent developments related to the question of learning MFGs and MFC solutions. We first clarified the definitions of several classical settings that have appeared in the literature. As far as we know, it is the first time that these settings are summarized and discussed in comparison with each other. Second, we proposed an overview of iterative methods to learn MFG and MFC solutions by updating the mean field and the policy. Starting from simple fixed-point iterations, we explained how these procedures can be enhanced by incorporating various smoothing methods. Along the way, we clarified the link with the framework of MDPs. Third, building on this connection with MDPs, we presented RL and deep RL methods for MFGs and MFC. Finally, we provided some numerical results on a simple benchmark problem.

Several aspects were not discussed in detail here, such as:

• games with multiple groups or sub-populations (Subramanian et al., 2018(Subramanian et al., , 2020aPerolat et al., 2021;Carmona et al., 2020;Cui and Koeppl, 2021b;Angiuli et al., 2022a;Mondal et al., 2022),

• static models and bandit problems (Gummadi et al., 2013;Iyer et al., 2014;Maghsudi and Hossain, 2017;Wang and Jia, 2021),

• games with strategic complementarities (Adlakha and Johari, 2013;Lee et al., 2020Lee et al., , 2021a),

• problems with more complex structures such as partially observable problems (Subramanian et al., 2020b), models with leader-follower or major-minor structures (Ghosh and Aggarwal, 2020), and so on,

• other types of equilibria (Muller et al., 2021;Wang et al., 2022),

• other forms of RL techniques such as model-based RL (Pasztor et al., 2021) or inverse reinforcement learning (Yang et al., 2018a;Chen et al., 2021bChen et al., , 2022.

Apart from the points covered in this survey, many directions remain to be investigated. For example, the question of approximation of the distribution has received relatively less interest than the question of approximating the policy and the value function. Efficiently representing and learning the distribution is important for mean field problems, particularly for large and complex environments for which exact tabular representations are not suitable.

Furthermore, the literature is quickly expanding in various directions and the numerical results are not always easy to compare. We hope that this survey will contribute to harmonizing the research on this topic although many aspects remain to be unified. Along these lines, having common benchmark problems and a common framework seem important. Recently, MFGs have been incorporated to the OpenSpiel library (Lanctot et al., 2019). 1 Last but not least, one of the main motivations to use RL methods for MFGs is to be able to compute Nash equilibria at a large scale. We thus hope that the methods presented here and their extensions will find concrete applications in the near future.


(2017); François-Lavet et al. (2018).

## Figure 3 :
3Environment for MFC and MFMDP.

## Figure 4 :
4Reading order: (a) the considered environment initial state in yellow, walls in white); (b) the log-density of a uniform policy (to illustrate entropy maximization).

## Figure 5 :
5Entropy maximization. From left to right: Terminal distribution induced by (a) Fixed Point; (b) Fictitious Play; (c) OMD; (d) Damped Fixed Point; (e) Softmax Fixed Point; (f) Softmax Fictitious Play; (g) Boltzmann Policy Iteration; (h) Exploitability curves for these methods.
https://github.com/deepmind/open_spiel
AcknowledgementsThe authors would like to thank their collaborators on the topic of this survey for fruitful discussions and collaborations, but also for contributions developing and maintaining the OpenSpiel code used in this survey, and in particular: Andrea Angiuli, Olivier Bachem, Theophile Cabannes, René Carmona, Gökçe Dayanikli, Romuald Élie, Jean-Pierre Fouque, Maximilien Germain, Sertan Girgin, Kenza Hamidouche, Ruimeng Hu, Ayush Jain, Marc Lanctot, Edward  Lockhart, Raphael Marinier, Paul Muller, Rémi Munos, Julien Pérolat, Huyên Pham, Georgios Piliouras, Mark Rowland, Zongjun Tan, Karl Tuyls.
Mean field games models of segregation. Y Achdou, M Bardi, M Cirant, Mathematical Models and Methods in Applied Sciences. 0127Achdou, Y., Bardi, M., and Cirant, M. (2017a). Mean field games models of segregation. Mathematical Models and Methods in Applied Sciences, 27(01).

PDE models in macroeconomics. Y Achdou, F Buera, J.-M Lasry, P.-L Lions, Moll , B , Proceedings of the Royal Society of London. Series A. Mathematical and Physical SciencesAchdou, Y., Buera, F., Lasry, J.-M., Lions, P.-L., and Moll, B. (2014). PDE models in macroeconomics. Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences.

Mean field games: numerical methods for the planning problem. Y Achdou, F Camilli, I Capuzzo-Dolcetta, SIAM Journal on Control and Optimization. 501Achdou, Y., Camilli, F., and Capuzzo-Dolcetta, I. (2012). Mean field games: numerical methods for the planning problem. SIAM Journal on Control and Optimization, 50(1).

Mean field games: numerical methods. Y Achdou, I Capuzzo-Dolcetta, SIAM Journal on Numerical Analysis. 348Achdou, Y. and Capuzzo-Dolcetta, I. (2010). Mean field games: numerical methods. SIAM Journal on Numerical Analysis, 48(3).

Y Achdou, P Cardaliaguet, F Delarue, A Porretta, F Santambrogio, Mean Field Games. Cetraro, ItalySpringer Nature2281Achdou, Y., Cardaliaguet, P., Delarue, F., Porretta, A., and Santambrogio, F. (2020). Mean Field Games: Cetraro, Italy 2019, volume 2281. Springer Nature.

A long-term mathematical model for mining industries. Y Achdou, P.-N Giraud, J.-M Lasry, P.-L Lions, Applied Mathematics & Optimization. 743Achdou, Y., Giraud, P.-N., Lasry, J.-M., and Lions, P.-L. (2016). A long-term mathematical model for mining indus- tries. Applied Mathematics & Optimization, 74(3).

Income and wealth distribution in macroeconomics: A continuous-time approach. Y Achdou, J Han, J.-M Lasry, P.-L Lions, Moll , B , National Bureau of Economic ResearchTechnical reportAchdou, Y., Han, J., Lasry, J.-M., Lions, P.-L., and Moll, B. (2017b). Income and wealth distribution in macroeco- nomics: A continuous-time approach. Technical report, National Bureau of Economic Research.

Mean field games for modeling crowd motion. Y Achdou, J.-M Lasry, Contributions to partial differential equations and applications. SpringerAchdou, Y. and Lasry, J.-M. (2019). Mean field games for modeling crowd motion. In Contributions to partial differential equations and applications, pages 17-42. Springer.

Mean field type control with congestion. Y Achdou, M Laurière, Applied Mathematics & Optimization. 373Achdou, Y. and Laurière, M. (2016). Mean field type control with congestion. Applied Mathematics & Optimization, 73(3).

Y Achdou, M Laurière, Mean field games and applications: Numerical aspects. Mean field games. Achdou, Y. and Laurière, M. (2020). Mean field games and applications: Numerical aspects. Mean field games, pages 249-307.

Mean field equilibrium in dynamic games with strategic complementarities. S Adlakha, R Johari, Operations Research61Adlakha, S. and Johari, R. (2013). Mean field equilibrium in dynamic games with strategic complementarities. Oper- ations Research, 61(4):971-989.

An extended mean field game for storage in smart grids. C Alasseur, I B Taher, A Matoussi, Journal of Optimization Theory and Applications. 1842Alasseur, C., Taher, I. B., and Matoussi, A. (2020). An extended mean field game for storage in smart grids. Journal of Optimization Theory and Applications, 184(2).

B Anahtarci, C D Kariksiz, N Saldi, arXiv:1912.13309Fitted Q-learning in mean-field games. arXiv preprintAnahtarci, B., Kariksiz, C. D., and Saldi, N. (2019). Fitted Q-learning in mean-field games. arXiv preprint arXiv:1912.13309.

Learning in discounted-cost and average-cost mean-field games. B Anahtarcı, C D Karıksız, N Saldi, arXiv:1912.13309arXiv preprintAnahtarcı, B., Karıksız, C. D., and Saldi, N. (2019). Learning in discounted-cost and average-cost mean-field games. arXiv preprint arXiv:1912.13309.

B Anahtarci, C D Kariksiz, N Saldi, arXiv:2003.12151Q-learning in regularized mean-field games. arXiv preprintAnahtarci, B., Kariksiz, C. D., and Saldi, N. (2020). Q-learning in regularized mean-field games. arXiv preprint arXiv:2003.12151.

Value iteration algorithm for mean-field games. B Anahtarcı, C D Karıksız, N Saldi, Systems & Control Letters. 143104744Anahtarcı, B., Karıksız, C. D., and Saldi, N. (2020). Value iteration algorithm for mean-field games. Systems & Control Letters, 143:104744.

Learning in discrete-time average-cost mean-field games. B Anahtarci, C D Kariksiz, N Saldi, 2021 60th IEEE Conference on Decision and Control (CDC). IEEEAnahtarci, B., Kariksiz, C. D., and Saldi, N. (2021). Learning in discrete-time average-cost mean-field games. In 2021 60th IEEE Conference on Decision and Control (CDC), pages 3048-3053. IEEE.

Reinforcement learning algorithm for mixed mean field control games. A Angiuli, N Detering, J.-P Fouque, Lin , J , arXiv:2205.02330arXiv preprintAngiuli, A., Detering, N., Fouque, J.-P., and Lin, J. (2022a). Reinforcement learning algorithm for mixed mean field control games. arXiv preprint arXiv:2205.02330.

Reinforcement learning for mean field games, with applications to economics. A Angiuli, J.-P Fouque, M Lauriere, arXiv:2106.13755Machine Learning And Data Sciences For Financial Markets. arXiv preprintTo appear inAngiuli, A., Fouque, J.-P., and Lauriere, M. (2021). Reinforcement learning for mean field games, with applica- tions to economics. To appear in Machine Learning And Data Sciences For Financial Markets (arXiv preprint arXiv:2106.13755).

Unified reinforcement Q-learning for mean field game and control problems. A Angiuli, J.-P Fouque, M Laurière, Mathematics of Control, Signals, and Systems. Angiuli, A., Fouque, J.-P., and Laurière, M. (2022b). Unified reinforcement Q-learning for mean field game and control problems. Mathematics of Control, Signals, and Systems, pages 1-55.

A Angiuli, C V Graves, H Li, J.-F Chassagneux, F Delarue, R Carmona, Cemracs 2017: numerical probabilistic approach to MFG. ESAIM: Proceedings and Surveys. 65Angiuli, A., Graves, C. V., Li, H., Chassagneux, J.-F., Delarue, F., and Carmona, R. (2019). Cemracs 2017: numerical probabilistic approach to MFG. ESAIM: Proceedings and Surveys, 65.

Thinking fast and slow with deep learning and tree search. T Anthony, Z Tian, D Barber, Proceedings of NeurIPS. NeurIPSAnthony, T., Tian, Z., and Barber, D. (2017). Thinking fast and slow with deep learning and tree search. In Proceedings of NeurIPS.

Deep reinforcement learning: A brief survey. K Arulkumaran, M P Deisenroth, M Brundage, A A Bharath, IEEE Signal Processing Magazine. 346Arulkumaran, K., Deisenroth, M. P., Brundage, M., and Bharath, A. A. (2017). Deep reinforcement learning: A brief survey. IEEE Signal Processing Magazine, 34(6):26-38.

Markets with a continuum of traders. R J Aumann, Econometrica: Journal of the Econometric Society. Aumann, R. J. (1964). Markets with a continuum of traders. Econometrica: Journal of the Econometric Society, pages 39-50.

Values of non-atomic games. R J Aumann, L S Shapley, Princeton University PressAumann, R. J. and Shapley, L. S. (2015). Values of non-atomic games. Princeton University Press.

Optimal incentives to mitigate epidemics: a stackelberg mean field game approach. A Aurell, R Carmona, G Dayanikli, M Lauriere, SIAM Journal on Control and Optimization. 602Aurell, A., Carmona, R., Dayanikli, G., and Lauriere, M. (2022). Optimal incentives to mitigate epidemics: a stackel- berg mean field game approach. SIAM Journal on Control and Optimization, 60(2):S294-S322.

Modeling tagged pedestrian motion: A mean-field type game approach. A Aurell, B Djehiche, Transportation Research Part B. 121MethodologicalAurell, A. and Djehiche, B. (2019). Modeling tagged pedestrian motion: A mean-field type game approach. Trans- portation Research Part B: Methodological, 121.

Mean-field games and dynamic demand management in power grids. F Bagagiolo, D Bauso, Dynamic Games and Applications. 42Bagagiolo, F. and Bauso, D. (2014). Mean-field games and dynamic demand management in power grids. Dynamic Games and Applications, 4(2).

Mean field Markov decision processes. N Bäuerle, arXiv:2106.08755arXiv preprintBäuerle, N. (2021). Mean field Markov decision processes. arXiv preprint arXiv:2106.08755.

Opinion dynamics in social networks through mean-field games. D Bauso, H Tembine, T Basar, SIAM Journal on Control and Optimization. 654Bauso, D., Tembine, H., and Basar, T. (2016a). Opinion dynamics in social networks through mean-field games. SIAM Journal on Control and Optimization, 54(6).

Density flow in dynamical networks via mean-field games. D Bauso, X Zhang, A Papachristodoulou, IEEE Transactions on Automatic Control. 623Bauso, D., Zhang, X., and Papachristodoulou, A. (2016b). Density flow in dynamical networks via mean-field games. IEEE Transactions on Automatic Control, 62(3):1342-1355.

The arcade learning environment: An evaluation platform for general agents. M G Bellemare, Y Naddaf, J Veness, M Bowling, Journal of Artificial Intelligence Research. 47Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. (2013). The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279.

A Markovian decision process. R Bellman, Journal of mathematics and mechanics. Bellman, R. (1957). A Markovian decision process. Journal of mathematics and mechanics, pages 679-684.

Mean field games and mean field type control theory. A Bensoussan, J Frehse, P Yam, Springer101Bensoussan, A., Frehse, J., Yam, P., et al. (2013). Mean field games and mean field type control theory, volume 101. Springer.

The master equation in mean field theory. A Bensoussan, J Frehse, S C P Yam, Journal de Mathématiques Pures et Appliquées. 1036Bensoussan, A., Frehse, J., and Yam, S. C. P. (2015). The master equation in mean field theory. Journal de Mathéma- tiques Pures et Appliquées, 103(6):1441-1474.

Mean field control and mean field game models with several populations. A Bensoussan, T Huang, M Laurière, arXiv:1810.00783arXiv preprintBensoussan, A., Huang, T., and Laurière, M. (2018). Mean field control and mean field game models with several populations. arXiv preprint arXiv:1810.00783.

Dynamic programming and optimal control. D Bertsekas, Athena scientific. 1Bertsekas, D. (2012). Dynamic programming and optimal control, volume 1. Athena scientific.

Stochastic optimal control: the discrete-time case. D Bertsekas, S E Shreve, Athena Scientific. 5Bertsekas, D. and Shreve, S. E. (1996). Stochastic optimal control: the discrete-time case, volume 5. Athena Scientific.

J F Bonnans, P Lavigne, L Pfeiffer, arXiv:2109.05785Generalized conditional gradient and learning in potential mean field games. arXiv preprintBonnans, J. F., Lavigne, P., and Pfeiffer, L. (2021). Generalized conditional gradient and learning in potential mean field games. arXiv preprint arXiv:2109.05785.

Stochastic approximation: a dynamical systems viewpoint. V S Borkar, Springer48Borkar, V. S. (2009). Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer.

Heads-up limit hold'em poker is solved. M Bowling, N Burch, M Johanson, O Tammelin, Science. 6218347Bowling, M., Burch, N., Johanson, M., and Tammelin, O. (2015). Heads-up limit hold'em poker is solved. Science, 347(6218).

On the implementation of a primal-dual algorithm for second order time-dependent mean field games with local couplings. L M Briceño Arias, D Kalise, Z Kobeissi, M Laurière, A Mateos González, F J Silva, ESAIM: Proceedings. 65Briceño Arias, L. M., Kalise, D., Kobeissi, Z., Laurière, M., Mateos González, A., and Silva, F. J. (2019). On the implementation of a primal-dual algorithm for second order time-dependent mean field games with local couplings. ESAIM: Proceedings, 65.

Proximal methods for stationary mean field games with local couplings. L M Briceño Arias, D Kalise, F J Silva, SIAM Journal on Control and Optimization. 562Briceño Arias, L. M., Kalise, D., and Silva, F. J. (2018). Proximal methods for stationary mean field games with local couplings. SIAM Journal on Control and Optimization, 56(2).

Iterative solution of games by fictitious play. Activity analysis of production and allocation. G W Brown, 13Brown, G. W. (1951). Iterative solution of games by fictitious play. Activity analysis of production and allocation, 13(1):374-376.

Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. N Brown, T Sandholm, Science. 6385360Brown, N. and Sandholm, T. (2017). Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Science, 360(6385).

Superhuman AI for multiplayer poker. N Brown, T Sandholm, Science. 6456365Brown, N. and Sandholm, T. (2019). Superhuman AI for multiplayer poker. Science, 365(6456).

Mean field games with nonlinear mobilities in pedestrian dynamics. M Burger, M Francesco, P Markowich, Wolfram , M.-T , Discrete and Continuous Dynamical Systems -Series B. 19Burger, M., Francesco, M., Markowich, P., and Wolfram, M.-T. (2013). Mean field games with nonlinear mobilities in pedestrian dynamics. Discrete and Continuous Dynamical Systems -Series B, 19.

Solving n-player dynamic routing games with congestion: a mean field approach. T Cabannes, M Lauriere, J Perolat, R Marinier, S Girgin, S Perrin, O Pietquin, A M Bayen, E Goubault, Elie , R , arXiv:2110.11943Extended abstract at AAMAS 2022 (long version: arXiv preprintCabannes, T., Lauriere, M., Perolat, J., Marinier, R., Girgin, S., Perrin, S., Pietquin, O., Bayen, A. M., Goubault, E., and Elie, R. (2021). Solving n-player dynamic routing games with congestion: a mean field approach. Extended abstract at AAMAS 2022 (long version: arXiv preprint arXiv:2110.11943).

A policy iteration method for mean field games. Simone Cacace, Fabio Camilli, Alessandro Goffi, ESAIM: COCV. 2785Cacace, Simone, Camilli, Fabio, and Goffi, Alessandro (2021). A policy iteration method for mean field games. ESAIM: COCV, 27:85.

Rates of convergence for the policy iteration method for mean field games systems. F Camilli, Q Tang, Journal of Mathematical Analysis and Applications. 5121126138Camilli, F. and Tang, Q. (2022). Rates of convergence for the policy iteration method for mean field games systems. Journal of Mathematical Analysis and Applications, 512(1):126138.

Deep blue. M Campbell, A J HoaneJr, F Hsu, Artificial intelligence. 1341-2Campbell, M., Hoane Jr, A. J., and Hsu, F.-h. (2002). Deep blue. Artificial intelligence, 134(1-2):57-83.

H Cao, X Guo, M Laurière, arXiv:2002.04112Connecting GANs, MFGs, and OT. arXiv preprintCao, H., Guo, X., and Laurière, M. (2020). Connecting GANs, MFGs, and OT. arXiv preprint arXiv:2002.04112.

The master equation and the convergence problem in mean field games. P Cardaliaguet, F Delarue, J.-M Lasry, P L Lions, 381201Cardaliaguet, P., Delarue, F., Lasry, J.-M., and Lions, P. L. (2019). The master equation and the convergence problem in mean field games, volume 381 of AMS-201.

Learning in mean field games: the fictitious play. P Cardaliaguet, S Hadikhanloo, ESAIM Cont. Optim. Calc. VarCardaliaguet, P. and Hadikhanloo, S. (2017). Learning in mean field games: the fictitious play. ESAIM Cont. Optim. Calc. Var.

Mean field game of controls and an application to trade crowding. P Cardaliaguet, C.-A Lehalle, Mathematics and Financial Economics. 123Cardaliaguet, P. and Lehalle, C.-A. (2018). Mean field game of controls and an application to trade crowding. Mathe- matics and Financial Economics, 12(3):335-363.

A segregation problem in multi-population mean field games. P Cardaliaguet, A Porretta, D Tonon, International Symposium on Dynamic Games and Applications. SpringerCardaliaguet, P., Porretta, A., and Tonon, D. (2016). A segregation problem in multi-population mean field games. In International Symposium on Dynamic Games and Applications. Springer.

A fully discrete semi-Lagrangian scheme for a first order mean field game problem. E Carlini, F J Silva, SIAM Journal on Numerical Analysis. 521Carlini, E. and Silva, F. J. (2014). A fully discrete semi-Lagrangian scheme for a first order mean field game problem. SIAM Journal on Numerical Analysis, 52(1).

A semi-Lagrangian scheme for a degenerate second order mean field game system. E Carlini, F J Silva, Discrete and Continuous Dynamical Systems. 35Carlini, E. and Silva, F. J. (2015). A semi-Lagrangian scheme for a degenerate second order mean field game system. Discrete and Continuous Dynamical Systems, 35(9).

R Carmona, arXiv:2012.05237Applications of mean field games in financial engineering and economic theory. arXiv preprintCarmona, R. (2020). Applications of mean field games in financial engineering and economic theory. arXiv preprint arXiv:2012.05237.

Mean field forward-backward stochastic differential equations. R Carmona, F Delarue, Electronic Communications in Probability. 18Carmona, R. and Delarue, F. (2013). Mean field forward-backward stochastic differential equations. Electronic Communications in Probability, 18:1-15.

Probabilistic Theory of Mean Field Games with Applications I. R Carmona, F Delarue, SpringerCarmona, R. and Delarue, F. (2018a). Probabilistic Theory of Mean Field Games with Applications I. Springer.

R Carmona, F Delarue, Probabilistic Theory of Mean Field Games with Applications II. SpringerCarmona, R. and Delarue, F. (2018b). Probabilistic Theory of Mean Field Games with Applications II. Springer.

Mean field games with common noise. The Annals of Probability. R Carmona, F Delarue, D Lacker, 44Carmona, R., Delarue, F., and Lacker, D. (2016). Mean field games with common noise. The Annals of Probability, 44(6):3740-3803.

Price of anarchy for mean field games. R Carmona, C V Graves, Z Tan, ESAIM: Proceedings and Surveys. 65Carmona, R., Graves, C. V., and Tan, Z. (2019a). Price of anarchy for mean field games. ESAIM: Proceedings and Surveys, 65:349-383.

Policy optimization for linear-quadratic zero-sum mean-field type games. R Carmona, K Hamidouche, M Laurière, Z Tan, 2020 59th IEEE Conference on Decision and Control (CDC). IEEECarmona, R., Hamidouche, K., Laurière, M., and Tan, Z. (2020). Policy optimization for linear-quadratic zero-sum mean-field type games. In 2020 59th IEEE Conference on Decision and Control (CDC), pages 1038-1043. IEEE.

Linear-quadratic zero-sum mean-field type games: Optimality conditions and policy optimization. R Carmona, K Hamidouche, M Laurière, Z Tan, Journal of Dynamics & Games. 84403Carmona, R., Hamidouche, K., Laurière, M., and Tan, Z. (2021). Linear-quadratic zero-sum mean-field type games: Optimality conditions and policy optimization. Journal of Dynamics & Games, 8(4):403.

Convergence analysis of machine learning algorithms for the numerical solution of mean field control and games: II -the finite horizon case. R Carmona, M Laurière, arXiv:1908.01613Annals of Applied Probability. To appear in. preprintCarmona, R. and Laurière, M. (2019). Convergence analysis of machine learning algorithms for the numerical solution of mean field control and games: II -the finite horizon case. To appear in Annals of Applied Probability (preprint arXiv:1908.01613).

Convergence analysis of machine learning algorithms for the numerical solution of mean field control and games i: the ergodic case. R Carmona, M Laurière, SIAM Journal on Numerical Analysis. 593Carmona, R. and Laurière, M. (2021). Convergence analysis of machine learning algorithms for the numerical solution of mean field control and games i: the ergodic case. SIAM Journal on Numerical Analysis, 59(3):1455-1485.

Linear-quadratic mean-field reinforcement learning: convergence of policy gradient methods. R Carmona, M Laurière, Z Tan, arXiv:1910.04295arXiv preprintCarmona, R., Laurière, M., and Tan, Z. (2019b). Linear-quadratic mean-field reinforcement learning: convergence of policy gradient methods. arXiv preprint arXiv:1910.04295.

Model-free mean-field reinforcement learning: mean-field MDP and mean-field Q-learning. R Carmona, M Laurière, Z Tan, arXiv:1910.12802arXiv preprintCarmona, R., Laurière, M., and Tan, Z. (2019c). Model-free mean-field reinforcement learning: mean-field MDP and mean-field Q-learning. arXiv preprint arXiv:1910.12802.

Convergence, fluctuations and large deviations for finite state mean field games via the master equation. A Cecchin, G Pelino, Stochastic Processes and their Applications. 129Cecchin, A. and Pelino, G. (2019). Convergence, fluctuations and large deviations for finite state mean field games via the master equation. Stochastic Processes and their Applications, 129(11):4510-4555.

Bertrand and Cournot mean field games. P Chan, R Sircar, Applied Mathematics & Optimization. 371Chan, P. and Sircar, R. (2015). Bertrand and Cournot mean field games. Applied Mathematics & Optimization, 71(3).

Fracking, renewables, and mean field games. P Chan, R Sircar, SIAM Review. 359Chan, P. and Sircar, R. (2017). Fracking, renewables, and mean field games. SIAM Review, 59(3).

Numerical method for FBSDEs of McKean-Vlasov type. J.-F Chassagneux, D Crisan, F Delarue, The Annals of Applied Probability. 293Chassagneux, J.-F., Crisan, D., and Delarue, F. (2019). Numerical method for FBSDEs of McKean-Vlasov type. The Annals of Applied Probability, 29(3):1640-1684.

Pessimism meets invariance: Provably efficient offline mean-field multi-agent rl. M Chen, Y Li, E Wang, Z Yang, Z Wang, T Zhao, Advances in Neural Information Processing Systems. 34Chen, M., Li, Y., Wang, E., Yang, Z., Wang, Z., and Zhao, T. (2021a). Pessimism meets invariance: Provably efficient offline mean-field multi-agent rl. Advances in Neural Information Processing Systems, 34.

Agent-level maximum entropy inverse reinforcement learning for mean field games. Y Chen, J Liu, B Khoussainov, arXiv:2104.14654arXiv preprintChen, Y., Liu, J., and Khoussainov, B. (2021b). Agent-level maximum entropy inverse reinforcement learning for mean field games. arXiv preprint arXiv:2104.14654.

Individual-level inverse reinforcement learning for mean field games. Y Chen, L Zhang, J Liu, S Hu, arXiv:2202.06401arXiv preprintChen, Y., Zhang, L., Liu, J., and Hu, S. (2022). Individual-level inverse reinforcement learning for mean field games. arXiv preprint arXiv:2202.06401.

A micro-macro traffic model based on mean-field games. G Chevalier, J Le Ny, R Malhamé, American Control Conference (ACC). IEEEChevalier, G., Le Ny, J., and Malhamé, R. (2015). A micro-macro traffic model based on mean-field games. In American Control Conference (ACC). IEEE.

Multi-population mean field games systems with neumann boundary conditions. M Cirant, Journal de Mathématiques Pures et Appliquées. 1035Cirant, M. (2015). Multi-population mean field games systems with neumann boundary conditions. Journal de Mathématiques Pures et Appliquées, 103(5):1294-1315.

Electrical vehicles in the smart grid: A mean field game analysis. R Couillet, S M Perlaza, H Tembine, M Debbah, IEEE Journal on Selected Areas in Communications. 630Couillet, R., Perlaza, S. M., Tembine, H., and Debbah, M. (2012). Electrical vehicles in the smart grid: A mean field game analysis. IEEE Journal on Selected Areas in Communications, 30(6).

Approximately solving mean field games via entropy-regularized deep reinforcement learning. K Cui, H Koeppl, PMLRProceedings of The 24th International Conference on Artificial Intelligence and Statistics. Banerjee, A. and Fukumizu, K.The 24th International Conference on Artificial Intelligence and Statistics130Cui, K. and Koeppl, H. (2021a). Approximately solving mean field games via entropy-regularized deep reinforcement learning. In Banerjee, A. and Fukumizu, K., editors, Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pages 1909-1917. PMLR.

Learning graphon mean field games and approximate Nash equilibria. K Cui, H Koeppl, arXiv:2112.01280arXiv preprintCui, K. and Koeppl, H. (2021b). Learning graphon mean field games and approximate Nash equilibria. arXiv preprint arXiv:2112.01280.

Exploration noise for learning linear-quadratic mean field games. F Delarue, A Vasileiadis, arXiv:2107.00839arXiv preprintDelarue, F. and Vasileiadis, A. (2021). Exploration noise for learning linear-quadratic mean field games. arXiv preprint arXiv:2107.00839.

A mean-field game of evacuation in multilevel building. B Djehiche, A Tcheukam, H Tembine, IEEE Transactions on Automatic Control. 1062Djehiche, B., Tcheukam, A., and Tembine, H. (2017a). A mean-field game of evacuation in multilevel building. IEEE Transactions on Automatic Control, 62(10).

Mean-field-type games in engineering. B Djehiche, A Tcheukam Siwe, H Tembine, AIMS Electronics and Electrical Engineering. 1Djehiche, B., Tcheukam Siwe, A., and Tembine, H. (2017b). Mean-field-type games in engineering. AIMS Electronics and Electrical Engineering, 1.

A mean field game analysis of SIR dynamics with vaccination. J Doncel, N Gast, B Gaujal, Probability in the Engineering and Informational Sciences. 362Doncel, J., Gast, N., and Gaujal, B. (2022). A mean field game analysis of SIR dynamics with vaccination. Probability in the Engineering and Informational Sciences, 36(2):482-499.

The exact law of large numbers for independent random matching. D Duffie, Y Sun, Journal of Economic Theory. 1473Duffie, D. and Sun, Y. (2012). The exact law of large numbers for independent random matching. Journal of Economic Theory, 147(3):1105-1139.

Mean-field moral hazard for optimal energy demand response management. R Elie, E Hubert, T Mastrolia, D Possamaï, Mathematical Finance. to appearElie, R., Hubert, E., Mastrolia, T., and Possamaï, D. (2019a). Mean-field moral hazard for optimal energy demand response management. Mathematical Finance (to appear).

Contact rate epidemic control of COVID-19: an equilibrium view. R Elie, E Hubert, G Turinici, Mathematical Modelling of Natural Phenomena. Elie, R., Hubert, E., and Turinici, G. (2020a). Contact rate epidemic control of COVID-19: an equilibrium view. Mathematical Modelling of Natural Phenomena.

A tale of a principal and many, many agents. R Elie, T Mastrolia, D Possamaï, Mathematics of Operations Research. 442Elie, R., Mastrolia, T., and Possamaï, D. (2019b). A tale of a principal and many, many agents. Mathematics of Operations Research, 44(2).

On the convergence of model free learning in mean field games. R Elie, J Perolat, M Laurière, M Geist, O Pietquin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Elie, R., Perolat, J., Laurière, M., Geist, M., and Pietquin, O. (2020b). On the convergence of model free learning in mean field games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7143-7150.

The derivation of ergodic mean field game equations for several populations of players. E Feleqi, Dynamic Games and Applications. 34Feleqi, E. (2013). The derivation of ergodic mean field game equations for several populations of players. Dynamic Games and Applications, 3(4):523-536.

Exploratory LQG mean field games with entropy regularization. D Firoozi, S Jaimungal, Automatica. 139110177Firoozi, D. and Jaimungal, S. (2022). Exploratory LQG mean field games with entropy regularization. Automatica, 139:110177.

Deep learning methods for mean field control problems with delay. J.-P Fouque, Z Zhang, Frontiers in Applied Mathematics and Statistics. 6Fouque, J.-P. and Zhang, Z. (2020). Deep learning methods for mean field control problems with delay. Frontiers in Applied Mathematics and Statistics, 6.

An introduction to deep reinforcement learning. Foundations and Trends® in Machine Learning. V François-Lavet, P Henderson, R Islam, M G Bellemare, J Pineau, 11François-Lavet, V., Henderson, P., Islam, R., Bellemare, M. G., and Pineau, J. (2018). An introduction to deep reinforcement learning. Foundations and Trends® in Machine Learning, 11(3-4):219-354.

The theory of learning in games. D Fudenberg, D K Levine, MIT Press Books1Fudenberg, D., Levine, D. K., et al. (1998). The theory of learning in games. MIT Press Books, 1.

Game theory. D Fudenberg, J Tirole, MIT pressFudenberg, D. and Tirole, J. (1991). Game theory. MIT press.

Addressing function approximation error in actor-critic methods. S Fujimoto, H Hoof, D Meger, PMLRInternational Conference on Machine Learning. Fujimoto, S., Hoof, H., and Meger, D. (2018). Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning, pages 1587-1596. PMLR.

Mean field for Markov decision processes: from discrete to continuous optimization. N Gast, B Gaujal, Le Boudec, J.-Y , IEEE Transactions on Automatic Control. 579Gast, N., Gaujal, B., and Le Boudec, J.-Y. (2012). Mean field for Markov decision processes: from discrete to continuous optimization. IEEE Transactions on Automatic Control, 57(9):2266-2280.

M Geist, J Pérolat, M Laurière, R Elie, S Perrin, O Bachem, R Munos, O Pietquin, arXiv:2106.03787Concave utility reinforcement learning: the mean-field game viewpoint. AAMAS 2022. arXiv preprintGeist, M., Pérolat, J., Laurière, M., Elie, R., Perrin, S., Bachem, O., Munos, R., and Pietquin, O. (2021). Concave utility reinforcement learning: the mean-field game viewpoint. AAMAS 2022 (arXiv preprint arXiv:2106.03787).

Numerical resolution of McKean-Vlasov FBSDEs using neural networks. M Germain, J Mikael, X Warin, Methodology and Computing in Applied Probability. Germain, M., Mikael, J., and Warin, X. (2022). Numerical resolution of McKean-Vlasov FBSDEs using neural networks. Methodology and Computing in Applied Probability, pages 1-30.

Model free reinforcement learning algorithm for stationary mean field equilibrium for multiple types of agents. A Ghosh, V Aggarwal, arXiv:2012.15377arXiv preprintGhosh, A. and Aggarwal, V. (2020). Model free reinforcement learning algorithm for stationary mean field equilibrium for multiple types of agents. arXiv preprint arXiv:2012.15377.

Socio-economic applications of finite state mean field games. D Gomes, R M Velho, Wolfram , M.-T , Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 372Gomes, D., Velho, R. M., and Wolfram, M.-T. (2014a). Socio-economic applications of finite state mean field games. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 372(2028).

Discrete time, finite state space mean field games. D A Gomes, J Mohr, R R Souza, Journal de mathématiques pures et appliquées. 933Gomes, D. A., Mohr, J., and Souza, R. R. (2010). Discrete time, finite state space mean field games. Journal de mathématiques pures et appliquées, 93(3):308-328.

On the existence of classical solutions for stationary extended mean field games. D A Gomes, S Patrizi, V Voskanyan, Nonlinear Analysis: Theory, Methods & Applications. 99Gomes, D. A., Patrizi, S., and Voskanyan, V. (2014b). On the existence of classical solutions for stationary extended mean field games. Nonlinear Analysis: Theory, Methods & Applications, 99:49-79.

A mean-field game approach to price formation. Dynamic Games and Applications. D A Gomes, J Saúde, Gomes, D. A. and Saúde, J. (2020). A mean-field game approach to price formation. Dynamic Games and Applica- tions.

Extended deterministic mean-field games. D A Gomes, V K Voskanyan, SIAM Journal on Control and Optimization. 542Gomes, D. A. and Voskanyan, V. K. (2016). Extended deterministic mean-field games. SIAM Journal on Control and Optimization, 54(2):1030-1055.

Deep learning. I Goodfellow, Y Bengio, A Courville, MIT pressGoodfellow, I., Bengio, Y., and Courville, A. (2016). Deep learning. MIT press.

Generative adversarial nets. I J Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in Neural Information Processing Systems 27 (NIPS). Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial nets. In Advances in Neural Information Processing Systems 27 (NIPS).

Existence and uniqueness of solutions for Bertrand and Cournot mean field games. P J Graber, A Bensoussan, Applied Mathematics & Optimization. 177Graber, P. J. and Bensoussan, A. (2018). Existence and uniqueness of solutions for Bertrand and Cournot mean field games. Applied Mathematics & Optimization, 77(1).

A mean-field game model for homogeneous flocking. P Grover, K Bakshi, E A Theodorou, Chaos: An Interdisciplinary Journal of Nonlinear Science. 28661103Grover, P., Bakshi, K., and Theodorou, E. A. (2018). A mean-field game model for homogeneous flocking. Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(6):061103.

Dynamic programming principles for mean-field controls with learning. H Gu, X Guo, X Wei, R Xu, arXiv:1911.07314arXiv preprintGu, H., Guo, X., Wei, X., and Xu, R. (2019). Dynamic programming principles for mean-field controls with learning. arXiv preprint arXiv:1911.07314.

H Gu, X Guo, X Wei, R Xu, arXiv:2002.04131Q-learning for mean-field controls. arXiv preprintGu, H., Guo, X., Wei, X., and Xu, R. (2020). Q-learning for mean-field controls. arXiv preprint arXiv:2002.04131.

Mean-field controls with q-learning for cooperative marl: convergence and complexity analysis. H Gu, X Guo, X Wei, R Xu, SIAM Journal on Mathematics of Data Science. 34Gu, H., Guo, X., Wei, X., and Xu, R. (2021a). Mean-field controls with q-learning for cooperative marl: convergence and complexity analysis. SIAM Journal on Mathematics of Data Science, 3(4):1168-1196.

Mean-field multi-agent reinforcement learning: A decentralized network approach. H Gu, X Guo, X Wei, R Xu, arXiv:2108.02731arXiv preprintGu, H., Guo, X., Wei, X., and Xu, R. (2021b). Mean-field multi-agent reinforcement learning: A decentralized network approach. arXiv preprint arXiv:2108.02731.

Mean field games and applications. O Guéant, J.-M Lasry, P.-L Lions, Paris-Princeton lectures on mathematical finance 2010. SpringerGuéant, O., Lasry, J.-M., and Lions, P.-L. (2011). Mean field games and applications. In Paris-Princeton lectures on mathematical finance 2010, pages 205-266. Springer.

Mean field analysis of multi-armed bandit games. R Gummadi, R Johari, S Schmit, Yu , J Y , Available at SSRN 2045842Gummadi, R., Johari, R., Schmit, S., and Yu, J. Y. (2013). Mean field analysis of multi-armed bandit games. Available at SSRN 2045842.

Learning mean-field games. X Guo, A Hu, R Xu, J Zhang, Advances in Neural Information Processing Systems. 32Guo, X., Hu, A., Xu, R., and Zhang, J. (2019). Learning mean-field games. Advances in Neural Information Processing Systems, 32.

A general framework for learning mean-field games. X Guo, A Hu, R Xu, J Zhang, arXiv:2003.06069arXiv preprintGuo, X., Hu, A., Xu, R., and Zhang, J. (2020a). A general framework for learning mean-field games. arXiv preprint arXiv:2003.06069.

X Guo, R Xu, T Zariphopoulou, arXiv:2010.00145Entropy regularization for mean field games with learning. arXiv preprintGuo, X., Xu, R., and Zariphopoulou, T. (2020b). Entropy regularization for mean field games with learning. arXiv preprint arXiv:2010.00145.

Learning in anonymous nonatomic games with applications to first-order mean field games. S Hadikhanloo, arXiv:1704.00378arXiv preprintHadikhanloo, S. (2017). Learning in anonymous nonatomic games with applications to first-order mean field games. arXiv preprint arXiv:1704.00378.

Learning in mean field games. S Hadikhanloo, Université Paris sciences et lettresPhD thesisHadikhanloo, S. (2018). Learning in mean field games. PhD thesis, PhD thesis. Université Paris sciences et lettres.

Finite mean field games: fictitious play and convergence to a first order continuous mean field game. S Hadikhanloo, F J Silva, Journal de Mathématiques Pures et Appliquées. 132Hadikhanloo, S. and Silva, F. J. (2019). Finite mean field games: fictitious play and convergence to a first order continuous mean field game. Journal de Mathématiques Pures et Appliquées, 132:369-397.

Mean-field games for distributed caching in ultradense small cell networks. K Hamidouche, W Saad, M Debbah, H V Poor, American Control Conference (ACC). IEEEHamidouche, K., Saad, W., Debbah, M., and Poor, H. V. (2016). Mean-field games for distributed caching in ultra- dense small cell networks. In 2016 American Control Conference (ACC). IEEE.

Deep fictitious play for stochastic differential games. R Hu, Communications in mathematical sciences. 192Hu, R. (2021). Deep fictitious play for stochastic differential games. Communications in mathematical sciences, 19(2).

Recent developments in machine learning methods for stochastic control and games. R Hu, M Laurière, 4096569SSRN preprintHu, R. and Laurière, M. (2022). Recent developments in machine learning methods for stochastic control and games. SSRN preprint:4096569.

A game-theoretic framework for autonomous vehicles velocity control: Bridging microscopic differential games and macroscopic mean field games. K Huang, X Di, Q Du, Chen , X , Discrete & Continuous Dynamical Systems -B. 2211Huang, K., Di, X., Du, Q., and Chen, X. (2017). A game-theoretic framework for autonomous vehicles velocity control: Bridging microscopic differential games and macroscopic mean field games. Discrete & Continuous Dy- namical Systems -B, 22(11).

Stabilizing traffic via autonomous vehicles: A continuum mean field game approach. K Huang, X Di, Q Du, Chen , X , 2019 IEEE Intelligent Transportation Systems Conference (ITSC). IEEEHuang, K., Di, X., Du, Q., and Chen, X. (2019). Stabilizing traffic via autonomous vehicles: A continuum mean field game approach. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pages 3269-3274. IEEE.

Large population stochastic dynamic games: closed-loop McKean-Vlasov systems and the Nash certainty equivalence principle. M Huang, R P Malhamé, P E Caines, Communications in Information & Systems. 63Huang, M., Malhamé, R. P., and Caines, P. E. (2006). Large population stochastic dynamic games: closed-loop McKean-Vlasov systems and the Nash certainty equivalence principle. Communications in Information & Systems, 6(3):221-252.

Nash-MFG equilibrium in a SIR model with time dependent newborn vaccination. E Hubert, G Turinici, Ricerche di Matematica. 167Hubert, E. and Turinici, G. (2018). Nash-MFG equilibrium in a SIR model with time dependent newborn vaccination. Ricerche di Matematica, 67(1).

Mean field equilibria of dynamic auctions with learning. Management Science. K Iyer, R Johari, M Sundararajan, 60Iyer, K., Johari, R., and Sundararajan, M. (2014). Mean field equilibria of dynamic auctions with learning. Manage- ment Science, 60(12):2949-2970.

An integral control formulation of mean field game based large scale coordination of loads in smart grids. A C Kizilkale, R Salhab, R P Malhamé, Automatica. 100Kizilkale, A. C., Salhab, R., and Malhamé, R. P. (2019). An integral control formulation of mean field game based large scale coordination of loads in smart grids. Automatica, 100.

On classical solutions to the mean field game system of controls. Z Kobeissi, Communications in Partial Differential Equations. 473Kobeissi, Z. (2022). On classical solutions to the mean field game system of controls. Communications in Partial Differential Equations, 47(3):453-488.

Mean-field-game model for botnet defense in cyber-security. V N Kolokoltsov, A Bensoussan, Applied Mathematics & Optimization. 743Kolokoltsov, V. N. and Bensoussan, A. (2016). Mean-field-game model for botnet defense in cyber-security. Applied Mathematics & Optimization, 74(3).

Corruption and botnet defense: a mean field game approach. V N Kolokoltsov, O A Malafeyev, International Journal of Game Theory. 347Kolokoltsov, V. N. and Malafeyev, O. A. (2018). Corruption and botnet defense: a mean field game approach. Inter- national Journal of Game Theory, 47(3).

Worst-case equilibria. E Koutsoupias, C Papadimitriou, Annual symposium on theoretical aspects of computer science. SpringerKoutsoupias, E. and Papadimitriou, C. (1999). Worst-case equilibria. In Annual symposium on theoretical aspects of computer science, pages 404-413. Springer.

Efficiency of the price formation process in presence of high frequency participants: a mean field game analysis. A Lachapelle, J.-M Lasry, C.-A Lehalle, P.-L Lions, Mathematics and Financial Economics. 310Lachapelle, A., Lasry, J.-M., Lehalle, C.-A., and Lions, P.-L. (2016). Efficiency of the price formation process in presence of high frequency participants: a mean field game analysis. Mathematics and Financial Economics, 10(3).

Limit theory for controlled McKean-Vlasov dynamics. D Lacker, SIAM Journal on Control and Optimization. 553Lacker, D. (2017). Limit theory for controlled McKean-Vlasov dynamics. SIAM Journal on Control and Optimization, 55(3):1641-1672.

On the convergence of closed-loop Nash equilibria to the mean field game limit. D Lacker, The Annals of Applied Probability. 304Lacker, D. (2020). On the convergence of closed-loop Nash equilibria to the mean field game limit. The Annals of Applied Probability, 30(4):1693-1761.

Rare Nash equilibria and the price of anarchy in large static games. D Lacker, K Ramanan, Mathematics of Operations Research. 442Lacker, D. and Ramanan, K. (2019). Rare Nash equilibria and the price of anarchy in large static games. Mathematics of Operations Research, 44(2):400-422.

Individual vaccination as Nash equilibrium in a SIR model with application to the 2009-2010 influenza A (H1N1) epidemic in France. L Laguzet, G Turinici, Bulletin of Mathematical Biology. 7710Laguzet, L. and Turinici, G. (2015). Individual vaccination as Nash equilibrium in a SIR model with application to the 2009-2010 influenza A (H1N1) epidemic in France. Bulletin of Mathematical Biology, 77(10):1955-1984.

Openspiel: A framework for reinforcement learning in games. M Lanctot, E Lockhart, J.-B Lespiau, V Zambaldi, S Upadhyay, J Pérolat, S Srinivasan, F Timbers, K Tuyls, S Omidshafiei, arXiv:1908.09453arXiv preprintLanctot, M., Lockhart, E., Lespiau, J.-B., Zambaldi, V., Upadhyay, S., Pérolat, J., Srinivasan, S., Timbers, F., Tuyls, K., Omidshafiei, S., et al. (2019). Openspiel: A framework for reinforcement learning in games. arXiv preprint arXiv:1908.09453.

Monte carlo sampling for regret minimization in extensive games. M Lanctot, K Waugh, M Zinkevich, M Bowling, 22Lanctot, M., Waugh, K., Zinkevich, M., and Bowling, M. (2009). Monte carlo sampling for regret minimization in extensive games. volume 22, pages 1078-1086.

Mean field games. J.-M Lasry, P.-L Lions, Japanese Journal of Mathematics. 21Lasry, J.-M. and Lions, P.-L. (2007). Mean field games. Japanese Journal of Mathematics, 2(1).

Numerical methods for mean field games and mean field type control. M Lauriere, Mean Field Games. 78221Lauriere, M. (2021). Numerical methods for mean field games and mean field type control. Mean Field Games, 78:221.

M Laurière, S Perrin, S Girgin, P Muller, A Jain, T Cabannes, G Piliouras, J Pérolat, R Élie, O Pietquin, arXiv:2203.11973Scalable deep reinforcement learning algorithms for mean field games. arXiv preprintLaurière, M., Perrin, S., Girgin, S., Muller, P., Jain, A., Cabannes, T., Piliouras, G., Pérolat, J., Élie, R., Pietquin, O., et al. (2022). Scalable deep reinforcement learning algorithms for mean field games. arXiv preprint arXiv:2203.11973.

Policy iteration method for time-dependent mean field games systems with non-separable hamiltonians. M Laurière, J Song, Q Tang, arXiv:2110.02552arXiv preprintLaurière, M., Song, J., and Tang, Q. (2021). Policy iteration method for time-dependent mean field games systems with non-separable hamiltonians. arXiv preprint arXiv:2110.02552.

Reinforcement learning for mean field games with strategic complementarities. K Lee, D Rengarajan, D Kalathil, S Shakkottai, PMLRInternational Conference on Artificial Intelligence and Statistics. Lee, K., Rengarajan, D., Kalathil, D., and Shakkottai, S. (2021a). Reinforcement learning for mean field games with strategic complementarities. In International Conference on Artificial Intelligence and Statistics, pages 2458-2466. PMLR.

Learning trembling hand perfect mean field equilibrium for dynamic mean field games. K.-Y Lee, D Rengarajan, D M Kalathil, S Shakkottai, CoRRLee, K.-Y., Rengarajan, D., Kalathil, D. M., and Shakkottai, S. (2020). Learning trembling hand perfect mean field equilibrium for dynamic mean field games. CoRR.

Controlling propagation of epidemics via mean-field control. W Lee, S Liu, H Tembine, W Li, S Osher, SIAM Journal on Applied Mathematics. 811Lee, W., Liu, S., Tembine, H., Li, W., and Osher, S. (2021b). Controlling propagation of epidemics via mean-field control. SIAM Journal on Applied Mathematics, 81(1):190-207.

Mean field game based control of dispersed energy storage devices with constrained inputs. F Li, R P Malhamé, Le Ny, J , 2016 IEEE 55th Conference on Decision and Control (CDC). IEEELi, F., Malhamé, R. P., and Le Ny, J. (2016). Mean field game based control of dispersed energy storage devices with constrained inputs. In 2016 IEEE 55th Conference on Decision and Control (CDC). IEEE.

Permutation invariant policy optimization for mean-field multi-agent reinforcement learning: A principled approach. Y Li, L Wang, J Yang, E Wang, Z Wang, T Zhao, H Zha, arXiv:2105.08268arXiv preprintLi, Y., Wang, L., Yang, J., Wang, E., Wang, Z., Zhao, T., and Zha, H. (2021). Permutation invariant policy optimization for mean-field multi-agent reinforcement learning: A principled approach. arXiv preprint arXiv:2105.08268.

Continuous control with deep reinforcement learning. T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, ICLR (Poster). Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2016). Continuous control with deep reinforcement learning. In ICLR (Poster).

APAC-Net: Alternating the population and agent control via two neural networks to solve high-dimensional stochastic mean field games. A T Lin, S W Fung, W Li, L Nurbekyan, S J Osher, arXiv:2002.10113arXiv preprintLin, A. T., Fung, S. W., Li, W., Nurbekyan, L., and Osher, S. J. (2020). APAC-Net: Alternating the population and agent control via two neural networks to solve high-dimensional stochastic mean field games. arXiv preprint arXiv:2002.10113.

Lecture at the collège de france. P.-L Lions, Lions, P.-L. (2006-2012). Lecture at the collège de france.

Distributed user association in energy harvesting dense small cell networks: A mean-field multi-armed bandit approach. S Maghsudi, E Hossain, IEEE Access. 5Maghsudi, S. and Hossain, E. (2017). Distributed user association in energy harvesting dense small cell networks: A mean-field multi-armed bandit approach. IEEE Access, 5:3513-3523.

Pipeline PSRO: A scalable approach for finding approximate Nash equilibria in large games. S Mcaleer, J Lanier, R Fox, P Baldi, Proceedings of NeurIPS. NeurIPSMcAleer, S., Lanier, J., Fox, R., and Baldi, P. (2020). Pipeline PSRO: A scalable approach for finding approximate Nash equilibria in large games. In Proceedings of NeurIPS.

Mean field energy games in wireless networks. F Mériaux, V Varma, S Lasaulce, 2012 Conference Record of the Forty Sixth Asilomar Conference on Signals, Systems and Computers (ASILOMAR). IEEEMériaux, F., Varma, V., and Lasaulce, S. (2012). Mean field energy games in wireless networks. In 2012 Conference Record of the Forty Sixth Asilomar Conference on Signals, Systems and Computers (ASILOMAR). IEEE.

Control Systems and Reinforcement Learning. S Meyn, Cambridge University PressMeyn, S. (2022). Control Systems and Reinforcement Learning. Cambridge University Press.

Decentralised learning in systems with many, many strategic agents. D Mguni, J Jennings, E Munoz De Cote, Proceedings of AAAI. AAAIMguni, D., Jennings, J., and Munoz de Cote, E. (2018). Decentralised learning in systems with many, many strategic agents. In Proceedings of AAAI.

Reinforcement learning for non-stationary discrete-time linear-quadratic meanfield games in multiple populations. E Miehling, T Başar, Dynamic Games and Applications. Miehling, E., Başar, T., et al. (2022). Reinforcement learning for non-stationary discrete-time linear-quadratic mean- field games in multiple populations. Dynamic Games and Applications, pages 1-47.

Model-free reinforcement learning for non-stationary mean field games. R K Mishra, D Vasal, S Vishwanath, 2020 59th IEEE Conference on Decision and Control (CDC). IEEEMishra, R. K., Vasal, D., and Vishwanath, S. (2020). Model-free reinforcement learning for non-stationary mean field games. In 2020 59th IEEE Conference on Decision and Control (CDC), pages 1032-1037. IEEE.

T M Mitchell, Machine learning. Mitchell, T. M. et al. (1997). Machine learning.

Playing atari with deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.5602arXiv preprintMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.

On the approximation of cooperative heterogeneous multi-agent reinforcement learning (marl) using mean field control (mfc). W U Mondal, M Agarwal, V Aggarwal, S V Ukkusuri, Journal of Machine Learning Research. 23129Mondal, W. U., Agarwal, M., Aggarwal, V., and Ukkusuri, S. V. (2022). On the approximation of cooperative hetero- geneous multi-agent reinforcement learning (marl) using mean field control (mfc). Journal of Machine Learning Research, 23(129):1-46.

Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. M Moravčík, M Schmid, N Burch, V Lisỳ, D Morrill, N Bard, T Davis, K Waugh, M Johanson, M Bowling, Science. 3566337Moravčík, M., Schmid, M., Burch, N., Lisỳ, V., Morrill, D., Bard, N., Davis, T., Waugh, K., Johanson, M., and Bowl- ing, M. (2017). Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508- 513.

Mean-field Markov decision processes with common noise and open-loop controls. M Motte, H Pham, arXiv:1912.07883arXiv preprintMotte, M. and Pham, H. (2019). Mean-field Markov decision processes with common noise and open-loop controls. arXiv preprint arXiv:1912.07883.

Learning equilibria in mean-field games: Introducing mean-field psro. P Muller, M Rowland, R Elie, G Piliouras, J Perolat, M Lauriere, R Marinier, O Pietquin, K Tuyls, arXiv:2111.08350AAMAS. 2022arXiv preprintMuller, P., Rowland, M., Elie, R., Piliouras, G., Perolat, J., Lauriere, M., Marinier, R., Pietquin, O., and Tuyls, K. (2021). Learning equilibria in mean-field games: Introducing mean-field psro. AAMAS 2022 (arXiv preprint arXiv:2111.08350).

Non-cooperative games. J Nash, Annals of mathematics. Nash, J. (1951). Non-cooperative games. Annals of mathematics, pages 286-295.

Equilibrium points in n-person games. J F Nash, Proceedings of the national academy of sciences. the national academy of sciences36Nash, J. F. (1950). Equilibrium points in n-person games. Proceedings of the national academy of sciences, 36(1):48- 49.

Synthesis of cucker-smale type flocking via mean field stochastic control theory: Nash equilibria. M Nourian, P E Caines, R P Malhamé, 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton). IEEENourian, M., Caines, P. E., and Malhamé, R. P. (2010). Synthesis of cucker-smale type flocking via mean field stochastic control theory: Nash equilibria. In 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 814-819. IEEE.

Mean field analysis of controlled cucker-smale type flocking: Linear analysis and perturbation equations. M Nourian, P E Caines, R P Malhamé, IFAC Proceedings Volumes. 44Nourian, M., Caines, P. E., and Malhamé, R. P. (2011). Mean field analysis of controlled cucker-smale type flocking: Linear analysis and perturbation equations. IFAC Proceedings Volumes, 44(1):4471-4476.

Normalizing flows for probabilistic modeling and inference. G Papamakarios, E Nalisnick, D J Rezende, S Mohamed, B Lakshminarayanan, Journal of Machine Learning Research. 2257Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., and Lakshminarayanan, B. (2021). Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):1-64.

F Parise, S Grammatico, B Gentile, J Lygeros, arXiv:1506.07719Network aggregative games and distributed mean field control via consensus theory. arXiv preprintParise, F., Grammatico, S., Gentile, B., and Lygeros, J. (2015). Network aggregative games and distributed mean field control via consensus theory. arXiv preprint arXiv:1506.07719.

Efficient model-based multi-agent mean-field reinforcement learning. B Pasztor, I Bogunovic, A Krause, arXiv:2107.04050arXiv preprintPasztor, B., Bogunovic, I., and Krause, A. (2021). Efficient model-based multi-agent mean-field reinforcement learn- ing. arXiv preprint arXiv:2107.04050.

Scaling up mean field games with online mirror descent. J Perolat, S Perrin, R Elie, M Laurière, G Piliouras, M Geist, K Tuyls, O Pietquin, arXiv:2103.00623AAMAS. 2022arXiv preprintPerolat, J., Perrin, S., Elie, R., Laurière, M., Piliouras, G., Geist, M., Tuyls, K., and Pietquin, O. (2021). Scaling up mean field games with online mirror descent. AAMAS 2022 (arXiv preprint arXiv:2103.00623).

S Perrin, M Laurière, J Pérolat, R Élie, M Geist, O Pietquin, arXiv:2109.09717Generalization in mean field games by learning master policies. AAAI'22. arXiv preprintPerrin, S., Laurière, M., Pérolat, J., Élie, R., Geist, M., and Pietquin, O. (2021a). Generalization in mean field games by learning master policies. AAAI'22 (arXiv preprint arXiv:2109.09717).

S Perrin, M Laurière, J Pérolat, M Geist, R Élie, O Pietquin, arXiv:2105.07933Mean field games flock! the reinforcement learning way. IJCAI 2021. arXiv preprintPerrin, S., Laurière, M., Pérolat, J., Geist, M., Élie, R., and Pietquin, O. (2021b). Mean field games flock! the reinforcement learning way. IJCAI 2021 (arXiv preprint arXiv:2105.07933).

Fictitious play for mean field games: Continuous time analysis and applications. S Perrin, J Pérolat, M Laurière, M Geist, R Elie, O Pietquin, In proc. of NeurIPSPerrin, S., Pérolat, J., Laurière, M., Geist, M., Elie, R., and Pietquin, O. (2020). Fictitious play for mean field games: Continuous time analysis and applications. In proc. of NeurIPS.

Markov decision processes: discrete stochastic dynamic programming. M L Puterman, John Wiley & SonsPuterman, M. L. (2014). Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons.

Variational inference with normalizing flows. D Rezende, S Mohamed, PMLRInternational conference on machine learning. Rezende, D. and Mohamed, S. (2015). Variational inference with normalizing flows. In International conference on machine learning, pages 1530-1538. PMLR.

An iterative method of solving a game. J Robinson, Annals of mathematics. Robinson, J. (1951). An iterative method of solving a game. Annals of mathematics, pages 296-301.

Introduction to the inefficiency of equilibria. Algorithmic game theory. T Roughgarden, E Tardos, 17Roughgarden, T. and Tardos, E. (2007). Introduction to the inefficiency of equilibria. Algorithmic game theory, 17:443-459.

A machine learning framework for solving high-dimensional mean field game and mean field control problems. L Ruthotto, S J Osher, W Li, L Nurbekyan, S W Fung, Proceedings of the National Academy of Sciences. 17117Ruthotto, L., Osher, S. J., Li, W., Nurbekyan, L., and Fung, S. W. (2020). A machine learning framework for solving high-dimensional mean field game and mean field control problems. Proceedings of the National Academy of Sciences, 117(17).

A mean field route choice game model. R Salhab, J Le Ny, R P Malhamé, 2018 IEEE Conference on Decision and Control (CDC). IEEESalhab, R., Le Ny, J., and Malhamé, R. P. (2018). A mean field route choice game model. In 2018 IEEE Conference on Decision and Control (CDC), pages 1005-1010. IEEE.

Energy-efficient resource management in ultra dense small cell networks: A mean-field approach. S Samarakoon, M Bennis, W Saad, M Debbah, M Latva-Aho, IEEE Global Communications Conference (GLOBECOM). Samarakoon, S., Bennis, M., Saad, W., Debbah, M., and Latva-Aho, M. (2015). Energy-efficient resource manage- ment in ultra dense small cell networks: A mean-field approach. In IEEE Global Communications Conference (GLOBECOM).

Some studies in machine learning using the game of checkers. A L Samuel, IBM Journal of Research and Development. 33Samuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM Journal of Research and Development, 3(3).

J Schaeffer, N Burch, Y Björnsson, A Kishimoto, M Müller, R Lake, P Lu, S Sutphen, Checkers is solved. 317Schaeffer, J., Burch, N., Björnsson, Y., Kishimoto, A., Müller, M., Lake, R., Lu, P., and Sutphen, S. (2007). Checkers is solved. Science, 317(5844).

Equilibrium points of nonatomic games. D Schmeidler, Journal of statistical Physics. 74Schmeidler, D. (1973). Equilibrium points of nonatomic games. Journal of statistical Physics, 7(4):295-300.

Programming a computer playing chess. C E Shannon, Philosophical Magazine, Ser. 731241Shannon, C. E. (1959). Programming a computer playing chess. Philosophical Magazine, Ser.7, 41(312).

Massive autonomous UAV path planning: A neural network based meanfield game theoretic approach. H Shiri, J Park, M Bennis, IEEE Global Communications Conference (GLOBECOM). Shiri, H., Park, J., and Bennis, M. (2019). Massive autonomous UAV path planning: A neural network based mean- field game theoretic approach. In IEEE Global Communications Conference (GLOBECOM).

Mastering the game of Go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, Nature. 7587529Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587).

A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Kumaran, T Graepel, T Lillicrap, K Simonyan, D Hassabis, Science. 6419632Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K., and Hassabis, D. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 632(6419).

Mastering the game of Go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, Nature. 7676550Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. (2017). Mastering the game of Go without human knowledge. Nature, 550(7676).

Opinion dynamics and stubbornness through mean-field games. L Stella, F Bagagiolo, D Bauso, G Como, 52nd IEEE Conference on Decision and Control. IEEEStella, L., Bagagiolo, F., Bauso, D., and Como, G. (2013). Opinion dynamics and stubbornness through mean-field games. In 52nd IEEE Conference on Decision and Control. IEEE.

Reinforcement learning in stationary mean-field games. J Subramanian, A Mahajan, Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. the 18th International Conference on Autonomous Agents and MultiAgent SystemsSubramanian, J. and Mahajan, A. (2019). Reinforcement learning in stationary mean-field games. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pages 251-259.

Reinforcement learning for mean-field teams. J Subramanian, R Seraj, A Mahajan, Workshop on Adaptive and Learning Agents at International Conference on Autonomous Agents and Multi-Agent Systems. Subramanian, J., Seraj, R., and Mahajan, A. (2018). Reinforcement learning for mean-field teams. In Workshop on Adaptive and Learning Agents at International Conference on Autonomous Agents and Multi-Agent Systems.

S G Subramanian, P Poupart, M E Taylor, N Hegde, arXiv:2002.02513Multi type mean field reinforcement learning. arXiv preprintSubramanian, S. G., Poupart, P., Taylor, M. E., and Hegde, N. (2020a). Multi type mean field reinforcement learning. arXiv preprint arXiv:2002.02513.

Partially observable mean field reinforcement learning. S G Subramanian, M E Taylor, M Crowley, P Poupart, abs/2012.15791CoRRSubramanian, S. G., Taylor, M. E., Crowley, M., and Poupart, P. (2020b). Partially observable mean field reinforcement learning. CoRR, abs/2012.15791.

The exact law of large numbers via Fubini extension and characterization of insurable risks. Y Sun, Journal of Economic Theory. 1261Sun, Y. (2006). The exact law of large numbers via Fubini extension and characterization of insurable risks. Journal of Economic Theory, 126(1):31-69.

Reinforcement Learning: An Introduction. R S Sutton, A G Barto, The MIT Press2nd editionSutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. The MIT Press, 2nd edition.

Algorithms for reinforcement learning. C Szepesvári, Synthesis lectures on artificial intelligence and machine learning. 41Szepesvári, C. (2010). Algorithms for reinforcement learning. Synthesis lectures on artificial intelligence and machine learning, 4(1):1-103.

Topics in propagation of chaos. A.-S Sznitman, Ecole d'été de probabilités de Saint-Flour XIX-1989. SpringerSznitman, A.-S. (1991). Topics in propagation of chaos. In Ecole d'été de probabilités de Saint-Flour XIX-1989, pages 165-251. Springer.

Linearly solvable mean-field traffic routing games. T Tanaka, E Nekouei, A R Pedram, K H Johansson, IEEE Transactions on Automatic Control. 662Tanaka, T., Nekouei, E., Pedram, A. R., and Johansson, K. H. (2020). Linearly solvable mean-field traffic routing games. IEEE Transactions on Automatic Control, 66(2):880-887.

H Tembine, R Tempone, P Vilanova, arXiv:1210.4657Mean-field learning: a survey. arXiv preprintTembine, H., Tempone, R., and Vilanova, P. (2012). Mean-field learning: a survey. arXiv preprint arXiv:1210.4657.

Reinforcement learning in non-stationary discretetime linear-quadratic mean-field games. M A Uz Zaman, K Zhang, E Miehling, Bas, T Ar, 2020 59th IEEE Conference on Decision and Control (CDC). IEEEuz Zaman, M. A., Zhang, K., Miehling, E., and Bas , ar, T. (2020). Reinforcement learning in non-stationary discrete- time linear-quadratic mean-field games. In 2020 59th IEEE Conference on Decision and Control (CDC), pages 2278-2284. IEEE.

Grandmaster level in starcraft ii using multi-agent reinforcement learning. O Vinyals, I Babuschkin, W M Czarnecki, M Mathieu, A Dudzik, J Chung, D H Choi, R Powell, T Ewalds, P Georgiev, Nature. 5757782Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354.

Breaking the curse of many agents: Provable mean embedding q-iteration for mean-field reinforcement learning. L Wang, Z Yang, Wang , Z , PMLRInternational Conference on Machine Learning. Wang, L., Yang, Z., and Wang, Z. (2020). Breaking the curse of many agents: Provable mean embedding q-iteration for mean-field reinforcement learning. In International Conference on Machine Learning, pages 10092-10103. PMLR.

Global convergence of policy gradient for linear-quadratic meanfield control/game in continuous time. W Wang, J Han, Z Yang, Wang , Z , PMLRInternational Conference on Machine Learning. Wang, W., Han, J., Yang, Z., and Wang, Z. (2021). Global convergence of policy gradient for linear-quadratic mean- field control/game in continuous time. In International Conference on Machine Learning, pages 10772-10782. PMLR.

X Wang, J Cerny, S Li, C Yang, Z Yin, H Chan, An , B , arXiv:2204.04930A unified perspective on deep equilibrium finding. arXiv preprintWang, X., Cerny, J., Li, S., Yang, C., Yin, Z., Chan, H., and An, B. (2022). A unified perspective on deep equilibrium finding. arXiv preprint arXiv:2204.04930.

Mean field equilibrium in multi-armed bandit game with continuous reward. X Wang, R Jia, arXiv:2105.00767arXiv preprintWang, X. and Jia, R. (2021). Mean field equilibrium in multi-armed bandit game with continuous reward. arXiv preprint arXiv:2105.00767.

Learning form delayed rewards. C Watkins, King's College, University of CambridgePh. D. thesisWatkins, C. (1989). Learning form delayed rewards. Ph. D. thesis, King's College, University of Cambridge.

Q-learning. C J Watkins, P Dayan, Machine learning. 83-4Watkins, C. J. and Dayan, P. (1992). Q-learning. Machine learning, 8(3-4):279-292.

Simple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 83Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229-256.

Learning while playing in mean-field games: Convergence and optimality. Q Xie, Z Yang, Z Wang, A Minca, PMLRProceedings of the 38th International Conference on Machine Learning. Meila, M. and Zhang, T.the 38th International Conference on Machine Learning139Xie, Q., Yang, Z., Wang, Z., and Minca, A. (2021). Learning while playing in mean-field games: Convergence and optimality. In Meila, M. and Zhang, T., editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 11436-11447. PMLR.

Mean field game-theoretic framework for interference and energy-aware control in 5G ultra-dense networks. C Yang, J Li, M Sheng, A Anpalagan, Xiao , J , IEEE Wireless Communications25Yang, C., Li, J., Sheng, M., Anpalagan, A., and Xiao, J. (2017). Mean field game-theoretic framework for interference and energy-aware control in 5G ultra-dense networks. IEEE Wireless Communications, 25(1).

Bayesian multi-type mean field multi-agent imitation learning. F Yang, A Vereshchaka, C Chen, Dong , W , Advances in Neural Information Processing Systems. 33Yang, F., Vereshchaka, A., Chen, C., and Dong, W. (2020). Bayesian multi-type mean field multi-agent imitation learning. Advances in Neural Information Processing Systems, 33:2469-2478.

Learning deep mean field games for modeling large population behavior. J Yang, X Ye, R Trivedi, H Xu, H Zha, International Conference on Learning Representations. Yang, J., Ye, X., Trivedi, R., Xu, H., and Zha, H. (2018a). Learning deep mean field games for modeling large population behavior. In International Conference on Learning Representations.

Mean field multi-agent reinforcement learning. Y Yang, R Luo, M Li, M Zhou, W Zhang, Wang , J , Proceedings of ICML. ICMLYang, Y., Luo, R., Li, M., Zhou, M., Zhang, W., and Wang, J. (2018b). Mean field multi-agent reinforcement learning. In Proceedings of ICML.

An overview of multi-agent reinforcement learning from game theoretical perspective. Y Yang, J Wang, arXiv:2011.00583arXiv preprintYang, Y. and Wang, J. (2020). An overview of multi-agent reinforcement learning from game theoretical perspective. arXiv preprint arXiv:2011.00583.

Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of Reinforcement Learning and Control. K Zhang, Z Yang, T Başar, Zhang, K., Yang, Z., and Başar, T. (2021). Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of Reinforcement Learning and Control, pages 321-384.

Regret minimization in games with incomplete information. M Zinkevich, M Johanson, M Bowling, C Piccione, 20Zinkevich, M., Johanson, M., Bowling, M., and Piccione, C. (2007). Regret minimization in games with incomplete information. volume 20, pages 1729-1736.